<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:cc="http://cyber.law.harvard.edu/rss/creativeCommonsRssModule.html">
    <channel>
        <title><![CDATA[The Airbnb Tech Blog - Medium]]></title>
        <description><![CDATA[Creative engineers and data scientists building a world where you can belong anywhere. http://airbnb.io - Medium]]></description>
        <link>https://medium.com/airbnb-engineering?source=rss----53c7c27702d5---4</link>
        <image>
            <url>https://cdn-images-1.medium.com/proxy/1*TGH72Nnw24QL3iV9IOm4VA.png</url>
            <title>The Airbnb Tech Blog - Medium</title>
            <link>https://medium.com/airbnb-engineering?source=rss----53c7c27702d5---4</link>
        </image>
        <generator>Medium</generator>
        <lastBuildDate>Fri, 19 Dec 2025 22:18:26 GMT</lastBuildDate>
        <atom:link href="https://medium.com/feed/airbnb-engineering" rel="self" type="application/rss+xml"/>
        <webMaster><![CDATA[yourfriends@medium.com]]></webMaster>
        <atom:link href="http://medium.superfeedr.com" rel="hub"/>
        <item>
            <title><![CDATA[GraphQL Data Mocking at Scale with LLMs and @generateMock]]></title>
            <link>https://medium.com/airbnb-engineering/graphql-data-mocking-at-scale-with-llms-and-generatemock-30b380f12bd6?source=rss----53c7c27702d5---4</link>
            <guid isPermaLink="false">https://medium.com/p/30b380f12bd6</guid>
            <category><![CDATA[networking]]></category>
            <category><![CDATA[llm]]></category>
            <category><![CDATA[engineering]]></category>
            <category><![CDATA[graphql]]></category>
            <category><![CDATA[ai]]></category>
            <dc:creator><![CDATA[Michael Rebello]]></dc:creator>
            <pubDate>Thu, 30 Oct 2025 17:01:54 GMT</pubDate>
            <atom:updated>2025-10-30T17:01:53.454Z</atom:updated>
            <content:encoded><![CDATA[<p><em>How Airbnb combines GraphQL infra, product context, and LLMs to generate and maintain convincing, type-safe mock data using a new directive.</em></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*wHF0IXmtHzIfzv-IZh_CKA.jpeg" /></figure><h3>Introduction</h3><p>Producing valid and realistic mock data for testing and prototyping with GraphQL has been a persistent challenge across the industry for years. Mock data is tedious to write and maintain, and attempts to improve the process, such as random value generation and field-level stubbing, fall short because they lack essential domain context to make test data realistic and meaningful. The time spent on this manual work ultimately takes away from what most engineers would like to focus on: building features.</p><p>In this post, weâ€™ll explore how weâ€™ve reimagined mocking GraphQL data at Airbnb by combining GraphQL validation, rich product and schema context, and LLMs to generate and maintain convincing, type-safe mock data. Our solution centers around a simple new GraphQL client directiveâ€Šâ€”â€Š@generateMockâ€Šâ€”â€Šthat engineers can add to any operation, fragment, or field. This approach eliminates the need for engineers to manually write and maintain mocks as queries evolve, freeing up time to focus on building theÂ product.</p><h3>Key challenges</h3><p>After meeting with Airbnb product engineers and analyzing results from internal surveys, we distilled the most common pain points around GraphQL mocking down into three key challenges:</p><ol><li><strong>Manually creating mocks is time consuming.</strong> GraphQL queries can grow to hundreds of lines, and hand-crafting mock response data is extremely tedious. Most engineers manually write mocks as either raw JSON files or by instantiating types generated from the GraphQL schema, while others modify copy-and-pasted JSON responses from the server. Although both of these methods can yield realistic-looking data that can be used for demos and snapshot tests, they require significant time investment and are prone to subtle mistakes.</li><li><strong>Prototyping &amp; demoing features without the server is hard.</strong> Typically, server and client engineers agree on a GraphQL schema early on in the feature development process. Once the schema has been established, however, the two groups split off and start working in parallel: Server engineers implement the logic to back the new schema and client engineers build the frontend UI, logic, and the queries that power them. This parallelization is particularly challenging for client engineers, since they canâ€™t actually test the UI theyâ€™re building until the server has fully implemented the schema. To unblock themselves, client engineers often hardcode data into views, leverage proxies to manipulate responses, or hack custom logic into the networking layer locally, resulting in wasted time andÂ effort.</li><li><strong>Mocks get out of sync with GraphQL queries over time.</strong> Since most mocks are hand-written, they are not tightly coupled to the underlying queries and schema they are supposed to represent. If a team builds a new feature, then comes back a few months later to add new functionality backed by additional GraphQL fields, engineers must remember to manually update their mock data. As there is no forcing function to guarantee mocks stay in sync with queries, mock data tends to shift further away from the production reality as time passesâ€Šâ€”â€Šdegrading the quality ofÂ tests.</li></ol><p>These challenges are not unique to Airbnb and are common across the industry. Although tooling like random value generators and local field resolvers can provide some assistance, they lack the domain knowledge and context needed to produce realistic, meaningful data for high-quality demos, quick product iteration, and reliableÂ testing.</p><h3>Goals</h3><p>When setting out to solve these challenges at Airbnb, we established three north-star goals:</p><ol><li><strong>Eliminate the need to hand-write mock data.</strong> Mock data should be generated automatically to free up engineers from needing to hand-craft and maintain mock GraphQLÂ data.</li><li><strong>Create highly realistic mock data.</strong> Mock data should match the user interface designs and look like real production data in order to support high-quality demos, which are highly valued at Airbnb for early feedback.</li><li><strong>Keep engineers in their local focus loops.</strong> Our solution should seamlessly integrate into engineersâ€™ current development processes so they can generate mocks without context-switching to a website, separate repository, or unfamiliar tool.</li></ol><h3>@generateMock: Schema + context + LLMs =Â magic</h3><p>To generate mock data while keeping engineers in their local focus loops, we introduced a new client GraphQL directive called @generateMock, which engineers can use to automatically generate mock data for a given GraphQL operation, fragment, orÂ field:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*llh_79Y2eUguxztCsbzIkg.png" /><figcaption>Example of @generateMock being specified on a GraphQLÂ query.</figcaption></figure><p>This directive accepts a few optional arguments that engineers can use to customize the generated mock data, and the directive itself can be repeated with different input arguments to generate different mock variations:</p><ul><li><em>id</em>: The identifier to use for the mock, as well as for naming generated helper functions. Useful when repeating the @generateMock directive to produce multipleÂ mocks.</li><li><em>hints</em>: Additional context or instructions on how the mock should look. For example, a hint might be â€œInclude travel entries for Barcelona, Paris, and Kyoto.â€ Under the hood, this information is fed to an LLM and heavily influences what the generated mock data looks like and how densely populated its fieldsÂ are.</li><li><em>designURL</em>: The URL of a design mockup of the screen that will render the mock data. Specifying this argument helps the LLM produce mock data that matches the design by generating matching names, addresses, and other similarÂ content.</li></ul><p>At Airbnb, engineers use a command line tool we call Niobe to generate code for their GraphQL queries and fragments. After modifying aÂ .graphql file locally, engineers run this code generator, then use the generated TypeScript/Kotlin/Swift files to send GraphQL requests. To generate mock data using @generateMock, engineers simply need to run Niobe code generation after adding the directiveâ€Šâ€”â€Šjust as they would after making any other GraphQLÂ change.</p><p>During code generation, Niobe produces both a JSON file containing the actual mock data for each @generateMock directive, as well as a source file that provides functions for loading and consuming mock data from demo apps, snapshot tests, and unit tests. As shown in the Swift code below, the <em>mockMixedStatusIndicators()</em> function is generated on the InboxSyncQueryâ€™s root <em>Data</em> type. It provides access to an instantiated type thatâ€™s populated with the generated mock data for <em>mixed_status_indicators</em>, allowing engineers to use the mock without having to load the JSON data manually:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*6stJEGGeLtkfKK_D6ulaoA.png" /><figcaption>Using a generated mock in a Swift unitÂ test.</figcaption></figure><p>Engineers are free to modify the generated mock JSON data as wellâ€Šâ€”â€Šas weâ€™ll see below, Niobe will avoid overwriting their modifications on subsequent generation invocations.</p><h3>What does mock data lookÂ like?</h3><p>The context that we provide to the LLM is vital to generating data that is realistic enough to use in demos. To this end, Niobe collects the following information and includes it in the context passed to theÂ LLM:</p><ul><li>The definitions of the query/fragment/fields being mocked (i.e., those marked with @generateMock and their dependencies).</li><li>The <em>subset</em> of the GraphQL schema being queried, as well as any associated documentation that is present as inline comments. This information enables the LLM to infer the types that are used by the query being mocked. Importantly, this isnâ€™t the <em>whole</em> schema, because including the full schema would likely overload the context windowâ€Šâ€”â€ŠNiobe traverses the schema and strips out types and fields that are not needed to resolve the query, along with any extra whitespace.</li><li>The URL for the image representation of the design document specified within <em>designURL</em>, if any. Niobe integrates with an internal API to generate a snapshot image of the provided node in the design document. The API pushes this snapshot to a storage bucket and provides a URL that Niobe feeds to the LLM, along with specialized instructions on how to useÂ it.</li><li>The additional <em>hints</em> specified in @generateMock.</li><li>The platform (e.g., â€œiOSâ€, â€œAndroidâ€, or â€œWebâ€) for which the mock data is being generated (for style specificity).</li><li>A list of Airbnb-hosted image URLs that the LLM can choose from if needed, along with short textual descriptions of each. This prevents the LLM from hallucinating image URLs that donâ€™t exist and ensures that the mock data contains valid URLs which can be properly loaded at runtime when prototyping orÂ demoing.</li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*yPJB6saOgVZ7kQErYjGtWQ.png" /><figcaption>Illustration of the various pieces of context that are passed to the LLM during mock generation.</figcaption></figure><p>All this information is consolidated into a prompt we fine-tuned against Gemini 2.5 Pro. We chose this model because of its 1-million token context window, plus the fact that in our internal tests this configuration performed significantly faster than comparable models while producing mock data of similar quality. Using this approach, weâ€™re able to produce highly realistic JSON mocks which, when loaded into the application, yield very convincing results as shownÂ below:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*1PXeYCxy9BVSWYtOI7edHQ.png" /><figcaption>Screenshot of a design mockup compared to a mock that was generated using @generateMock.</figcaption></figure><p>The data in the screenshot on the right looks quite realistic, but if you look closely you may notice that the data is indeed mockedâ€Šâ€”â€Šall the photos are coming from the seed data set that we feed theÂ LLM.</p><h3>How itÂ works</h3><p>When an engineer uses the Niobe CLI to generate code for their GraphQL files, Niobe automatically performs mock generation as the final step of this process, as shown in the flowchart below:</p><ul><li>If the @generateMock directive includes a <em>designURL</em>, Niobe validates the URL to ensure it includes a <em>node-id</em>, then uses an internal API to produce an image snapshot of that particular node. The API, in turn, pushes this snapshot to a storage bucket and provides Niobe with itsÂ URL.</li><li>Next, the CLI aggregates all the context described in the section aboveâ€Šâ€”â€Šincluding the URL of the design snapshotâ€Šâ€”â€Šand crafts a prompt to send to the LLM. This prompt is then sent to the Gemini 2.5 Pro model, and results are streamed back to the client in order to show a progress indicator in theÂ CLI.</li><li>Once the mock JSON response has been received from the LLM, Niobe performs a validation step against this data by passing the GraphQL schema, client GraphQL document, and JSON data to the graphql <a href="https://www.npmjs.com/package/graphql">NPM package</a>â€™s <em>graphqlSync</em> function.</li><li>If the validation produces errors (for example, if the LLM hallucinated an invalid enum value or failed to populate a required field), Niobe aggregates these errors and feeds them back into the LLM along with the initial mock data. This retry mechanism is used to essentially â€œself-healâ€ and fix invalid mock data.<br>â€“ This step is <em>critical</em> to reliably generating mock data. By placing the LLM within our existing GraphQL infrastructure, weâ€™re able to enforce a set of guardrails through this validation step and provide strong guarantees that the mock data produced at the end of the pipeline is fully validâ€Šâ€”â€Šsomething that wouldnâ€™t be possible by using a tool outside our GraphQL infrastructure like ChatGPT.<br>â€“ Finally, once the mock data has been validated, Niobe writes it to a JSON file, alongside a companion source file which provides functions for loading the mock from application code.</li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*i1gTnZfrP0qZmQrn3N-jkQ.png" /><figcaption>Flowchart of how mock generation works under theÂ hood.</figcaption></figure><h3>@respondWithMock: Unblocking client development</h3><p>In addition to generating realistic mock data with @generateMock, we also wanted to empower client engineers to iterate on features without waiting for the backend server implementation. A second directive, @respondWithMock, works alongside @generateMock to make this possible:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*GDhohfDu-AgMKgN3yBH4LA.png" /><figcaption>Simple example of using the @respondWithMock directive.</figcaption></figure><p>When this directive is present, Niobe alters the code thatâ€™s generated alongside the mock data to include extra details about this annotation. At runtime, the GraphQL client uses this to load the generated mock data, then seamlessly returns the mocked response <em>instead</em> of using data from the server. This effectively allows client engineers to unblock themselves from waiting on the server implementation, since they can easily use locally mocked data when querying unimplemented fields. The screenshot of the inbox screen earlier in this post is actually a real screenshot that was taken by generating with these two directives and running the Airbnb app in an iOS simulatorâ€Šâ€”â€Šno manual mocking, proxying, or response modification needed!</p><p>@respondWithMock can also be specified on <em>individual fields</em>. When used on fields within a query instead of on the query itself, the GraphQL client will actually request all fields from the server <em>except those annotated with @respondWithMock</em>, then patch in locally mocked data for the remaining fieldsâ€Šâ€”â€Šproducing a hybrid of production and mock data, and making it possible for client engineers to develop against new (unimplemented) fields in existing queries. Engineers can even repeat this directive and use query input variables to decide if and when to return a specific generated mock at runtime, as shownÂ below:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*6JRec3pvDGoc8_nMYS7HIg.png" /><figcaption>Using @respondWithMock with conditionals and on individual fields.</figcaption></figure><h3>Schema evolution: Keeping mocksÂ truthful</h3><p>The final challenge we addressed was the issue of keeping mocks in sync with queries as they evolve over time. Since Niobe manages mock data that is generated via the @generateMock directive, it can be smart about maintaining that mock data. As part of mock generation, Niobe embeds two extra keys in each generated JSONÂ file:</p><ol><li>A hash of the client entity being mocked (i.e., the GraphQL query document).</li><li>A hash of the input arguments to @generateMock.</li></ol><figure><img alt="" src="https://cdn-images-1.medium.com/max/852/1*P0Fps2QTSpsnTP_shlqNfQ.png" /><figcaption>Niobe embeds version hashes in mock data in order to determine when a given mock needs to beÂ updated.</figcaption></figure><p>Each time code generation runs, Niobe determines whether existing mocksâ€™ hashes differ from what their current hashes should be based on the GraphQL document. If they match, it skips mock generation for those types. On the other hand, if one of the hashes changed, Niobe intelligently updates that mock by including the existing mock in the context provided to the LLM, along with instructions on how to modifyÂ it.</p><p>Itâ€™s important that Niobe doesnâ€™t unnecessarily modify existing mock data for fields that are unchanged and still valid, since doing so could overwrite manual tweaks that were made to the JSON by engineers or break existing tests that rely on this data. To avoid this, we provide the LLM with a diff of what changed in the query, and tuned the prompt to focus on that diff and avoid making spurious changes to unrelated fields.</p><p>Finally, each client codebase includes an automated check that ensures mock version hashes are up to date when code is submitted. This provides a guarantee that all generated mocks stay in sync with queries as they evolve over time. When engineers encounter these validation failures, they just re-run code generation locallyâ€Šâ€”â€Šno manual updates required.</p><h3>Conclusion</h3><p><em>â€œ@generateMock has significantly sped up my local development and made working with local data much more enjoyable.â€â€Šâ€”â€ŠSenior SoftwareÂ Engineer</em></p><p>By integrating highly contextualized LLMsâ€Šâ€”â€Šinformed by the GraphQL schema, product context, and UX designsâ€Šâ€”â€Šdirectly into existing GraphQL tooling, weâ€™ve unlocked the ability to generate valid and realistic mock data while eliminating the need for engineers to manually hand-write and maintain mocks. The directive-driven approach of @generateMock and @respondWithMock allows engineers to build clients before the server implementation is complete while keeping them in their focus loops and providing a guarantee that mock data stays in sync as queriesÂ evolve.</p><p>In just the past few months, Airbnb engineers have generated and merged over 700 mocks across iOS, Android, and Web using @generateMock, and we plan to roll out internal support for backend services soon. These tools have fundamentally changed how engineers mock GraphQL data for tests and prototypes at Airbnb, allowing them to focus on building product features rather than crafting and maintaining mockÂ data.</p><h3>Acknowledgments</h3><p>Special thanks to Raymond Wang and Virgil King for their contributions bringing @generateMock support to Web and Android clients, as well as to many other engineers and teams at Airbnb who participated in design reviews, built supporting infrastructure, and provided usage feedback.</p><p>Does this type of work interest you? Check out our open rolesÂ <a href="https://careers.airbnb.com/">here</a>.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=30b380f12bd6" width="1" height="1" alt=""><hr><p><a href="https://medium.com/airbnb-engineering/graphql-data-mocking-at-scale-with-llms-and-generatemock-30b380f12bd6">GraphQL Data Mocking at Scale with LLMs and @generateMock</a> was originally published in <a href="https://medium.com/airbnb-engineering">The Airbnb Tech Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[From Static Rate Limiting to Adaptive Traffic Management in Airbnbâ€™s Key-Value Store]]></title>
            <link>https://medium.com/airbnb-engineering/from-static-rate-limiting-to-adaptive-traffic-management-in-airbnbs-key-value-store-29362764e5c2?source=rss----53c7c27702d5---4</link>
            <guid isPermaLink="false">https://medium.com/p/29362764e5c2</guid>
            <category><![CDATA[engineering]]></category>
            <category><![CDATA[cloud-storage]]></category>
            <category><![CDATA[key-value-store]]></category>
            <category><![CDATA[cloud-services]]></category>
            <category><![CDATA[infrastructure]]></category>
            <dc:creator><![CDATA[Shravan Gaonkar]]></dc:creator>
            <pubDate>Thu, 09 Oct 2025 16:01:55 GMT</pubDate>
            <atom:updated>2025-10-09T16:01:55.061Z</atom:updated>
            <content:encoded><![CDATA[<p>How Airbnb hardened Mussel, our key-value store, with smarter traffic controls to stay fast and reliable during trafficÂ spikes.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*kBx_8QLd7El4TZ2nrouYUw.jpeg" /></figure><p>By <a href="https://www.linkedin.com/in/shravangaonkar/">Shravan Gaonkar</a>, <a href="https://www.linkedin.com/in/caseygetz/">Casey Getz</a>, <a href="https://www.linkedin.com/in/wonheec/">WonheeÂ Cho</a></p><h3>Introduction</h3><p>Every request lookup on Airbnb, from stays, experiences, and services search to customer support inquiries ultimately hits <a href="https://medium.com/airbnb-engineering/mussel-airbnbs-key-value-store-for-derived-data-406b9fa1b296">Mussel</a>, our multi-tenant key-value store for derived data. Mussel operates as a proxy service, deployed as a fleet of stateless dispatchersâ€Šâ€”â€Šeach a Kubernetes pod. On a typical day, this fleet handles millions of predictable point and range reads. During peak events, however, it must absorb several-fold higher volume, terabyte-scale bulk uploads, and sudden bursts from automated bots or DDoS attacks. Its ability to reliably serve this volatile mix of traffic is therefore critical to both the Airbnb user experience and the stability of the many services that power our platform.</p><p>Given Musselâ€™s traffic volume and its role in core Airbnb flows, <a href="https://en.wikipedia.org/wiki/Quality_of_service">quality of service</a> (QoS) is one of the productâ€™s defining features. The first-generation QoS system was primarily an isolation tool. It relied on a Redis-backed counter, client quota based rate-limiter, that checked a callerâ€™s requests per second (QPS) against a configurable fixed quota. The goal was to prevent a single misbehaving client from overwhelming the service and causing a complete outage. For this purpose, it was simple and effective.</p><p>However, as the service matured, our goal shifted from merely preventing meltdowns to maximizing goodputâ€Šâ€”â€Šthat is, getting the most useful work done without degrading performance. A system of fixed, manually configured quotas canâ€™t achieve this, as it canâ€™t adapt in real time to shifting traffic patterns, new query shapes, or sudden threats like a DDoS attack. A truly effective QoS system needs to be adaptive, automatically exerting prioritized backpressure when it senses the system has reached its useful capacity.</p><p>To better match our QoS system to the realities of online traffic and maximize goodput, over time we evolved it to add several newÂ layers.</p><ul><li><strong>Resource-aware rate control (RARC)</strong>: Charges each request in <em>request units</em> (RU) that reflect rows, bytes, and latency, not justÂ counts.</li><li><strong>Load shedding with criticality tiers</strong>: Guarantees that high-priority traffic (e.g., customer support, trust and safety) stays responsive when capacity evaporates.</li><li><strong>Hot-key detection &amp; DDoS mitigation</strong>: Detects skewed access patterns in real time and then shields the backendâ€Šâ€”â€Šwhether the surge is legitimate or a DDoS burstâ€Šâ€”â€Šby caching or coalescing the duplicate requests before they reach the storageÂ layer.</li></ul><p>What follows is an engineerâ€™s view of how these layers were designed, deployed, and battle-tested, and why the same ideas may apply to any multi-tenant system that has outgrown simple QPSÂ limits.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*XVDMQb8i2pQEUogiZipFbQ.jpeg" /><figcaption>Progression Timeline</figcaption></figure><h3>Background: Life with Client Quota RateÂ Limiter</h3><p>When Mussel launched, rate-limiting was entirely handled via simple QPS rate-limiting using a Redis-based distributed counter service. Each caller received a static, per-minute quota, and the dispatcher incremented a Redis key for every incoming request. If the keyâ€™s value exceeded the callerâ€™s quota, the dispatcher returned an HTTP 429. The design was simple, predictable, and easy toÂ operate.</p><p>Two architectural details made this feasible. First, Mussel and its storage engine were tightly coupled; backend effort correlated reasonably well with the number of calls at the front door. Second, the traffic mix was modest in size and variety, so a single global limit per caller rarely causedÂ trouble.</p><p>As adoption grew, two limitations becameÂ clear.</p><ol><li><strong>Cost variance:</strong> A one-row lookup and a 100,000-row scan were treated equally, even though their load on the backend differed by orders of magnitude. The system couldnâ€™t distinguish high-value cheap work from low-value expensive work.</li><li><strong>Traffic skew:</strong> Per-caller rate limits provided isolation at the client level, but were blind to the dataâ€™s access pattern. When a single key became â€œhotâ€â€Šâ€”â€Šfor example, a popular listing accessed by thousands of different callers simultaneouslyâ€Šâ€”â€Šthe aggregate traffic could overwhelm the underlying storage shard, even if each individual caller remained within its quota. This created a localized bottleneck that degraded performance for the entire cluster, impacting clients requesting completely unrelated data. Isolation by <em>caller</em> was insufficient to prevent this kind of resource contention.</li></ol><p>Addressing these gaps meant shifting from a <em>request-counting</em> mindset to a <em>resource-accounting</em> mindset and designing controls that reflect the real cost of each operation.</p><h3>Resource-aware rateÂ control</h3><p>A fair quota system must account for the real work a request imposes on the storage layer. Resource-aware rate control (RARC) meets this need by charging operations in <em>request units</em> (RU) rather than raw requests perÂ second.</p><p>A request unit blends four observable factors: fixed per-call overhead, rows processed, payload bytes, andâ€Šâ€”â€Šcruciallyâ€Šâ€”â€Šlatency. Latency captures effects that rows and bytes alone miss: two one-megabyte reads can differ greatly in cost if one hits cache and the other triggers disk. In practice, we use a linear model. For both reads and writes, the costÂ is:</p><pre><br>RU_read = 1 + w_r Ã— bytes_read + w_l Ã— latency_ms<br>RU_write = 6 + w_b Ã— bytes_written / 4096 bytes + w_l Ã— latency_ms<br><br>Weight factors w_r, w_b, and w_l come from load-test calibration <br>based on the compute, network and disk I/O. <br>bytes_read, bytes_written and latency is measured per request</pre><p>Although approximate, the formula separates operations whose surface metrics look similar yet load the backend very differently.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*VuIeZSMzyRqXuXpfSzM_yg.jpeg" /><figcaption>Impact of Latency on RU computation</figcaption></figure><p>Each dispatcher continues to rely on rate-limiter for distributed counting, but the counter now represents request-unit tokens instead of raw QPS. At the start of every epoch, the dispatcher adds the callerâ€™s static RU quota to a local token bucket and immediately debits that bucket by the RU cost of each incoming request. When the bucket is empty, the request is rejected with HTTP 419. Because all dispatchers follow the same procedure and epochs are short, their buckets remain closely aligned without additional coordination.</p><p>Adaptive protection is handled in the separate load-shedding layer; backend latency influences which traffic is dropped or delayed, not the size of the periodic RU refill. This keeps rate accounting straightforwardâ€Šâ€”â€Šstatic quotas expressed in request unitsâ€Šâ€”â€Šwhile still reacting quickly when the storage layer shows signs ofÂ stress.</p><h3>Load shedding: Staying healthy when capacity evaporates or developsÂ hotspots</h3><p>Rate limits based on request units excel at smoothing normal traffic, but they adjust on a scale of seconds. When the workload shifts fasterâ€Šâ€”â€Ša bot floods a key, a shard stalls, or a batch job begins a full-table scanâ€Šâ€”â€Šthose seconds are enough for queues to balloon and service-level objectives to slip. To bridge this reaction-time gap, Mussel uses a load-shedding safety net that combines three real-time signals: (1) traffic criticality, (2) a latency ratio, and (3) a CoDel-inspired queueingÂ policy.</p><p>The latency ratio is a ratio that serves as a real-time indicator of stress on the system stress. Each dispatcher computes this ratio by dividing the long-term p95 latency by the short-term p95 latency. A stable system has a ratio near 1.0; a value dropping towards 0.3 indicates that latency is rising sharply. When that threshold is crossed, the dispatcher temporarily increases the RU cost applied to a designated client class so that its token bucket drains faster and the request rate naturally backs off. If the ratio keeps falling, the same penalty can be expanded to additional classes until latency returns to a safeÂ range.</p><p>The estimate uses the constant-memory PÂ² algorithm [1], requiring no raw sample storage or cross-node coordination.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*GWv-z2hXM6RiW6sRJKzYCg.png" /><figcaption>Latency response over time and illustration of throttling</figcaption></figure><p>The Control-Delay (CoDel) thread pool tackles the second hazard: queue buildup <em>inside the dispatcher itself</em> [2]. It monitors the time a request <em>waits</em> in the queue. If that sojourn time proves the system is already saturated, the request fails early, freeing up memory and threads for higher-priority work. An optional latency penalty can also be applied to RU accounting, charging more for queries from callers that persistently trigger the latencyÂ ratio.</p><p>Together, these layersâ€Šâ€”â€Šcriticality, a real-time latency ratio, and adaptive queueingâ€Šâ€”â€Šform a shield that lets guest-facing traffic ride out backend hiccups. In practice, this system has cut recovery times by about half and keeps dispatchers stable without human intervention.</p><h3>Hot-key detection and DDoSÂ defence</h3><p>Request-unit limits and load shedding keep client usage fair, but they cannot stop a stampede of identical reads aimed at one record. Imagine a listing that hits the front page of a major news outlet: tens of thousands of guests refresh their browser, all asking for the same key. A misconfigured crawlerâ€Šâ€”â€Šor a deliberate botnetâ€Šâ€”â€Šcan generate the same access pattern, only faster. The result is shard overload, a full dispatcher queue, and rising latency for unrelated work.</p><p>Mussel neutralises this amplification with a three-step<strong> </strong>hot-key defence layer<strong>:</strong> real-time detection, local caching, and request coalescing.</p><h3>Real-time detection in constantÂ space</h3><p>Every dispatcher streams incoming keys into an in-memory <em>top-k</em> counter. The counter is a variant of the Space-Saving algorithm [2] popularized in Brian Hayesâ€™s â€œBritney Spears Problemâ€ essay [4]. In just a few megabytes, it tracks approximate hit counts, maintains a frequency-ordered heap, and surfaces the hottest keys in real time in each individual dispatcher.</p><h3>Local caching and request coalescing</h3><p>When a key crosses the hot threshold, the dispatcher serves it from a process-local LRU cache. Entries expire after roughly three seconds, so they vanish as soon as demand cools; no global cache is required. A cache miss can still arrive multiple times in the same millisecond, so the dispatcher tracks in-flight reads for hot keys. New arrivals attach to the pending future; the first backend response then fans out to all waiters. In most cases only <em>one</em> request per hot key per dispatcher pod ever reaches the storageÂ layer.</p><h3>Impact in production</h3><p>In a controlled DDoS drill that targeted a small set of keys at â‰ˆ million-QPS scale, the hot-key layer collapsed the burst to a trickleâ€Šâ€”â€Šeach dispatcher forwarded only an occasional request, well below the capacity of any individual shardâ€Šâ€”â€Šso the backend never felt theÂ surge.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*DuwP-cqJEnQcbD64RGUTYA.png" /><figcaption>Hotkeys detected and served from dispatcher cache in realÂ time</figcaption></figure><h3>Retrospective and key takeaways</h3><p>The journey from a single QPS counter to a layered, cost-aware QoS stack has reshaped how Mussel handles traffic and, just as importantly, how engineers think about fairness and resilience. A few themes surface when we look back across the stages described above.</p><p>The first is the value of early, visible impact. The initial release of request-unit accounting went live well before load shedding or hot-key defence. Soon after deployment it automatically throttled a caller whose range scans had been quietly inflating cluster latency. That early win validated the concept and built momentum for the deeper changes that followed.</p><p>A second lesson is to prefer to keep control loops local. All the key signalsâ€Šâ€”â€ŠPÂ² latency quantiles, the Space-Saving top-k counter, and CoDel queue delayâ€Šâ€”â€Šrun entirely inside each dispatcher. Because no cross-node coordination is required, the system scales linearly and continues to protect capacity even if the control plane is itself underÂ stress.</p><p>Third, effective protection works on<strong> </strong>two different time-scales<strong>.</strong> Per-call RU pricing catches micro-spikes; the latency ratio and CoDel queue thresholds respond to macro slow-downs. Neither mechanism alone would have kept latency flat during the last controlled DDoS drill, but in concert they absorbed the shock and recovered withinÂ seconds.</p><p>Finally, QoS is a living system. Traffic patterns evolve, back-end capabilities improve, and new workloads appear. Planned next steps include database-native resource groups and automatic quota tuning from thirty-day usage curves. The principles that guided this projectâ€Šâ€”â€Šmeasure true cost, react locally and quickly, layer defencesâ€Šâ€”â€Šform a durable template, but the implementation will continue to grow with the platform it protects.</p><p>Does this type of work interest you? Weâ€™re hiring, check out open rolesÂ <a href="https://careers.airbnb.com/">here</a>.</p><h3>ğŸ“š References</h3><ol><li>Raj Jain and Imrich Chlamtac. 1985. The PÂ² algorithm for dynamic calculation of quantiles and histograms without storing observations. <em>Communications of the ACM</em>, <strong>28</strong>(10), 1076â€“1085.<a href="https://doi.org/10.1145/4372.4378"> https://doi.org/10.1145/4372.4378</a></li><li>Erik D. Demaine, Alejandro LÃ³pez-Ortiz, and J. Ian Munro. 2002. Frequency estimation of internet packet streams with limited space. In <em>Algorithmsâ€Šâ€”â€ŠESA 2002: 10th Annual European Symposium</em>, Rome, Italy, September 17â€“21, 2002. Rolf H. MÃ¶hring and Rajeev Raman (Eds.). <em>Lecture Notes in Computer Science</em>, Vol. <strong>2461</strong>. Springer, 348â€“360.</li><li>Kathleen M. Nichols and Van Jacobson. 2012. Controlling queue delay. <em>Communications of the ACM</em>, <strong>55</strong>(7), 42â€“50.<a href="https://doi.org/10.1145/2209249.2209264"> https://doi.org/10.1145/2209249.2209264</a></li><li>Brian Hayes. 2008. Computing science: The Britney Spears problem. <em>American Scientist</em>, <strong>96</strong>(4), 274â€“279. <a href="https://www.americanscientist.org/article/the-britney-spears-problem">https://www.americanscientist.org/article/the-britney-spears-problem</a></li></ol><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=29362764e5c2" width="1" height="1" alt=""><hr><p><a href="https://medium.com/airbnb-engineering/from-static-rate-limiting-to-adaptive-traffic-management-in-airbnbs-key-value-store-29362764e5c2">From Static Rate Limiting to Adaptive Traffic Management in Airbnbâ€™s Key-Value Store</a> was originally published in <a href="https://medium.com/airbnb-engineering">The Airbnb Tech Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Building a Next-Generation Key-Value Store at Airbnb]]></title>
            <link>https://medium.com/airbnb-engineering/building-a-next-generation-key-value-store-at-airbnb-0de8465ba354?source=rss----53c7c27702d5---4</link>
            <guid isPermaLink="false">https://medium.com/p/0de8465ba354</guid>
            <category><![CDATA[engineering]]></category>
            <category><![CDATA[migration]]></category>
            <category><![CDATA[infrastructure]]></category>
            <category><![CDATA[storage]]></category>
            <category><![CDATA[sql]]></category>
            <dc:creator><![CDATA[Shravan Gaonkar]]></dc:creator>
            <pubDate>Wed, 24 Sep 2025 16:02:09 GMT</pubDate>
            <atom:updated>2025-09-24T23:47:22.333Z</atom:updated>
            <content:encoded><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*sUS0d9nGKa-WQupVS8Wb4g.jpeg" /></figure><p>How we completely rearchitected Mussel, our storage engine for derived data, and lessons learned from the migration from Mussel V1 toÂ V2.</p><p>By <a href="https://www.linkedin.com/in/shravangaonkar/">Shravan Gaonkar</a>, <a href="https://www.linkedin.com/in/chandramoulir/">Chandramouli Rangarajan</a>, <a href="https://www.linkedin.com/in/yanhan-zhang/">YanhanÂ Zhang</a></p><p>How we completely rearchitected Mussel, our storage engine for derived data, and lessons learned from the migration from Mussel V1 toÂ V2.</p><p>Airbnbâ€™s core key-value store, internally known as Mussel, bridges offline and online workloads, providing highly scalable bulk load capabilities combined with single-digit millisecond reads.</p><p>Since first writing about Mussel in a 2022 <a href="https://medium.com/airbnb-engineering/mussel-airbnbs-key-value-store-for-derived-data-406b9fa1b296">blog post</a>, we have completely deprecated the storage backend of the original system (what we now call Mussel v1) and have replaced it with a NewSQL backend which we are referring to as Mussel v2. Mussel v2 has been running successfully in production for a year, and we wanted to share why we undertook this rearchitecture, what the challenges were, and what benefits we got fromÂ it.</p><h3>Why rearchitect</h3><p>Mussel v1 reliably supported Airbnb for years, but new requirementsâ€Šâ€”â€Šreal-time fraud checks, instant personalization, dynamic pricing, and massive dataâ€Šâ€”â€Šdemand a platform that combines real-time streaming with bulk ingestion, all while being easy toÂ manage.</p><h3>Key Challenges withÂ v1</h3><p>Mussel v2 solves a number of issues with v1, delivering a scalable, cloud-native key-value store with predictable performance and minimal operational overhead.</p><ul><li><strong>Operational complexity:</strong> Scaling or replacing nodes required multi-step Chef scripts on EC2; v2 uses Kubernetes manifests and automated rollouts, reducing hours of manual work toÂ minutes.</li><li><strong>Capacity &amp; hotspots:</strong> Static hash partitioning sometimes overloaded nodes, leading to latency spikes. V2â€™s dynamic range sharding and presplitting keep reads fast (p99 &lt; 25ms), even for 100TB+Â tables.</li><li><strong>Consistency flexibility:</strong> v1 offered limited consistency control. v2 lets teams choose between immediate or eventual consistency based on their SLAÂ needs.</li><li><strong>Cost &amp; Transparency:</strong> Resource usage in v1 was opaque. v2 adds namespace tenancy, quota enforcement, and dashboards, providing cost visibility andÂ control.</li></ul><h3>New architecture</h3><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*yjXZcYHPpQdl-peEhfEAmA.png" /></figure><p>Mussel v2 is a complete re-architecture addressing v1â€™s operational and scalability challenges. Itâ€™s designed to be automated, maintainable, and scalable, while ensuring feature parity and an easy migration for 100+ existing userÂ cases.</p><h3>Dispatcher</h3><p>In Mussel v2, the Dispatcher is a stateless, horizontally-scalable Kubernetes service that replaces the tightly coupled, protocol-specific design of v1. It translates client API calls into backend queries/mutations, supports dual-write and shadow-read modes for migration, manages retries and rate limits, and integrates with Airbnbâ€™s service mesh for security and service discovery.</p><p><strong>Reads</strong> are simplified: Each dataname maps to a logical table, enabling optimized point lookups, range/prefix queries, and stale reads from local replicas to reduce latency. Dynamic throttling and prioritization maintain performance under changingÂ traffic.</p><p><strong>Writes</strong> are persisted in Kafka for durability first, with the Replayer and Write Dispatcher applying them in order to the backend. This event-driven model absorbs bursts, ensures consistency, and removes v1â€™s operational overhead. Kafka also underpins upgrades, bootstrapping, and migrations until CDC and snapshotting mature.</p><p>The architecture suits derived data and replay-heavy use cases today, with a long-term goal of shifting ingestion and replication fully to the distributed backend database to bring down latency and simplify operations.</p><p>Bulk load<strong><br></strong>Bulk load remains essential for moving large datasets from offline warehouses into Mussel for low-latency queries. v2 preserves v1 semantics, supporting both â€œmergeâ€ (add to existing tables) and â€œreplaceâ€ (swap datasets) semantics.</p><p>To maintain a familiar interface, v2 keeps the existing Airflow-based onboarding and transforms warehouse data into a standardized format, uploading to S3 for ingestion. <a href="https://airbnb.io/projects/airflow/">Airflow</a> is an open-source platform for authoring, scheduling, and monitoring data pipelines. Created at Airbnb, it lets users define workflows in code as directed acyclic graphs (DAGs), enabling quick iteration and easy orchestration of tasks for data engineers and scientists worldwide.</p><p>A stateless controller orchestrates jobs, while a distributed, stateful worker fleet (Kubernetes StatefulSets) performs parallel ingestion, loading records from S3 into tables. Optimizationsâ€Šâ€”â€Šlike deduplication for replace jobs, delta merges, and insert-on-duplicate-key-ignoreâ€Šâ€”â€Šensure high throughput and efficient writes at AirbnbÂ scale.</p><h3>TTL</h3><p>Automated data expiration (TTL) can help support data governance goals and storage efficiency. In v1, expiration relied on the storage engineâ€™s compaction cycle, which struggled atÂ scale.</p><p>Mussel v2 introduces a topology-aware expiration service that shards data namespaces into range-based subtasks processed concurrently by multiple workers. Expired records are scanned and deleted in parallel, minimizing sweep time for large datasets. Subtasks are scheduled to limit impact on live queries, and write-heavy tables use max-version enforcement with targeted deletes to maintain performance and dataÂ hygiene.</p><p>These enhancements provide the same retention functionality as v1 but with far greater efficiency, transparency, and scalability, meeting Airbnbâ€™s modern data platform demands and enabling future useÂ cases.</p><h3>The migration process</h3><h3>Challenge</h3><p>Mussel stores vast amounts of data and serves thousands of tables across a wide array of Airbnb services, sustaining mission-critical read and write traffic at high scale. Given the criticality of Mussel to Airbnbâ€™s online traffic, our migration goal was straightforward but challenging: Move all data and traffic from Mussel v1 to v2 with zero data loss and no impact on availability to our customers.</p><h3>Process</h3><p>We adopted a blue/green migration strategy, but with notable complexities. Mussel v1 didnâ€™t provide table-level snapshots or CDC streams, which are standard in many datastores. To bridge this gap, we developed a custom migration pipeline capable of bootstrapping tables to v2, selected by usage patterns and risk profiles. Once bootstrapped, dual writes were enabled on a per-table basis to keep v2 in sync as the migration progressed.</p><p>The migration itself followed several distinctÂ stages:</p><ul><li><strong>Blue Zone:</strong> All traffic initially flowed to v1 (â€œBlueâ€). This provided a stable baseline as we migrated data behind theÂ scenes.</li><li><strong>Shadowing (Green):</strong> Once tables were bootstrapped, v2 (â€œGreenâ€) began shadowing v1â€Šâ€”â€Šhandling reads/writes in parallel, but only v1 responded. This allowed us to check v2â€™s correctness and performance withoutÂ risk.</li><li><strong>Reverse:</strong> After building confidence, v2 took over active traffic while v1 remained on standby. We built automatic circuit breakers and fallback logic: If v2 showed elevated error rates or lagged behind v1, we could instantly return traffic to v1 or revert to shadowing.</li><li><strong>Cutover:</strong> When v2 passed all checks, we completed the cutover on a dataname-by-dataname basis, with Kafka serving as a robust intermediary for write reliability throughout.</li></ul><p>To further de-risk the process, migration was performed one table at a time. Every step was reversible and could be fine-tuned per table or group of tables based on their risk profile. This granular, staged approach allowed for rapid iteration, safe rollbacks, and continuous progress without impacting the business.</p><h3>Migration pipeline</h3><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*4Q-yjBQu8jwWSv0pkkNIHQ.jpeg" /></figure><p>As described in our previous blog post, the v1 architecture uses Kafka as a replication logâ€Šâ€”â€Šdata is first written to Kafka, then consumed by the v1 backend. During the data migration to v2, we leveraged the same Kafka stream to maintain eventual consistency between v1 andÂ v2.</p><p>To migrate any given table from v1 to v2, we built a custom pipeline consisting of the following steps:</p><ol><li><strong>Source data sampling</strong>: We download backup data from v1, extract the relevant tables, and sample the data to understand its distribution.</li><li><strong>Create pre-split table on v2</strong>: Based on the sampling results, we create a corresponding v2 table with a pre-defined shard layout to minimize data reshuffling during migration.</li><li><strong>Bootstrap</strong>: This is the most time-consuming step, taking hours or even days depending on table size. To bootstrap efficiently, we use Kubernetes StatefulSets to persist local state and periodically checkpoint progress.</li><li><strong>Checksum verification</strong>: We verify that all data from the v1 backup has been correctly ingested intoÂ v2.</li><li><strong>Catch-up</strong>: We apply any lagging messages that accumulated in Kafka during the bootstrap phase.</li><li><strong>Dual writes</strong>: At this stage, both v1 and v2 consume from the same Kafka topic. We ensure eventual consistency between the two, with replication lag typically within tens of milliseconds.</li></ol><p>Once data migration is complete and we enter dual write mode, we can begin the read traffic migration phase. During this phase, our dispatcher can be dynamically configured to serve read requests for specific tables from v1, while sending shadow requests to v2 for consistency checks. We then gradually shift to serving reads from v2, accompanied by reverse shadow requests to v1 for consistency checks, which also enables quick fallback to v1 responses if v2 becomes unstable. Eventually, we fully transition to serving all read traffic fromÂ v2.</p><h3>Lessons learned</h3><p>Several key insights emerged from this migration:</p><ul><li><strong>Consistency complexity:</strong> Migrating from an eventually consistent (v1) to a strongly consistent (v2) backend introduced new challenges, particularly around write conflicts. Addressing these required features like write deduplication, hotkey blocking, and lazy write repairâ€Šâ€”â€Šsometimes trading off storage cost or read performance.</li><li><strong>Presplitting is critical:</strong> As we shifted from hash-based (v1) to range-based partitioning (v2), inserting large consecutive data could cause hotspots and disrupt our v2 backend. To prevent this, we needed to accurately sample the v1 data and presplit it into multiple shards based on v2â€™s topology, ensuring balanced ingestion traffic across backend nodes during data migration.</li><li><strong>Query model adjustments:</strong> v2 doesnâ€™t push down range filters as effectively, requiring us to implement client-side pagination for prefix and rangeÂ queries.</li><li><strong>Freshness vs. cost:</strong> Different use cases required different tradeoffs. Some prioritized data freshness and used primary replicas for the latest reads, while others leveraged secondary replicas to balance staleness with cost and performance.</li><li><strong>Kafkaâ€™s role:</strong> Kafkaâ€™s proven stable p99 millisecond latency made it an invaluable part of our migration process.</li><li><strong>Building in flexibility:</strong> Customer retries and routine bulk jobs provided a safety net for the rare inconsistencies, and our migration design allowed for per-table stage assignments and instant reversibilityâ€Šâ€”â€Škey for managing risk atÂ scale.</li></ul><p>As a result, we migrated more than a petabyte of data across thousands of tables with zero downtime or data loss, thanks to a blue/green rollout, dual-write pipeline, and automated fallbacksâ€Šâ€”â€Šso the product teams could keep shipping features while the engine under themÂ evolved.</p><h3>Conclusion and nextÂ steps</h3><p>What sets Mussel v2 apart is the way it fuses capabilities that are usually confined to separate, specialized systems. In our deployment of Mussel V2, we observe that this system can simultaneously</p><ol><li>ingest tens of terabytes in bulk dataÂ upload,</li><li>sustain 100 k+ streaming writes per second in the same cluster,Â and</li><li>keep p99 reads under 25Â ms</li></ol><p>â€” all while giving callers a simple dial to toggle stale reads on a per-namespace basis. By pairing a NewSQL backend with a Kubernetes-native control plane, Mussel v2 delivers the elasticity of object storage, the responsiveness of a low-latency cache, and the operability of modern service meshesâ€Šâ€”â€Šrolled into one platform. Engineers no longer need to stitch together a cache, a queue, and a datastore to hit their SLAs; Mussel provides those guarantees out of the box, letting teams focus on product innovation instead of data plumbing.</p><p>Looking ahead, weâ€™ll be sharing deeper insights into how weâ€™re evolving quality of service (QoS) management within Mussel, now orchestrated cleanly from the Dispatcher layer. Weâ€™ll also describe our journey in optimizing bulk loading at scaleâ€Šâ€”â€Šunlocking new performance and reliability wins for complex data pipelines. If youâ€™re passionate about building large-scale distributed systems and want to help shape the future of data infrastructure at Airbnb, take a look at our <a href="https://careers.airbnb.com/">Careers page</a>â€Šâ€”â€Šweâ€™re always looking for talented engineers to join us on thisÂ mission.</p><h3>References</h3><ol><li><a href="https://medium.com/airbnb-engineering/mussel-airbnbs-key-value-store-for-derived-data-406b9fa1b296">https://medium.com/airbnb-engineering/mussel-airbnbs-key-value-store-for-derived-data-406b9fa1b296</a></li></ol><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=0de8465ba354" width="1" height="1" alt=""><hr><p><a href="https://medium.com/airbnb-engineering/building-a-next-generation-key-value-store-at-airbnb-0de8465ba354">Building a Next-Generation Key-Value Store at Airbnb</a> was originally published in <a href="https://medium.com/airbnb-engineering">The Airbnb Tech Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Viaduct, Five Years On: Modernizing the Data-Oriented Service Mesh]]></title>
            <link>https://medium.com/airbnb-engineering/viaduct-five-years-on-modernizing-the-data-oriented-service-mesh-e66397c9e9a9?source=rss----53c7c27702d5---4</link>
            <guid isPermaLink="false">https://medium.com/p/e66397c9e9a9</guid>
            <category><![CDATA[data]]></category>
            <category><![CDATA[graphql]]></category>
            <category><![CDATA[engineering]]></category>
            <category><![CDATA[open-source]]></category>
            <category><![CDATA[api]]></category>
            <dc:creator><![CDATA[Adam Miskiewicz]]></dc:creator>
            <pubDate>Wed, 17 Sep 2025 17:01:58 GMT</pubDate>
            <atom:updated>2025-09-18T01:59:27.141Z</atom:updated>
            <content:encoded><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*q_owFEOLfQlioFXHP7UqBA.avif" /></figure><h4>A more powerful engine and a simpler API for our data-oriented mesh</h4><p>By: Adam Miskiewicz, RaymieÂ Stata</p><p>In November 2020 we <a href="https://medium.com/airbnb-engineering/taming-service-oriented-architecture-using-a-data-oriented-service-mesh-da771a841344">published</a> a post about Viaduct, our data-oriented service mesh. Today, weâ€™re excited to announce Viaduct is available as open-source software (OSS) at <a href="https://github.com/airbnb/viaduct">https://github.com/airbnb/viaduct</a>.</p><p>Before we talk about OSS, hereâ€™s a quick update on Viaductâ€™s adoption and evolution at Airbnb over the last five years. Since 2020, traffic through Viaduct has grown by a factor of eight. The number of teams hosting code in Viaduct has doubled to 130+ (with hundreds of weekly active developers). The codebase hosted by Viaduct has tripled to over <strong>1.5M</strong> lines (plus about the same in test code). Weâ€™ve achieved all this while keeping operational overhead constant, halving incident-minutes, and keeping costs growing linearly withÂ QPS.</p><h3>Whatâ€™s theÂ same?</h3><p>Three principles have guided Viaduct since day one and still anchor the project: a <strong>central schema</strong> served by <strong>hosted business logic</strong> via a <strong>re-entrant</strong> API.</p><p><strong>Central schema <br></strong>Viaduct serves our central schema: a single, integrated schema connecting all of our domains across the company. While that schema is developed in a <em>decentralized</em> manner by many teams, itâ€™s one, highly connected graph. Over 75% of Viaduct requests are internal because Viaduct has become a â€œoneâ€‘stopâ€ data-oriented mesh connecting developers to all of our data and capabilities.</p><p><strong>Hosted business logic <br></strong>From the beginning, weâ€™ve encouraged teams to host their business logic directly in Viaduct. This runs counter to what many consider to be best practices in GraphQL, which is that GraphQL servers should be a thin layer over microservices that host the real business logic. Weâ€™ve created a serverless platform for hosting business logic, allowing our developers to focus on writing business logic rather than on operational issues. As noted by Katie, an engineer on our MediaÂ team:</p><blockquote>â€œAs we migrate our media APIs into Viaduct, weâ€™re looking forward to retiring a handful of standalone services. Centralizing everything means less overhead, fewer moving parts, and a much smoother developer experience!â€</blockquote><p><strong>Re-entrancy</strong></p><p>At the heart of our developer experience is what we call <em>re-entrancy</em>: Logic hosted on Viaduct composes with other logic hosted on Viaduct by issuing GraphQL fragments and queries. Re-entrancy has been crucial for maintaining modularity in a large codebase and avoiding classic monolithÂ hazards.</p><h3>Whatâ€™s changed?</h3><p>For most of Viaductâ€™s history, evolution has been bottom-up and reactive to immediate developer needs. We added capabilities incrementally, which helped us move fast, but also produced multiple ways to accomplish similar tasks (some wellâ€‘supported, others not) and created a confusing developer experience, especially for new teams. Another side-effect of this reactive approach has been a lack of architectural integrity. The interfaces between the layers of Viaduct, described in more detail below, are loose and often arbitrary, and the abstraction boundary between the Viaduct framework and the code that it hosts is weak. As a result, it has become increasingly difficult to make changes to Viaduct without disrupting our customerÂ base.</p><p>To address these issues, over a year ago we launched a major initiative we call <strong>â€œViaduct Modernâ€</strong>, a ground-up overhaul of both the developer-facing API and the execution engine.</p><h3>Tenant API</h3><p>One driving principle of Viaduct Modern has been to simplify and rationalize the API we provide to developers in Viaduct, which we call the <strong>â€œTenant APIâ€</strong>. The following diagram captures the decision tree one faced when deciding how to implement functionality in the oldÂ API:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*0Gujn6vAspuKh3ZuPjlK-g.png" /><figcaption>Viaductâ€™s original complex programming model</figcaption></figure><p>Each oval in this diagram represents a different mechanism for writing code. In contrast, the new API offers just two mechanisms: node resolvers and field resolvers.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*xuL1H_ylQR5PU2eh2JuXEA.png" /><figcaption>Viaduct Modernâ€™s simplerÂ model</figcaption></figure><p>The choice between the two is driven by the schema itself, not adâ€‘hoc distinctions based on a featureâ€™s behavior. We unified the APIs for both resolver types wherever possible, which simplifies dev experience. After four years evolving the API in a useâ€‘caseâ€‘driven manner, we distilled the best ideas into a single simple surface (and left the mistakesÂ behind).</p><h3>Tenant modularity</h3><p>Strong abstraction boundaries are essential in any large codebase. Microservices achieve this via service definitions and RPC API boundaries; Viaduct achieves it via <strong>modules</strong> plus <strong>reâ€‘entrancy</strong>.</p><p>Modularity in the central schema and hosted code has evolved. Initially, all we had was a vague set of conventions for organizing code into team-owned directories. There was no formal concept of a module, and schema and code were kept in separate source directories with unenforced naming conventions to connect the two. Over time, we evolved that into a more formal abstraction we call a â€œtenant module.â€ A tenant module is a unit of schema together with the code that implements that schema, and crucially, is owned by a single team. While we encourage rich graphâ€‘level connections across modules, we <strong>discourage direct code dependencies</strong> between modules. Instead, modules compose via GraphQL fragments and queries. Viaduct Modern extends and simplifies these reâ€‘entrancy tools.</p><p>Letâ€™s look at an example. Imagine two teams, a â€œCore Userâ€ team that owns and manages the basic profile data of users, and then a â€œMessagingâ€ team that operates a messaging platform for users to interact with each other. In our example, the Messaging team would like to define a <em>displayName</em> field on a <em>User</em>, which is used in their user interface. This would look something likeÂ this:</p><p><strong>Core UserÂ team</strong></p><pre>type User implements Node {<br>  id: ID!<br>  firstName: String<br>  lastName: String<br>  â€¦ <br>}</pre><pre>class UserResolver : Nodes.User() {<br>    @Inject<br>    val userClient: UserServiceClient<br><br>    @Inject<br>    val userResponseMapper: UserResponseMapper<br><br>    override suspend fun resolve(ctx.Context): User {<br>        val r = userClient.fetch(ctx.id)<br>        return userResponseMapper(r)<br>    } <br>}</pre><p>This is the base definition of the <em>User</em> type that lives in the Core User teamâ€™s module. This base definition defines the first- and last-name fields (among many others), and itâ€™s the Core User teamâ€™s responsibility to materialize thoseÂ fields.</p><p><strong>Messaging team</strong></p><pre>extend type User {<br>  displayName: String @resolver<br>}</pre><pre>@Resolver(&quot;firstName lastName&quot;)<br>class DisplayNameResolver : UserResolvers.DisplayName() {<br>    override suspend fun resolve(ctx: Context): String {<br>        val f = ctx.objectValue.getFirstName()<br>        val l = ctx.objectValue.getLastName()<br>        return &quot;$f ${l.first()}.&quot;<br>    }<br>}</pre><p>The Messaging team can then extend the <em>User </em>type with the display name field, and also indicates that they intend to provide a resolver for it. The code has an <em>@Resolver </em>annotation that indicates which fields of the <em>User</em> object it needs to implement the <em>displayName</em> field. The Messaging team doesnâ€™t need to understand which module these fields come from, and their code doesnâ€™t depend on code from the Core User team. Instead, the Messaging team states their data needs in a declarative fashion.</p><h3>Framework modularity</h3><p>A major goal of Viaduct Modern has been to make the framework itself more modular. We want to enable faster improvements to Viaductâ€Šâ€”â€Šespecially regarding performance and reliabilityâ€Šâ€”â€Šwithout extensive changes to application code. Viaduct is composed of three main layers: the GraphQL execution engine, the tenant API, and the hosted application code. While these layers made sense, the interfaces between them were weak, making Viaduct difficult to update. The new design is focused on creating strong abstraction boundaries between these layers to improve flexibility and maintainability:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*qvOOmI-Hs7-PMa52kSACqQ.png" /><figcaption>Viaduct Modernâ€™s modularÂ design</figcaption></figure><p>The most significant change is the boundary between the <strong>engine</strong> and the developer-facing <strong>tenant API</strong>. In the previous system, that boundary hardly existed. Viaduct Modern defines a strong <strong>engine API</strong> whose core is a <strong>dynamicallyâ€‘typed</strong> representation of GraphQL values (input and output objects as simple maps from field name â†’ value). The tenant API, by contrast, is <strong>statically typed</strong>: we generate Kotlin classes for every GraphQL type in the central schema. In the new architecture, generated types are thin wrappers over the dynamic representation. The tenant API forms the bridge between the engineâ€™s untyped world and tenantsâ€™ typedÂ world.</p><p>This separation lets us evolve the engine in relative isolation (to improve latency, throughput, and reliability) and evolve the tenant API in relative isolation (to improve dev experience). Large changes will still cross the boundary, but as Viaduct Modern stabilizes, that should beÂ rare.</p><h3>Migration without a â€œbigÂ bangâ€</h3><p>Viaduct Modern would be a nonâ€‘starter if it required a stepâ€‘function migration of a million+ lines of code. To enable gradual migration, weâ€™re shipping <strong>two tenant APIs</strong> sideâ€‘byâ€‘sideâ€Šâ€”â€Šthe new <strong>Modern</strong> API and the existing <strong>Classic</strong> APIâ€Šâ€”â€Šboth on top of the new engine. This lets teams realize performance/cost wins from the engine immediately, while adopting the ergonomic wins of the Modern API overÂ time.</p><p>Shipping two APIs has also improved the engine API design: building two tenant runtimes simultaneously forced us to keep the engineâ€™s concerns clean and general. Over time, we expect to build additional tenant APIs on the engine (e.g., <strong>TypeScript</strong>).</p><h3>Other improvements</h3><p>Viaduct has seen a lot of other improvements since 2020. The story is too long to be told in this post, but to list some of the highlights:</p><ul><li><strong>Observability.</strong> Hosting software from 100+ teams means our framework has to make ownership and attribution crystal clear. In the old system, there is no clear dividing line between tenant code and framework code, so our instrumentation includes a bit of guesswork in its attribution to parties. The new architecture draws a crisp boundary, which enables deeper, more accurate attribution.</li><li><strong>Build time.</strong> Weâ€™re schemaâ€‘first: Developers write schema as source, and Viaduct generates code to provide a strongly typed, ergonomic surface. At our scale, build time is a constant battle. Over the years weâ€™ve made numerous investments to improve build time, including direct-to-bytecode code generation that bypasses lengthy compilation of generated code. We anticipate that the improved modularity in Viaduct Modern will keep build times in check as the codebaseÂ grows.</li><li><strong>Dispatcher.</strong> We run Viaduct as a horizontally-scaled Kubernetes app. To mitigate blast radius, we use a dispatcher that routes operations to deployment shards, applying <a href="https://aws.amazon.com/blogs/architecture/shuffle-sharding-massive-and-magical-fault-isolation/">shuffle sharding</a>. It also simplifies isolating offline vs. online traffic and hosting experimental framework builds. (We donâ€™t currently plan to openâ€‘source the dispatcher as itâ€™s tightly tied to Airbnbâ€™s serving framework, but we may talk more about our strategies here in theÂ future!)</li></ul><h3>Open-sourcing Viaduct</h3><p>Our intent from the start of Viaduct Modern was to openâ€‘source it. We believe that setting out to build software for the world will result in higher quality software for ourselves. Also, as a significant consumer of open-source software, we feel an obligation to give back. Last but not least, while weâ€™ve learned a lot by working with Airbnb developers, we think Viaduct can be massively improved by incorporating ideas and contributions from the wider community.</p><p>Weâ€™re openâ€‘sourcing at an interesting moment. Viaduct is <strong>mature and battleâ€‘tested</strong> <em>and also</em> <strong>new and evolving</strong>. The new engine is now in full production, while the new tenant API is still in alpha. Weâ€™ve implemented a small but robust kernel of the new API and are using it in a few (demanding) use cases. Weâ€™re investing heavily in the Modern API and migrating our own workloads to it, but itâ€™s early days. By openâ€‘sourcing early, we hope to grow a community who will shape the API with us together.</p><h3>Is Viaduct forÂ you?</h3><p>Although our use case proves Viaduct can scale to massive graphs, we think itâ€™s also a great GraphQL server when youâ€™re just starting. Weâ€™ve emphasized developer ergonomics from day one, and we believe Viaduct provides one of the best environments for building GraphQL solutions. Whether youâ€™re operating a super-graph today or just kicking the tires, weâ€™d love for you to try Viaduct Modern and tell us what worksâ€Šâ€”â€Šand whatÂ doesnâ€™t.</p><p>â€”</p><p>Thanks to the entire Viaduct team, and especially Aileen Chen and Raymie Stata, for the tireless work on ViaductÂ Modern.</p><p><em>All product names, logos, and brands are property of their respective owners. All company, product and service names used in this website are for identification purposes only. Use of these names, logos, and brands does not imply endorsement.</em></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=e66397c9e9a9" width="1" height="1" alt=""><hr><p><a href="https://medium.com/airbnb-engineering/viaduct-five-years-on-modernizing-the-data-oriented-service-mesh-e66397c9e9a9">Viaduct, Five Years On: Modernizing the Data-Oriented Service Mesh</a> was originally published in <a href="https://medium.com/airbnb-engineering">The Airbnb Tech Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Taming Service-Oriented Architecture Using A Data-Oriented Service Mesh]]></title>
            <link>https://medium.com/airbnb-engineering/taming-service-oriented-architecture-using-a-data-oriented-service-mesh-da771a841344?source=rss----53c7c27702d5---4</link>
            <guid isPermaLink="false">https://medium.com/p/da771a841344</guid>
            <category><![CDATA[java]]></category>
            <category><![CDATA[microservices]]></category>
            <category><![CDATA[infrastructure]]></category>
            <category><![CDATA[graphql]]></category>
            <dc:creator><![CDATA[Adam Miskiewicz]]></dc:creator>
            <pubDate>Tue, 16 Sep 2025 18:37:58 GMT</pubDate>
            <atom:updated>2025-09-16T18:37:35.139Z</atom:updated>
            <content:encoded><![CDATA[<p>Introducing Viaduct, Airbnbâ€™s data-oriented serviceÂ mesh</p><p>By: Raymie Stata, Arun Vijayvergiya, Adam Miskiewicz</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*KUevf-1aGcLQyit3wzmh8w.jpeg" /></figure><p>At Hasuraâ€™s <a href="https://hasura.io/enterprisegraphql/">Enterprise GraphQL Conf</a> on October 22, we presented Viaduct, what weâ€™re calling a <em>data-oriented service mesh </em>that we believe will bring a step function improvement in the modularity of our microservices-based Service-Oriented Architecture (SOA). In this blog post, we describe the philosophy behind Viaduct and provide a rough sketch of how it works. Please <a href="https://www.youtube.com/watch?v=xxk9MWCk7cM">watch the presentation</a> for a more detailedÂ look.</p><h3>Massive SOA Dependency Graphs</h3><p>For a while, <strong>S</strong>ervice-<strong>O</strong>riented <strong>A</strong>rchitectures have been moving towards ever larger numbers of small microservices. Modern applications can consist of thousands to tens of thousands of microservices connected in unconstrained ways. As a result, itâ€™s not uncommon to see dependency graphs like the following:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/896/1*y2E1EmsS3paNhoChYGnJIw.png" /></figure><p>This particular dependency graph happens to be from Airbnb, but itâ€™s not uncommon. <a href="https://twitter.com/werner/status/741673514567143424">Amazon</a>, <a href="https://medium.com/refraction-tech-everything/how-netflix-works-the-hugely-simplified-complex-stuff-that-happens-every-time-you-hit-play-3a40c9be254b">Netflix</a>, and <a href="https://eng.uber.com/microservice-architecture/">Uber</a> are examples of those that shared similarly tangled dependency graphs.</p><p>These dependency graphs are reminiscent of <a href="https://en.wikipedia.org/wiki/Spaghetti_code">spaghetti code</a>, just at the microservices level. Similar to how spaghetti code becomes harder and harder to modify over time, so does spaghetti SOA. To help manage the larger number of services inherent in a microservices-based architecture, we need organizing principles as well as technical measures to implement those principles. At Airbnb, we undertook an effort to find such principles and measures. Our investigations led us to the concept of a <em>data-oriented service mesh</em>,<em> </em>which we believe brings a new level of modularity toÂ SOA.</p><h3>Procedure- vs Data-Oriented Design</h3><p>Organizing large programs into modular units is not a new problem in software engineering. Up until the 1970s, the main paradigm of software organization focused on grouping code into procedures and procedures into modules. In this approach, modules publish a public API to be used by code outside of the module; behind this public API, modules hide their internal, helper procedures and other implementation details. Languages such as Pascal and C are based on this paradigm.</p><p>Starting in the â€™80s, the paradigm shifted to organizing software primarily around data, not procedures. In this approach, modules define classes of objects that encapsulate an internal representation of an object accessed via a public API of methods<em> </em>on the object. Languages such as Simula and Clu pioneered this form of organization.</p><p>SOA is a step back to more procedure-oriented designs. Todayâ€™s microservice is a collection of procedural endpointsâ€Šâ€”â€Ša classic, 1970s-style module. We believe that SOA needs to evolve to support data-oriented design, and that this evolution can be enabled by transitioning our service mesh from a procedural orientation to a data orientation.</p><h3>Viaduct: A Data-Oriented ServiceÂ Mesh</h3><p>Central to modern, scalable SOA applications is a <em>service mesh</em> (e.g., <a href="https://istio.io/">Istio</a>, <a href="https://linkerd.io/">Linkerd</a>), which routes service invocations to instances of microservices that can handle them. The current industry standard for service meshes is to organize exclusively around remote procedure invocations without knowing anything about the data that makes up the application architecture. Our vision is to replace these procedure-oriented service meshes with service meshes organized aroundÂ <em>data.</em></p><p>At Airbnb, we are using <a href="https://graphql.org/">GraphQL</a>â„¢ï¸ to build a data-oriented service mesh called <em>Viaduct.</em> A Viaduct service mesh is defined in terms of a GraphQL schema consisting of:</p><ul><li><em>Types</em> (and <em>interfaces</em>) describing data managed within your serviceÂ mesh</li><li><em>Queries</em> (and <em>subscriptions</em>) providing means to access that data, which is abstracted from the service entry points that provide theÂ data</li><li><em>Mutations </em>providing ways to update data, again abstracted from service entryÂ points</li></ul><p>The types (and interfaces) in the schema define a single graph across all of the data managed within the service mesh. For example, at an eCommerce company, a service meshâ€™s schema may define a field productById(id: ID) that returns results of type Product. From this starting point, a single query allows a data consumer to navigate to information about the productâ€™s manufacturer, e.g., productById { manufacturer }; reviews of the product, e.g. productById { reviews }; and even the authors of those reviews, e.g., productById { reviews { author }Â }.</p><p>The data elements requested by such a query may come from many different microservices. In a procedure-oriented service mesh, the data consumer would need to take these services as explicit dependencies. In our data-oriented service mesh, it is the service mesh, i.e., Viaduct, not the data consumer, that knows which services provide which data element. Viaduct abstracts away the service dependencies from any single consumer.</p><h3>Putting Schema at theÂ Center</h3><p>In our talk we discuss how, unlike other distributed GraphQL systems like <a href="https://graphql-modules.com/">GraphQL Modules</a> or <a href="https://www.apollographql.com/docs/federation/">Apollo Federation</a>, Viaduct deals with the schema as a single artifact and has implemented several primitives that allow us to keep a unified schema while still allowing for many teams to collaborate on that schema productively. As Viaduct replaces more and more of our underlying procedure-oriented service mesh, its schema captures the data managed by our application more and more completely. We have taken advantage of this â€œcentral schema,â€ as we call it, as a place to define the APIs of some of our microservices. In particular, we have started using GraphQL for the API of some microservices. For these microservices, their GraphQL schemas are defined as a subset of the central schema. In the future, we want to take this idea further, using the central schema to define the schema of data stored in our database.</p><p>Among other things, using the central schema to define our APIs and database schemas will solve one of the bigger challenges of large-scale SOA applications: data agility. In todayâ€™s SOA applications, a change to a database schema often needs to be manually reflected in the APIs of two, three, and sometimes even more layers of microservices before it can be exposed to client code. Such changes can require weeks of coordinating among multiple teams. By deriving service APIs and database schemas from a single, central schema, a database schema change like this can be propagated to client code with a singleÂ update.</p><h3>Going Serverless</h3><p>Often in large SOA applications, there are many stateless â€œderived-dataâ€ services and â€œbackend-for-frontendâ€ services that take raw data from lower-level services and transform it into data thatâ€™s more appropriate for presentation in clients. Stateless logic like this is a good fit for the serverless computing model, which eliminates the operational overhead of microservices altogether and instead hosts logic in a â€œcloud functionsâ€ fabric.</p><p>Viaduct has a mechanism for computing what we call â€œderived fieldsâ€ using serverless cloud functions that operate on top of<em> </em>the graph without knowledge of the underlying services. These functions allow us to move transformational logic out of the service mesh and into stateless containers, keeping our graph clean and reducing the number and complexity of services weÂ need.</p><h3>Conclusion</h3><p>Viaduct is built on <a href="https://www.graphql-java.com/">graphql-java</a> and supports fine-grained field selection via GraphQL selection sets. It uses modern data-loading techniques, employs reliability techniques such as short-circuiting and soft dependencies, and implements an intra-request cache. Viaduct provides <em>data observability, </em>allowing us to understand, down to the field level, what services consume what data. As a GraphQL interface, Viaduct allows us to take advantage of a large ecosystem of open source tooling, including live IDEs, mock servers, and schema visualizers.</p><p>Viaduct started powering production workflows at Airbnb over a year ago. We started from scratch with a clean schema consisting of a handful of entities and have grown it to include 80 core entities that are able to power 75% of our modern APIÂ traffic.</p><p>As mentioned in the introduction, more details on the motivation and technology behind Viaduct can be found in our <a href="https://www.youtube.com/watch?v=xxk9MWCk7cM">presentation</a>.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=da771a841344" width="1" height="1" alt=""><hr><p><a href="https://medium.com/airbnb-engineering/taming-service-oriented-architecture-using-a-data-oriented-service-mesh-da771a841344">Taming Service-Oriented Architecture Using A Data-Oriented Service Mesh</a> was originally published in <a href="https://medium.com/airbnb-engineering">The Airbnb Tech Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Migrating Airbnbâ€™s JVM Monorepo to Bazel]]></title>
            <link>https://medium.com/airbnb-engineering/migrating-airbnbs-jvm-monorepo-to-bazel-33f90eda51ec?source=rss----53c7c27702d5---4</link>
            <guid isPermaLink="false">https://medium.com/p/33f90eda51ec</guid>
            <category><![CDATA[monorepo]]></category>
            <category><![CDATA[migration]]></category>
            <category><![CDATA[engineering]]></category>
            <category><![CDATA[infrastructure]]></category>
            <category><![CDATA[bazel]]></category>
            <dc:creator><![CDATA[Thomas Bao]]></dc:creator>
            <pubDate>Wed, 13 Aug 2025 17:01:58 GMT</pubDate>
            <atom:updated>2025-08-25T15:27:54.529Z</atom:updated>
            <content:encoded><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*rSkIrKYhc8Bwcv2xLE1mxA.jpeg" /></figure><p><strong>By: </strong>Jack Dai, Howard Ho, Loc Dinh, Stepan Goncharov, Ted Tenedorio, and ThomasÂ Bao</p><p>At Airbnb, we recently completed migrating our largest repo, the JVM monorepo, to Bazel. This repo contains <strong>tens of millions of lines</strong> of Java, Kotlin, and Scala code that power the vast array of backend services and data pipelines behind airbnb.com.</p><p><strong>Migration in numbers (4.5 years ofÂ work):</strong></p><ul><li>Build CSAT: 38% â†’Â 68%</li><li><strong>3â€“5x </strong>faster local build and testÂ times</li><li><strong>2â€“3x </strong>faster IntelliJÂ syncs</li><li><strong>2â€“3x</strong> faster deploys to the development environment</li></ul><p>In this blog post, weâ€™ll discuss the <strong>why</strong>, share some highlights on the <strong>how</strong>, and finish off with <strong>key learnings</strong>.</p><h3>Why Bazel?</h3><p>Before the migration, our JVM monorepo used Gradle as its build system. We decided to migrate to<strong> </strong>Bazel because it offered three key advantages: speed, reliability, and a uniform build infrastructure layer.</p><h4>Speed</h4><p><em>Bazelâ€™s cacheable, portable actions allow us to scale performance with remote execution</em></p><p>In 2021, builds of large services often took &gt;20 minutes locally and pre-merge CI p90 was 35Â minutes.</p><p>Building with Gradle was near its limit. We had already vertically scaled to high-end AWS machines on CI and remote development machines for developers of large services. In CI, we also used heuristics to split project builds and tests across multiple machines. However, this was inefficient, because of machine underutilization and duplication of sharedÂ tasks.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*y__bh8jYeKCNGmCW" /></figure><p>Bazel remote execution allowed us to scale to thousands of parallel actions. This was far more efficient than our sharding heuristics. Remote build execution (RBE) workers are also short-lived, which results in better machine utilization and cost efficiency. In addition, <a href="https://blog.bazel.build/2021/04/07/build-without-the-bytes.html">Build without the Bytes</a> allows downloading only a subset of files, greatly reducing download volume (in Gradle, every cached artifact needs to be downloaded). Finally and most importantly, local builds are significantly faster thanks toÂ RBE.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*qEJJw_dqvH--5nS1" /></figure><p>In addition, <a href="https://docs.gradle.org/current/userguide/build_lifecycle.html#sec:configuration">Gradle configuration</a> of some large projects often took minutes due to it being single-threaded. Bazel analysis, in contrast, runs in parallel, in part because its configuration language, Starlark, is constrained to be side-effect-free.</p><h4>Reliability</h4><p><em>Bazelâ€™s hermeticity ensures reliable, repeatable builds</em></p><p>Gradle tasks have access to the full file system, which can lead to serious unintended consequences at scale. One example we ran into was when a developer updated a task to clean up recent files in the /tmp/ directory. This created a race condition with other Gradle tasks that used the /tmp/ directory and caused CI to fail when thousands of Gradle tasks had to beÂ rerun.</p><p>Bazel solves this issue with sandboxing, which ensures that only specified inputs are available to a build action. If a file isnâ€™t declared as an input, it simply doesnâ€™t exist in the sandboxed environment.</p><p>Gradle tasks also implicitly depend on the machineâ€™s resources. Gradle builds run on local machines and CI machines of different sizes. This can lead to resource contention when a task is run on a smaller machine or when the cache is cold and thousands of tasks are run on the sameÂ machine.</p><p>Remote build execution (RBE) solves this by running actions in identical containers with strict resource limits. We also configured both local and CI builds to use RBE, which greatly reduces environment differences.</p><h4>Shared infrastructure</h4><p>Airbnb currently has a collection of language- and platform-specific repos, such as <a href="https://medium.com/airbnb-engineering/adopting-bazel-for-web-at-scale-a784b2dbe325">web</a>, <a href="https://medium.com/airbnb-engineering/migrating-our-ios-build-system-from-buck-to-bazel-ddd6f3f25aa3">iOS</a>, Python, and Go, all of which are now on Bazel. Unifying on Bazel enables a uniform build infrastructure layer across repos, which includes:</p><ul><li>Remote caching</li><li>Remote build execution</li><li>Affected targets calculation</li><li>Instrumentation &amp; logging from the <a href="https://bazel.build/docs/build-event-protocol">Build EventÂ Protocol</a></li></ul><h3>How did weÂ migrate?</h3><h4>Proof ofÂ concept</h4><p>As a first milestone, we wanted to show that we could build a service and run its unit tests in Bazel. Because this was a proof of concept, we wanted to <strong>minimize disruption to engineers</strong>. Therefore, the Bazel build <em>co-existed </em>with Gradle<strong>. </strong>As a result, developers could choose between using Gradle or BazelÂ locally<em>.</em></p><p>We needed to prove that developers would <em>choose</em> to opt in to using Bazel over Gradle. It wasnâ€™t enough for Bazel to be faster, developers had to willingly opt in to usingÂ Bazel.</p><p>For this proof of concept, we chose Airbnbâ€™s GraphQL monolith platform, <a href="https://medium.com/airbnb-engineering/taming-service-oriented-architecture-using-a-data-oriented-service-mesh-da771a841344">Viaduct</a>, which had the following important properties:</p><ol><li>It was one of Airbnbâ€™s largest and most complex services. If we could migrate Viaduct to Bazel, then we could likely migrate the rest of the monorepo.</li><li>Slow builds were a major pain point, so Bazel could have a largeÂ impact.</li><li>Viaduct has 300 product engineers modifying its code every month, so improving Viaductâ€™s build speed would be a substantial productivity win.</li><li>Because of (2) and (3) above, Viaductâ€™s core infrastructure team was eager to partner withÂ us.</li></ol><p>To achieve a working Viaduct build with Bazel, we did two things. First, we had to port much of Viaductâ€™s build logic from Gradle to Bazel. Second, because we decided to maintain co-existing builds and the Gradle build graph was still changing, we decided to build an automated build file generator (which weâ€™ll cover in detail in a separate section).</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*8d_f127RDGo6fHBHcqmN3g.png" /></figure><p>Importantly, even though we were able to locally build the service 2â€“4x faster with Bazel, many developers did not yet want toÂ switch.</p><p>In talking with the serviceâ€™s owners, we discovered a number of missing integrations and bugs. It took us an additional few months to address these pain points, after which Viaduct developers willingly switched from Gradle toÂ Bazel.</p><h4>Scaling builds and tests withÂ Bazel</h4><p>The proof of concept showed that Bazel was superior to Gradle for one of Airbnbâ€™s largest services and a large audience of developers. Now we wanted to scale it to the rest of Airbnbâ€™s JVM monorepo.</p><p>We decided to scale breadth-first, getting all of the repo compiling and testing in Bazel. Again, to minimize disruption, Bazel builds co-existed with Gradle, which had two important benefits.</p><p>First, developers could still use Bazel for local development and get most of its benefits even though their code was still built with Gradle for deployment. Second, we could always disable Bazel if it was negatively impacting developers. For example, when Bazel infrastructure like the remote cache or remote execution cluster experienced an incident, we could and did disable Bazel, letting users fall back toÂ Gradle.</p><p>However, a major downside was that both Gradle and Bazel build graphs had to be maintained. Manually maintaining a Bazel build graph would have degraded the developer experience. As a result, we invested further in automated build file generation, so that developers didnâ€™t need to manually maintain Bazel buildÂ files.</p><h4>Automated build file generation</h4><p>For our build file generator, we were heavily inspired by <a href="https://github.com/bazel-contrib/bazel-gazelle">Gazelle</a>, which generates Bazel build files by parsing source files to build a dependency graph.</p><p>Although we considered extending Gazelle to support JVM languages, we had very strict performance requirements and needed to handle dependency cycles. This ultimately led us to build our own automated build file generator.</p><p>Because we had to maintain <em>co-existing build graphs</em>, we needed to run the build file generator on every commit before merging into mainline. This meant it had to run as fast as possible to not significantly degrade the developer experience. To achieve this, we implemented external caching to speed up the automated build file generation.</p><p>Similar to Gazelle, the build file generator parses Java, Kotlin, and Scala source files for package and import statements and symbol declarations to build a file-level dependency graph.</p><p>In CI, we publish a cached index of the repository at each mainline commit. When a user runs sync-configs, it downloads this cache and only re-scans directories which have changed since the merge base. This greatly improves performance for the common case where users only modify a small set ofÂ files.</p><p>In addition, with this build file generator, we were able to support a more fine-grained build graph, which resulted in &gt;10x more Bazel targets than Gradle projects. This enabled faster builds through more parallel builds and less cache invalidation. However, one challenge of moving to a more fine-grained build graph is the possibility of introducing compilation cycles; sync-configs is able to detect this automatically and merge compilation units when necessary.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*UnuZOwRc8X-lT3onIEYHiA.png" /></figure><p>Even after the migration, the build file generator remains in use. It improves the developer experience by automatically fixing build file configurations and removing unused dependencies. In contrast, when we were on Gradle, users manually maintained ~4,500 Gradle files, which led to unused dependencies, a bloated dependency graph, and slower builds with fewer cacheÂ hits.</p><h4>Porting buildÂ logic</h4><p>In addition to JVM source files, our repo has a large amount of build logic such as code generation owned by multiple teams. These often took the form of GradleÂ plugins.</p><p>Because we now had automated build file generation, when porting the build logic, we also had to integrate it with the build file generator. Also, because we had more granular targets, a single line of Gradle config now might need to apply to 10+ Bazel buildÂ files.</p><p>As a result, our build file generator architecture supported plugins similar to Gazelle extensions. These plugins were triggered by the presence of specific files such as Thrift or GraphQL files. These plugins could also generate new build targets such as codegenÂ actions.</p><p>In some cases, the Gradle logic was manually added as a one-off or not easily inferred from the file structure. As a result, we also supported generator <em>directives</em> similar to Gazelle, such as adding dependencies or setting attributes.</p><p>Initially, our team ported much of this build logic ourselves with minimal help from service owners. As Bazel adoption grew, owners of complex build logic were incentivized to migrate to Bazel, because it was faster and more reliable than Gradle. In the process, they often wrote their own build file generator plugins, highlighting the extensibility of our generator.</p><h4>Third-party library multi-version support</h4><p>Another major issue we hit on the road to 100% compilation and testing with Bazel was multiple versions of the same third-party library.</p><p>Initially, we specified a single version of each library. The build file generator would add dependencies from this universe.</p><p>However, in Gradle, each sub-project within the monorepo could use different versions of third-party libraries. As a result, when compiling against a single version, compilation or testing could fail with a missingÂ symbol.</p><p>To bring multi-version support to our Bazel system, we built tooling to generate multiple maven_install rules and added a custom <a href="https://bazel.build/extending/aspects">aspect</a> to resolve conflicts at the targetÂ level.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*_U1LAeex52squvmt" /></figure><p><em>Multiple versions of Guava in Bazel before we added conflict resolution</em></p><p>Once we had this capability, we systematically synced library versions from Gradle so that each build targetâ€™s classpath more closely matched its Gradle counterpart.</p><p>To learn more about our approach, see our <a href="https://www.youtube.com/watch?v=Ui4YtqWhqYU">BazelCon 2022 talk</a>. Since giving this talk, we have made improvements like moving the resolution to analysis-time for better IDE support and adding more user-friendly tooling for updating libraries.</p><h4>Migrating theÂ deploys</h4><p>As we got the vast majority of projects building and passing unit tests with Bazel in CI, we began to focus on migrating deployments. The Bazel-built jars were not identical to the Gradle-built jars. As a result, we needed a strategy to ensure the deployments were safe. We started with services.</p><h4>Services</h4><p>To verify the correctness of deploying services with Bazel, we used startup and integration tests. Of ~700 services, ~100 encountered startup or integration test failures. The majority of failures were missing dependencies that were loaded via Java reflection, usually from config files or other files. As a result, we were able to fix a number of these issues by parsing files for classes that would be loaded via reflection, and then adding the required dependency.</p><p>Another major source of errors was differing library versions, which could lead to missing symbol errors at runtime. In Gradle, users manually specified dependencies and their versions. However, in Bazel, build files were generated from source and dependencies were inferred from import statements, which didnâ€™t specify the version. We solved many of these errors forcing third-party library versions to match those of the GradleÂ project.</p><p>After taking into account reflected classes and syncing versions, only a single-digit percentage of services hit production runtime issues that required more in-depth manual work toÂ fix.</p><h4>Data pipelines and otherÂ projects</h4><p>In addition to services, we had 450 data pipelines and ~50 other projects that were deployed to either a Spark cluster or a FlinkÂ runtime.</p><p>Similar to services, we were able to catch a number of issues using tests. In particular, data pipelines on Airbnbâ€™s data engineering paved path have CI tests that run a small version of the Spark pipeline locally. For these ~400 paved-path pipelines, after passing CI tests, only about 3% had production issues at runtime. As a result, we were able to very quickly migrate the paved-path pipelines.</p><p>As with services, we had a few remaining deployables that were a bit more bespoke and had to be individually deployed and monitored to verify correctness.</p><h3>What did weÂ learn?</h3><h4>Customer partnership</h4><p>Early in the migration, we identified key pilot services that had large opportunities for impact and an appetite to invest in migrating to a new build system. For example, Viaduct had complex build logic leading to slow builds and reliability issues. In addition, many developers contributed to the service, so improving their builds had a large impact on Airbnbâ€™s developer experience.</p><p>Partnering with pilot teams was incredibly valuable. They were early adopters and made significant contributions in the form of reporting and debugging issues, profiling performance bottlenecks, and suggesting features. The pilot teams also became advocates and provided internal support, helping motivate the rest of the Airbnb developer community.</p><h4>Dangers of premature optimization</h4><p>This migration took 4.5 years. With the benefit of hindsight we think we could have drastically improved the migration timeline if we had <strong>migrated first, before improving</strong><em>.</em></p><p>Although increasing build granularity improved build times, it increased the time to migrate. Specifically, increased build granularity greatly increased the number of configuration files, making it much harder to manage configurations manually. This forced more functionality into automated build file generation, which increased its complexity.</p><p>If we had migrated first and <em>then</em> optimized the build granularity, we believe we could have migrated sooner, enabling users to get benefits from Bazel sooner and reducing the time spent maintaining two co-existing builds.</p><p>Similarly, build granularity also made it harder to match deploy jars between Gradle and Bazel. This led to spending more time testing deployments and fixing runtimeÂ issues.</p><p>On a more positive note, we accelerated the migration by deciding to support multiple third-party library versions and implementing version resolution. This enabled us to sync versions from Gradle to Bazel, which fixed a large number of build and runtimeÂ issues.</p><p>Towards the end, one major takeaway in the migration was, <strong>by default, we should try to imitate what was there before</strong><em>. </em>In our case, deviating from Gradle usually added technical risk, and should be carefully considered, especially its downstream consequences.</p><p>As engineers, we often want to improve things. However, during migrations, improvements can have non-obvious consequences and potentially significantly slow down the migration.</p><h3>ğŸ Conclusion</h3><p>After 4.5 years, we fully migrated Airbnbâ€™s biggest repo from Gradle to Bazel, achieving:</p><ul><li>Build CSAT: 38% â†’Â 68%</li><li><strong>3â€“5x </strong>faster local build and testÂ times</li><li><strong>2â€“3x</strong> faster IntelliJÂ syncs</li><li><strong>2â€“3x</strong> faster deploys to the development environment</li></ul><p>Finally, now that several Airbnb repos are on Bazel, weâ€™re able to share <a href="https://www.youtube.com/watch?v=RpSVBtyoYCY">common infrastructure</a> such as remote build caching, remote build execution, affected targets calculation, andÂ more.</p><p>Interested in helping us solve problems like these at Airbnb? Learn more about our open engineering rolesÂ <a href="https://careers.airbnb.com/">here</a>.</p><h3>Acknowledgments</h3><p>Additionally, thank you Janusz Kudelka, Kumail Hussain, Meghan Dow, Pawel Lipski, Peimin Liu, Tomasz Pacuszka, and various other internal and external partners.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=33f90eda51ec" width="1" height="1" alt=""><hr><p><a href="https://medium.com/airbnb-engineering/migrating-airbnbs-jvm-monorepo-to-bazel-33f90eda51ec">Migrating Airbnbâ€™s JVM Monorepo to Bazel</a> was originally published in <a href="https://medium.com/airbnb-engineering">The Airbnb Tech Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Seamless Istio Upgrades at Scale]]></title>
            <link>https://medium.com/airbnb-engineering/seamless-istio-upgrades-at-scale-bcb0e49c5cf8?source=rss----53c7c27702d5---4</link>
            <guid isPermaLink="false">https://medium.com/p/bcb0e49c5cf8</guid>
            <category><![CDATA[open-source]]></category>
            <category><![CDATA[kubernetes]]></category>
            <category><![CDATA[engineering]]></category>
            <category><![CDATA[infrastructure]]></category>
            <category><![CDATA[istio]]></category>
            <dc:creator><![CDATA[Rushy R. Panchal]]></dc:creator>
            <pubDate>Thu, 07 Aug 2025 17:01:42 GMT</pubDate>
            <atom:updated>2025-08-07T17:01:41.777Z</atom:updated>
            <content:encoded><![CDATA[<h4><strong>How Airbnb upgrades tens of thousands of pods on dozens of Kubernetes clusters to new IstioÂ versions</strong></h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*44AVFDg8R66nWj4cAU8vCA.jpeg" /></figure><p>Airbnb has been running IstioÂ® at scale since 2019. We support workloads running on both Kubernetes and virtual machines (using <a href="https://istio.io/latest/docs/ops/deployment/vm-architecture/">Istioâ€™s mesh expansion</a>). Across these two environments, we run tens of thousands of pods, dozens of Kubernetes clusters, and thousands of VMs. These workloads send tens of millions of QPS at peak through Istio. Our <a href="https://www.youtube.com/watch?v=6kDiDQW5YXQ">IstioCon 2021 talk</a> describes our journey onto Istio and our <a href="https://www.youtube.com/watch?v=1D8lg36ZNHs">KubeCon 2021 talk</a> goes into further detail on our architecture.</p><p>Istio is a foundational piece of our architecture, which makes ongoing maintenance and upgrades a challenge. Despite that, we have upgraded Istio a total of 14 times. This blog post will explore how the Service Mesh team at Airbnb safely upgrades Istio while maintaining high availability.</p><h4>Challenges</h4><p>Airbnb engineers collectively run thousands of different workloads. We cannot reasonably coordinate the teams that own these, so our upgrades must function independently of individual teams. We also cannot monitor all of these at once, and so we must minimize risk through gradual rollouts.</p><p>With that in mind, we designed our upgrade process with the following goals:</p><ol><li>Zero downtime for workloads and users. This is the <em>seamless</em> part of the upgradeâ€Šâ€”â€Ša workload owner doesnâ€™t need to be in the loop for Istio upgrades.</li><li>Gradual rollouts with the ability to control which workloads are upgraded or reverted.</li><li>We must be able to roll back an upgrade across all workloads, without coordinating every workloadÂ team.</li><li>All workloads should be upgraded within some definedÂ time.</li></ol><h4>Architecture</h4><p>Our deployment consists of one management cluster, which runs Istiod and contains all workload configuration for the mesh (VirtualServices, DestinationRules, and so forth), and multiple workload clusters, which run user workloads. VMs run separately, but their Istio manifests are still deployed to the management cluster in their own namespaces. We use Sidecar mode exclusively, meaning that every workload runs istio-proxyâ€Šâ€”â€Šwe do not yet runÂ <a href="https://istio.io/latest/docs/ambient/overview/">Ambient</a>.</p><figure><img alt="A diagram of Istio deployment architecture at Airbnb. There is a single configuration cluster containing Istio custom resources like VirtualServices, Istiod, and Istiodâ€™s ConfigMaps. There are two workload clusters, each running user workloads, and alongside those a number of VMs. Workloads and VMs communicate with each other." src="https://cdn-images-1.medium.com/max/1024/0*-qAuZZZxdsi-oJSO" /></figure><h4>Upgrade Process</h4><p>At a high level, we follow <a href="https://istio.io/latest/docs/setup/upgrade/canary/">Istioâ€™s canary upgrade model</a>. This involves running two versions (or Istio revisions) of Istiod simultaneously: the current version and the new version that we are upgrading to. Both form one logical service mesh, so workloads connected to one Istiod can communicate with workloads connected to another Istiod and vice versa. Istiod versions are managed using different revision labelsâ€Šâ€”â€Šfor example, 1â€“24â€“5 for Istio 1.24.5 and 1â€“25â€“2 for IstioÂ 1.25.2.</p><p>An upgrade involves both Istiod, the control plane, and istio-proxy, the data plane sidecar, running on all pods and VMs. While Istio supports connecting an <a href="https://istio.io/latest/docs/releases/supported-releases/#control-planedata-plane-skew">older istio-proxy to a newer Istiod</a>, we do not use this. Instead, we atomically roll out the new istio-proxy version to a workload along with the configuration of which Istiod to connect to. For example, the istio-proxy built for version 1.24 will only connect to 1.24â€™s Istiod and the istio-proxy built for 1.25 will only connect to 1.25â€™s Istiod. This reduces a dimension of complexity during upgrades (cross-version data planeâ€Šâ€”â€Šcontrol plane compatibility).</p><p>The first step of our upgrade process is to deploy the new Istiod, with a new revision label, onto the management cluster. Because all workloads are explicitly pinned to a revision, no workload will connect to this new Istiod, so this first step has noÂ impact.</p><p>The rest of the upgrade comprises all of the effort and riskâ€Šâ€”â€Šworkloads are gradually shifted to run the new istio-proxy version and connect to the newÂ Istiod.</p><figure><img alt="Istio with multiple revisions. There is a config cluster with Istio custom resources like VirtualServices, multiple Istiod deployments, and the Istiod ConfigMaps for each of those. Workloads can connect to either Istiod." src="https://cdn-images-1.medium.com/max/1024/0*k7_eTtEPqBiDKM3t" /><figcaption><em>Multiple Istio revisions, with some workloads connected to different revisions.</em></figcaption></figure><h3>Rollout specification</h3><p>We control what version of istio-proxy workloads run through a file called rollouts.yml. This file specifies workload namespaces (as patterns) and the percentage distribution of Istio versions:</p><pre># &quot;production&quot; is the default; anything not matching a different pattern will match this.<br>production:<br>  1-24-5: 100<br><br>&quot;.*-staging&quot;:<br>  1-24-5: 75<br>  1-25-2: 25<br><br># A pinned namespace; our end-to-end verification workload.<br>istio-e2e:<br>  1-25-2: 100</pre><p>This spec dictates the desired state of all namespaces. A given namespace is first mapped to a bucket (based on the longest pattern that matches) and then a version is chosen based on the distribution for that bucket. The distribution applies at the namespace level, not the pod (or VM) level. ForÂ example,</p><pre>&quot;.*-staging&quot;:<br>  1-24-5: 75<br>  1-25-2: 25</pre><p>means that 75% of the namespaces with the suffix -stagingwill be assigned to 1â€“24â€“5 and the remaining 25% will be assigned to 1â€“25â€“2. This assignment is deterministic, using consistent hashing. The majority of our upgrade process involves updating rollouts.yml and then monitoring.</p><p>This process allows us to selectively upgrade workloads. We can also upgrade environments separately and ensure that only a certain percentage of those environments are on the new version. This gives us time to bake an upgrade and learn of potential regressions.</p><p>The rest of this post will describe the mechanism through which a change to rollouts.yml is applied to thousands of workloads, for both Kubernetes andÂ VMs.</p><h3>Kubernetes</h3><p>Each Istio revision has a corresponding <a href="https://istio.io/latest/docs/setup/additional-setup/sidecar-injection/#automatic-sidecar-injection">MutatingAdmissionWebhook for sidecar injection</a> on every workload cluster. This webhook selects pods specifying the label istio.io/rev=&lt;revision&gt; and injects the istio-proxy and istio-init containers into those pods. Notably, the istio-proxy container contains the PROXY_CONFIG environment variable, which sets the discoveryAddress to the Istiod revision. This is how the istio-proxyversion and the configuration for which Istiod to connect to are deployed atomicallyâ€Šâ€”â€Šentirely by the sidecar injector.</p><p>Every workloadâ€™s Deployment has this revision label. For example, a workload configured to use Istio 1.24.5 will have the label istio.io/rev=1â€“24â€“5in its pod template; thus pods for that Deployment will be mutated by the MutatingAdmissionWebhook for IstioÂ 1.24.5.</p><p>This setup is the standard method of upgrading Istio, but requires that every Deployment specifies a revision label. To perform an upgrade across thousands of workloads, every team would have to update this label and deploy their workload. We could neither perform a rollback across all workloads nor reasonably expect an upgrade to complete to 100%, both for the same reasonâ€Šâ€”â€Šrelying on every workload toÂ deploy.</p><h4>Krispr</h4><p>To avoid having to update workloads individually, a workloadâ€™s configuration never directly specifies the revision label in source code. Instead, we use <a href="https://medium.com/airbnb-engineering/a-krispr-approach-to-kubernetes-infrastructure-a0741cff4e0c">Krispr, a mutation framework built in-house</a>, to inject the revision label. Krispr gives us the ability to decouple infrastructure component upgrades from workload deployments.</p><p>Airbnb workloads that run on Kubernetes use an internal API to define their workload, instead of specifying Kubernetes manifests. This abstraction is then compiled into Kubernetes manifests during CI. Krispr runs as part of this compilation and mutates those Kubernetes manifests. One of those mutations injects the Istio revision label into the pod specification of each Deployment, reading rollouts.ymlto decide which label to inject. If a team sees any issue with their workload when they deploy, they can roll back and thus also roll back the Istio upgradeâ€Šâ€”â€Šall without involving the Service MeshÂ team.</p><p>In addition, Krispr runs during pod admission. If a pod is being admitted from a Deployment that is more than two weeks old, Krispr will re-mutate the pod and accordingly update the podâ€™s revision label if needed. Combined with the fact that our Kubernetes nodes have a maximum lifetime of two weeks, thus ensuring that any given podâ€™s maximum lifetime is also two weeks, we can guarantee that an Istio upgrade completes. A majority of workloads will be upgraded when they deploy (during the Krispr run in CI) and for those that donâ€™t deploy regularly, the natural pod cycling and re-mutation will ensure they are upgraded in at most fourÂ weeks.</p><p>In summary, per workload:</p><ol><li>During CI, Krispr mutates the Kubernetes manifests of a workload to add the Istio revision label, based on rollouts.yml.</li><li>When a pod is admitted to a cluster, Krispr will re-mutate the pod if its Deployment is more than two weeks old and update the Istio revision label ifÂ needed.</li><li>The revision-specific Istio MutatingAdmissionWebhook will mutate the pod by injecting the sidecar and associated discoveryAddress.</li></ol><h3>Virtual machines</h3><p>On VMs, we deploy an artifact that contains istio-proxy, a script to run istio-iptables (similar to the istio-init container), and the Istiod discoveryAddress. By packaging istio-proxy and the discoveryAddress in the same artifact, we can atomically upgradeÂ both.</p><p>Installation of this artifact is the responsibility of an on-host daemon called mxagent. It determines what version to install by polling a set of key-value tags on the VM (such as <a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Using_Tags.html">EC2 tags on AWS</a> or <a href="https://cloud.google.com/compute/docs/tag-resources">resource tags on GCP</a>). These tags mimic the istio.io/rev label for Kubernetes-based workloads. Whenever they change, mxagent will download and install the artifact corresponding to that version. Thus, upgrading istio-proxy on a VM just involves updating these tags on that VM; mxagent will take care of theÂ rest.</p><p>Our VM workloads are largely infrastructure platforms that donâ€™t typically have code deployed at regular intervals. As such, VMs donâ€™t support a deploy-time upgrade (in the way that Kubernetes workloads can be upgraded when they deploy). Similarly, teams cannot roll back these workloads themselves, but this has been acceptable, given that there are just a handful of such infrastructure platforms.</p><p>The tag updates are managed by a central controller, mxrc, which scans for outdated VMs. If rollouts.yml would result in a different set of resource tags for a VM, the controller will update the tags accordingly. This roughly corresponds to Krisprâ€™s pod admission-time mutationâ€Šâ€”â€Šhowever, with the caveat that VMs are mutable and long-lived, and thus are upgraded in-place.</p><p>For safety, mxrc takes into account the health of the VM, namely in the form of the <a href="https://istio.io/latest/docs/reference/config/networking/workload-group/#ReadinessProbe">readiness probe status on the WorkloadEntry</a>. Similar to Kubernetesâ€™ maxUnavailable semantics, mxrc aims to keep the number of unavailable VMs (that is, unhealthy VMs plus those with in-progress upgrades) below a defined percentage. It gradually performs these upgrades, aiming to upgrade all the VMs for a workload in twoÂ weeks.</p><p>At the end of two weeks, all VMs will match the desired state in rollouts.yml.</p><h3>Conclusion</h3><p>Keeping up-to-date with open-source software is a challenge, especially at scale. Upgrades and other Day-2 operations often become an afterthought, which furthers the burden when upgrades are eventually necessary (to bring in security patches, remain within support windows, utilize new features, and so forth). This is particularly true with Istio, where a version reaches end-of-life supportÂ rapidly.</p><p>Even with the complexity and scale of our service mesh, we have successfully upgraded Istio 14 times. This was made possible due to designing for maintainability, building a process that ensures zero downtime, and derisking through the use of gradual rollouts. Similar processes are in use for a number of other foundational infrastructure systems atÂ Airbnb.</p><h3>Future work</h3><p>As Airbnbâ€™s infrastructure continues to evolve and grow, weâ€™re looking at a few key projects to evolve our serviceÂ mesh:</p><ul><li>Utilizing <a href="https://istio.io/latest/docs/ambient/overview/">Ambient mode</a> as a more cost-effective and easier-to-manage deployment model of Istio. In particular, this simplifies upgrades by not needing to touch workload deployments atÂ all.</li><li>Splitting our singular production mesh into multiple meshes in order to separate fault domains, provide better security isolation boundaries, and scale Istio further. For upgrades, this would further reduce the blast radius, as some meshes that only run low-risk workloads (such as staging) could be upgradedÂ first.</li></ul><p>If this type of work interests you, we encourage you to apply for an <a href="https://careers.airbnb.com/">open position</a>Â today.</p><h3>Acknowledgements</h3><p>All of our work with Istio is thanks to many different people, including: Jungho Ahn, Stephen Chan, Weibo He, Douglas Jordan, Brian Wolfe, Edie Yang, Dasol Yoon, and YingÂ Zhu.</p><p><em>All product names, logos, and brands are property of their respective owners. All company, product and service names used in this website are for identification purposes only. Use of these names, logos, and brands does not imply endorsement.</em></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=bcb0e49c5cf8" width="1" height="1" alt=""><hr><p><a href="https://medium.com/airbnb-engineering/seamless-istio-upgrades-at-scale-bcb0e49c5cf8">Seamless Istio Upgrades at Scale</a> was originally published in <a href="https://medium.com/airbnb-engineering">The Airbnb Tech Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Achieving High Availability with distributed database on Kubernetes at Airbnb]]></title>
            <link>https://medium.com/airbnb-engineering/achieving-high-availability-with-distributed-database-on-kubernetes-at-airbnb-58cc2e9856f4?source=rss----53c7c27702d5---4</link>
            <guid isPermaLink="false">https://medium.com/p/58cc2e9856f4</guid>
            <category><![CDATA[infrastructure]]></category>
            <category><![CDATA[kubernetes]]></category>
            <category><![CDATA[sql]]></category>
            <category><![CDATA[engineering]]></category>
            <category><![CDATA[database]]></category>
            <dc:creator><![CDATA[Artem Danilov]]></dc:creator>
            <pubDate>Mon, 28 Jul 2025 17:57:46 GMT</pubDate>
            <atom:updated>2025-07-28T18:00:59.220Z</atom:updated>
            <content:encoded><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*X2h18kiTtfcbRXQo" /></figure><h3>Introduction</h3><p>Traditionally, organizations have deployed databases on costly, high-end standalone servers using sharding for scaling as a strategy. As data demands grew, the limitations of this strategy became increasingly evident with increasingly longer and more complex maintenance projects.</p><p>Increasingly distributed horizontally scalable databases are not uncommon and many of them are open source. However, running these databases reliably in the cloud with high availability, low latency and scalability, all at a reasonable cost is a problem many companies are trying toÂ solve.</p><p>We chose an innovative strategy of deploying<strong> a distributed database cluster across multiple Kubernetes clusters in a cloud environment</strong>. Although currently an uncommon design pattern due to its complexity, this strategy allowed us to achieve target system reliability and operability.</p><p>In this post, weâ€™ll share how we overcame challenges and the best practices weâ€™ve developed for this strategy and we believe these best practices should be applicable to any other strongly consistent, distributed storageÂ systems.</p><h3>Managing Databases on Kubernetes</h3><p>Earlier this year, we integrated an open source horizontally scalable, distributed SQL database into our infrastructure.</p><p>While Kubernetes is a great tool for running stateless services, the use of Kubernetes for stateful servicesâ€Šâ€”â€Šlike databasesâ€Šâ€”â€Šis challenging, particularly around node replacement and upgrades.</p><p>Since Kubernetes lacks knowledge of data distribution across nodes, each node replacement requires careful data handling to prevent data quorum loss and service disruption, this includes copying the data before replacing aÂ node.</p><p>At Airbnb, we opted to attach storage volumes to nodes using AWS EBS, this allows quick volume reattachment to new virtual machines upon node replacement. Thanks to Kubernetesâ€™ Persistent Volume Claims (<a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#binding">PVC</a>), this reattachment happens automatically. In addition we need to allow time for a new storage node to catch up with the clusterâ€™s current state before moving to the next node replacement. For this, we rely on the custom <a href="https://kubernetes.io/docs/concepts/extend-kubernetes/operator/">k8s operator</a><a href="https://github.com/pingcap/tidb-operator">,</a> which allows us to customize various Kubernetes operations according to specifics of the application.</p><h3>Coordinating Node Replacement</h3><p>Node replacements occur for various reasons, from <a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-retirement.html">AWS instance retirement</a> to Kubernetes upgrades or configuration changes. To address these cases, we categorize node replacement events into threeÂ groups:</p><ol><li><strong>Database-initiated events:</strong> Such as config changes or version upgrades.</li><li><strong>Proactive infrastructure events:</strong> Like instance retirements or node upgrades.</li><li><strong>Unplanned infrastructure failures:</strong> Such as a node becoming unresponsive.</li></ol><p>To safely manage node replacements for database-initiated events, we implemented a a custom check in the k8s-operator that verifies that all nodes are up and running before deleting anyÂ pod.</p><p>In order to serialize it with the second group initiated by infrastructure, we implemented <a href="https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/">an admission hook</a> in k8s to intercept pod eviction. This admission hook rejects any attempt to evict the pod, but assigns a custom annotation on the pod which our customer database k8s-operator watches and acts on to safely delete the pod serializing it with any database-initiated node replacements described above.</p><p>Node replacements due to unplanned infrastructure failure events like hardware failure, canâ€™t be coordinated. But we can still improve availability by ensuring that any node replacement event from the first two groups will be blocked until the failed hardware is replaced.</p><p>In our infrastructure the k8s operator handles both proactive and infrastructure-triggered node replacements, maintaining data consistency in the presence of node replacements and ensuring that unplanned events donâ€™t impact ongoing maintenance.</p><h3>Kubernetes Upgrades</h3><p>Regular Kubernetes upgrades are essential but can be high-risk operations, especially for databases. Cloud managed Kubernetes might not offer rollbacks once the control plane is upgraded, posing a potential disaster recovery challenge if something goes wrong. While our approach involves using self-managed Kubernetes clusters, which does allow rolling back the control plane, a bad Kubernetes upgrade could still cause service disruption till rollback is completed.</p><h3>Ensuring Fault Tolerance with Multiple Kubernetes clusters</h3><p>At Airbnb, we think the best way to achieve high regional availability is to deploy each database across three independent Kubernetes clusters, each within a different AWS availability zone (<a href="https://docs.aws.amazon.com/whitepapers/latest/aws-fault-isolation-boundaries/availability-zones.htm">AZ</a>). AWS uses availability zones not just for independent power, networking, and connectivity, but they also do rollouts zone by zone. Our Kubernetes cluster alignment with AWS AZ also means that any underlying infrastructure issues or bad deployments have a limited blast radius as they are restricted to a single AZ. Internally, we also deploy a new configuration or a new database version to a part of the logical cluster running in a single Kubernetes cluster in one AZÂ first.</p><p>While this setup adds complexity, it significantly boosts availability by limiting the blast radius of any issues stemming from faulty deployments at every layerâ€Šâ€”â€Šwhether database, Kubernetes, or AWS infrastructure.</p><p>For instance, recently, a faulty config deployment in our infrastructure abruptly terminated all VMs of a specific type in our staging Kubernetes cluster, deleting most of the query layer pods. However, since the disruption was isolated to a single Kubernetes cluster, two-thirds of our query layer nodes remained operational, preventing anyÂ impact.</p><p>We also overprovision our database clusters to ensure that, even if an entire AZ, Kubernetes cluster, or all storage nodes within a zone goes down, we still have sufficient capacity to handleÂ traffic.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*1hSoKkktmPABPkTj" /></figure><h3>Leveraging AWS EBS for Reliability and LatencyÂ Handling</h3><p>EBS offers two key benefits for our deployment: rapid reattachment during node replacements and superior durability compared to local disks. With EBS, we confidently run a highly available cluster using only three replicas, maintaining reliability without needing additional redundancy.</p><p>However, EBS can occasionally experience tail latency spikes, with p99 latency reaching up to 1 second. To mitigate this, we implemented a storage read timeout session variable, allowing queries to transparently retry against other storage nodes during EBS latency spikes. By default the database we use sends all requests and retries to the leader. To enable retries on storage nodes with healthy EBS, we have to allow reads from both leader and replica reads, but prefer the closest one for the original request. This brings the added benefit of reduced latency and no cross-AZ network costs, as we have a replica in each AZ. Finally, for use cases that permit it, we leverage stale reads feature, enabling reads to be served independently by the replica without requiring synchronous calls to the leader, which may be experiencing an EBS latency spike at the time of theÂ read.</p><h3>Conclusion: Exploring Open Source Databases on Kubernetes</h3><p>Our journey running a distributed database on Kubernetes has empowered us to achieve high availability, low latency, scalability, and lower maintenance costs. By leveraging the operator pattern, multi-cluster deployments, AWS EBS, and stale reads, weâ€™ve demonstrated that even open source distributed storage systems can thrive in cloud environments.</p><p>We already operate several database clusters in production in the described setup, with the largest one handling 3M QPS across 150 storage nodes, storing over 300+ TB of data spread across 4M internal shards. All this with 99.95% availability thanks to techniques described in thisÂ post.</p><p>For other companies considering to run open-source databases on Kubernetes, the opportunities are immense. Embrace the challenge, run open-source databases to shape these tools for enterprise use. The future of scalable, reliable data management in the cloud lies in collaboration and open-source innovationâ€Šâ€”â€Šnow is the time to lead and participate.</p><h3>Acknowledgments</h3><p>Thanks to Abhishek Parmar, Brian Wolfe, Chen Ding, Daniel Low, Hao Luo, Xiaomou Wang for collaboration and Shylaja Ramachandra forÂ editing.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=58cc2e9856f4" width="1" height="1" alt=""><hr><p><a href="https://medium.com/airbnb-engineering/achieving-high-availability-with-distributed-database-on-kubernetes-at-airbnb-58cc2e9856f4">Achieving High Availability with distributed database on Kubernetes at Airbnb</a> was originally published in <a href="https://medium.com/airbnb-engineering">The Airbnb Tech Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Understanding and Improving SwiftUI Performance]]></title>
            <link>https://medium.com/airbnb-engineering/understanding-and-improving-swiftui-performance-37b77ac61896?source=rss----53c7c27702d5---4</link>
            <guid isPermaLink="false">https://medium.com/p/37b77ac61896</guid>
            <category><![CDATA[ios]]></category>
            <category><![CDATA[swiftui]]></category>
            <category><![CDATA[performance]]></category>
            <category><![CDATA[engineering]]></category>
            <category><![CDATA[mobile]]></category>
            <dc:creator><![CDATA[Cal Stephens]]></dc:creator>
            <pubDate>Tue, 24 Jun 2025 16:43:07 GMT</pubDate>
            <atom:updated>2025-09-06T22:52:14.099Z</atom:updated>
            <content:encoded><![CDATA[<p>New techniques weâ€™re using at Airbnb to improve and maintain performance of SwiftUI features atÂ scale</p><p>By <a href="https://www.linkedin.com/in/calstephens/">Cal Stephens</a>, <a href="https://www.linkedin.com/in/miguel-jimenez-b98216112">MiguelÂ Jimenez</a></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*qBYJ9abMpZyuODmbHkYD5Q.jpeg" /></figure><p>Airbnb <a href="https://medium.com/airbnb-engineering/unlocking-swiftui-at-airbnb-ea58f50cde49">first adopted SwiftUI in 2022</a>, starting with individual components and later expanding to entire screens and features. Weâ€™ve seen major improvements to engineersâ€™ productivity thanks to its declarative, flexible, and composable architecture. However, adopting SwiftUI has brought new challenges related to performance. For example, there are many common code patterns in SwiftUI that can be inefficient, and many small papercuts can add up to a large cumulative performance hit. To begin addressing some of these issues at scale, weâ€™ve created new tooling for proactively identifying these cases and statically validating correctness.</p><h3>SwiftUI feature architecture atÂ Airbnb</h3><p>Weâ€™ve been leveraging declarative UI patterns at Airbnb for many years, using our UIKit-based <a href="https://medium.com/airbnb-engineering/introducing-epoxy-for-ios-6bf062be1670">Epoxy library</a> and <a href="https://medium.com/airbnb-engineering/introducing-epoxy-for-ios-6bf062be1670#fbe0">unidirectional data flow</a> systems. When adopting SwiftUI in our screen layer, we decided to continue using our existing unidirectional data flow library. This simplified the process of incrementally adopting SwiftUI within our large codebase, and we find it improves the quality and maintainability of features.</p><p>However, we noticed that SwiftUI features using our unidirectional data flow library didnâ€™t perform as well as we expected, and it wasnâ€™t immediately obvious to us what the problem was. Understanding SwiftUIâ€™s performance characteristics is an important requirement for building performant features, especially when venturing outside of the â€œstandardâ€ SwiftUIÂ toolbox.</p><h3>Understanding SwiftUI viewÂ diffing</h3><p>When working with declarative UI systems like SwiftUI, itâ€™s important to ensure the framework knows which views need to be re-evaluated and re-rendered when the state of the screen changes. Changes are detected by diffing the viewâ€™s stored properties any time its parent is updated. Ideally the viewâ€™s body will only be re-evaluated when its properties actuallyÂ change:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*LvNBJSor0RDThlW3Oq7mWw.png" /></figure><p>However, this behavior is not always the reality (more on why in a moment). Unnecessary view body evaluations hurt performance by performing unnecessary work.</p><p>How do you know how often a viewâ€™s body is re-evaluated in a real app? An easy way to visualize this is with a modifier that applies a random color to the view every time itâ€™s rendered. When testing this on various views in our appâ€™s most performance-sensitive screens, we quickly found that many views were re-evaluated and re-rendered more often than necessary:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*yNrr8e8yI9RcKg6Z-VUhEA.gif" /></figure><h4>The SwiftUI view diffing algorithm</h4><p>SwiftUIâ€™s built-in diffing algorithm is often overlooked and not officially documented, but it has a huge impact on performance. To determine if a viewâ€™s body needs to be re-evaluated, SwiftUI uses a reflection-based diffing algorithm to compare each of the viewâ€™s stored properties:</p><ol><li>If a type is <em>Equatable</em>, SwiftUI compares the old and new values using the typeâ€™s <em>Equatable</em> conformance. Otherwise:</li><li>SwiftUI compares value types (e.g., structs) by recursively comparing each instance property.</li><li>SwiftUI compares reference types (e.g., classes) using reference identity.</li><li>SwiftUI attempts to compare closures by identity. However, most closures cannot be compared reliably.</li></ol><p>If all of the viewâ€™s properties compare as equal to the previous value, then the body isnâ€™t re-evalulated and the content isnâ€™t re-rendered. Values using SwiftUI property wrappers like<em> @State </em>and <em>@Environment</em> donâ€™t participate in this diffing algorithm, and instead trigger view updates through different mechanisms.</p><p>When reviewing different views in our codebase, we found several common patterns that confounded SwiftUIâ€™s diffing algorithm:</p><ol><li>Some types are inherently not supported, like closures.</li><li>Simple data types stored on the view may be unexpectedly compared by reference instead of byÂ value.</li></ol><p>Hereâ€™s an example SwiftUI view with properties that interact poorly with the diffing algorithm:</p><pre>struct MyView: View {<br>  /// A generated data model that is a struct with value semantics,<br>  /// but is copy-on-write and wraps an internal reference type.<br>  /// Compared by reference, not by value, which could cause unwanted body evaluations.<br>  let dataModel: CopyOnWriteDataModel<br><br>  /// Other miscellaneous properties used by the view. Typically structs, but sometimes a class.<br>  /// Unexpected comparisons by reference could cause unwanted body evaluations.<br>  let requestState: MyFeatureRequestState<br><br>  /// An action handler for this view, part of our unidirectional data flow library. <br>  /// Wraps a closure that routes the action to the screen&#39;s action handler.<br>  /// Closures almost always compare as not-equal, and typically cause unwanted body evaluations. <br>  let handler: Handler&lt;MyViewAction&gt;<br><br>  var body: some View { ... }<br>}</pre><p>If a view contains any value that isnâ€™t diffable, the entire view becomes non-diffable. Preventing this in a scalable way is almost impossible with existing tools. This finding also reveals the performance issue caused by our unidirectional data flow library: action handling is closure-based, but SwiftUI canâ€™t diff closures!</p><p>In some cases, like with the action handlers from our unidirectional data flow library, making the value diffable would require large, invasive, and potentially undesirable architecture changes. Even in simpler cases, this process is still time consuming, and thereâ€™s no easy way to prevent a regression from creeping in later on. This is a big obstacle when trying to improve and maintain performance at scale in large codebases with many different contributors.</p><h3>Controlling SwiftUI viewÂ diffing</h3><p>Fortunately, we have another option: If a view conforms to Equatable, SwiftUI will diff it using its Equatable conformance <em>instead</em> of using the default reflection-based diffing algorithm.</p><p>The advantage of this approach is that it lets us selectively decide which properties should be compared when diffing our view. In our case, we know that the handler object doesnâ€™t affect the content or identity of our view. We only want our view to be re-evalulated and re-rendered when the <em>dataModel</em> and <em>requestState</em> values are updated. We can express that with a custom <em>Equatable</em> implementation:</p><pre>// An Equatable conformance that makes the above SwiftUI view diffable.<br>extension MyView: Equatable {<br>  static func ==(lhs: MyView, rhs: MyView) -&gt; Bool {<br>    lhs.dataModel == rhs.dataModel<br>      &amp;&amp; lhs.requestState == rhs.requestState<br>      // Intentionally not comparing handler, which isn&#39;t Equatable.<br>  }<br>}</pre><p>However:</p><ol><li>This is a lot of additional boilerplate for engineers to write, especially for views with lots of properties.</li><li>Writing and maintaining a custom conformance is error-prone. You can easily forget to update the <em>Equatable</em> conformance when adding new properties later, which would causeÂ bugs.</li></ol><p>So, instead of manually writing and maintaining <em>Equatable</em> conformances, we created a new<em> @Equatable </em>macro that generates conformances forÂ us.</p><pre>// A sample SwiftUI view that has adopted @Equatable<br>// and is now guaranteed to be diffable.<br>@Equatable<br>struct MyView: View {<br>  // Simple data types must be Equatable, or the build will fail.<br>  let dataModel: CopyOnWriteDataModel<br>  let requestState: MyFeatureRequestState<br><br>  // Types that aren&#39;t Equatable can be excluded from the<br>  // generated Equatable conformance using @SkipEquatable,<br>  // as long as they donâ€™t affect the output of the view body.<br>  @SkipEquatable let handler: Handler&lt;MyViewAction&gt;<br><br>  var body: some View { ... }<br>}</pre><p>The <em>@Equatable</em> macro generates an <em>Equatable</em> implementation that compares all of the viewâ€™s stored instance properties, excluding properties with SwiftUI property wrappers like<em>@State </em>and <em>@Environment</em> that trigger view updates through other mechanisms. Properties that arenâ€™t <em>Equatable</em> and donâ€™t affect the output of the view body can be marked with <em>@SkipEquatable</em> to exclude them from the generated implementation. This allows us to continue using the closure-based action handlers from our unidirectional data flow library without impacting the SwiftUI diffingÂ process!</p><p>After adopting the <em>@Equatable</em> macro on a view, that view is guaranteed to be diffable. If an engineer adds a non-<em>Equatable</em> property later, the build will fail, highlighting a potential regression in the diffing behavior. This effectively makes the <em>@Equatable</em> macro a sophisticated linterâ€Šâ€”â€Šwhich is really valuable for scaling these performance improvements in a codebase with many components and many contributors, since it makes it less likely for regressions to slip inÂ later.</p><h3>Managing the size of viewÂ bodies</h3><p>Another essential aspect of SwiftUI diffing is understanding that SwiftUI can only diff proper View structs. Any other code, such as computed properties or helper functions that generate a SwiftUI view, cannot beÂ diffed.</p><p>Consider the following example:</p><pre>// Complex SwiftUI views are often simplified by<br>// splitting the view body into separate computed properties.<br>struct MyScreen: View {<br>  /// The unidirectional data flow state store for this feature.<br>  @ObservedObject var store: StateStore&lt;MyState, MyAction&gt;<br><br>  var body: some View {<br>    VStack {<br>      headerSection<br>      actionCardSection<br>    }<br>  }<br><br>  private var headerSection: some View {<br>    Text(store.state.titleString)<br>      .textStyle(.title)<br>  }<br><br>  private var actionCardSection: some View {<br>    VStack {<br>      Image(store.state.cardSelected ? &quot;enabled&quot; : &quot;disabled&quot;)<br>      Text(&quot;This is a selectable card&quot;)<br>    }<br>    .strokedCard(.roundedRect_mediumCornerRadius_12)<br>    .scaleEffectButton(action: {<br>      store.handle(.cardTapped) <br>    })<br>  }<br>}</pre><p>This is a common way to organize complex view bodies, since it makes the code easier to read and maintain. However, at runtime, SwiftUI effectively inlines the views returned from the properties into the main view body, as if we insteadÂ wrote:</p><pre>// At runtime, computed properties are no different<br>// from just having a single, large view body!<br>struct MyScreen: View {<br>  @ObservedObject var store: StateStore&lt;MyState, MyAction&gt;<br><br>  // Re-evaluated every time the state of the screen is updated.<br>  var body: some View {<br>    VStack {<br>      Text(store.state.titleString)<br>        .textStyle(.title)<br><br>      VStack {<br>        Image(store.state.cardSelected ? &quot;enabled&quot; : &quot;disabled&quot;)<br>        Text(&quot;This is a selectable card&quot;)<br>      }<br>      .strokedCard(.roundedRect_mediumCornerRadius_12)<br>      .scaleEffectButton(action: {<br>        store.handle(.cardTapped) <br>      })<br>    }<br>  }<br>}</pre><p>Since all of this code is part of the same view body, all of it will be re-evaluated when any part of the screenâ€™s state changes. While this specific example is simple, as the view grows larger and more complicated, re-evaluating it will become more expensive. Eventually there would be a large amount of unnecessary work happening on every screen update, hurting performance.</p><p>To improve performance, we can implement the layout code in separate SwiftUI views. This allows SwiftUI to properly diff each child view, only re-evaluating their bodies when necessary:</p><pre>struct MyScreen: View {<br>  @ObservedObject var store: StateStore&lt;MyState, MyAction&gt;<br><br>  var body: some View {<br>    VStack {<br>      HeaderSection(title: store.state.titleString)<br>      CardSection(<br>       isCardSelected: store.state.isCardSelected,<br>       handler: store.handler,<br>      )<br>    }<br>  }<br>}<br><br>/// Only re-evaluated and re-rendered when the title property changes.<br>@Equatable<br>struct HeaderSection: View {<br>  let title: String<br><br>  var body: some View {<br>    Text(title)<br>      .textStyle(.title)<br>  }<br>}<br><br>/// Only re-evaluated and re-rendered when the isCardSelected property changes.<br>@Equatable<br>struct CardSection: View {<br>  let isCardSelected: Bool<br>  @SkipEquatable let handler: Handler&lt;MyAction&gt;<br><br>  var body: some View {<br>    VStack {<br>      Image(store.state.isCardSelected ? &quot;enabled&quot; : &quot;disabled&quot;)<br>      Text(&quot;This is a selectable card&quot;)<br>    }<br>    .strokedCard(.roundedRect_mediumCornerRadius_12)<br>    .scaleEffectButton(action: {<br>      handler.handle(.cardTapped) <br>    })<br>  }<br>}</pre><p>By breaking the view into smaller, diffable pieces, SwiftUI can efficiently update only the parts of the view that actually changed. This approach helps maintain performance as a feature grows moreÂ complex.</p><h4>View body complexity lintÂ rule</h4><p>Large, complex views arenâ€™t always obvious during development. Easily available metrics like total line count arenâ€™t a good proxy for complexity. To help engineers know when itâ€™s time to refactor a view into smaller, diffable pieces, we created a custom <a href="https://github.com/realm/SwiftLint">SwiftLint</a> rule that parses the view body using <a href="https://github.com/swiftlang/swift-syntax">SwiftSyntax</a> and measures its complexity. We defined the view complexity metric as a value that increases every time you compose views using computed properties, functions, or closures. With this rule we automatically trigger an alert in Xcode when a view is getting too complex. (The complexity limit is configurable, and we currently allow a maximum complexity level ofÂ 10.)</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*Tdo1L8qZf81FWFeJaNY6yQ.png" /><figcaption>The rule shows as a warning during local Xcode builds alerting engineers as early as possible. In this screenshot, the complexity limit is set to 3, and this specific view has a complexity ofÂ 5.</figcaption></figure><h3>Conclusion</h3><p>With an understanding of how SwiftUI view diffing works, we can use an <em>@Equatable</em> macro to ensure view bodies are only re-evaluated when the values inside views actually change, break views into smaller parts for faster re-evaluation, and encourage developers to refactor views before they get too large andÂ complex.</p><p>Applying these three techniques to SwiftUI views in our app has led to a large reduction in unnecessary view re-evaluation and re-renders. Revisiting the examples from earlier, you see far fewer re-renders in the search bar and filterÂ panel:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*tWhEXK5kyFP5KYPCUSvp6Q.gif" /></figure><p>Using results from our <a href="https://medium.com/airbnb-engineering/airbnbs-page-performance-score-on-ios-36d5f200bc73">page performance score</a> system, weâ€™ve found that adopting these techniques in our most complicated SwiftUI screens really does improve performance for our users. For example, we reduced <a href="https://medium.com/airbnb-engineering/airbnbs-page-performance-score-on-ios-36d5f200bc73#4c63">scroll hitches</a> by<strong> </strong>15% on our main Search screen by adopting <em>@Equatable</em> on its most important views, and breaking apart large view bodies into smaller diffable pieces. These techniques also give us the flexibility to use a feature architecture that best suits our needs without compromising performance or imposing burdensome limitations (e.g., completely avoiding closures in SwiftUIÂ views).</p><p>Of course, these techniques arenâ€™t a silver bullet. Itâ€™s not necessary for all SwiftUI features to use them, and these techniques by themselves arenâ€™t enough to guarantee great performance. However, understanding how and why they work serves as a valuable foundation for building performant SwiftUI features, and makes it easier to spot and avoid problematic patterns in your ownÂ code.</p><p>If youâ€™re interested in joining us on our quest to make the best iOS app in the App Store, please see our <a href="https://careers.airbnb.com/">careers</a> page for open iOSÂ roles.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=37b77ac61896" width="1" height="1" alt=""><hr><p><a href="https://medium.com/airbnb-engineering/understanding-and-improving-swiftui-performance-37b77ac61896">Understanding and Improving SwiftUI Performance</a> was originally published in <a href="https://medium.com/airbnb-engineering">The Airbnb Tech Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Load Testing with Impulse at Airbnb]]></title>
            <link>https://medium.com/airbnb-engineering/load-testing-with-impulse-at-airbnb-f466874d03d2?source=rss----53c7c27702d5---4</link>
            <guid isPermaLink="false">https://medium.com/p/f466874d03d2</guid>
            <category><![CDATA[tech]]></category>
            <category><![CDATA[infrastructure]]></category>
            <category><![CDATA[engineering]]></category>
            <category><![CDATA[technology]]></category>
            <category><![CDATA[load-testing]]></category>
            <dc:creator><![CDATA[Chenhao Yang]]></dc:creator>
            <pubDate>Mon, 09 Jun 2025 17:45:39 GMT</pubDate>
            <atom:updated>2025-06-09T17:45:39.642Z</atom:updated>
            <content:encoded><![CDATA[<p>Comprehensive Load Testing with Load Generator, Dependency Mocker, Traffic Collector, andÂ More</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*3LijjQrJDLVA_ptfeRe83g.jpeg" /></figure><p>Authors: <a href="https://www.linkedin.com/in/chenhao-yang-9799b022/">Chenhao Yang</a>, <a href="https://www.linkedin.com/in/haoyue-wang-a722509a/">Haoyue Wang</a>, <a href="https://www.linkedin.com/in/xiaoyawei/">Xiaoya Wei</a>, <a href="https://www.linkedin.com/in/zhijie-guan/">Zay Guan</a>, <a href="https://www.linkedin.com/in/yaolin-chen-591a31339/">Yaolin Chen</a> and <a href="https://www.linkedin.com/in/fei-yuan/">FeiÂ Yuan</a></p><p>System-level load testing is crucial for reliability and efficiency. It identifies bottlenecks, evaluates capacity for peak traffic, establishes performance baselines, and detects errors. At a company of Airbnbâ€™s size and complexity, weâ€™ve learned that load testing needs to be robust, flexible, and decentralized. This requires the right set of tools to enable engineering teams to do self-service load tests that integrate seamlessly withÂ CI.</p><p>Impulse is one of our internal load-testing-as-a-service frameworks. It provides tools that can generate synthetic loads, mock dependencies, and collect traffic data from production environments. In this blog post, weâ€™ll share how Impulse is architected to minimize manual effort, seamlessly integrate with our observability stack, and empower teams to proactively address potential issues.</p><h3>Architecture</h3><p>Impulse is a comprehensive load testing framework that allows service owners to conduct context-aware load tests, mock dependencies, and collect traffic data to ensure the systemâ€™s performance under various conditions. It includes the following components:</p><ol><li><strong>Load generator</strong> to generate context-aware requests on the fly, for testing different scenarios with synthetic or collected traffic.</li><li><strong>Dependency mocker</strong> to mock the downstream responses with latency, so that the load testing on the service under test (SUT) doesnâ€™t need to involve certain dependent services. This is especially crucial when the dependencies are vendor services that donâ€™t support load testing, or if the team wants to regression load test their service during day-to-day deployment without affecting downstreams.</li><li><strong>Traffic collector</strong> to collect both the upstream and downstream traffic from the production environment, and then apply the resulting data to the test environment.</li><li><strong>Testing API generator</strong> to wrap asynchronous workflows into synchronous API calls for loadÂ testing.</li></ol><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*SFDblGijyiLQfI7C5RpPXw.png" /><figcaption>Figure 1: The Impulse framework and its four main components</figcaption></figure><p>Each of these four tools are independent, allowing service owners the flexibility to select one or more components for their load testingÂ needs.</p><h4>Load generator</h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/977/1*WD4EWyWHDQMf_7nDIGkAuA.png" /><figcaption>Figure 2: Containerized load generator</figcaption></figure><p><em>Context aware</em></p><p>When load testing, requests made to the SUT often require some information from the previous response or need to be sent in a specific order. For example, if an update API needs to provide an <em>entity_id</em> to update, we must ensure the entity already exists in the testing environment context.</p><p>Our load generator tool allows users to write arbitrary testing logic in Java or Kotlin and launch containers to run these tests at scale against the SUT. Why write code instead of DSL/configuration logic?</p><ul><li>Flexibility: Programming languages are more expressive than DSL and can better support complex contextual scenarios.</li><li>Reusability: The same testing code can be used in other tests, e.g., integration tests.</li><li>Developer proficiency: Low/no learning curve to onboard, donâ€™t need to learn how to write testingÂ logic.</li><li>Developer experience: IDE support, testing, debugging, etc.</li></ul><p>Here is an example of synthetic context-aware testÂ case:</p><pre>class HelloWorldLoadGenerator : LoadGenerator {<br>   override suspend fun run() {<br>       val createdEntity = sutApiClient.create(CreateRequest(name=&quot;foo&quot;, ...)).data<br><br>       // request with id from previous response (context)<br>       val updateResponse = sutApiClient.update(UpdateRequest(id=createdEntity.id, name=&quot;bar&quot;))<br>       <br>       // ... other operations<br>       <br>       // clean up<br>       sutApiClient.delete(DeleteRequest(id=createdEntity.id))<br>   }<br>}</pre><p><em>Decentralized</em></p><p>The load generator is decentralized and containerized, which means each time a load test is triggered, a set of new containers will be created to run the test. This design has several benefits:</p><ul><li>Isolation: Load testing runs between different services are isolated from each other, eliminating any interference.</li><li>Scalability: The number of containers can be scaled up or down according to the traffic requirements.</li><li>Cost efficiency: The containers are short-lived, as they only exist during the load testingÂ run.</li></ul><p>Whatâ€™s more, as our services are cloud based, a subtle point is that the Impulse framework will evenly distribute the workers among all our data centers, and the load will be emitted evenly from all the workers. Impulseâ€™s load generator ensures the overall trigger per second (TPS) is as configured. Based on this, we can better leverage the locality settings in load balancers, which can better mimic the real traffic distribution in production.</p><p><em>Execution</em></p><p>The load generator is designed to be executed in the CI/CD pipeline, which means we can trigger load testing automatically. Developers can configure the testing spec in multiple phases, e.g., a warm up phase, a steady state phase, a peak phase, etc. Each phase can be configured with:</p><ul><li>Test cases toÂ run</li><li>TPS (trigger per second) of each testÂ case</li><li>Test duration</li></ul><h4>Dependency mocker</h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*QXMa3Nj3-aSUE_EOvlv1xw.png" /><figcaption>Figure 3: Dependency mocker</figcaption></figure><p>Impulse is a decentralized framework where each service has its own dependency mocker. This can eliminate interference between services and reduce communication costs. Each dependency mocker is an out-of-process service, which means the SUT behaves just as it does in production. We run the mockers in separate instances to avoid any impact on the performance of the SUT. The mock servers are all short livedâ€Šâ€”â€Šthey only start before tests run and shut down afterwards to save costs and maintenance effort. The response latency and exceptions are configurable and the number of mocker instances can be adjusted on demand to support large amounts ofÂ traffic.</p><p>Other noteworthy features:</p><ul><li>You can selectively stub some of the dependencies. Currently, stubbing is supported for HTTP JSON, Airbnb Thrift, and Airbnb GraphQL dependencies.</li><li>The dependency mockers support use cases beyond load testing. For instance, integration tests often rely on other services or third-party API calls, which may not guarantee a stable testing environment or might only support ideal scenarios. Dependency mockers can address this by offering predefined responses or exceptions to fully test thoseÂ flows.</li></ul><p>Impulse supports two options for generating mock responses:</p><ol><li>Synthetic response: The response is generated by user logic, as in integration testing; the difference is that the response comes from a remote (out-of-process) server with simulated latency.<br>- Similar to the load generator, the logic is written in Java/Kotlin code and contains request matching and response generation.<br>- Latency can be simulated using p95/p99Â metrics.</li><li>Replay response: The response is replayed from the production downstream recording, supported by the traffic collector component.</li></ol><p>Here is an example of a synthetic response with latency inÂ Kotlin:</p><pre>downstreamsMocking.every(<br>      thriftRequest&lt;FooRequest&gt;().having { it.message == &quot;hello&quot; }<br>    ).returns { request -&gt;<br>      ThriftDownstream.Response.thriftEncoded(<br>        HttpStatus.OK,<br>        FooResponse.builder.reply(&quot;${request.message} world&quot;).build()<br>      )<br>    }.with {<br>      delay = latencyFromP95(p95=500.miliseconds, min=200.miliseconds, max=2000.miliseconds)<br>    }</pre><h4>Traffic collector</h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*EImg3JUEGzbos5r3_6U-FQ.png" /><figcaption>Figure 4: Traffic collector</figcaption></figure><p>The traffic collector component is designed to capture both upstream and downstream traffic, along with the relationships between them. This approach allows Impulse to accurately replay production traffic during load testing, avoiding inconsistencies in downstream data or behavior. By replicating downstream responsesâ€Šâ€”â€Šincluding production-like latency and errorsâ€Šâ€”â€Švia the dependency mocker, the system ensures high-fidelity load testing. As a result, services in the testing environment behave identically to those in production, enabling more realistic and reliable performance evaluations.</p><h4>Testing API generator</h4><p>We rely heavily on event-driven, asynchronous workflows that are critical to our business operations. These include processing events from a message queue (MQ) and executing delayed jobs. Most of the MQ events/jobs are emitted from synchronous flows (e.g., API calls), so theoretically they can be covered by API load testing. However, the real world is more complex. These asynchronous flows often involve long chains of event and job emissions originating from various sources, making it difficult to replicate and test them accurately using only API-based methods.</p><p>To address this, the testing API generator component creates HTTP APIs during the CI stage according to the event or job schema. These APIs act as wrappers around the underlying asynchronous flows and are registered exclusively in the testing environment. This setup enables load testing toolsâ€Šâ€”â€Šsuch as load generatorsâ€Šâ€”â€Što send traffic to these synthetic APIs, allowing asynchronous flows to be exercised as if they were synchronous. As a result, itâ€™s possible to perform targeted, realistic load testing on asynchronous logic that would otherwise be hard to simulate.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/660/1*F3qllm7qqMu4N2k0bBFbdQ.png" /><figcaption>Figure 5: Testing API generator for asyncÂ flows</figcaption></figure><p>The goal of the testing API generator is to help developers identify performance bottlenecks and potential issues in their async flow implementations and under high traffic conditions. It does this by enabling direct load testing of async flows without involving middleware components like MQs. The rationale is that developers typically aim to evaluate the behavior of their own logic, not the middleware, which is usually already well-tested. By bypassing these components, this approach simplifies the load testing process and empowers developers to independently manage and execute their ownÂ tests.</p><h4>Integration with other testing frameworks</h4><p>Airbnb emphasizes product quality, utilizing versatile testing frameworks that cover integration and API tests across development, staging, and production environments, and integrate smoothly into CI/CD pipelines. The modular design of Impulse facilitates its integration with these frameworks, offering systematic serviceÂ testing.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*649CFxbpASxHotVVbqdQkQ.png" /><figcaption>Figure 6: How Impulse interfaces with other internal testing frameworks</figcaption></figure><h3>Conclusion</h3><p>In this blog post, we shared how Impulse and its four core components help developers perform self-service load testing at Airbnb. As of this writing, Impulse has been implemented in several customer support backend services and is currently under review with different teams across the company who are planning to leverage Impulse to conduct loadÂ testing.</p><p>Weâ€™ve received a lot of good feedback in the process. For example: â€œ<em>Impulse helps us to identify and address potential issues in our service. During testing, it detected an ApiClientThreadToolExhaustionException caused by thread pool pressure. Additionally, it alerted us about occasional timeout errors in client API calls during service deployments. Impulse helped us identify high memory usage in the main service container, enabling us to fine-tune the memory allocation and optimize our serviceâ€™s resource usage. Highly recommend utilizing Impulse as an integral part of the development and testing processes.</em>â€</p><h3>Acknowledgments</h3><p>Thanks to Jeremy Werner, Yashar Mehdad, Raj Rajagopal, Claire Cheng, Tim L., Wei Ji, Jay Wu, Brian Wallace for support on the ImpulseÂ project.</p><p>Does this type of work interest you? Check out our open rolesÂ <a href="https://careers.airbnb.com/">here</a>.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=f466874d03d2" width="1" height="1" alt=""><hr><p><a href="https://medium.com/airbnb-engineering/load-testing-with-impulse-at-airbnb-f466874d03d2">Load Testing with Impulse at Airbnb</a> was originally published in <a href="https://medium.com/airbnb-engineering">The Airbnb Tech Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
    </channel>
</rss>