<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:cc="http://cyber.law.harvard.edu/rss/creativeCommonsRssModule.html">
    <channel>
        <title><![CDATA[NYT Open - Medium]]></title>
        <description><![CDATA[How we design and build digital products at The New York Times. - Medium]]></description>
        <link>https://open.nytimes.com?source=rss----51e1d1745b32---4</link>
        <image>
            <url>https://cdn-images-1.medium.com/proxy/1*TGH72Nnw24QL3iV9IOm4VA.png</url>
            <title>NYT Open - Medium</title>
            <link>https://open.nytimes.com?source=rss----51e1d1745b32---4</link>
        </image>
        <generator>Medium</generator>
        <lastBuildDate>Fri, 19 Dec 2025 22:27:53 GMT</lastBuildDate>
        <atom:link href="https://open.nytimes.com/feed" rel="self" type="application/rss+xml"/>
        <webMaster><![CDATA[yourfriends@medium.com]]></webMaster>
        <atom:link href="http://medium.superfeedr.com" rel="hub"/>
        <item>
            <title><![CDATA[Designing a Digital New York Times Museum]]></title>
            <link>https://open.nytimes.com/designing-a-digital-times-museum-for-all-405331352189?source=rss----51e1d1745b32---4</link>
            <guid isPermaLink="false">https://medium.com/p/405331352189</guid>
            <category><![CDATA[product]]></category>
            <category><![CDATA[intern]]></category>
            <category><![CDATA[design]]></category>
            <category><![CDATA[product-design]]></category>
            <category><![CDATA[archive]]></category>
            <dc:creator><![CDATA[The NYT Open Team]]></dc:creator>
            <pubDate>Wed, 15 Oct 2025 18:30:40 GMT</pubDate>
            <atom:updated>2025-10-16T00:34:00.011Z</atom:updated>
            <content:encoded><![CDATA[<h4>How five product design interns created an award-winning virtual museum experience for employees</h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*TkpbB5_zR3GSXWYSeqaBQQ.jpeg" /><figcaption>Illustration by Lucas¬†Burtin</figcaption></figure><p><strong>By Nuoran Chen, Mina Chung, Frannie Ello, Christina Su, and Bella¬†Rinne</strong></p><p>A key element of The New York Times Product Design Internship Program is a cohort project where all the interns work as a group to create a deliverable that is presented to the entire product design team. We, the summer 2024 cohort, were tasked with designing a proof-of-concept that brings the internal company museum in the NYC office to remote employees.</p><p>The Times Museum is home to multiple artifacts where visitors can freely roam from each display to understand how their stories intertwine. From an official letter authorizing the publication of the leaked Pentagon Papers to physical effects of our war correspondents, each display is unique and speaks to the history behind The Times‚Äô mission to bring independent journalism to¬†all.</p><p>Our group began our work with a tour of The Times Museum led by retired journalist and museum curator, <a href="https://www.nytimes.com/by/david-w-dunlap">David W. Dunlap</a>. On this tour, David guided us through each artifact in chronological order, describing the stories behind each piece and additional facts that were not fully captured with labels. His guided tour was essential for understanding the details behind The New York Times history and journalistic evolution. This tour informed two key requirements for the virtual¬†museum.</p><ol><li><strong>A guided tour with pathways for autonomous exploration</strong>: Visitors should experience the museum in order as it was curated while also being able to explore independently.</li><li><strong>Highlight important artifacts</strong>: We want to reduce the cognitive load for visitors and prominent artifacts so visitors are not overwhelmed when they enter the virtual¬†museum.</li></ol><p>With these defined requirements, we began the design¬†process.</p><p>We held multiple brainstorming sessions to prioritize features and propose user flows in this virtual museum, but quickly found ourselves at a roadblock. For many weeks, we focused on nailing down our key features without bringing our ideas to visual sketches and designs. Not only did this make it challenging to scope our work, but it made it challenging for us to get feedback from other designers.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*ocK45EteCnD3nvJWSVisZA.jpeg" /><figcaption><em>Putting our ideas to visuals helped with deciding the scope of our¬†project.</em></figcaption></figure><p>We divided responsibilities to each team member: Bella took charge of onboarding to the museum, Nuoran and Mina shared the interactive map and navigation, and Christina and Frannie worked together on the artifact pages. Additionally, each team member rotated responsibility in preparing for our biweekly workshops with designers who gave us feedback on our deliverables.</p><p>To bring the New York Times museum to a digital audience, we realized the first challenge was how to display artifacts online. How can users navigate the space and discover artifacts that interest them? With this question in mind, we began a deep dive into inspiration. We explored various museums‚Äô digital websites and then drew from the company‚Äôs internal communication platform to ensure our design reflected the brand‚Äôs editorial integrity. The early explorations based on these inspirations show the spectrum of displaying artifacts‚Ää‚Äî‚Ääin a more editorial way or spatial¬†way.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*D86GSdDVSQM8GjIyB6OxFw.jpeg" /><figcaption><em>Low-fidelity iterations of the museum navigation.</em></figcaption></figure><p>After discussion, we agreed that since the physical museum already exists, our goal was to recreate its sense of physical presence‚Ää‚Äî‚Äähelping users feel connected to the artifacts and experience the excitement of walking through the space. To achieve this digitally, Nuoran used photogrammetry to stitch together photos of the museum, creating a 3D environment where users can either follow a guided tour or explore¬†freely.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*8nHCTDBKNXHA04krZwmy2g.gif" /><figcaption><em>Creating a digital twin of the museum using photogrammetry</em></figcaption></figure><p>This 3D foundation led us to adopt a spatial UI, which ensures the UI doesn‚Äôt distract from the artifacts; it becomes part of the physical space itself. Nuoran and Mina designed unified spatial UI components, such as the floor-plan map and the side panel for filtering and navigation. These navigation methods ensured a cohesive and accessible experience. Upon landing, users enter a panoramic 3D view that visualizes the museum, map and the side rail. This helps users understand where they‚Äôre located in the space and helps them efficiently navigate between highlighted artifacts.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*tjVgUweNfTQOfdml4PXM3g.gif" /><figcaption><em>Visitors land onto a 3D map where they can understand the physical space of the¬†museum.</em></figcaption></figure><p>Users can feel as if they‚Äôre walking through the museum by following visual cues such as locator pins embedded in the 3D space or directional arrows on artifact cards that guide exploration and highlight key artifacts.</p><p>To support this system, Bella designed an onboarding flow that introduces users to each component‚Äôs functionality before they begin their¬†journey.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*lEftq7cAeEsoeApzPh0tNw.gif" /><figcaption><em>Onboarding that tells users how to navigate through the digital¬†museum</em></figcaption></figure><p>The second design focus was creating interactive experiences around the highlighted artifacts. With limited time, we only focused on ten key artifacts that best represent the museum while minimizing visitor decision fatigue. David provided the list of artifacts, and we began exploring ways to present them in the virtual space. Our initial approach used editorial-style layouts with static images and text, but we quickly realized this fell short. These artifacts are powerful touchstones of journalism‚Äôs history, and much of their presence is lost on a flat screen. Unlike physical museums where artifacts are behind glass and out of reach, virtual environments offer opportunities for more immersive interaction and deeply engaging storytelling.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*WozMSqSB_gFJW9Kk4vD6xg.gif" /><figcaption><em>Earlier artifact page iterations felt static and focused on the¬†text.</em></figcaption></figure><p>Realizing that, we regrouped and had a workshop to brainstorm interactive features that could make key artifacts more vibrant and help tell their stories more effectively. Christina and Frannie tried out different animations to bring these ideas to life. For example, the <em>‚ÄúMan Walking on the Moon‚Äù</em> artifact includes three newspaper front pages, each with a different headline‚Ää‚Äî‚Äähighlighting how journalists in the analog era adapted to rapidly unfolding events. We stacked these newspapers together so users can click on tags to switch between the headlines, mimicking the experience of flipping through real newspapers. For Barney Darnton‚Äôs story, rather than stacking all his related objects together, we chose to display them across a single page representing his role as a World War II correspondent in the Pacific Theater. Users can zoom in on individual items to explore his story in greater¬†depth.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*OLraX__r3_z5RNATfwpF5g.gif" /></figure><p>Instead of treating the ten artifacts as separated experiences, we organized them in chronological order under a unifying theme: ‚ÄúIntegrity of the Times.‚Äù This structure forms the basis of the guided tour we offer users from the start. To make this tour‚Äôs storytelling more engaging, we introduced a scavenger hunt element throughout the experience. As users explore each artifact, they can collect clues that unlock the full story behind it which adds a layer of interactivity that makes the journey more educational and gamified.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*1arEaUTRCOf2RoP81SrslA.gif" /></figure><p>As we wrapped up this project, our team understood the responsibility of designing a virtual museum and the rich history that comes with it. Throughout our design process, we used real imagery and content from the museum in our sketches, constantly questioning if our ideas were the best way to showcase a specific artifact. <em>Is this interaction too playful for this artifact? Does this artifact need more visuals to tell the story?</em> As product designers, we often think about design consistency at the system-level, but as designers working at The New York Times, it is this kind of editorial thinking that brings our work to life. We ended this project by presenting it to the Product Design team, The Times Brand and R&amp;D teams. Our work went on to earn the <a href="https://ifdesign.com/en/winner-ranking/project/virtual-new-york-times-museum/702910">2025 iF Design Award </a>and the <a href="https://www.idsa.org/awards-recognition/idea/idea-gallery/virtual-new-york-times-museum/">2025 IDEA Bronze Prize</a>, highlighting the strength of our approach and storytelling as recognized by the broader design community.</p><p>These awards could not have been won without each member of the team and the strength of our collaboration in this project. Sharing project management responsibilities among this team of designers each step of the way helped with decision-making and keeping us prepared for deadlines. Lastly, for designers looking to hone their process, it can be daunting to jump into designs before everything is answered; however, sometimes visuals are necessary provocations that help a team align on an¬†idea.</p><p><em>A special ‚Äòthank you‚Äô to former product designers Bella Rinne and Christina Su for being a part of this project and contributing to this article</em>. <em>We could not have done this project and won this award without you¬†two!</em></p><p><em>Nuoran Chen is an associate product designer who has worked on The New York Times‚Äô internal design system and currently designs mobile subscription experiences across different Times products.</em></p><p><em>Mina Chung is an associate product designer who has worked on the core news home page and currently designs current subscribers‚Äô experience managing and upgrading their digital subscription across different NYT product surfaces.</em></p><p><em>Frannie Ello is an associate product designer who designs across visual formats and video experiences for the news web and app platforms.</em></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=405331352189" width="1" height="1" alt=""><hr><p><a href="https://open.nytimes.com/designing-a-digital-times-museum-for-all-405331352189">Designing a Digital New York Times Museum</a> was originally published in <a href="https://open.nytimes.com">NYT Open</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Scaling Subscriptions at The New York Times with Real-Time Causal Machine Learning]]></title>
            <link>https://open.nytimes.com/scaling-subscriptions-at-the-new-york-times-with-real-time-causal-machine-learning-5f23a7b24ff4?source=rss----51e1d1745b32---4</link>
            <guid isPermaLink="false">https://medium.com/p/5f23a7b24ff4</guid>
            <category><![CDATA[data-science]]></category>
            <category><![CDATA[data]]></category>
            <category><![CDATA[causal-inference]]></category>
            <category><![CDATA[machine-learning]]></category>
            <dc:creator><![CDATA[Rohit Supekar]]></dc:creator>
            <pubDate>Fri, 03 Oct 2025 15:19:24 GMT</pubDate>
            <atom:updated>2025-10-06T01:45:28.397Z</atom:updated>
            <content:encoded><![CDATA[<h4>How real-time algorithms and causal ML transformed our digital subscription funnel from static paywalls to dynamic, millisecond decision-making</h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*UITh9uwuoXVxScBnf9JnJQ.gif" /><figcaption>Illustration by <a href="https://mathieulabrecque.com/">Mathieu Labrecque</a></figcaption></figure><p>The New York Times became a subscription-first news and lifestyle service with the launch of its paywall in 2011. Since then, our subscription strategy has evolved substantially. Initially, users could access a limited number of free articles per month before they encountered the paywall. In 2019, we began personalizing this number using a Machine Learning (ML) model‚Ää‚Äî‚Ää<a href="https://open.nytimes.com/how-the-new-york-times-uses-machine-learning-to-make-its-paywall-smarter-e5771d5f46f8">The Dynamic Meter</a>. In the past few years, we have replaced this model with real-time algorithms that decide, typically within milliseconds, whether to grant access. These algorithms are tailored to balance and optimize the tradeoff between several business Key Performance Indicators (KPIs), while also allowing us the flexibility to adjust for any business constraints. This article further details the motivation behind these algorithms and their design based upon principles from causal machine learning and multi-objective optimization.</p><h3><strong>Our subscription funnel</strong></h3><p>The New York Times has a tiered subscription funnel (Figure 1), consisting of unregistered, registered, and subscribed users. This funnel is designed to provide non-subscribers with limited access to our content, allowing them to discover our offerings. At other times, the content may be blocked by a digital ‚Äúwall‚Äù. We have two types of walls‚Ää‚Äî‚Ääa registration wall that asks a user to register for a free account or log in, and a paywall that asks a user to subscribe. A large number of users are unregistered‚Ää‚Äî‚Ääthey may be shown either a registration wall or a paywall. Once a user is in the registered state, they can be shown only a¬†paywall.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*0tTyFxroIibVwYrsKNXu0Q.png" /><figcaption>Figure 1: The New York Times subscription funnel</figcaption></figure><h3><strong>Optimizing the subscription funnel</strong></h3><p>Optimizing who sees the registration wall or the paywall‚Ää‚Äî‚Ääand when‚Ää‚Äî‚Ääis a very relevant question for our business. While blocking access encourages users to subscribe, we do not want to block all our content because it prevents users from discovering the breadth and depth of our journalism.</p><p>Our holistic business strategy is made up of distinct KPIs for registered and unregistered users. For registered users, the business KPIs are two-fold and related to engagement and conversion, similar to our previous¬†<a href="https://open.nytimes.com/how-the-new-york-times-uses-machine-learning-to-make-its-paywall-smarter-e5771d5f46f8">model</a>.</p><p>For unregistered users, we are interested in subscriptions as well as registrations. Post registration, we can offer tailored experiences to provide users with a more rewarding experience. In addition, we consider <a href="https://en.wikipedia.org/wiki/Bounce_rate">bounce rate</a> as a relevant KPI, for better user experience and since it is often relevant for SEO (Search Engine Optimization).</p><p>Due to such competing business KPIs, the decision to show a registration wall or a paywall is nuanced. Our goal is to build a smart system driven by ML that can decide when to show a paywall, a registration wall, or allow access. As an example, if a user is highly likely to only register but not subscribe, we might show them a registration wall. If they are not likely to subscribe or register, but might engage, the best decision is to allow access so that they get interested in our¬†content.</p><p>Beyond sophisticated machine learning methods, the success of our approach is possible due to a close collaboration with our business leadership in clearly defining business KPIs. The data-driven culture at The Times enables this seamless collaboration. Our shared understanding of the fundamental tradeoffs in our business is crucial for building customized ML models with business constraints.</p><h3><strong>Machine learning for optimizing the subscription funnel</strong></h3><p>Machine learning has been a key component of how the Times‚Äô paywall has been operating. We published an article on our previous <a href="https://open.nytimes.com/how-the-new-york-times-uses-machine-learning-to-make-its-paywall-smarter-e5771d5f46f8">Dynamic Meter model</a> that used to personalize the number of free articles every registered user could access in a month. This model was implemented at the start of each month as a batch process to balance and maximize KPIs for subscription as well as engagement. While very successful for our strategy, this approach did not allow us to take real-time data, as well as article information, into account for making a paywalling decision.</p><p>To leverage the capabilities provided by our Machine Learning Platform, we revamped our algorithms to be real-time and took a holistic approach to apply machine learning in optimizing our subscription funnel. We also apply custom manual rules that can override algorithmic outcomes‚Ää‚Äî‚Ääfor instance, designating certain content as open access for public¬†service.</p><h3><strong>Modeling</strong></h3><p>From the perspective of prescriptive ML, different user types have different sets of ‚Äúactions‚Äù. For unregistered users, the actions include (1) showing a registration wall, (2) showing a paywall, or (3) allowing access. For a registered user, the actions are (1) showing a paywall or (2) allowing access. Since the actions, as well as the number and nature of objectives, are different for our two user groups, we decided to build separate prescriptive models. Here, we describe the construction of the models holistically.</p><p>For each user type, we have a set of supervised predictors <em>f·µ¢(ùêó, a)</em>, where <em>i</em> represents a specific objective, such as propensity to subscribe or engage with an article. These predictors take in <em>ùêó</em>, a vector of features available at inference time, and an action <em>a</em>, such as showing a paywall or allowing¬†access.</p><p>The supervised predictors <em>f·µ¢(ùêó, a)</em> are trained on data collected by a Randomized Control Trial (RCT), which is set up to randomly take an action with equal probability. The RCT is always on and the collected data from it ensures that the supervised predictors are causal, which means they are able to make predictions about counterfactual actions for any given request. This supervised learning setup‚Ää‚Äî‚Ääwhere the action <em>a</em> is used as a feature‚Ää‚Äî‚Ääis typically referred to as an <a href="https://causalml.readthedocs.io/en/latest/methodology.html#s-learner">S-Learner</a>.</p><p>Since we have multiple objectives, we take a convex linear combination to construct a single objective using weight factors <em>Œ¥·µ¢</em> such that <em>‚àë ·µ¢ Œ¥·µ¢ = 1</em>. These weight factors determine how much we value one objective over the others and are chosen based on certain business constraints that we detail¬†below.</p><p>The models then take an action <em>a*</em> as per the policy in Equation¬†1.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*r8XP2CuCUSE7Nyg_PG1hhw.png" /></figure><p><em>a*</em> is the action that maximizes the combined objectives.</p><p>Our modeling approach is schematically shown in Figure 2. Real-time features are ingested into supervised predictors, which predict the objectives for each action. These predictions are then multiplied by their weight factors and combined. Finally, the action is chosen that leads to the largest objective.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*6tOWgAWkVsux3cRFgrIEfg.png" /><figcaption>Figure 2: A schematic for our algorithmic approach that determines whether to show a registration wall, a paywall, or allow access to a user accessing a specific article. Using a real-time feature vector <em>ùêó, supervised predictors return different objectives for each action. These objectives are combined using pre-determined weight factors based on business constraints. The final chosen action maximizes the combined objective. This process is typically executed within milliseconds when a user accesses an¬†article.</em></figcaption></figure><h3><strong>Respecting business constraints</strong></h3><p>The weight factors in our models crucially determine the rates at which registration walls and paywalls are shown, which are often determined by business constraints. For example, for the registered user model, a constraint might be a desired paywall rate in aggregate over a day. For the unregistered user model, we may have multiple constraints, such as those on registration wall rate and paywall¬†rate.</p><p>We solve an optimization problem to figure out the weight factors by backtesting on a recent RCT dataset. By applying a trained model on this dataset, we can construct functions <em>r‚Çñ(Œ¥‚ÇÅ, Œ¥‚ÇÇ,¬†‚Ä¶; œá) </em>that return any statistics about the actions that the model takes on the dataset. Our problem is to match <em>r‚Çñ</em> with a business-specified target <em>t‚Çñ</em> by continuously changing the weight factors <em>Œ¥·µ¢</em>(s). For example, <em>t‚ÇÅ</em> might represent a paywall rate. Correspondingly, <em>r‚ÇÅ</em> returns the paywall rate that the model would have achieved if it were applied to the historical RCT¬†dataset.</p><p>We pose the above problem as yet another multi-objective optimization problem to minimize the losses in Equation 2, which are the squared errors between the achieved and the target¬†rates.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*Vktf7PDQhetX6Z0FVeW3dg.png" /></figure><p>The Pareto optimization problem for these losses is in Equation¬†3.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*7ziYo3DDqTEyjaOJ4_cjPQ.png" /></figure><p>We may solve Equation 3 in its native multi-objective formulation, or combine the individual losses and treat it as a single objective problem. Our investigations showed that the loss landscape is rugged and steep, and we usually have 3‚Äì5 parameters to optimize. We tried several derivative-free optimization algorithms like <a href="https://link.springer.com/article/10.1007/s10589-010-9329-3">Nelder-Mead simplex algorithm</a> and <a href="https://arxiv.org/abs/1807.02811">Bayesian optimization</a>, and found that <a href="https://www.egr.msu.edu/~kdeb/papers/c2014022.pdf">U-NSGA-III</a> worked the¬†best.</p><p>After a sufficiently low loss is achieved, the model is deployed to production with the weight factors that are found. We run this optimization as frequently as every day to adapt to the changing traffic patterns. This approach also allows us the flexibility to respond to business requirements at a rapid pace. Any changes typically involve a simple edit to our configuration files to reflect the modified targets, and the tuning of weight factors is triggered right¬†after.</p><p>To anticipate the impact of weight factor changes on outcomes such as subscription rate, we utilize <a href="https://arxiv.org/abs/2106.07695">Inverse Probability Weighting (IPW)</a>, specifically Haj√®k estimation, similar to our <a href="https://open.nytimes.com/how-the-new-york-times-uses-machine-learning-to-make-its-paywall-smarter-e5771d5f46f8">previous work</a>. This estimation also helps us inform our stakeholders about the expected impact of changing any business constraints, such as the paywall¬†rate.</p><p>The training of the supervised predictors and the tuning of the weight factors operate in a control flow as shown in Figure 3. We only deploy a new model if the optimization loss for respecting business constraints is sufficiently low. Otherwise, our team gets alerted while the previously trained model continues to remain in production and serve¬†traffic.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*yhONASebsHkbv33sEfdY2g.png" /><figcaption>Figure 3: A pictorial representation of the control flow for training supervised predictors and tuning the weight factors to respect business constraints. This process is executed on a schedule every day so that the algorithm continuously adapts to changing traffic patterns and maintains business constraints.</figcaption></figure><h3><strong>Performance measurement</strong></h3><p>To ensure that our productionalized models are performant, we compare their performance against our constantly running RCT. This helps us understand how much better the personalization of the model is as compared to a purely random¬†policy.</p><p>Say we are operating with 3 KPIs. From the RCT data, we can develop a 3-dimensional plane with its extreme points being the average of data points where only a specific action was taken (registration wall, paywall, or allowed access). Any intermediate percentages of actions would correspond to a point on this plane. If the model is better than the randomized policy, then, in this space of objectives, the model point lies on the side of the RCT plane where the objectives are increasing. This is schematically shown in Figure 4. The dotted red lines help us define the performance improvement for each KPI over the random policy. As we vary the weight factors to adjust for business constraints, the model point moves around along the <a href="https://en.wikipedia.org/wiki/Pareto_front">Pareto¬†front</a><strong>.</strong></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*V3NMrJDAvHTDTEjWjnAIdA.png" /><figcaption>Figure 4: A schematic representation for comparing model performance against the Randomized Control Trial when we are considering three competing KPIs (Key Performance Indicators). Examples of such KPIs include subscription rate, registration rate, and engagement. The model point lies ‚Äúabove‚Äù the RCT surface, thus indicating that its policy is better than a purely random policy. While holding any two KPIs constant, the model does better in terms of the third¬†KPI.</figcaption></figure><p>In addition, we also conduct A/B tests to validate any improvements or changes we make to the models as compared to their old counterparts.</p><h3><strong>Conclusion</strong></h3><p>The New York Times registration wall and paywall are now driven by dynamic real-time algorithms. These algorithms are inherently causal and learn the efficacy of these walls based on a variety of factors using Randomized Control Trial (RCT) data. They are designed in a customized way to balance the tradeoff between multiple KPIs, while also helping provide an explicit control over business constraints. As a result, these models have achieved a boost in subscription rate and registration rate while maintaining the rates at which registration walls and paywalls are¬†shown.</p><p><a href="https://www.linkedin.com/in/rsupekar/"><em>Rohit Supekar</em></a><em> is a Lead Machine Learning Scientist at The New York Times, focusing on algorithmic targeting for access and messaging systems. He is passionate about developing and deploying ML solutions, drawing on his doctoral research in applied mathematics and scientific machine learning.</em></p><p><em>Many thanks to the Algorithmic Targeting, Machine Learning Platform, and Meter teams for their invaluable contributions to this collaborative effort.</em></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=5f23a7b24ff4" width="1" height="1" alt=""><hr><p><a href="https://open.nytimes.com/scaling-subscriptions-at-the-new-york-times-with-real-time-causal-machine-learning-5f23a7b24ff4">Scaling Subscriptions at The New York Times with Real-Time Causal Machine Learning</a> was originally published in <a href="https://open.nytimes.com">NYT Open</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[The New York Times Games‚Äô Path to Dark Mode]]></title>
            <link>https://open.nytimes.com/the-new-york-times-games-path-to-dark-mode-345dfe464e1a?source=rss----51e1d1745b32---4</link>
            <guid isPermaLink="false">https://medium.com/p/345dfe464e1a</guid>
            <category><![CDATA[design]]></category>
            <category><![CDATA[games]]></category>
            <category><![CDATA[development]]></category>
            <category><![CDATA[dark-mode]]></category>
            <dc:creator><![CDATA[The NYT Open Team]]></dc:creator>
            <pubDate>Thu, 25 Sep 2025 15:25:52 GMT</pubDate>
            <atom:updated>2025-09-25T15:24:30.929Z</atom:updated>
            <content:encoded><![CDATA[<h4><em>How we designed a deceptively complex¬†feature</em></h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*2IrBHYTCuNopOZNZI4KYnA.jpeg" /><figcaption>Illustration by Alessandro Gottardo</figcaption></figure><p><strong>By Joel Urena, Jenna Kim, Kenneth Ofosu, Shaka Clark, Raven Adaramola, Dylan¬†Campbell</strong></p><p>At least once a day, our players requested a Dark Mode feature for The New York Times Games, particularly those who enjoy playing at night. The bright screens were negatively impacting their satisfaction and hindering accessibility. Our players often wondered why this seemingly simple feature‚Ää‚Äî‚Ää<em>just a switch from white to black</em>‚Ää‚Äî‚Äätook so long to¬†deliver.</p><p>People may think designing for Dark Mode is about inverting colors from white to black, one of the points players often cited when asking for Dark Mode. But there are numerous factors that make up little decisions when designing and implementing Dark Mode. Not only do we have to consider the color accessibility, we have to make intentional color decisions to preserve the brand of each game and The New York Times¬†Games.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*hsYW6kpGTSohh2-EWpTpyQ.png" /><figcaption><em>A compilation of negative Games app reviews that reference dark¬†mode</em></figcaption></figure><h3>Primer: Design Systems &amp; Feature Discovery</h3><p>In the Spring of 2023, The New York Times Games was evolving quickly and we found ourselves with a user experience that was vastly different depending on which game you played and where you played it. We began to rectify this problem with the development of a Design Systems strategy. Like most other tech orgs, we hypothesized that if we began approaching product design and front-end development systematically, surely that would orient us toward a more consistent product experience, right? The answer: yes, eventually.</p><p>From a business and user impact perspective, the clearest Design Systems opportunity to prioritize first was Dark Mode. However, given the maturity of The New York Times Games digital product, the operational complexity we were tackling was massive. At the time, we had eight live games spread across three surfaces: The New York Times Games apps, the The New York Times news app and on the web. We also had ancillary experience for some of these games in the form of our companion articles (e.g: <a href="https://www.nytimes.com/spotlight/spelling-bee-forum">Spelling Bee Forum</a>, <a href="https://www.nytimes.com/spotlight/daily-crossword-column">Daily Crossword‚Äôs Wordplay¬†column</a>).</p><p>What we thought would be a simple color switch became an extensive exercise in paying down years of design debt. We discovered that each game had its own distinct user experience, with different fonts, color palettes, and component styles. Implementing a consistent Dark Mode across everything meant we couldn‚Äôt just add a new theme; we had to audit every screen and standardize the design from the ground up. This complexity forced us to make a critical decision: should we spend years paying down this debt, or could we find a way to scope Dark Mode in a way that delivered immediate value to our players? Our solution was to prioritize the user-facing experience over a complete systemic overhaul, giving us a path to delivery while continuing to chip away at the underlying issues.</p><p>We made the strategic choice to restrict Dark Mode to the Games app, which allowed us to narrow our technical and design focus to typical mobile and tablet display sizes. This decision also served to distinguish the Games app experience from our other platforms, aligning with our long-standing objective of establishing it as the premier destination for playing our¬†games.</p><p>We then began unpacking the remaining scope. Product managers developed a visual to help cross-functional teams get a sense of what work needed to be done to get us to ship our first Dark Mode experience.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*di6wTNBmhRlFkqQAr89D-Q.png" /><figcaption><em>Path to Dark Mode In¬†Apps</em></figcaption></figure><p>Once we understood the work ahead, our next step was to pitch the project to Games leadership for a spot on the roadmap. We publish our games using a hybrid strategy‚Ää‚Äî‚Ääour apps are a native shell built around our web-based games‚Ää‚Äî‚Ääwhich meant we needed to align our web and app teams. Given the competing priorities we faced on the Games App roadmap, this was a hard sell. We ultimately struck a balance between the two teams by aligning on a UX that leveraged our hybrid model to give users the maximum amount of flexibility when determining their theme preferences. Just like with Wordle, every game would have its own Dark Mode¬†setting.</p><h3>Designing the¬†System</h3><p>Now that it was approved, we wondered, ‚Äúwhat are steps to actually get to Dark Mode on Games? How does engineering come into play as designers are exploring?‚Äù We decided to create a strategic chart outlining the high-level steps to the feature: <strong>Alignment, Foundation, Exploration, Integration</strong>. There were still unanswered questions, such as whether to launch all the Dark Mode features at once or to keep the scope narrow by launching it for each game individually. Creating this outline of the design strategy helped us gain clarity that we need to launch Dark Mode individually, reducing the complexity of the project by breaking things down, a common pitfall when working on design systems. This approach also mitigated risk as it allowed us to go through visual quality assurance for each¬†game.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*eZRXcJRgV4iMaA4D4i0Umg.png" /><figcaption><em>Product strategy¬†diagram</em></figcaption></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*o9FO4w6AckmpZks11l--9w.png" /></figure><p>To ensure <strong>Alignment</strong>, we presented to the wider team on the importance of investing in design tokens. Because the wider team was so accustomed to using hex values for colors, it was crucial to change the way designers and engineers work with¬†color.</p><p>For <strong>Foundation </strong>work, we conducted an audit of all game surfaces to identify any misalignments or discoveries. The New York Times Games product expanded significantly from its initial focus on The New York Times Crossword to include over eight distinct games. This growth, however, occurred without a unified design system. Consequently, each game surface utilized unique hex values, even when visual similarities were present.This investigation again proved the importance of design systems. To address this problem, the hex values used within our products were assembled into primitive/base tokens which will later be used to create semantic tokens which are more complex, intricate tokens used for more specific use cases. Also, thinking about how to help other designers discover the color they want efficiently in the future and understanding the complexity of our surfaces, we made a decision to establish three separate color component libraries with semantic libraries: Games Home Surfaces, Games Brand, Gameplay Color libraries.</p><ul><li><em>Games Home colors </em>are general, foundational colors primarily used for utilitarian purposes for text, background, and¬†stroke.</li><li><em>Game Brand colors</em> consist of design tokens used for brand purposes, such as Spelling Bee Yellow, Connections Purple, The Mini Blue,¬†etc.</li><li><em>Gameplay Color</em> tokens are color tokens used for gameboards and game interactions.</li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*Rd4eTzPLsAVUyGsUyY3kIw.png" /></figure><p>When the time came for diving into <strong>Exploration</strong>, we approached each game as a separate project. We held each other accountable by reviewing each others‚Äô design tokens. We discussed and consulted with one another on names for alignment‚Ää‚Äî‚Ääbeginning the development of a ‚Äòshared language‚Äô for the system that we could leverage cross-functionally. We put strong emphasis on <em>playability</em> in gameplay through multiple rounds of prototypes to make sure the game experience of a light mode feels the same as the dark mode. For example, tiles within Connections have a very distinct beige color and do pass the accessibility test on a dark black background. However, it can feel overly bright on a dark background, so we adjusted the saturation of the beige to have a consistent game experience.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*a1bT8n8qu-ltL9a3Uehf3A.png" /></figure><p>As we were getting close to handing off the designs to engineers, we also implemented a way for users to switch preferences within Settings since not all users prefer to have Dark Mode at night or they would prefer to have only certain games in Dark Mode. We wanted to deliver a flexible game experience where users can have a choice and be in control of what mode they want for each¬†game.</p><h3>Putting the Pieces¬†Together</h3><p>Our design system was engineered with the primary goal of ensuring consistency and scalability across both our internal and external products, beginning with our games. By providing a unified set of design tokens and reusable components, it bridges the gap between design and development, enabling our team to deliver a cohesive user experience. While still in development, the system is already proven to be a valuable tool for maintaining visual and functional consistency across our products. Collaboration has been at the heart of this effort, with several members of our engineering, product, and design functions working closely to align on requirements, standards and best practices.</p><p>The design system is built using React with CSS modules for styling, and it is documented in Storybook to ensure ease of use for all stakeholders. We leverage TypeScript for type safety, Vite as a build tool for fast and efficient development, and Playwright for visual regression testing to maintain quality. Design tokens, such as spacing, colors (used to implement dark mode) and typography are implemented using CSS variables to ensure scalability and flexibility. To enforce code quality and consistency, we use Stylelint and ESLint, integrated with Git hooks via tools like lint-staged and husky. These tools ensure that every contribution adheres to our standards, making the system reliable and maintainable. We then created a React Provider and utilized React hooks, which enabled us to roll out the new palette per game and per surface, to ensure we did not have to add the new feature to all surfaces and games at once. Overall our implementation strategy worked, allowing us to incrementally add color tokens without users noticing the difference. Many of the challenges we faced in the project related to our previous css and styles decisions in our code¬†base.</p><p>One of the most rewarding aspects of building the design system has been the collaborative process. Weekly and ad-hoc meetings with engineers, engineering managers, product managers, and designers allowed us to align on requirements, share knowledge, and roadmap future improvements. This has fostered a shared sense of ownership and ensured that the system meets the needs of all users. Looking ahead, our vision is for the design system to become the single source of truth for all design-related information, encompassing both design tokens and shared components. By continuing to iterate and expand, we aim to make the system an indispensable resource for our team and a foundation for scalable, high-quality product development.</p><h3>Takeaways</h3><p>Building Dark Mode for The New York Times Games revealed two significant wins. First, what started as a focused effort to improve workflow through design systems evolved into a powerful tool for consistency and scalability across our products. We‚Äôre now extending this foundational work to other elements like fonts, spacing, buttons and shared patterns.</p><p>Second, the data clearly shows the positive impact of Dark Mode on our players. Users who engage with Dark Mode exhibit higher engagement, playing and completing more puzzles. Our strategic decision to prioritize the user-facing experience allowed us to deliver immediate value while systematically addressing underlying design¬†debt.</p><p>Our efforts have delivered a feature that directly addresses our players‚Äô long-standing desire for a more comfortable nighttime solving experience, allowing them to enjoy their favorite puzzles well into the¬†evening.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*6CY9TY3d82kFNCDWeAe8nw.png" /></figure><p><em>Special thanks¬†to:</em></p><p><em>Jennifer Scheerer &amp; Jessica Gerson for design support and leadership. Michael Beach &amp; Blake Spikestein for product support and leadership. William Frohn for motion design. Ian Hipschman, Ashby Thornwell, Lauren Yew, Goran Svorcan, Ihor Shamin, Katie Leavitt and the entire Games App Squad for their partnership. Emily Ngo for the Data Insights and Experimentation Support. Nick Ritenour, Coryn Brown and The New York Times Marketing team for their partnership</em></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=345dfe464e1a" width="1" height="1" alt=""><hr><p><a href="https://open.nytimes.com/the-new-york-times-games-path-to-dark-mode-345dfe464e1a">The New York Times Games‚Äô Path to Dark Mode</a> was originally published in <a href="https://open.nytimes.com">NYT Open</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Using Provocations to Shake the Status Quo]]></title>
            <link>https://open.nytimes.com/using-provocations-to-shake-the-status-quo-7e884b866310?source=rss----51e1d1745b32---4</link>
            <guid isPermaLink="false">https://medium.com/p/7e884b866310</guid>
            <category><![CDATA[figma]]></category>
            <category><![CDATA[engineering]]></category>
            <category><![CDATA[visualization]]></category>
            <category><![CDATA[design]]></category>
            <category><![CDATA[ux-design]]></category>
            <dc:creator><![CDATA[The NYT Open Team]]></dc:creator>
            <pubDate>Tue, 22 Jul 2025 16:23:11 GMT</pubDate>
            <atom:updated>2025-07-22T16:23:11.612Z</atom:updated>
            <content:encoded><![CDATA[<h4>The bold approach NYT Cooking used to define a strategy for recipe¬†cards.</h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*hOaZxaNhjA2fAFt9Dz2H_Q.jpeg" /><figcaption>Illustration by Ben¬†Denzer</figcaption></figure><p><strong>By Jayne¬†Lee</strong></p><p>In The New York Times Cooking, ‚Äúcards,‚Äù or the containers that represent our content, are the first impression of our brand. They‚Äôre the window into our recipes and users rely on them to evaluate and choose what to¬†cook.</p><p>NYT Cooking users are particularly attracted to our appetizing food photos. In every research session, participants get distracted by a delicious-looking dish while answering the moderator‚Äôs questions. People tend to browse with their stomachs first and then see if the recipe specifications meet their personal criteria (For example, do I have enough time to cook¬†this?).</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*puGPGkTNj0c0zIUN" /></figure><p>A problem we encountered: our beautiful photography was muddied by the page‚Äôs gray background. The white card containers sitting on top of the gray page forced your eyes to focus on the container instead of the photo. The visual hierarchy was at odds with the preferred reading and browsing order of our audience: photo first, then recipe information.</p><p>Recipe information varies, so the card needs to afford that variability. Using containers forced us to keep our cards the same height, resulting in unnecessary whitespace. Since the cards were so tall, this limited the amount of recipes a user could see at once, increasing the time users scanned for¬†recipes.</p><p>We needed to highlight our photography, modernize the format and streamline the recipe evaluation process.</p><p>While designing solutions, I realized that I had more questions than answers. Answering each question would have significantly bloated the project scope. Some examples were: What order of recipe information is most helpful when deciding between recipes? Are recipe bylines important? Which presentation of ratings is more effective?</p><p>Because the priority was to update the format rather than improve comprehension, and to avoid bloating the scope of the project, I started with design <strong>provocations</strong> over design specs. Because cards are systematic, they appear across many surfaces of our product. Provocations gave me confidence in my design decisions without having to design every card for every¬†context.</p><p>Provocations are high-fidelity designs that may look and feel like design specs, but only gesture at solving a problem. The solutions are rough concepts that haven‚Äôt been validated with research and¬†data.</p><p>Temporarily solving my questions via provocations gave me enough information to focus on my main objective: highlight the photography and modernize the format. Think of it like Ikea‚Äôs approach to building furniture: you nail pieces together just enough for the furniture to be stable temporarily. This way you can make adjustments at the end if pieces are misaligned, before final assembly. I knew that I could return to properly solve the questions later¬†on.</p><p>Provocations can inspire more creative and unexpected solutions because they liberate me from the constraints momentarily. To make the provocations useful and not just beautiful rough concepts, I came up with principles that mapped to each provocation.</p><p>Codifying the ideas into principles allows the team, especially non-designers, to understand the intention and goals of the new¬†ideas.</p><p>Here is an¬†example:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*rasbtN43QxxOm_2Q" /></figure><h3>Lead with visuals; use text conservatively</h3><ul><li><strong>Make our visuals do the heavy-lifting.<br></strong>Cooking is a sensorial experience. Highlighting our delicious photography and showing how easy our recipes are to make, will inspire users and help them make decisions faster.</li><li><strong>Our visuals speak the loudest.<br></strong>Ensure visuals are displayed at a large enough scale to entice users, while still giving users the ability to choose. Text is supplementary to the visual, not in competition.</li></ul><p>These provocations served as a vision for how our cards should evolve over time. They allowed us to see where our cards were headed in the future and reverse-engineer small, conservative tweaks we could make to our product now. The expectation of the first iteration was not to move the needle, but to improve the overall product‚Äôs quality of life. We removed the old card containers, changed the page background color, grouped the recipe information together and rounded the corners on our photography.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*ndMQY3qsDsJALpzS" /></figure><ul><li>The old card containers were a fixed height, regardless of the amount of content within it. This created unnecessary whitespace, made our cards taller, and required users to scroll more to see more¬†recipes.</li><li>Removing those meant some recipe information would look disconnected from the card. Grouping all of the information together allows users to scan and make decisions faster.</li><li>We rounded the corners of our photography to signify the clickability affordance, now that the card container was no longer visually present. The rounded corners introduce an overall sense of warmth to the product and visually aligns with the other products within the NYT¬†suite.</li></ul><p>Making these small tweaks put the focus more on our photography, captivating more stomachs and eyes. Our early data shows users can now find and cook new recipes at a much faster rate. And our research participants can stay happily distracted.</p><p>Provocations helped us define a direction for how our cards would evolve over time. It showed us what could be shipped now and how to continue building toward the future. Shifting to provocations allowed us to be more innovative and set a confident, clear strategy of where to go¬†next.</p><p><em>Jayne Lee is a lead product designer for The New York Times Cooking app, driving innovation through design. She strives to create experiences that are both functional and beautiful.</em></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=7e884b866310" width="1" height="1" alt=""><hr><p><a href="https://open.nytimes.com/using-provocations-to-shake-the-status-quo-7e884b866310">Using Provocations to Shake the Status Quo</a> was originally published in <a href="https://open.nytimes.com">NYT Open</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Developing an Internal Tool for Our Puzzle Editor]]></title>
            <link>https://open.nytimes.com/developing-an-internal-tool-for-our-puzzle-editor-d5dc7a9a6464?source=rss----51e1d1745b32---4</link>
            <guid isPermaLink="false">https://medium.com/p/d5dc7a9a6464</guid>
            <category><![CDATA[web-development]]></category>
            <category><![CDATA[internal-tools]]></category>
            <category><![CDATA[puzzle]]></category>
            <category><![CDATA[code]]></category>
            <category><![CDATA[dashboard]]></category>
            <dc:creator><![CDATA[The NYT Open Team]]></dc:creator>
            <pubDate>Mon, 02 Jun 2025 15:54:44 GMT</pubDate>
            <atom:updated>2025-06-02T15:54:44.772Z</atom:updated>
            <content:encoded><![CDATA[<h4>How we developed a dashboard tool created to help ease the workflow of managing puzzles for our Connections editor.</h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*hHwPctJuLkCU-hW_nurB_g.jpeg" /><figcaption>Illustration by Su Yun¬†Song</figcaption></figure><p><strong>By Shafik Quoraishee and Wyna¬†Liu</strong></p><p>In the game Connections, every puzzle is a meticulously crafted challenge designed to captivate our audience and spark intellectual curiosity. Developing these puzzles can sometimes be a time consuming and intricate task. Each puzzle requires planning, beginning with conceptualizing fresh categories and plausible misleads, followed by testing the combinations for balance and solvability, and concluding with refinement and publication-ready formatting. The process requires both creativity and quality¬†control.</p><p><a href="https://en.wikipedia.org/wiki/Wyna_Liu">Wyna Liu</a>, the editor of Connections has the responsibility of constructing and reviewing multiple puzzles spanning various dates, ensuring that each board remains consistent, fresh and challenging to our puzzle solvers. This is a challenging endeavor where there isn‚Äôt much room for error. In order to address the challenge, we developed the Connections Reference Dashboard‚Ää‚Äî‚Ääan in company tool aimed at streamlining data management while providing the puzzle editor with an intuitive, aesthetically pleasing interface that enhances the daily workflow.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*bleB6X_GIsHXBv7E" /></figure><p>There were two considerations in developing this tool. Firstly was technical work in handling a dynamically changing payload of puzzle data. We wanted to create a rich and visually resonating interface that was easy to navigate and gave a bit of the feel of the Connections game¬†itself.</p><p>Therefore, everything from the board results to the search interface was designed with these ergonomics in mind. We wanted to create a level of tactility to the tool which was reminiscent of and which reduced the number of manual steps needed to cross reference both categories and words in individual boards.</p><p>The primary functionality that Wyna was after was the ability to quickly identify words that have appeared in previous Connections boards, as well as their contexts‚Ää‚Äî‚Ääthe categories they were members of, and the other categories that belonged to that board. Connections is a puzzle built around the novel ‚Äòmisleads‚Äô. A ‚Äúmislead‚Äù in the game refers to the specific way words are presented or combined within a particular puzzle that might tempt a player to form an incorrect group. An example is the word ‚ÄúARCHER‚Äù, which might mislead you to group it with ‚ÄúBOW‚Äù, ‚ÄúARROW‚Äù, and ‚ÄúTARGET‚Äù (for ‚Äúarchery terms‚Äù), when its intended category is actually ‚ÄúTV SHOWS‚Äù with words like ‚ÄúLOST‚Äù and ‚ÄúFRASIER.‚Äù While words and categories can be repeated over time, the misleads ideally should not. With more than 700 puzzles, keeping track of what has run, on what date, and in what context, has been a vital part of the construction workflow.</p><p>Previously, there was no comprehensive search view to assist in checking this easily, either in Google Sheets, where the game is constructed, or in our internal admin tool, where the game is published. The dashboard tool provides all the necessary information at-a-glance, which has been an enormous time saver, especially since searching for words that have previously run is done multiple times while constructing each Connections board.</p><h3><strong>The Back¬†End</strong></h3><p>The backend, built with <a href="https://flask.palletsprojects.com/en/stable/">Flask</a>, serves as the cornerstone of the dashboard. It is responsible for fetching puzzle data from external sources, caching it locally, and ensuring that the data is available in real time for the frontend. One of the key components of our backend is the data caching mechanism, which minimizes unnecessary network calls by checking if data for a given date is already available. If it is not, the system fetches the data from the NYT Connections API and caches it on disk. This not only speeds up subsequent requests but also provides redundancy against network issues. For example, consider the function below, which checks for cached data before fetching new¬†data:</p><pre>if resp.status_code == 200:<br>   data = resp.json()<br>   if data and isinstance(data.get(&quot;categories&quot;), list):<br>            puzzle_obj = {}<br>            ALL_PUZZLES[date_str] = puzzle_obj<br>            try:<br>                with open(cache_filename, &quot;w&quot;, encoding=&quot;utf-8&quot;) as f:<br>                    json.dump(puzzle_obj, f, ensure_ascii=False, indent=2)<br>            except Exception as e:<br>                print(f&quot;[fetch_puzzle_and_cache] Write error {date_str}: {e}&quot;)<br>            return puzzle_obj</pre><p>In addition to this caching strategy, an auto-fetch mechanism has been implemented to preload upcoming puzzles. This ensures that even future, unpublished boards are available for planning and review. The auto-fetch function calculates a date range that includes several weeks into the future and then iterates through that range to fetch and cache each¬†puzzle.</p><pre>&lt;script setup&gt;<br>import { reactive, watch, onMounted } from &#39;vue&#39;;<br>import axios from &#39;axios&#39;;<br><br>const state = reactive({<br>  includeUnpublished: true,<br>  puzzles: [],<br>  loading: false,<br>});<br><br>const fetchPuzzles = async () =&gt; {<br>  state.loading = true;<br>  try {<br>    const { data } = await axios.get(‚Äòapi‚Äô&#39;, {<br>      params: {<br>        start: state.dateRange.start,<br>        end: state.dateRange.end,<br>        includeUnpublished: state.includeUnpublished,<br>      },<br>    });<br>    state.puzzles = data.puzzleData;<br>  } catch (error) {<br>    console.error(&#39;Error fetching puzzles:&#39;, error);<br>  }<br>  state.loading = false;<br>};<br><br>onMounted(fetchPuzzles);<br><br>watch(<br>  [() =&gt; state.dateRange, () =&gt; state.includeUnpublished],<br>  fetchPuzzles<br>);<br>&lt;/script&gt;</pre><p>The above snippet is an example of how the front end communicates with the server component we set up, through the data api, and all updates occur seamlessly, and allow the addition of filtering parameters that allow for checking whether the results should contain unpublished boards.</p><h3><strong>The Front¬†End</strong></h3><p>The layout is created using <a href="https://en.wikipedia.org/wiki/Vue.js">Vue.js</a>, employing a grid system to structure four vertical columns, each containing a heading and a list of related items. Each column is encapsulated as an individual Vue component or dynamically rendered from an array of category objects. Data management typically involves an array of objects, with each object containing a category label (e.g., ‚ÄúCONSUMED‚Äù, ‚ÄúALSO‚Äù) and an associated list of¬†terms.</p><p>Components such as &lt;CategoryColumn&gt; accept props like title and items, displaying each entry within styled containers. Conditional styling for special cells, such as highlighting ‚ÄúHORSE‚Äù in yellow, is managed through props or reactive state, signaling active or selected¬†items.</p><p><a href="https://unocss.dev/">UnoCSS</a> is utilized for styling, ensuring uniformity and rapid development with concise, utility-first CSS classes. The grid layout leverages CSS Grid or Flexbox, providing clear borders, appropriate padding, and interactive hover¬†states.</p><p>On the frontend, the choice of Vue.js significantly contributes to a clean, responsive design. The Vue-based interface is intuitive, adapting smoothly across multiple devices and screen sizes. Its reactive nature ensures immediate reflection of changes from the puzzle editor‚Ää‚Äî‚Ääsuch as adjustments in date ranges or toggling puzzle visibility‚Ää‚Äî‚Ääwith no noticeable delay.</p><pre>&lt;template&gt;<br>  &lt;div class=&quot;my-4 p-4 border rounded-lg bg-white shadow&quot;&gt;<br>    &lt;input<br>      v-model=&quot;searchTerm&quot;<br>      type=&quot;text&quot;<br>      placeholder=&quot;Search for a word...&quot;<br>      class=&quot;w-full p-2 border rounded focus:outline-none focus:ring focus:border-blue-300&quot;<br>    /&gt;<br>    &lt;ul v-if=&quot;filteredWords.length&quot; class=&quot;mt-4 space-y-2&quot;&gt;<br>      &lt;li<br>        v-for=&quot;word in filteredWords&quot;<br>        :key=&quot;word&quot;<br>        @click=&quot;selectWord(word)&quot;<br>        class=&quot;cursor-pointer p-2 bg-blue-100 hover:bg-blue-200 rounded&quot;<br>      &gt;<br>        {{ word }} (found in {{ getFrequency(word) }} puzzles)<br>      &lt;/li&gt;<br>    &lt;/ul&gt;<br>    &lt;div v-else class=&quot;mt-4 text-gray-500&quot;&gt;No words match your search.&lt;/div&gt;<br>  &lt;/div&gt;<br>&lt;/template&gt;</pre><p>Vue‚Äôs built-in directives like <a href="https://vuejs.org/guide/components/v-model.html">v-model</a>, <a href="https://www.w3schools.com/vue/vue_v-if.php">v-if</a>,<a href="https://vuejs.org/guide/essentials/list"> v-for</a>, and <a href="https://vuejs.org/guide/essentials/event-handling">@click</a> dramatically simplify the process of building interactive components. In our live word search feature, these directives let us handle input binding, conditional rendering, list generation, and event handling‚Ää‚Äî‚Ääall in a few lines of clean, declarative markup. This approach reduces boilerplate and eliminates manual DOM manipulation, allowing the puzzle editor to interact with a responsive, real-time interface without the overhead of complex logic or state¬†wiring.</p><h3><strong>Search Functionality</strong></h3><p>An advancement in the puzzle review process brought about by this dashboard is the ability to look up the construction history of puzzles in terms of how often duplicate words occurred. What was once a time-consuming procedure has been streamlined into an efficient workflow. We are able to, through the backend, look up duplicates through the use of a word-frequency count. This allows one to observe how often puzzles with recurring words are present and the spacing in time of puzzles with duplicate words occur. Take for instance ‚ÄòBALL‚Äô, which occurred 23 times so far through the history of connections.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*Lv-et845769XCaxq" /></figure><p>We also created a convenient method of looking up the frequency count of all words used in connections in descending order of their usage. This not only provides interesting construction information about connections, but could also lead to interesting statistical analysis, such as separation distance between words over time, least frequency used words, and most frequently used¬†words.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*xr-RAjBVp7510g69" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*pz4b-7OYJRzWBXfT" /></figure><p>The search itself is multi-facetted with several capabilities. The user can search through all the words in all the boards over the duration and lifetime of Connections through using a basic word search for exact matches, and for words contained as parts of other words through the use of regex search. Not only words on their own can be searched for, but categories and category titles as well, which present different snapshots into how words were previously used and in what combination with other words in specific groupings. This can help with organizing future puzzles and providing insights to new category possibilities.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/600/1*ZgdW6v_QTbTNgxHFZ0w9NQ.gif" /></figure><h3><strong>Collaboration with¬†Wyna</strong></h3><p>I was excited to collaborate with Wyna on this opportunity as it was an amazing experience in working with editorial to make useful tools to come to life, and give me as a developer the feeling im building something positive that contributes to the mission of the Games Team bringing joy to our users by helping the creative process (in addition to working on the actual¬†games).</p><p>In order to build the tool to be as useful to Wyna as possible, I had to make sure to really work the board to be rapidly prototypable so different combinations of optimal systems could be swapped in and out. Sometimes the optimal product and tool isn‚Äôt known until you go through several iterations.</p><p>And unsurprisingly to me, the process was not only an incredible learning experience, and an opportunity to work with Wyna, but also a ton of fun, and an opportunity to exercise my skills in tool building, which as a developer I feel compelled to build systems that provide joy to¬†others.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*NTSnOZ9kJlSGNzG4" /><figcaption>The original dashboard specs. Figma has nothing on pencil and¬†paper.</figcaption></figure><h3><strong>Future Advancements</strong></h3><p>Looking ahead, the architecture of the Connections Reference Dashboard is designed to be future-proof and scalable. The modular approach‚Ää‚Äî‚Ääwhere the backend and frontend operate as separate yet integrated components‚Ää‚Äî‚Ääallows for easy enhancements and the addition of new features over time. This flexibility means that as the needs of the puzzle editor evolve or as new challenges arise, the dashboard can be updated and expanded without disrupting the existing¬†workflow</p><p>Happy puzzling!</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=d5dc7a9a6464" width="1" height="1" alt=""><hr><p><a href="https://open.nytimes.com/developing-an-internal-tool-for-our-puzzle-editor-d5dc7a9a6464">Developing an Internal Tool for Our Puzzle Editor</a> was originally published in <a href="https://open.nytimes.com">NYT Open</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[How The New York Times Game Designer Heidi Erwin Creates Variety Puzzles]]></title>
            <link>https://open.nytimes.com/how-new-york-times-game-designer-heidi-erwin-creates-variety-puzzles-41b9bf0abb2b?source=rss----51e1d1745b32---4</link>
            <guid isPermaLink="false">https://medium.com/p/41b9bf0abb2b</guid>
            <category><![CDATA[creativity]]></category>
            <category><![CDATA[puzzle]]></category>
            <category><![CDATA[games]]></category>
            <category><![CDATA[design]]></category>
            <category><![CDATA[game-design]]></category>
            <dc:creator><![CDATA[The NYT Open Team]]></dc:creator>
            <pubDate>Tue, 27 May 2025 15:13:33 GMT</pubDate>
            <atom:updated>2025-05-28T15:26:48.366Z</atom:updated>
            <content:encoded><![CDATA[<h4>An in-depth look at the process of making the weekly ‚ÄúBrain Ticklers‚Äù for The New York Times Gameplay newsletter.</h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*VXoc787GGktvYR_M-3FC_g.jpeg" /><figcaption>Illustration by Claire Merchlinsky</figcaption></figure><p><strong>By Heidi¬†Erwin</strong></p><p>Most of my work as a Senior Game Designer at The New York Times is oriented around the design and development of larger puzzle games, but one unexpected and delightful part of my job for the past two years has been writing variety riddles for The New York Times Gameplay <a href="https://www.nytimes.com/newsletters/gameplay">newsletter</a>. As someone who loves to see the process behind the scenes of the media I enjoy, I wanted to share the experience of creating Brain Ticklers.</p><p><strong>What are Brain Ticklers?</strong></p><p>‚ÄúBrain Ticklers‚Äù is inherited from Will Shortz, and how his variety puzzles have run over the years. Will‚Äôs variety puzzles are typically word puzzles, and ask solvers to anagram phrases or build words from other words, for instance. In fact, I sometimes catch myself writing ‚ÄúBrian Tickler‚Äù by accident in my TODO list; I guess Brian‚Äôs a hidden fictional character associated with these puzzles who exists only in my¬†mind.</p><p>Brain Ticklers are variety puzzles that could run in print (do not require a digital interactive format to be solved), whether that puzzle asks the solver to use deductive logic, wordplay, lateral thinking, visual analysis, or something else. We run one each week in The New York Times Gameplay newsletter, as well as in other parts of the print paper. Here‚Äôs one that ran shortly after my puzzles began running in the newsletter in January of¬†2023:</p><p><strong>Move the following five letters into the grid below, such that you spell two words that form a phrase meaning ‚Äúpersonal perspective.‚Äù Be creative!</strong></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/720/0*o8UzQIPkeTQ3wpA1" /><figcaption>The answer to this one is at the end of this blog¬†post!</figcaption></figure><p><strong>Process Overview</strong></p><p>The end-to-end process for creating a Brain Tickler generally involves the following:</p><ul><li>A source of inspiration</li><li>A first¬†draft</li><li>Editing</li><li>A final¬†graphic.</li></ul><p>More on each of those¬†steps‚Ä¶</p><p><strong>Inspiration</strong></p><p>Inspiration could be anywhere! One of my favorite parts of writing these puzzles is that I feel encouraged to look at the world through different lenses when I‚Äôm out and¬†about.</p><p>Inspiration could come from a sign on the street in the real world (that‚Äôs right gamers, I‚Äôm touching grass), a format restriction, a puzzle I play online‚Ää‚Äî‚Ääthe world is full of puzzle potential.</p><p>Three contexts in which ideas for Brain Ticklers spawn for me are 1. Being out and about interacting with the world, 2. NYT Games team activities that prompt thinking about puzzles, and 3. other media (art, books, games, puzzles).</p><p>For example, here‚Äôs a Brain Tickler from¬†2024:</p><p><strong>What item might be seen with each of these five¬†shapes?</strong></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*T-J3n2mlFCnAovFw" /></figure><p>Solution: A bicycle. They‚Äôre all bike rack¬†shapes!</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*ZfozppVYrkUleh8N" /></figure><p>This puzzle was inspired by the bike racks I was seeing on runs around Queens. I started photographing them for reference; you can tell that the puzzle graphic pulls pretty directly from¬†these!</p><p>There are several opportunities to participate in new game ideation within The New York Times Games team. One of these is the game jams the team hosts, where people on the team put aside their other work for a couple days to ideate, prototype, and pitch. At one point, some work friends and I pitched a Venn diagram puzzle game during game jam, which did not turn into a full game, but did inspire this Brain Tickler (solution at¬†end):</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*q_wJ0Q3wZA0xjk6_" /></figure><p>I‚Äôve also been inspired by the formats of other cool puzzles out in the world. In March 2023, we ran 5 puzzles for a ‚ÄúMarch Matchsticks‚Äù puzzle series (like March Madness). These puzzles riff off of the classic matchstick puzzle format. Here are two from our month of matches (answers at the end of this blog¬†post):</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*raSNMBAc4fCszOGP" /></figure><p>In the puzzle below, 18 matches spell out the word ‚Äúsled.‚Äù Rotate one thing to ‚Äòmake friends.‚Äô</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*6k1iGvkEz3KHovv-" /></figure><p><strong>Editing</strong></p><p>Every few weeks or so, when I have anywhere from three to eight new puzzles drafted, I hop on call with our Puzzle Editor Sam Ezersky, where he plays the puzzles in real¬†time.</p><p>Watching someone else solve a puzzle in real time is helpful in shaping it further: Sometimes it becomes immediately obvious that the setup of a puzzle is unclear if I observe Sam heading down an unintended path. But on top of that playtester feedback, it‚Äôs awesome to witness Sam‚Äôs puzzle brain in¬†action.</p><p>A recent example: I proposed a Brain Tickler where solvers were asked to untangle letter sequences to reveal four phrases of the format ‚Äú____ in ____.‚Äù Sam took one look at ‚ÄúTJIUMSTE‚Äù and said, ‚ÄúJust in time.‚Äù This was followed by seeing ‚ÄúWLAIAITIDNYG,‚Äù immediately thinking out loud, ‚ÄúLying in wait? No! Lady in waiting!‚Äù and then rapid-fire recognizing ‚ÄúCEDHITIOERF‚Äù and ‚ÄúLSOANW‚Äù as ‚ÄúEditor in chief‚Äù and ‚ÄúSon in law.‚Äù Sometimes I wonder if Sam solving a puzzle really says anything about whether the puzzle is fair to the average solver, but fortunately he definitely also has puzzle design sensibilities tailored to a general audience. Reviewing puzzles with Sam is a moment to test the accessibility of a puzzle so we can adjust the framing, presentation, or even concept, as¬†needed.</p><p>Here‚Äôs another recent puzzle that became more elegant during the editing¬†process.</p><p>What I initially presented to¬†Sam:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*dwLgoGCNpNZlQXiw" /></figure><p>After Sam‚Äôs feedback:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*hSnkT0eG5Cn6m6-B" /></figure><p>[Spoiler Warning] The solution is that Marie likes Juliet, because Marie likes words that start with a shortened month name: aprons (Apr), mayonnaise (May), jungles (Jun), and Juliet¬†(Jul).</p><p>He noticed that April, May, and June were all consecutive, and offered up ‚ÄúRomeo and Juliet‚Äù as an alternate fourth pair to continue the consecutive months using the ‚ÄúJul‚Äù in ‚ÄúJuliet‚Äù for July. This is the kind of small adjustment that makes a puzzle that‚Äôs mostly solid feel tighter and more¬†elegant.</p><p>For this type of puzzle, I include an easter egg where the character‚Äôs name hints at the quality of words they like. In this case, Marie also begins with a shortened month name:¬†Mar.</p><p>Here are two more of this puzzle format for you to¬†solve.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*8srogUMgBEzfRXR1" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*g_SuIX7dBiXV8OvR" /></figure><p>In terms of graphics, the Brain Tickler graphics are fairly simple, and typically I make them in Figma. For some puzzles, being precise with graphics matters¬†more:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*wpK38mAn7VDbUTSO" /></figure><p>My younger self would be in awe at the opportunity to work with so many brilliant puzzle minds, all in one place. Working on these puzzles has made me a stronger designer and solver, and I feel gratitude for all of the thought that goes into puzzles across the team, and all of the thought solvers put into playing our games: Humans make our puzzles what they¬†are.</p><p><em>Below is the answer to the puzzle from the start of this¬†piece:</em></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/720/0*EsQZ1XcyN3g3VEfl" /></figure><p><em>And here are other answers to puzzles in this¬†post:</em></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*nfN_Iq7Tie65f2o0" /></figure><p>Rotate the image 180 degrees. It now reads as¬†‚Äúpals.‚Äù</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*HIJN7LtpDU-R_j0_" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*uzpmYG3uZ-mYmEtb" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*RGuXnZFNAnHdv5nh" /></figure><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=41b9bf0abb2b" width="1" height="1" alt=""><hr><p><a href="https://open.nytimes.com/how-new-york-times-game-designer-heidi-erwin-creates-variety-puzzles-41b9bf0abb2b">How The New York Times Game Designer Heidi Erwin Creates Variety Puzzles</a> was originally published in <a href="https://open.nytimes.com">NYT Open</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Accessibility: Requirements, Not Features]]></title>
            <link>https://open.nytimes.com/accessibility-requirements-not-features-76d9758665cd?source=rss----51e1d1745b32---4</link>
            <guid isPermaLink="false">https://medium.com/p/76d9758665cd</guid>
            <category><![CDATA[web-accessibility]]></category>
            <category><![CDATA[accessibility]]></category>
            <category><![CDATA[product-design]]></category>
            <category><![CDATA[product]]></category>
            <category><![CDATA[a11y]]></category>
            <dc:creator><![CDATA[The NYT Open Team]]></dc:creator>
            <pubDate>Fri, 25 Apr 2025 15:31:25 GMT</pubDate>
            <atom:updated>2025-04-25T15:35:14.956Z</atom:updated>
            <content:encoded><![CDATA[<h4>Shipping fast often means shrinking scope‚Ää‚Äî‚Ääbut accessibility deserves permanence, not compromise. At The New York Times, product managers embedded it into how our teams think, build, and deliver using the power of <strong>the three P‚Äôs: People, Process and Products.</strong></h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*EBM2JcEWi0Kt0lWnfSCqgw.jpeg" /><figcaption>Illustration by Klaus¬†Kremmerz</figcaption></figure><p><strong>By Erica Vendetti, Rachel Dixon, David Leininger and Matt Argomaniz</strong></p><p>As Product Managers, we‚Äôre always trying to deepen how we meet the needs of our users and the future of our business. To deliver the features our users want, we‚Äôre always thinking about both who uses our products and how our products are used. For our Platform Product Managers, that can mean thinking about the needs of our colleagues‚Ää‚Äî‚Ääother Product Managers, Data Analysts, Program Managers, Engineers, and Designers. And for our User-Facing Product Managers, that can mean thinking about our readers. <strong>To make products accessible to all, accessibility should be a requirement, not a¬†feature.</strong></p><p>But when you build at the <a href="https://medium.com/timesopen/design-at-the-speed-of-news-7394cb37110f">speed of news</a>, teams must streamline features as they build. To launch a feature quickly, teams continuously refine the scope of feature development. And defining what‚Äôs a feature vs a requirement is a delicate dance for Agile teams. Shifting our teams‚Äô perspective so accessibility is a requirement in all development work is a major undertaking.<strong> To become champions of accessibility in tech, we focused on the three P‚Äôs: People, Process and Products.</strong></p><h3>People: Leverage Internal Experts, Groups and Individuals to Share Knowledge</h3><p>The New York Times aims to foster a culture that enables our mission, business and people to thrive. Finding both the time and opportunity to share knowledge is a top down priority, and the company has several staff-led groups that work to connect and uplift diverse perspectives. Three of these groups‚Ää‚Äî‚Ääwhich are invested in making technology more inclusive and accessible‚Ää‚Äî‚Ääcame together to share knowledge and best practices:</p><ul><li>The Product function‚Äôs DEI Speaker Series Squad, which organizes discussions around building accessible products and being inclusive product managers,</li><li>Women+ in Tech, which focuses on diversity in the technology, and</li><li>The Accessibility Team, which empowers teams to build world leading digital products that are accessible from the¬†start.</li></ul><p>Sometimes, the key to success is simply connecting the right people. By leveraging these internal communities, we can build on existing efforts and share knowledge quickly in multiple spaces. This approach helps us tap into the time people were already dedicating for learning and a large engaged community of folks already interested in inclusive practices.</p><h3>Process: Train Your Teams on Web Accessibility Development Standards</h3><p>Making digital resources accessible means taking steps to prevent and eliminate obstacles that limit some users from fully using our digital products and tools. When it comes to technology, that means being able to support diverse abilities‚Ää‚Äî‚Ääincluding temporary, situational, and age related disabilities. And, of course, improving accessibility improves the user experience for everyone! If you‚Äôve ever increased the brightness on your screen or used dark mode, you‚Äôre benefiting from accessibility tooling.</p><p>To share our knowledge about accessibility requirements, we hosted internal talks, certifications, and created self-guided syllabus learning for various levels. In our first talk, we shared four tips to help standardize accessibility across any product. These tips can easily be added as acceptance criteria from Product to Engineering to make accessible features.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*itfcRc8viExy0K1I" /><figcaption><em>Four tips to standardize accessibility for any product. (1) The heading hierarchy must follow a logical sequence. (2) Button and link text must make sense on their own so the call to action is clear. (3) Keyboard only navigation must be fully functional with tab, shift+tab, space and arrow keys. (4) Use voice over with your product to improve navigation for blind¬†readers.</em></figcaption></figure><p>In our second talk, we shared how to adapt these tips for Platform teams. This helped us build cohesion across the Product and Engineering functions. We were now effectively bridging gaps between user-interface accessibility and internal tooling accessibility.</p><p>In our final session, we organized a self-led certificate program that was open to all staff during <a href="https://open.nytimes.com/innovating-from-home-maker-week-at-the-new-york-times-50ecfa38aca1">Maker Week</a>. Maker Week is an annual event where staff can take a break from their usual work and focus on creative self-directed projects. Our Product experts shared steps for how to perform an accessibility audit for web-based products. Hosting the training at the same time that folks were already building a lot of new tools and applications meant that people also had space to explore accessibility requirements. It was a¬†win-win!</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*FXoICpld6CO0GemH" /><figcaption><em>Accessibility Audit report indicating what tests passed, failed, or not yet¬†run</em></figcaption></figure><p>Product: Create Internal Tools to Standardize Accessible Features</p><p>Our technology teams have invested in creating an internal Accessibility Guide, a resource maintained by a team of internal specialists who monitor Web Content Accessibility Guidelines (WCAG) standards. This guide outlines how designers, engineers and product managers across all platforms can meet WCAG 2.2 AA standards.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*Nrwl0RNlbloTyF3c" /><figcaption><em>Times Product Language accessibility guidance on writing good accessible names for¬†elements</em></figcaption></figure><p>By sharing this guide in all of our learning sessions, we‚Äôve removed one of the biggest barriers‚Ää‚Äî‚Ääfinding the right resources. And because accessibility needs can change with each project, the Product Function‚Äôs DEI Speaker Series Squad wanted to make sure there was a sustainable way for people to continue their journey. As folks learn how to use the accessibility audit tool, they can volunteer to become a team and department accessibility lead by joining the Accessibility Champions Program. This internal program has three levels to guide learning from general interest all the way to subject matter¬†experts.</p><p>The Product function‚Äôs DEI Speaker Series Squad, Women+ In Tech, and the Accessibility Team are among many internal champions aiming to create spaces where curious learners and teams can make technology even more inclusive. This is just our starting point‚Ää‚Äî‚Ääwe‚Äôre excited to keep building and sharing more together.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=76d9758665cd" width="1" height="1" alt=""><hr><p><a href="https://open.nytimes.com/accessibility-requirements-not-features-76d9758665cd">Accessibility: Requirements, Not Features</a> was originally published in <a href="https://open.nytimes.com">NYT Open</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[How The New York Times systematically migrated from Enzyme into React Testing Library]]></title>
            <link>https://open.nytimes.com/how-the-new-york-times-systematically-migrated-from-enzyme-into-react-testing-library-b3ea538d001c?source=rss----51e1d1745b32---4</link>
            <guid isPermaLink="false">https://medium.com/p/b3ea538d001c</guid>
            <dc:creator><![CDATA[The NYT Open Team]]></dc:creator>
            <pubDate>Mon, 03 Mar 2025 17:59:29 GMT</pubDate>
            <atom:updated>2025-03-05T15:43:06.328Z</atom:updated>
            <content:encoded><![CDATA[<h4>How we navigated the shift from Enzyme to React Testing Library at The New York¬†Times.</h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*FLhT6mM8mWf2gBn3UWXiQQ.jpeg" /><figcaption>Illustration by Iris¬†Lei</figcaption></figure><p><strong>By Felipe¬†Buena√±o</strong></p><p>As part of our efforts to maintain and improve the functionality and performance of The New York Times core website, we recently upgraded our React library from <a href="https://open.nytimes.com/enhancing-the-new-york-times-web-performance-with-react-18-d6f91a7c5af8">React 16 into React 18</a>. One of the biggest challenges we faced in the process was transforming our codebase from the <a href="https://enzymejs.github.io/enzyme/">Enzyme</a> test utility into the <a href="https://testing-library.com/docs/react-testing-library/intro/">React Testing¬†Library</a>.</p><p>There are some major key differences in unit testing between these two libraries. Enzyme generates a string representation of the DOM tree. React objects are accessible through Enzyme methods. On the other hand, React Testing Library generates an actual representation of the DOM tree, where React elements are no longer reachable but are rendered as part of the DOM as a¬†whole.</p><p>When making updates to our codebase‚Äôs testing library, we needed to ensure that site traffic was steady and there were no disruptions. With millions of visitors reaching out to our site daily, we had to be extra cautious. In order to successfully migrate our testing library, we implemented a systematic approach that updated test files individually in a consistent manner.</p><p>Our project had relied on Enzyme since its very beginning in May of 2016. However, given the push by the React platform to <a href="https://www.piotrstaniow.pl/goodbye-enzyme">shift towards the React Testing Library</a>, we needed to make this transition soon. In addition to migrating hundreds of files within our core codebase, our roadmap had other priorities with higher user¬†impact.</p><h3>Three possible approaches to a migration path</h3><p>So, how do you move hundreds of files from one testing philosophy to¬†another?</p><p>There are three¬†ways:</p><ol><li>The bulldozer approach</li><li>The consensus approach</li><li>The piecemeal approach</li></ol><h3>The bulldozer approach</h3><p>The bulldozer approach is a top-down solution that updates all the files in one fell swoop. What that means is opening a single pull request that contains multiple changes across hundreds of files. This is a good solution when working in isolation. But because the code base it‚Äôs updated constantly we run the risk of having to resolve conflicts at high frequency. Also, this approach does not invite collaboration with others who may also be interested in helping deprecate Enzyme.</p><h3>The consensus approach</h3><p>The consensus approach is a highly collaborative approach that requires ownership by engineers for each file. This is a great solution for new or highly visible projects.</p><h3>The piecemeal approach</h3><p>This approach requires working on individual files and making strategic, impactful changes continuously until we reach the last corner of the codebase. The strategy here is to start with the most basic file, that is, the file that requires few changes. Then move on to the next most <em>basic file</em>. When these updates are made on a consistent basis i.e. when a change is made daily, momentum will start to build up to a point that making changes becomes part of the daily¬†routine.</p><h3>Our solution: the piecemeal approach</h3><p>We decided to go with the piecemeal approach. It allows you to create momentum and consensus at the same time. This approach has an important requirement: work needs to start at the most basic file, which, in this context, is the file that requires only one or two lines of change, or the file with the least number of tests. Once this file is updated, the first step in the migration process begins. For instance when you need to assert the visibility of an element in React Testing Library vs Enzyme, you must figure out how to retrieve the element then test for visibility.</p><p>Here is an example of how that may look¬†like:</p><p>Given a react component:</p><pre>import React from &#39;react&#39;;<br><br><br>function Content() {<br> return &lt;div&gt;Important content goes here!&lt;/div&gt;;<br>}<br><br>export default Content;</pre><p>This is how unit tests for this code might look like using¬†Enzyme:</p><pre>import React from &#39;react&#39;;<br>import Adapter from &#39;enzyme-adapter-react-16&#39;;<br>import { shallow, configure } from &#39;enzyme&#39;;<br>import Content from &#39;../Content&#39;;<br><br><br>configure({ adapter: new Adapter() });</pre><pre>describe(&#39;Content&#39;, () =&gt; {<br> test(&#39;renders a component&#39;, () =&gt; {<br>   const content = shallow(&lt;Content /&gt;);<br>   const div = content.find(&#39;div&#39;);<br>   expect(div.text()).toBe(&#39;Important content goes here!&#39;);<br> });<br>});</pre><p>This is how that same code would look like for React Testing¬†Library:</p><pre>import React from &#39;react&#39;;<br>import { render } from &#39;@testing-library/react&#39;;<br>import &#39;@testing-library/jest-dom&#39;;<br>import Content from &#39;../Content&#39;;<br><br><br>describe(&#39;Content&#39;, () =&gt; {<br> test(&#39;renders a component&#39;, () =&gt; {<br>   const content = render(&lt;Content /&gt;);<br>   const div = content.container.querySelector(&#39;div&#39;);<br>   expect(div.innerHTML).toBe(&#39;Important content goes here!&#39;);<br> });<br>});</pre><p>Notice that aside from the obvious changes around the methods used to access the component, React Testing Library leverages the DOM to access the elements within the component.</p><p>Later, when working on a test that requires you to click an element, you would already know how to find an element using the previous test as a template. At this point, you would need to figure out now how to fire the¬†event.</p><p>Given the following React component:</p><pre>import React, { useState } from &#39;react&#39;;<br><br><br>function ButtonContent() {<br> const [count, setCount] = useState(0);<br><br><br> return (<br>   &lt;div&gt;<br>     &lt;button id=&quot;btn&quot; onClick={() =&gt; setCount(count + 1)}&gt;<br>       Button count {count}<br>     &lt;/button&gt;<br>   &lt;/div&gt;<br> );<br>}<br><br><br>export default ButtonContent;</pre><p>This is how unit tests for this code might look like using¬†Enzyme:</p><pre>import React from &#39;react&#39;;<br>import Adapter from &#39;enzyme-adapter-react-16&#39;;<br>import { mount, configure } from &#39;enzyme&#39;;<br>import ButtonContent from &#39;../ButtonContent&#39;;<br><br><br>configure({ adapter: new Adapter() });</pre><pre> test(&#39;how element content changes by updating the state of the component&#39;, () =&gt; {<br>   const buttonContent = mount(&lt;ButtonContent /&gt;);<br>   const button = buttonContent.find(&#39;#btn&#39;);<br>   button.simulate(&#39;click&#39;, {<br>     preventDefault() {},<br>   });<br><br>   expect(button.html()).toContain(&#39;Button count 1&#39;);<br> });<br>});</pre><p>This is how that same code would look like for React Testing¬†Library:</p><pre>import React from &#39;react&#39;;<br>import { render } from &#39;@testing-library/react&#39;;<br>import &#39;@testing-library/jest-dom&#39;;<br>import ButtonContent from &#39;../ButtonContent&#39;;<br><br><br>describe(&#39;ButtonContent&#39;, () =&gt; {<br> test(&#39;how element content changes by updating the state of the component&#39;, () =&gt; {<br>   const buttonContent = render(&lt;ButtonContent /&gt;);<br>   const button = buttonContent.container.querySelector(&#39;#btn&#39;);<br>   button.click();<br>   <br>  expect(button.innerHTML).toBe(&#39;Button count 1&#39;);<br> });<br>});</pre><p>Notice the similarities with the first test. When it comes to this test, you need to focus on clicking the¬†element.</p><p>Later when more complexity is required by a test that evaluates visibility, interactivity and network interaction you would use the previous two tests as templates then figure out how to retrieve the response to evaluate¬†it.</p><p>Given the following React component:</p><pre>import React, { useState } from &#39;react&#39;;<br><br><br>function InteractiveContent() {<br> const [isVisible, setIsVisible] = useState(false);<br> function loadImage(event) {<br>   const img = event.target.previousElementSibling;<br>   img.src = &#39;https://placehold.co/400&#39;;<br><br><br>   img.onload = () =&gt; {<br>     setIsVisible(true);<br>   };<br> }<br><br> return (<br>   &lt;div&gt;<br>     &lt;img id=&quot;myImg&quot; alt=&quot;load image&quot; is-visible={isVisible.toString()} /&gt;<br>     &lt;button id=&quot;btn&quot; onClick={loadImage}&gt;<br>       Click to show an image<br>     &lt;/button&gt;<br>   &lt;/div&gt;<br> );<br>}<br><br>export default InteractiveContent;</pre><p>This is how unit tests for this code might look like using¬†Enzyme:</p><pre>import React from &#39;react&#39;;<br>import Adapter from &#39;enzyme-adapter-react-16&#39;;<br>import { mount, configure } from &#39;enzyme&#39;;<br>import InteractiveContent from &#39;../InteractiveContent&#39;;<br><br><br>configure({ adapter: new Adapter() });<br><br><br>describe(&#39;InteractiveContent&#39;, () =&gt; {<br> test(&#39;should load an image after a button is clicked&#39;, () =&gt; {<br>   const interactiveContent = mount(&lt;InteractiveContent /&gt;);<br>   const button = interactiveContent.find(&#39;#btn&#39;);<br>   button.simulate(&#39;click&#39;, {<br>     preventDefault() {},<br>   });<br><br><br>   const image = interactiveContent.find(&#39;img&#39;);<br>   image.simulate(&#39;load&#39;);<br><br>   expect(image.html()).toContain(&#39;https://placehold.co/400&#39;);<br> });<br>});</pre><p>This is how that same code would look like for React Testing¬†Library:</p><pre>import React from &#39;react&#39;;<br>import { render, act } from &#39;@testing-library/react&#39;;<br>import &#39;@testing-library/jest-dom&#39;;<br>import InteractiveContent from &#39;../InteractiveContent&#39;;<br><br><br>describe(&#39;InteractiveContent&#39;, () =&gt; {<br> test(&#39;should load an image after a button is clicked&#39;, () =&gt; {<br>   act(() =&gt; {<br>     const interactiveContent = render(&lt;InteractiveContent /&gt;);<br>     const button = interactiveContent.container.querySelector(&#39;#btn&#39;);<br>     button.click();<br>     const image = interactiveContent.container.querySelector(&#39;#myImg&#39;);<br>     image.onload();<br><br>     expect(image.src).toBe(&#39;https://placehold.co/400&#39;);<br>   });<br> });<br>});</pre><p>Although this test is more complex than the first two tests, all you need to do is focus on firing the onload event for the image since you‚Äôve already shown how to access an element and click it from the previous¬†tests.</p><p>By the time a couple of files have been updated the differences between Enzyme and React Testing Library become trivial to resolve. Also a clear <em>pattern</em> of work will be established. Every time you make changes to a single test, you will start to identify ways you have updated your code to accommodate the new library. Solving more complex test cases becomes easier once the process has undergone several interactions. As the files become more difficult to resolve, solutions can often be found by looking back at previous¬†work.</p><p>Since the updates are small and well-documented in the previous tests, integrating people into this work requires little effort. Because of this ability to onboard new people, consensus is built naturally as people can join in and help contribute to completing the migration.</p><p>The piecemeal approach was time-consuming since it required us to work on a single file at the time but allowed us to migrate with minimal disruption. This approach also gave maintainable progress, which meant that no bug was introduced while migrating. Most importantly, though, this process allowed for collaboration. It gave people interested in joining this project a template they could follow, making the process more manageable and less stressful. It allowed us to maintain our development pace for other critical features.</p><p>Migrating from Enzyme to React Testing Library was a significant but impactful undertaking for our core web application. We could complete the migration without disrupting our roadmap by choosing a methodical, piecemeal approach. This experience taught us the value of strategic planning and balancing technical debt with ongoing development needs.</p><p><em>Felipe Buena√±o is a senior software engineer on the core Web Platforms team at The New York Times. He brings fifteen years of experience in web development. He is known for his strong passion for code refactoring. His contributions have been recognized with awards, including the 2023 NYT Maker‚Äôs Week Best of Use AI and the Q1 NYT Publisher‚Äôs Award for AMP Decommission.</em></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=b3ea538d001c" width="1" height="1" alt=""><hr><p><a href="https://open.nytimes.com/how-the-new-york-times-systematically-migrated-from-enzyme-into-react-testing-library-b3ea538d001c">How The New York Times systematically migrated from Enzyme into React Testing Library</a> was originally published in <a href="https://open.nytimes.com">NYT Open</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[How The New York Times Incorporates Editorial Judgement in Algorithms to Curate Home Screen Content]]></title>
            <link>https://open.nytimes.com/how-the-new-york-times-incorporates-editorial-judgement-in-algorithms-to-curate-home-screen-content-85f48209fdad?source=rss----51e1d1745b32---4</link>
            <guid isPermaLink="false">https://medium.com/p/85f48209fdad</guid>
            <dc:creator><![CDATA[The NYT Open Team]]></dc:creator>
            <pubDate>Wed, 02 Oct 2024 18:27:14 GMT</pubDate>
            <atom:updated>2025-03-03T19:39:45.558Z</atom:updated>
            <content:encoded><![CDATA[<h4>A look into how editorially-driven algorithms assist content curation on The New York Times home¬†page.</h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*WTljcsMoiN2pMl9KKfTI0g.gif" /><figcaption>Illustration by Vivek¬†Thakker</figcaption></figure><p><strong>By Zhen¬†Yang</strong></p><p>Whether on the web or the app, the home page of The New York Times is a crucial gateway, setting the stage for readers‚Äô experiences and guiding them to the most important news of the day. The Times publishes over 250 stories daily, far more than the 50 to 60 stories that can be featured on the home page at a given time. Traditionally, editors have manually selected and programmed which stories appear, when and where, multiple times daily. This manual process presents challenges:</p><ul><li>How can we provide readers a relevant, useful, and fresh experience each time they visit the home¬†page?</li><li>How can we make our editorial curation process more efficient and scalable?</li><li>How do we maximize the reach of each story and expose more stories to our¬†readers?</li></ul><p>To address these challenges, The Times has been actively developing and testing editorially-driven algorithms to assist in curating home page content. These algorithms are editorially driven in that a human editor‚Äôs judgment or input is incorporated into every aspect of the algorithm‚Ää‚Äî‚Ääincluding deciding where on the home page the stories are placed, informing the rankings, and potentially influencing and overriding algorithmic outputs when necessary. From the get-go, we‚Äôve designed algorithmic programming to elevate human curation, not to replace¬†it.</p><h3>Which parts of the home page are algorithmically programmed?</h3><p>The Times began using algorithms for content recommendations <a href="https://www.niemanlab.org/2011/02/you-are-what-you-read-nyt-cto-marc-frons-on-the-papers-new-article-recommendation-engine/">in 2011</a> but only recently started applying them to home page modules. For years, we only had one algorithmically-powered module, ‚ÄúSmarter Living‚Äù, on the home page, and later ‚ÄúPopular in The Times.‚Äù Both were positioned relatively low on the page. Three years ago, the formation of <a href="https://www.nytco.com/press/introducing-the-experiments-and-personalization-team/">a cross-functional team</a>‚Ää‚Äî‚Ääincluding newsroom editors, product managers, data scientists, data analysts, and engineers‚Ää‚Äî‚Ääbrought the momentum needed to advance our responsible use of algorithms. Today, nearly half of the home page is programmed with assistance from algorithms that help promote news, features, and sub-brand content, such as The Athletic and Wirecutter (see Figure 1 for the position of algorithmic modules on the home page). Some of these modules, such as the Features Module located at the top right of the home page on the web version, are in highly visible locations. During major news moments, editors can also deploy algorithmic modules to display additional coverage to complement a main module of stories near the top of the page (the topmost news package of Figure 1 is an example of this in¬†action).</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/560/0*5qcQl1qM-U9ukVAT" /></figure><h3>How is editorial judgment incorporated into algorithmic programming?</h3><p>Algorithmic programming comprises three steps (Figure 2): (1) Pooling: creating a pool of eligible stories for the specific module; (2) Ranking: sorting stories by a ranking mechanism; and (3) Finishing: applying editorial guardrails and business rules to ensure the final output of stories meets our standards. Editorial judgment is incorporated into all of these steps, in different ways.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/686/0*PPp5-yzTSoFtIpG8" /></figure><p>To make an algorithmic recommendation, we first need a pool of articles eligible to appear in a given home page module. A pool can be either manually curated by editors or automatically generated via a query based on rules set by the newsroom.</p><p>A pool typically includes more stories than the number of slots available in the module, so we need a mechanism to rank them to determine which ones to show first and in what order. While there are various ways to rank stories, the algorithm we frequently use on the home page is a <em>contextual bandit</em>, a reinforcement learning method (see our previous<a href="https://open.nytimes.com/how-the-new-york-times-is-experimenting-with-recommendation-algorithms-562f78624d26"> blog post</a> for more information). While in its simplest form, a bandit recommends the same set of engaging articles to all users; the ‚Äúcontextual‚Äù version uses additional reader information (e.g., reading history or geographical location) to adjust the recommendations and make the experience more relevant for each reader. For an example of the geo-personalized bandit, see our previous<a href="https://open.nytimes.com/how-the-new-york-times-is-experimenting-with-recommendation-algorithms-562f78624d26"> blog¬†post</a>.</p><p>To prioritize mission-driven and significant stories, we use several approaches to quantify editorial importance. One approach is having editors assign a rank to each story in the pool, with more recent and newsworthy stories generally considered more important. Another method infers a story‚Äôs importance based on its past promotion on the homepage, where stories that remain in prominent positions for longer are rated as more important. Regardless of the approach, editorial importance can be combined with the bandit to ensure that editorial judgment is incorporated into the ranking process, thus prioritizing stories deemed important by the newsroom.</p><p>Once we have a ranked list of stories, we make final adjustments based on predetermined rules developed with our newsroom partners before stories are shown to readers. One such intervention we developed is a <em>Pinning</em> function that allows editors to override the algorithm and pin important stories at the top. Other important examples are our ‚Äú<em>Already-Read</em>‚Äù and <em>‚ÄúAlready-Seen‚Äù</em> filters, which deprioritize stories that the user has already read or seen a certain number of times (Figure 2). This finishing step ensures that editorial judgment shapes the final output and that we maintain a dynamic and fresh user experience.</p><h3>How do we set up an algorithmically powered module on the home¬†page?</h3><p>The process begins with clearly defining editorial intentions, standards and boundaries as well as reader goals, and then designing algorithms appropriately. To illustrate the process, consider the above-mentioned Features Module (Figure 1): Content in this module is among the most widely-read on the home page. The goal of algorithmic programming for this module is to increase engagement by presenting readers with freshly published features and columns and also to ensure that the most relevant and engaging stories are subsequently displayed.</p><p>After several rounds of experimentation and extensive collaboration with editors, we realized that more features had to be built to achieve the intended reader experience and for the newsroom to be comfortable with integrating algorithms into their process for programming the home page. Together, we built and launched the following features, which are cornerstones in accelerating the use of algorithmic programming on the home¬†page:</p><h3>Exposure boosting</h3><p>While pinning is an effective tool for increasing the exposure of important stories, it is also a rather blunt one: a pinned story is shown to all readers until unpinned by an editor. To meet a desire by home page editors for a ‚Äúsofter‚Äù and more dynamic solution, we developed an <em>Exposure Boosting</em> capability. While a ‚Äúboosted‚Äù story also initially appears at the top of the module, it gradually moves down the slots over time‚Ää‚Äî‚Ääat a rate predetermined by editors‚Ää‚Äî‚Ääuntil it becomes subject to the algorithm‚Äôs bandit again (Figure 3: Exposure Boosting).</p><h3>Smart refreshing</h3><p>Another way to increase the exposure of our stories while ensuring that readers are presented with fresh content is by removing articles that the user has seen several times but has not clicked on‚Ää‚Äî‚Ääthis assumes that the reader is not interested in the story displayed and the algorithm instead shows the next story on the list. When an article is shown to a user, whether they click on it or not, it‚Äôs called an impression. This rather rudimentary logic has its drawbacks: Frequent visitors might experience recommendations refreshing too often, causing a disorienting ‚Äúslot machine‚Äù effect. They could also quickly exhaust all recommendations, resulting in a static module. At the same time, infrequent users, who don‚Äôt reach the impression limit, might see the same stories on distant subsequent visits, leading to a home page that would feel¬†stale.</p><p>These potential issues were especially of concern for high-traffic modules like the Features Module. To address them, we developed a capability called <em>Smart Refreshing</em>. This feature creates a more stable experience for frequent visitors by only increasing the impression counter if a certain amount of time has passed since the last impression. Effectively, impressions occurring less than that amount of time are collapsed into a single impression (Figure 3: Smart Refreshing). For infrequent visitors, Smart Refreshing limits staleness by automatically refreshing recommendations after a set period since their first impression, even if the impression limit was not reached. Home page editors decide on the interval between impressions and the maximum duration a story remains after its initial view based on editorial judgment and A/B¬†testing.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*99DKNHW7guqB_0-g" /></figure><h3>Exposure Minimums</h3><p>In response to concerns from editors that some stories risk not getting enough exposure under purely algorithmic programming, we developed Exposure Minimums. This capability gives the newsroom the reassurance that all stories (particularly less popular ones) receive a minimum number of impressions on the home page before the algorithm takes over their programming. This guarantee helps set editorial expectations for story exposure and has enabled the rollout of algorithms on prominent sections of the home page, such as the Features Module. Typically, higher minimum values increase story exposure but can interfere with algorithm optimization, reducing overall engagement. To find the right balance between exposure and engagement, the exposure minimum is determined in collaboration with our newsroom partners and through A/B¬†testing.</p><h3>Algo Visibility tools</h3><p>One blocker we encountered while trying to scale algorithmic programming was the lack of visibility for editors regarding reader experience and story performance. One of the biggest challenges was feedback from the newsroom that editors and reporters couldn‚Äôt tell if their stories would appear in an algorithmic module on the home page. With the ‚ÄúAlready-Read‚Äù filter in place, their stories, which they would have read, wouldn‚Äôt show up on the home¬†page.</p><p>To address this, our product designer, engineers and data scientists partnered with home page editors to conceptualize and build a browser extension that allows editors to track all the algorithmic modules on the home page, preview different A/B testing variants, and review all the stories that have been selected and eligible for promotion for each module. Our engineers also built a tool that sends automated alerts to editors about changes in algorithmic programming, including new stories added to the pool and any headline or summary updates. Additionally, the Data Science team developed a dashboard to provide near-real-time analytics for stories that were algorithmically programmed.</p><p>After rigorously testing each of these new features and getting editors familiar with these tools and concepts, we permanently implemented algorithmic programming for the Features Module in the spring of 2024. This approach not only streamlined the editorial workflow (daily updates to the module were reduced by a third), it also gave stories that had a longer shelf life more time on the home page, lifting overall engagement. Our product colleagues were delighted that powering the Features Module with an algorithm also helped increase engagement with our sub-brands such as Wirecutter and¬†Cooking.</p><h3>Taking algorithmic programming further‚Ää‚Äî‚Ääbreaking news and¬†more</h3><p>The strong foundation we built by incorporating editorial thinking into our algorithms, coupled with the trust we cultivated, led to more demand from the newsroom for more algorithmic programming tools. Today, editors are also using a tailored set of algorithmic modules to power a secondary set of stories for a topic or news event. These modules are completely self-service for editors, and have been particularly useful during major news events, when the volume of coverage produced often exceeds the amount of real estate on the home page (See Figure 4 for a couple of examples).</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*9vz24V3cTO_VcWXK" /></figure><p>Currently, algorithmic programming recommends stories within individual modules on the home page. Next, we want to explore and test reordering modules based on a mix of editorial importance, engagement, and personalization signals. We believe this approach can further improve a reader‚Äôs experience and amplify our journalism.</p><p><em>Zhen Yang is a Data Scientist on the Algorithmic Recommendations team at The New York Times. Outside of work, Zhen enjoys playing ping pong, rope adventure, and traveling.</em></p><p><em>Celia Eddy, Alex Saez, Derrick Ho, and Christopher Wiggins contributed to this¬†post.</em></p><p><strong><em>Acknowledgements:</em></strong><em> We acknowledge the invaluable contributions of SubX Programmed Surfaces (Product), Home Personalization and Experiments (Newsroom), Algorithmic Recommendations (Data Science), Machine Learning Platform (Engineering), Personalization (Engineering), Programming (Engineering), and Messaging and Personalization (Engineering) team. Together, we designed and built these editorially-driven algorithms, advancing our use of algorithms on the home page to assist content curation. This achievement would not have been possible without their dedication and teamwork.</em></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=85f48209fdad" width="1" height="1" alt=""><hr><p><a href="https://open.nytimes.com/how-the-new-york-times-incorporates-editorial-judgement-in-algorithms-to-curate-home-screen-content-85f48209fdad">How The New York Times Incorporates Editorial Judgement in Algorithms to Curate Home Screen Content</a> was originally published in <a href="https://open.nytimes.com">NYT Open</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Enhancing The New York Times Web Performance with React 18]]></title>
            <link>https://open.nytimes.com/enhancing-the-new-york-times-web-performance-with-react-18-d6f91a7c5af8?source=rss----51e1d1745b32---4</link>
            <guid isPermaLink="false">https://medium.com/p/d6f91a7c5af8</guid>
            <category><![CDATA[react]]></category>
            <category><![CDATA[javascript]]></category>
            <category><![CDATA[web-performance]]></category>
            <dc:creator><![CDATA[The NYT Open Team]]></dc:creator>
            <pubDate>Wed, 26 Jun 2024 18:03:14 GMT</pubDate>
            <atom:updated>2024-06-26T18:03:14.590Z</atom:updated>
            <content:encoded><![CDATA[<h4>How upgrading to React 18 energized The New York Times website‚Ää‚Äî‚Ääand how we tackled some of the challenges we faced along the¬†way.</h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*rT1gMg-9ensw1FDxaAaq_Q.gif" /><figcaption>Illustration by Ben¬†Hickey</figcaption></figure><p><strong>By Ilya¬†Gurevich</strong></p><p>As software engineers at The New York Times, we place a high value on page performance, SEO, and keeping up to date with the latest technology. With those priorities in mind, the release of React 18 stood out to us as a significant and tangible leap forward in the ever-expanding world of web development. For our React-based sites, the upgrade promised a performance boost and access to exciting new features. Last winter, we set out to embrace the powers of React 18 on our flagship core news site. Along the way, we encountered some unique peculiarities‚Ää‚Äî‚Ääboth in React and in our own site‚Ää‚Äî‚Ääthat we had to learn to navigate through. In the end, we achieved big performance gains and unlocked a world of future improvements that we‚Äôre still exploring.</p><p>Before we dive into our process for upgrading, let‚Äôs take a look at a few of the major benefits and changes in React¬†18:</p><ul><li><strong>Smoother Rendering with Concurrent Mode:</strong> React 18 introduces Concurrent Mode, a paradigm shift that allows for simultaneous rendering of updates and user interactions. This translates to smoother animations, less screen jank and cumulative layout shift, and a more responsive user experience.</li><li><strong>Automatic Batching and Transitions:</strong> To take full advantage of concurrency, React 18 automatically batches state updates within a single render cycle, optimizing performance. It does so by breaking up tasks in the main thread, which is a big shift from prior mechanics, where almost all tasks were synchronously executed. The introduction of new <a href="https://react.dev/reference/react/useTransition">useTransition</a> hooks also allows engineers to ensure that certain states will update without blocking the¬†UI.</li><li><strong>Exciting New Features:</strong> React 18 paves the way for exciting functionalities like server-side rendering and streaming updates through <a href="https://react.dev/reference/rsc/server-components">react server components</a> and selective hydration, opening doors to innovative UI patterns and faster initial¬†renders.</li></ul><p>The performance gains were particularly important to us because they promised significant improvements in our Interaction to Next Paint (INP) scores. INP is a measure of page responsiveness and is the newest Core Web Vital, a set of metrics that Google uses to rank websites in search results. SEO scores are vital for a news organization, and improving our INP scores had been a difficult challenge for us, making the React upgrade a high-priority (and high-stakes) initiative.</p><h3>Our Migration Process</h3><ol><li><strong>Removing Deprecated Dependencies</strong></li></ol><p>Before we could get started with the migration itself, we needed to remove a deprecated Enzyme testing library that was incompatible with React 18. To do that, we had to manually migrate all of our test files to the more up-to-date library, <a href="https://testing-library.com/docs/react-testing-library/intro/"><strong>@testing-library/react</strong></a><strong>. </strong>In terms of time commitment, this might have been the biggest piece of the entire project. Enzyme was used in hundreds of test files across our repository, and it required a significant manual effort and dozens of pull requests to fully replace it. We accomplished this effort over the course of several months with incremental pull requests in order to accommodate other product work and avoid developer fatigue. At the end of the effort, we definitely felt like experts in the <a href="https://testing-library.com/docs/react-testing-library/intro/"><strong>@testing-library/react</strong></a><strong> </strong>API, and we were thankful to move on to the React 18 upgrade¬†itself.</p><p><strong>2. Foundation Setting</strong></p><p>With the test file migration out of the way, we could begin work on integrating React 18. In order to accomplish this safely, we first started by upgrading all of our major dependencies, types and tests to conform to React 18, without implementing the latest features themselves. This involved simply upgrading everything from @types/react, react-test-renderer, react-dom, and @testing-library to the latest versions in our package.json files across our repository. Upgrading all major dependencies also involved refactoring some test and type definitions to conform to the latest versions as¬†well.</p><p><strong>3. Turning on the¬†Engines</strong></p><p>Once we felt confident in our package upgrades, we were ready to safely integrate the new functionality of React 18. To turn the features into reality, we needed to utilize the latest APIs: <a href="https://react.dev/reference/react-dom/client/createRoot">createRoot</a> and <a href="https://react.dev/reference/react-dom/client/hydrateRoot">hydrateRoot</a>. We have several instances across multiple web servers where we‚Äôve integrated React Hydration, with a set of shared UI components rendered between all of them, so it was important for us to enable React 18 functionality in as many places as we could. At first glance, it looks as simple as changing references from <strong>ReactDOM.hydrate</strong> to <strong>hydrateRoot. </strong>But was it¬†really?</p><p><strong>Unexpected Challenges</strong></p><p>As developers, it‚Äôs easy to get overconfident when you hit the ‚Äúdeploy to production‚Äù button. Your end-to-end integration and unit tests are passing, you‚Äôve covered QA across various surfaces and devices, and you‚Äôre moments away from getting that latest feature out the door. We all felt that way when we initially deployed the latest version of React to The New York Times website. Soon after our initial deployment of the new upgrades, we encountered a problem with some highly trafficked content, namely on a content-type we call ‚Äúembedded interactives‚Äù.</p><h3>Adapting Embedded Interactives to React¬†18</h3><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*yqosD8m1lIRtZRh4" /><figcaption>A custom embedded interactive built by our graphics developers: <a href="https://www.google.com/url?q=https://www.nytimes.com/article/hurricane-norma-baja-california.html&amp;sa=D&amp;source=docs&amp;ust=1719425021820790&amp;usg=AOvVaw0Wy7DlTRRsTiyaAjg31xhe">https://www.nytimes.com/article/hurricane-norma-baja-california.html</a></figcaption></figure><p>At The New York Times, we use custom embedded interactives rendered server-side with <a href="https://react.dev/reference/react-dom/components/common#dangerously-setting-the-inner-html">dangerouslySetInnerHTML</a>. These interactives have their own HTML, links, and scripts, running independently of the React tree. This allows editors and journalists to inject one-off, self-contained visual and <a href="https://www.nytimes.com/2023/10/27/business/kanye-west-adidas-yeezy.html">interactive elements</a> into our pages without having to alter or re-deploy core infrastructure. Embedded interactives are the key to some of our most impactful reporting, but they can also pose real challenges for developers.</p><p>A simplified example might look something like this (where script tags will modify the DOM as soon as the page has¬†opened):</p><pre>const embeddedInteractiveString = `<br>  &lt;div id=&quot;server-test&quot;&gt;server&lt;/div&gt;<br>  &lt;script&gt;<br>    document.addEventListener(&quot;DOMContentLoaded&quot;, () =&gt; {<br>      const serverTestElement = document.getElementById(&quot;server-test&quot;);<br>      serverTestElement.textContent = &quot;client&quot;;<br>    });<br>  &lt;/script&gt;<br>`;<br>return &lt;div dangerouslySetInnerHTML={{ __html: embeddedInteractiveString }} /&gt;;</pre><p>In this setup, the script modifies the ‚Äúserver-test‚Äù element‚Äôs content from ‚Äúserver‚Äù to ‚Äúclient‚Äù after page load. This works because browser-rendered scripts execute before React hydrates the DOM. It‚Äôs essentially a ‚Äúblack box,‚Äù where we trust the injected HTML and its scripts to behave as intended.</p><p><strong>The Hydration Hurdle</strong></p><p>Enter React 18, with its stricter hydration mismatch requirements. Under the new rules, any DOM modifications between the initial browser load and client-side hydration trigger a fallback to client-side rendering. In our example, even though the script tag modifies the ‚Äúserver-test‚Äù element before hydration, in a hydration mismatch, React will discard the server-rendered content and fall back to client-side rendering, essentially nullifying the script‚Äôs impact. In previous versions of React, even if there was a hydration mismatch, the React team opted to leave the version of the DOM in an <a href="https://github.com/facebook/react/issues/27848#issuecomment-1883713988">invalid state</a> as opposed to re-rendering entirely on the client, which is why we didn‚Äôt experience any issues in the¬†past.</p><p>In practice, what does this mean? Well, when rendering components on the client using the dangerouslySetInnerHTML prop, any bit of HTML containing a &lt;script&gt; tag inside of it will not run due to browser security considerations. This means that any embedded interactive that is re-rendered on the client due to hydration mismatches using the dangerouslySetInnerHTML prop will essentially render as if the javascript had never been executed. In our example above, the text content will change from ‚Äúserver‚Äù to ‚Äúclient,‚Äù but on a hydration mismatch, it will re-render as ‚Äúserver.‚Äù This ended up making some of our embedded interactives look wildly different from the expected¬†render.</p><p><strong>Expected:</strong></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*iL0ORQWV_QeM9ty5" /></figure><p><strong>Actual:</strong></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/778/0*kcvw3Bu9X7OLN-1x" /></figure><p><strong>So what do we¬†do?</strong></p><p>Given that React 18 was significantly more sensitive to hydration mismatches than React 16, we essentially had two choices in front of us. The first was to fix all potential hydration mismatches in our website. The second was to adapt embedded interactives to re-mount on the client as a fallback should a hydration mismatch occur. This left us in a bit of a dilemma. The New York Times has published millions of articles with hundreds of different components and tens of thousands of custom embedded interactives. Of course we wanted to fix all of our hydration mismatches, but how could we do so¬†safely?</p><p>In the end, we decided to tackle both problems at the same¬†time.</p><h3>Extracting and Executing Embedded Interactive Scripts¬†Manually</h3><p>We know that script tags, when added via the innerHTML prop (or during a client-side re-render), will not run automatically because of browser security considerations. So how do we get around this? Script tags will only run when manually appended or replaced as a childNode to another element in the DOM. This means that in order to properly run script tags, we must first extract and remove them from the interactive HTML and then<strong> append them back into the right location in the embedded interactive</strong> HTML when the component re-renders.</p><pre>// This function replaces script tags in generic html with empty placeholders.<br>// This allows us to replace the script tag reference in-place later on client-mount with the actual script.<br>export const addsPlaceholderScript = (scriptText, id, scriptCounter) =&gt; {<br>  let replacementToken = &#39;&#39;;<br>  let hoistedText = scriptText;<br><br>  replacementToken = `&lt;script id=&quot;${id}-script-${scriptCounter}&quot;&gt;&lt;/script&gt;`;<br>  hoistedText = hoistedText.replace(&#39;&lt;script&#39;, `&lt;script id=&quot;${id}-script-${scriptCounter}&quot;`);<br><br>  return {<br>    replacementToken,<br>    hoistedText,<br>  };<br>};<br><br>// This function extracts and removes `&lt;script&gt;` tags from an interactive HTML string<br>// and returns an object containing:<br>// - `scriptsToRunOnClient`: An array of script texts to be run on client-mount.<br>// - `scriptlessHtml`: The modified HTML string with scripts removed with empty script references.<br>export const extractAndReplace = (html, id) =&gt; {<br>  const SCRIPT_REGEX = /&lt;script[\s\S]*?&gt;[\s\S]*?&lt;\/script&gt;/gi;<br>  let lastMatchAdjustment = 0;<br>  let scriptlessHtml = html;<br>  let match;<br>  const scriptsToRunOnClient = [];<br>  let scriptCounter = 0;<br>  while ((match = SCRIPT_REGEX.exec(html))) {<br>    const [matchText] = match;<br>    if (matchText) {<br>      let hoistedText = matchText;<br>      let replacementToken = &#39;&#39;;<br>      ({ hoistedText, replacementToken } = addsPlaceholderScript(hoistedText, id, scriptCounter));<br>      scriptCounter += 1;<br>      const start = match.index - lastMatchAdjustment;<br>      const end = match.index + matchText.length - lastMatchAdjustment;<br>      scriptlessHtml = `${scriptlessHtml.substring(<br>        0,<br>        start<br>      )}${replacementToken}${scriptlessHtml.substring(end, scriptlessHtml.length)}`;<br>      scriptsToRunOnClient.push(hoistedText);<br>      lastMatchAdjustment += matchText.length - replacementToken.length;<br>    }<br>  }<br><br>  return {<br>    scriptsToRunOnClient,<br>    scriptlessHtml,<br>  };<br>};<br><br>// Run script on client<br>const runScript = (clonedScript) =&gt; {<br>    const script = document.getElementById(document.getElementById(`${clonedScript.id}`))<br>    script.parentNode.replaceChild(clonedScript, script);<br>}</pre><p>You may be asking, <em>why not keep scripts on the server and then re-run them on the client</em>? One reason why this is not possible in some scenarios is that some script tags declare variables globally instead of within a function closure. If you were to pre-render those script tags on the server and then re-run them on the client, you would encounter errors due to redeclaration of global variables, which is not possible.</p><p>That initial solution fixed many of our embedded interactives. Unfortunately, not every interactive plays well with arbitrarily-ordered script execution. Here‚Äôs where we navigate some¬†nuances:</p><p><strong>Script Load¬†Ordering</strong></p><p>Some interactive scripts, when appended back to the embedded interactive HTML, must be loaded in the correct order. Previous script execution strategies automatically assumed that all &lt;script&gt; tags had already been declared and pre-rendered on the server. Now that we are stripping out script tags and re-mounting them on the client, some inherent logic based on these principles are going to break. Let‚Äôs walk through an¬†example.</p><pre>&lt;script&gt;<br>  const <br>results = document.getElementById(&quot;RESULTS_MANIFEST&quot;).innerHTML.ELECTION_RESULTS;<br>  // do additional logic with results<br>&lt;/script&gt;<br>&lt;div&gt;<br>  Interactive DOM Content Goes here<br>&lt;/&gt;div&gt;<br>&lt;script id=&quot;RESULTS_MANIFEST&gt;{&quot;ELECTION_RESULTS&quot;: [&#39;result1&#39;, &#39;result2&#39;, ....]}&lt;/script&gt;</pre><p>In the scenario above, we have an initial script that searches for another script tag by ID and then utilizes some existing logic based on the innerHTML of the second script tag. In previous iterations, since script tags used to be pre-rendered on the server, there wouldn‚Äôt be any issue referencing a script tag by ID as the script tag would be available in the DOM by¬†default.</p><p>For optimal interaction, script execution needs to follow a specific order when re-appended to the DOM. This involves:</p><ol><li>Appending non-functional manifest scripts containing static data¬†first.</li><li>Executing scripts with src attributes asynchronously next.</li><li>Finally, appending and executing scripts with vanilla JavaScript in their innerHTML.</li></ol><p>This sequencing prevents scripts from referencing each other before they‚Äôre properly¬†loaded.</p><pre>// Parses the provided script tag, returning a priority for sorting.<br>// Priority 1: for JSON or other metadata content.<br>// Priority 2: for other vanilla JS or src contents<br>export const getPriority = template =&gt; {<br>  let priority;<br>  try {<br>    JSON.parse(template.innerHTML);<br>    priority = 1;<br>  } catch (err) {<br>    priority = 2;<br>  }<br>  return priority;<br>};<br><br><br>scripts.sort((a, b) =&gt; getPriority(a) - getPriority(b));</pre><h3>Immediate Performance Benefits</h3><p>After integrating these very fine-tuned‚Ää‚Äî‚Ääalmost surgical‚Ää‚Äî‚Äämanipulations of our embedded interactive code, we felt that we were able to safely release React 18 into the wild again. While we would never be able to extensively QA nearly 40,000 custom-created embedded interactives, we were able to rely on a few reusable templates that the graphics team often returns to. This let us validate specific behavior within our Svelte or Adobe Illustrator-based embedded interactives. In the long term, we‚Äôre committed to squashing our remaining hydration mismatches and achieving complete peace of mind. But in the short term, we were ready to push the ‚Äúdeploy‚Äù button¬†again.</p><p>Once we released the new features (and spent an hour nervously monitoring internal alerts for any issues), we saw almost immediate performance improvements.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*MtMev3JHmEYYsbne" /></figure><p>As you can see from this chart, INP scores in the p75 range dropped by roughly¬†30%!</p><p>Before the upgrade, one of our biggest challenges had been the frequent re-renders our news site went through as it loaded pages. That caused a poor user experience (and sub-par INP scores) when the user tried to interact with the still-loading page.</p><p>After the React 18 upgrade, our re-renders were cut essentially in¬†half!</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/980/0*_jaE9Y5ugAi1TfSS" /></figure><p>These two very visible and important improvements are the direct result of React 18‚Äôs automatic batching and concurrency features. This gave us a very clear and positive indication that we were moving in the right direction.</p><h3>Where We Go From¬†Here</h3><p>The integration of React 18 has already resulted in significant improvements for us, opening the door to a wealth of previously unavailable possibilities. We are now focused on exploring the potential benefits of new features such as <a href="https://react.dev/reference/react/startTransition">startTransition</a> and <a href="https://react.dev/reference/rsc/server-components">React Server Components</a>. Our core intention is to continuously bring our INP scores down and improve overall functionality. However, we‚Äôre mindful of questions we still need to answer about these enhancements. For now, our primary commitment is ensuring the stable and reliable performance of the current React version we¬†use.</p><p>Based on our results on the news site, we felt confident to pursue upgrades for some of our other sites, where we saw similar performance gains. We were able to get our INP scores out of the ‚Äúpoor‚Äù zone before <a href="https://developers.google.com/search/blog/2023/05/introducing-inp">Google‚Äôs March deadline</a>, and saw no negative SEO results when it became part of their search algorithm. We like to think that our readers are enjoying the slightly-snappier experience. And our newsroom continues to put out <a href="https://www.nytimes.com/2024/03/24/world/asia/india-sugar-cane-fields-child-labor-hysterectomies.html">powerful</a> and <a href="https://www.nytimes.com/2024/05/02/arts/music/song-copyright-sheet-music-ed-sheeran-marvin-gaye.html">interesting</a> interactives every day, without having to give their rendering framework a second¬†thought.</p><p><em>Ilya Gurevich is a Senior Software Engineer with over 10 years of experience in both startup and enterprise environments. He is currently part of the core Web Platforms Team since joining the Times in 2019. He manages the centralized NodeJS platform that powers the main site, and also works on the developer experience, tooling, and build process for a multi-workspace monorepo with over a hundred active contributors. Previously, he worked on the Times‚Äô cutting-edge real-time collaborative text editor tailored for reporters and¬†editors.</em></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=d6f91a7c5af8" width="1" height="1" alt=""><hr><p><a href="https://open.nytimes.com/enhancing-the-new-york-times-web-performance-with-react-18-d6f91a7c5af8">Enhancing The New York Times Web Performance with React 18</a> was originally published in <a href="https://open.nytimes.com">NYT Open</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
    </channel>
</rss>