<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:cc="http://cyber.law.harvard.edu/rss/creativeCommonsRssModule.html">
    <channel>
        <title><![CDATA[Stories by Pinterest Engineering on Medium]]></title>
        <description><![CDATA[Stories by Pinterest Engineering on Medium]]></description>
        <link>https://medium.com/@Pinterest_Engineering?source=rss-ef81ef829bcb------2</link>
        <image>
            <url>https://cdn-images-1.medium.com/fit/c/150/150/1*iAV-apeVpCJ1h6Znt1AzCg.jpeg</url>
            <title>Stories by Pinterest Engineering on Medium</title>
            <link>https://medium.com/@Pinterest_Engineering?source=rss-ef81ef829bcb------2</link>
        </image>
        <generator>Medium</generator>
        <lastBuildDate>Fri, 19 Dec 2025 22:11:59 GMT</lastBuildDate>
        <atom:link href="https://medium.com/@Pinterest_Engineering/feed" rel="self" type="application/rss+xml"/>
        <webMaster><![CDATA[yourfriends@medium.com]]></webMaster>
        <atom:link href="http://medium.superfeedr.com" rel="hub"/>
        <item>
            <title><![CDATA[LLM-Powered Relevance Assessment for Pinterest Search]]></title>
            <link>https://medium.com/pinterest-engineering/llm-powered-relevance-assessment-for-pinterest-search-b846489e358d?source=rss-ef81ef829bcb------2</link>
            <guid isPermaLink="false">https://medium.com/p/b846489e358d</guid>
            <category><![CDATA[machine-learning]]></category>
            <category><![CDATA[search]]></category>
            <category><![CDATA[pinterest]]></category>
            <category><![CDATA[experimentation]]></category>
            <category><![CDATA[engineering]]></category>
            <dc:creator><![CDATA[Pinterest Engineering]]></dc:creator>
            <pubDate>Wed, 10 Dec 2025 20:02:11 GMT</pubDate>
            <atom:updated>2025-12-10T20:02:11.655Z</atom:updated>
            <content:encoded><![CDATA[<p>Han Wang | Machine Learning Engineer; Alex Whitworth | Staff Data Scientist; Pak Ming Cheung | Sr. Staff Machine Learning Engineer; Zhenjie Zhang | Sr. Staff Machine LearningÂ Engineer</p><h3>Introduction</h3><p>Search relevance measures how well search results align with a userâ€™s search query. For personalized search systems, itâ€™s important to ensure that displayed content is pertinent to the userâ€™s information needs, rather than over-relying on the userâ€™s past engagement. At Pinterest Search, we track whole-page relevance in online A/B experiments to evaluate new ranking models and ensure a high-quality user experience.</p><p>Relevance measurement typically relies on human annotations, but is limited by the low availability of human labels and the high marginal cost of generating them. This led to measurement designs and sample sizes that could only detect significant topline metric movements, but were insufficient to measure heterogeneous treatment effects or small toplineÂ effects.</p><p>In this blog, we present our methodology at Pinterest Search to scale the labeling capabilities with LLMs and address these bottlenecks. We fine-tune open-source LLMs on relevance prediction tasks using human-annotated labels, then utilize the fine-tuned LLMs to evaluate the ranking results across experimental groups in online A/B experiments. This approach not only significantly reduces labeling costs and improves evaluation efficiency, but also unlocks opportunities to further improve metric quality by scaling up the query sets and refining the samplingÂ design.</p><h3>Methodology</h3><p>At Pinterest, we measure the semantic relevance between queries and Pins using a 5-level guideline: Highly Relevant (L5), Relevant (L4), Marginally Relevant (L3), Irrelevant (L2), and Highly Irrelevant (L1). We use this guideline to measure the whole-page relevance for our searchÂ system.</p><h3>Fine-tuned LLMs as Relevance Model</h3><p>We use a cross-encoder model architecture to predict a Pinâ€™s relevance to a given query, as illustrated in Figure 1. We fine-tune open-source LLMs on human-annotated data to optimize their performance on relevance prediction task. To support search queries and Pins across multiple languages, we leveraged multilingual LLMs to take advantage of their cross-lingual transfer capabilities. We formalize the relevance prediction as a multiclass classification problem based on the 5-scale relevance guideline, minimizing the point-wise cross-entropy loss during training.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*ibgqQEssW3lmqg1Qsoq7uw.png" /><figcaption><strong>Figure 1</strong>: The cross-encoder architecture for LLM-based search relevance model. Take the encoder language models (e.g., <a href="https://arxiv.org/abs/1810.04805">BERT</a>-based models) for illustration.</figcaption></figure><p>To effectively represent each Pin for relevance prediction, we leverage a comprehensive set of textual features, including Pin titles and descriptions, <a href="https://arxiv.org/abs/2201.12086">BLIP</a> image captions, linked page titles and descriptions, user-curated board titles where the Pin has been saved, and highly-engaged query tokens associated with the Pin. These features together form a robust text representation crucial for accurate relevance assessment.</p><p>We experiment with various language models, including <a href="https://huggingface.co/google-bert/bert-base-multilingual-cased">multilingual BERT-base</a>, <a href="https://huggingface.co/google-t5/t5-base">T5-base</a>, <a href="https://huggingface.co/microsoft/mdeberta-v3-base">mDeBERTa-V3-base</a>, <a href="https://huggingface.co/FacebookAI/xlm-roberta-large">XLM-RoBERTa-large</a>, and <a href="https://huggingface.co/meta-llama/Meta-Llama-3-8B">Llama-3â€“8B</a>. The comparative performance of various LLMs and ablation studies on Pin text features can be found in a <a href="https://medium.com/pinterest-engineering/improving-pinterest-search-relevance-using-large-language-models-4cd938d4e892">previous blog</a>. We then use this fine-tuned search relevance model to generate 5-dimensional relevance scores and use the label corresponding to the highest score (argmax) for relevance assessment.</p><h3>Stratified SamplingÂ Design</h3><p>LLM labeling significantly reduces relevance labeling costs as well as labeling time, which enables much larger sampling designs. Therefore, we propose a stratified query sampling design that enables measurement of heterogeneous treatment effects and reduces minimum detectable effects (MDEs) by an order of magnitude.</p><p>Stratification plays an important role in sampling-based measurement. First, stratification ensures the sample population is representative of the whole population. In addition, if the strata are chosen such that each stratum is relatively homogeneous, variance reduction can be achieved. We adopted the in-house query-to-interest model based on <a href="https://arxiv.org/abs/1910.01108">DistilBERT</a> combined with the popularity segment, a measure of how many users issue each specific query, to determine the strata. Prior to LLM labeling, stratified query sampling with human annotations was impractical, as it required a large number of queries to adequately represent each fine-grained stratum.</p><p>We evaluate the impact of these changes on experiment sensitivity by evaluating the MDE for our experimentation system. The MDE is the smallest change in a metric that an experiment can reliably detect given the sample size, statistical power (Î²= 0.8), and significance level (Î±=0.05) chosen for the test. It can be derived asÂ below</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*1QAT3UeCSgm8LDYF1qui0w.png" /></figure><p>Since the typical experiment for most online platforms has a small effect, achieving small MDEs is a critical factor in team velocity and shipping new features to our users. Before the introduction of LLM labeling, relevance measurement had large MDEs (e.g. 1.3%-1.5%). These large MDEs were primarily the result of the constraints on our sampling designs imposed by the high cost and time consumption of human labeling. The introduction of LLM labeling enabled us to redesign our sampling approach. We increased our sample sizes, moved from simple random sampling (SRS) to stratified sampling, and now use a stratified sampling estimator. <a href="https://www.jstor.org/stable/2342192">Optimal allocation</a> is used to allocate sample units to strata. These changes enabled us to reduce our MDEs to â‰¤Â 0.25%.</p><p>The MDE reduction can be expressed in terms of reduction in variance and increased sample size. We present these results in Table 1. The vast majority of reduction comes from the variance reduction due to stratification. This is consistent with prior findings at Pinterest that most variance in relevance occurs across queries. Previous work has found substantial variation in relevance due to query interest and query popularity.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*FjsF9LTCIWH1t-Shdjuudw.png" /><figcaption><strong>Table 1: </strong>Improvement in metric sensitivity (MDE).</figcaption></figure><h3>Relevance Measurement withÂ LLMs</h3><p>To measure the relevance impact of an A/B experiment on search ranking, we take a stratified sample of paired search queries from control and treatment experiment groups, ensuring that the sample is representative of overall user usage. The use of paired samples blocks between-query differences, which is an important source of variation in experiment measurement.</p><p>For each query in our paired sample, we retain the top K search results and generate LLM-based relevance labels. We then compute sDCG@K for each query and aggregate query-level metrics to derive topline experiment metrics. The sDCG@K metric is a variant of the standard nDCG@K, where we assume an infinite supply of highly relevant (L5) documents for sDCG@K computation (see Equation 2). We use K=25 throughout our evaluation.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*9em8zYMfFyeVLZAhrzlBFA.png" /></figure><p>Lastly, we calculate heterogeneous effects by query popularity and query interest (e.g. beauty, womenâ€™s fashion, art, etc), utilizing a <a href="https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/j.2517-6161.1995.tb02031.x">Benjamini-Hochberg procedure</a> to control the false discovery rate. The LLM-based relevance measurement procedure at Pinterest Search is illustrated in FigureÂ 2.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*asEJx-QzUzajryLEixRPhw.png" /><figcaption><strong>Figure 2</strong>: Components of LLM-based relevance measurement at Pinterest Search.</figcaption></figure><h3>Results</h3><p>We use XLM-RoBERTa-large as the LLM backbone for our relevance model. The model is lightweight yet delivers high-quality predictions. Inference runs on a single A10G GPU, allowing us to label 150,000 rows within 30 minutes. While the Llama-3â€“8B model offers slight improvement in accuracy, its inference time and cost increase 6 times. Therefore, we select XLM-RoBERTa-large as it offers a good balance between prediction quality and inference efficiency. The validation results are presented below.</p><h4>Alignment with HumanÂ Labels</h4><p>We conducted a rigorous validation of the metrics derived from LLM labeling. On Pin-level evaluation, LLM-generated labels and human labels yield an exact match rate of 73.7%, with 91.7% of ratings deviating at most by 1 point. These results underscore the high alignment between the relevance labels produced by LLMs and those from human annotators. To measure alignment between LLMs and human labels, we also compute and report the rank-based correlation Kendallâ€™s Ï„ and Spearmanâ€™s Ï to assess the correlation between the two rankings at query-level sDCG@K metric. To understand the performance on queries with different popularity, we also categorize the queries into 4 popularity segments based on search volume: head, torso, tail, and single. The results are summarized in Table 2. We achieve Kendallâ€™s Ï„&gt;0.5 and Spearmanâ€™s Ï&gt;0.65 for all query popularity segments, indicating a strong alignment across all segments.</p><p>In addition to Kendallâ€™s Ï„ and Spearmanâ€™s Ï, we also validate the query-level sDCG@K error distribution. Here, the error refers to the difference between the sDCG@K metric derived from LLM labels and human labels. According to Table 2, the overall error is below 0.01, with the 10-th and 90-th percentiles falling within the range of [-0.1, 0.1]. We also visualize the error distribution in Figure 3. The error is tightly centered around 0, indicating its negligible magnitude and that the average bias will approach 0 as the size of the query setÂ grows.</p><p>For experimental evaluation, we need to calculate the metric difference between the control and treatment groups. Therefore, we also validate how well these metric differences align in paired comparison. As shown on the right-hand side of Figure 3, the errors in paired differences are even more centered around 0 with lighter tails, indicating that LLM-based labeling provides highly reliable estimates of paired differences for A/B experiment assessment.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*W8ZQ3gKmT5uj7dsV48_ZAw.png" /><figcaption><strong>Table 2</strong>: Query-level LLM vs human labels alignment for different query segments in US market relevance evaluation.</figcaption></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*aAX0xljm_8wo450VCqlcDQ.png" /><figcaption><strong>Figure 3</strong>: Query-level bias distribution for single group (left) and paired differences (right) in US market relevance evaluation.</figcaption></figure><h4>Performance on Non-English Queries</h4><p>We fine-tuned multilingual LLMs on human-annotated data, with the majority of query-Pin pairs in English. As a result, careful validation is required for non-English queries to extend LLM-based relevance assessment to those queries. For this analysis, we focus on France (FR) and Germany (DE)Â markets.</p><p>The query-level metric alignment is summarized in Table 3. The overall Kendallâ€™s Ï„ and Spearmanâ€™s Ï are approximately 0.47 and 0.61, respectively. While these rank-based correlations are lower than those observed for English queries, they are still considered strong according to existing literature. The distribution of query-level metric errors is shown in Figure 4. Similar to the results of the US market, the errors are tightly concentrated around 0 for both countries, indicating a low average bias, with an even smaller bias for paired differences. These results provide confidence that the LLM-based relevance assessment is also suitable for non-English queries. Expanding relevance evaluation to countries beyond the US leads to further reductions in labeling costs and improvements in evaluation efficiency.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*u4KO0xUTSyYm5FpFstyJLA.png" /><figcaption><strong>Table 3</strong>: Query-level LLM vs human labels alignment for different query segments in France (FR) and Germany (DE) markets relevance evaluation.</figcaption></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*6fCQi6wnOWeuEjTVRpKW3A.png" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*nh4DLG3eLdkeCyXcUnTzVQ.png" /><figcaption><strong>Figure 4</strong>: Query-level bias distribution for single group (left) and paired differences (right) in France (top) and Germany (bottom) markets relevance evaluation.</figcaption></figure><h3>Summary</h3><p>In this work, we explore the use of LLM-based relevance labeling to generate query-level relevance metrics for online A/B experiments evaluation. We demonstrate that fine-tuned LLMs achieve low bias on query-level ğ‘ ğ·ğ¶ğº@ğ¾ metrics and paired differences. Transition to LLM-based relevance assessment enables us to scale up the evaluation query set and redesign the sampling strategy to improve the quality of relevance metrics for online experiment evaluation. We have successfully deployed the LLM-based relevance assessment at Pinterest Search, significantly reducing the manual annotation costs and turnaround time, while achieving an order of magnitude reduction in MDEs for improved detection of relevance shifts. For more details, please refer to our <a href="https://arxiv.org/abs/2509.03764">fullÂ paper</a>.</p><h3>Future Work</h3><p>We will explore using Visual Language Models (VLMs) to better leverage raw images for relevance prediction. Additionally, the observed performance gap with non-English queries highlights opportunities to further improve the multilingual capabilities of our LLM-based relevance model. We leave it for futureÂ work.</p><h3>Acknowledgement</h3><ul><li>Search: Maggie Yang, Mukuntha Narayanan, Jinfeng Rao, Krishna Kamath, Kurchi SubhraÂ Hazra</li><li>Relevance Measurements Tooling: Maria Alejandra Morales Gutierrez (former), Miguel Madera, Pedro Sanchez, Jorge Amigon, Francisco Navarrete</li></ul><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=b846489e358d" width="1" height="1" alt=""><hr><p><a href="https://medium.com/pinterest-engineering/llm-powered-relevance-assessment-for-pinterest-search-b846489e358d">LLM-Powered Relevance Assessment for Pinterest Search</a> was originally published in <a href="https://medium.com/pinterest-engineering">Pinterest Engineering Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[How Pinterest Built a Realâ€‘Time Radar for Violative Content using AI]]></title>
            <link>https://medium.com/pinterest-engineering/how-pinterest-built-a-real-time-radar-for-violative-content-using-ai-d5a108e02ac2?source=rss-ef81ef829bcb------2</link>
            <guid isPermaLink="false">https://medium.com/p/d5a108e02ac2</guid>
            <category><![CDATA[ai]]></category>
            <category><![CDATA[engineering]]></category>
            <category><![CDATA[pinterest]]></category>
            <category><![CDATA[foundational-model]]></category>
            <category><![CDATA[trust-and-safety]]></category>
            <dc:creator><![CDATA[Pinterest Engineering]]></dc:creator>
            <pubDate>Mon, 08 Dec 2025 17:02:56 GMT</pubDate>
            <atom:updated>2025-12-10T18:46:10.203Z</atom:updated>
            <content:encoded><![CDATA[<p><em>Faisal Farooq | Sr. Director Trust Engineering; Aravindh Manickavasagam | Staff Technical Program Manager; Attila Dobi | Sr. Staff Data Scientist</em></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/738/1*NhUi6IO44OyZJSSlX8gSaQ.png" /></figure><p>People come to Pinterest to find ideas they feel good about. To keep that experience safe, we need to know not just what gets reported, but what people actually saw. Thatâ€™s what we call prevalence: the percentage of all views, on a given day, that went to content that violates a policy. Prevalence complements reporting by covering its blind spots, helping us spot underâ€‘reported harms, track trends, and tell whether interventions work.</p><h3><strong>Why Prevalence Matters</strong></h3><p>Historically, our Trust &amp; Safety teams leveraged multiple indicators to understand the extent of policy violating content on the platform. â€œIn-app user reportsâ€ confirmed by human reviewers served as a key indicator alongside other measures of potential harm. While user reports are invaluable because they come directly from our community, a reportâ€‘only metric is incomplete.</p><p><strong>What a Reportsâ€‘only View Canâ€™t TellÂ Us</strong></p><ul><li>Some harms are underâ€‘reported (for example, selfâ€‘harm) due to stigma or sensitivity.</li><li>Users who seek harmful content donâ€™t report what theyâ€™reÂ seeking.</li><li>Rare categories generate few reports, so we lack statistical power to track progress or detect emergingÂ threats.</li><li>Scaling report handling with human review adds cost andÂ latency.</li></ul><p>Prevalence fills those gaps. Instead of waiting for reports, we measure what people actually saw each day by sampling based on user impressions and labeling them at scale. This gives us a stable, statistically powered view across a broader spectrum of policies, independent of enforcement thresholds, so that we can monitor risks, set goals, track progress, and actÂ sooner.</p><p><strong>Addressing Historical Challenges with Prevalence Measurement</strong></p><p>Historically, measuring prevalence was expensive because it relied on human review. We could only run large, platformâ€‘representative studies infrequently (roughly every six months and not always on a fixed cadence), so pre/post comparisons after interventions were slow and often hard to trust. Secondly, human review can oftentimes be unstable. In order to reach reliable and stable decisions, we required at least two independent reviewers per item, plus adjudication on disagreements, which further increased cost andÂ latency.</p><p>To address these cost and latency constraints while enabling more frequent and reliable measurement, we built an AI-assisted workflow that allows us to focus on measuring the daily user experience.</p><h3><strong>What WeÂ Measure</strong></h3><p>We estimate how often violating content is seen vs. how much is posted on the platform because â€œimpact comes from exposure.â€ A single violating Pin might be posted once but seen a million times, or not seen at all. Measuring the share of views that went to policyâ€‘violating content better reflects what people actually experienced on Pinterest. We report this daily, with 95% confidence intervals to show precision.</p><p><strong>Metric</strong>: prevalence on a given day is: (<strong># views of content that violates a given policy</strong>) / (<strong># totalÂ views</strong>).</p><p>For example, if 10 out of 100,000 views in a sample are policyâ€‘violating, the estimated prevalence is 0.01% for that policy area thatÂ day.</p><p><strong>Policies andÂ segments</strong></p><p>The measurement can be further broken down so that the team can act onÂ it.</p><ul><li>Policy area, e.g. Adult Content, Selfâ€‘harm, Graphic Violence.</li><li>Subâ€‘policy, e.g. within Adult Content we separate nudity from explicit sexualÂ content.</li><li>Surface, e.g. Homefeed vs. Search vs. RelatedÂ Pins.</li><li>Other segments, where appropriate (such as content age, geography or user ageÂ buckets)</li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*v_F5SC-wIn0V5BJ-Q5wBUg.png" /><figcaption>Figure 1: Illustrative Example <strong>[not a real production chart]</strong>â€Šâ€”â€ŠMonthly average of adult content prevalence throughout 2024 and 2025. The tool emoji (ğŸ› ï¸) highlights enforcement updates. The metric is responsive to product interventions.</figcaption></figure><h3>Methods at aÂ Glance</h3><h4>Sampling</h4><ul><li>We sample images from the daily user impressions stream, using risk scores from our production enforcement models to improve sampling efficiency, not as labels or inclusion criteria <em>(the same models that remove policyâ€‘violating content)</em>.</li><li><strong>What if some content is missing scores? </strong>As a failsafe, missing scores are imputed with the dayâ€™s median so fresh content stays inÂ frame.</li><li><strong>Inclusion probabilities:</strong> For each content unit i we assign a sample probability</li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*L2tsIUDywm_kPhKUsD60OA.png" /><figcaption>Equation 1: where by default <em>Î³ </em>= 1,<em> Î½ </em>= 1 but both are tunable. Setting <em>Î³ </em>= 0 yields impression weighted sampling; <em>Î³ </em>= <em>Î½ </em>= 0 yields random sampling. The corresponding, normalized sampling probably is Ï€áµ¢Â âˆ<em>w</em>áµ¢</figcaption></figure><ul><li><strong>Implementation detail</strong>: We implement this with weighted reservoir sampling using an index definedÂ by:</li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*nvQiws1yMNb5hSNZXMZwpA.png" /><figcaption>Equation 2: <em>w</em>áµ¢ is the sample probability for content_i from equation 1.<br>Uáµ¢ represents a uniform random number from (0,1]. The parameters <em>Î³</em>, <em>Î½</em> are tunable, set toÂ 1.</figcaption></figure><ul><li><strong>Why ML-assisted sampling, and why it stays unbiased:<br>â€¢ Scores as a lens, not the ruler: </strong>Production risk scores focus label budget on highâ€‘risk, highâ€‘exposure candidates; the estimator then reâ€‘weights to remove that lensing so the prevalence statistic reflects impressions, not the modelâ€™s threshold. This decouples measurement from enforcement.<br><strong>â€¢ Design Consistent Estimator:</strong> We use inverseâ€‘probability weighting to keep daily impressionâ€‘weighted prevalence designâ€‘consistent and comparable over time. In practice we use the Hansenâ€“Hurwitz ratio for PPSâ€‘withâ€‘replacement and the Horvitzâ€“Thompson ratio for withoutâ€‘replacement, which remain unbiased even if thresholds or calibration drift.</li></ul><h4>Labeling atÂ Scale</h4><ul><li>We bulkâ€‘label the sample with a multimodal LLM (vision + text) using prompts reviewed by policy subject matter experts (SMEs). The system logs decisions, brief rationales, and full lineage (policy version, prompt, and model IDs) for auditability.</li><li>Calibration and de-biasing are critical to maintaining measurement accuracy at scale. We employ human validation of strategically sampled subsets immediately after launch as part of post-launch metric evaluation. These calibration samples are designed to capture edge cases and potential AI blind spots that could introduce systematic bias into our prevalence estimates. Before launching to production the LLM must meet a minimum decision quality requirement relative to human review. Additionally, LLM + prompt quality is periodically checked against Subject Matter Expertâ€‘labeled gold sets (ground truth) to detect model drift and ensure the labeler/classifier remains accurate and aligned with current violations policy. This continuous monitoring process allows us to maintain measurement conviction as content patterns and policy interpretations evolve.</li><li>LLMâ€‘assisted labeling lets us run large, daily probability samples across more policy areas at a fraction of the latency (15x faster) and orders of magnitude lower operational cost than a humanâ€‘only workflow at comparable decision quality-while also preserving statistical validity and governance.</li></ul><h3>System Overview</h3><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*TBPSAEXj27_7bp4-gdhLMA.png" /><figcaption>Figure 2: Illustration of the prevalence measurement workflow.</figcaption></figure><h3>Implementation Notes</h3><ul><li><strong>Inputs:</strong> Engagement at the entity Ã— day level (e.g., impressions, Pin clicks, hides, reports) plus the latest production risk scores, used only as auxiliary signals. Missing scores are imputed with the dayâ€™sÂ median.</li><li><strong>Sampling: </strong>We use a weighted reservoir sampler that gives more chances to items with higher impressions and higher risk scores, while still preserving an unbiased estimate when we reweight the sample [<a href="https://utopia.duth.gr/~pefraimi/research/data/2007EncOfAlg.pdf">1</a>]. This emulates probability proportionalâ€‘toâ€‘size with replacement (PPSWR) and yields unbiased estimators when paired with the right weights [<a href="https://drive.google.com/file/d/1rd9Lqw6GxQ2h5vU3kdRvi278KzvYSIIC/view?usp=sharing">2</a>]. A toggle also supports pure random sampling for validation studies.</li><li><strong>Labeling:</strong> The LLM returns any policyâ€‘defined label hierarchy (e.g., {safe, not_safe, unsure}). The workflow records token usage and perâ€‘run cost for budgeting and is modelâ€‘agnostic for future flexibility.</li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*OHgA_tOUYL7cyW99RuA_Rg.png" /><figcaption>Figure 3: Examples of how the LLM labels an image according to the Adult ContentÂ policy.</figcaption></figure><ul><li><strong>Estimation:</strong> We compute overall prevalence and pivots, persist estimates, weights, and labels to production stores, and write diagnostics/lineage for audits. The dashboard surfaces the point estimate, 95% CI, CI width, and effective sampleÂ size.</li></ul><h3>Dashboard andÂ Alerting</h3><ul><li><strong>Cards:</strong> Daily prevalence with 95% CI, sample positive rate (for monitoring sampling efficiency), auxiliary score distributions for context, and run health/lineage (prompt/model/taxonomy/metric versions).</li><li><strong>Pivots</strong>: The dashboard owners can slice prevalence by policy area, surface (Homefeed, Search, etc.), and selected subâ€‘policies.</li><li><strong>Validation:</strong> A random subsample of labels routes to an internal human validation queue for continual checks of the AIâ€™s decision quality; a config switch enables pure random sampling to sanityâ€‘check assumptions.</li></ul><h3>Impact</h3><p>AI-assisted prevalence measurement has transformed how we understand and respond to platform safety challenges:</p><ul><li><strong>Proactive Risk Detection and Response:</strong> We now have continuous measurement without historical blind spots. Dramatically faster labeling turnaround enables real-time monitoring, providing clearer understanding of platform risk and user experience. This leads to faster root cause analysis when issues emerge and more proactive identification of emerging trends before they scale.<br><strong>â€¢ Faster Product Iteration and Data Driven Policy:</strong> Prevalence measurement provides immediate insight into how product launches impact platform trust and safety, enabling us to course-correct quickly and build more effective enforcements and interventions. The system also creates a valuable feedback loop for policy development and prompt tuning, helping us understand how clear and enforceable our policies are in practice.<br><strong>â€¢ Strategic Decision Making Beyond Monitoring:<br>â€Šâ€”â€ŠBenchmarking and goal setting</strong>: establishing measurable targets for platform health and tracking progress<br>â€Šâ€”â€Š<strong>Cross-team alignment</strong>: providing shared metrics that unite product, policy, and enforcement teams around common objectives<br>â€Šâ€”â€Š<strong>Data-driven resource allocation:</strong> directing enforcement efforts where theyâ€™ll have the greatest impact<br>â€Šâ€”â€Š<strong>Precise intervention measurement:</strong> with reliable prevalence baselines established, we can now A/B test enforcement strategies with statistical confidence to optimize policy interventions based on measurable outcomes</li></ul><h3>Constraints and Tradeâ€‘offs</h3><ul><li><strong>Rare categories can have wide daily CIs:</strong> we adapt Î³ [Equation 1], stratify, or pool to weekly as needed; the dashboard exposes CI width so owners can budgetÂ labels.</li><li><strong>Policy/prompt drift:</strong> Prompt and data versioning + selective time period based label backfills keep the series interpretable.</li><li><strong>LLM decision quality stability:</strong> LLM decision quality stability is required for metric conviction. We regularly run randomâ€‘sampling validations and monitor LLM outputs to detect and address potential decision qualityÂ drift.</li><li><strong>Cost guardrails: </strong>Token usage and perâ€‘run cost per model/metric variant are tracked and periodically evaluated for cost efficiency.</li></ul><h3>Future Focus</h3><ul><li><strong>Pivots</strong>: Expand pivoting ability to viewer country, age,Â etc.</li><li><strong>Cost optimization:<br>â€Šâ€”â€ŠMulti step LLM labeling</strong> <strong>process</strong>: A first layer decides if labels are safe/unsafe using a short prompt. The second layer labels the unsafe items against a longer, comprehensive policy-prompt.<br>â€Šâ€”â€Š<strong>LLM Fine Tuning: </strong>LLMâ€™s fine tuned with SME labeled data have yielded improved performance during evaluations.</li><li><strong>Human-in-the-loop denoising/debiasing</strong>: Create an active denoising/debiasing system leveraging human review and SME labeling in the loop (prompt tuning, fineâ€‘tuning, and label correction). The objective is to minimize LLM-human bias and reduce variance introduced by suboptimal decisionÂ quality.</li><li>Further generalizing the pipeline for companyâ€‘wide measurement applications, further refining metric versioning and validation, developing prevalence based A/B testing guardrails.</li></ul><h3>Acknowledgements</h3><p>Standing up a Trust and Safety prevalence radar has required sustained crossâ€‘functional work across Trust &amp; Safety teams including engineering, product and operations. Hereâ€™s an incomplete list of folks and teams who helped us design, ship, and operationalize daily prevalence:</p><ul><li>Data Science: Xiaohan Yang, Zehao Xu, Yuqi Tian, Minli Zang, Huan Yu, Robert Paine, Kevin Oâ€™Sullivan, Wenjun Wang, BenjaminÂ Thompson</li><li>Risk Intelligence: Jenny Bi, Mairead Oâ€™Doherty, Antons Tocilins-Ruberts</li><li>Product: Monica Bhide, Prachi Wadekar, DanÂ Towne</li><li>TPM: Nayan Patel, Helene Labriet-Gross</li><li>Operations: Abby Beckman, Jessica Flora, Aaron Stein-Chester, CarolÂ Davis</li><li>Policy: Stanley Washington, Francesca Anzola</li><li>Engineering: Vikram Deshpande, Ahmed Fayez, NamanÂ Makhija</li><li>Leadership: Faisal Farooq, Andrey Gusev, Sriram Subramanian</li></ul><p><strong>References:</strong></p><ul><li>[Reservoir sampling trick] <a href="https://utopia.duth.gr/~pefraimi/research/data/2007EncOfAlg.pdf">Weighted Random Sampling (2005; Efraimidis, Spirakis)</a></li><li>[Hansen Hurwitiz 1943] <a href="https://drive.google.com/file/d/1rd9Lqw6GxQ2h5vU3kdRvi278KzvYSIIC/view?usp=sharing">On the theory of sampling from finite populations.pdf</a></li></ul><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=d5a108e02ac2" width="1" height="1" alt=""><hr><p><a href="https://medium.com/pinterest-engineering/how-pinterest-built-a-real-time-radar-for-violative-content-using-ai-d5a108e02ac2">How Pinterest Built a Realâ€‘Time Radar for Violative Content using AI</a> was originally published in <a href="https://medium.com/pinterest-engineering">Pinterest Engineering Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Improving Quality of Recommended Content through Pinner Surveys]]></title>
            <link>https://medium.com/pinterest-engineering/improving-quality-of-recommended-content-through-pinner-surveys-eebca8a52652?source=rss-ef81ef829bcb------2</link>
            <guid isPermaLink="false">https://medium.com/p/eebca8a52652</guid>
            <category><![CDATA[engineering]]></category>
            <category><![CDATA[surveys]]></category>
            <category><![CDATA[pinterest]]></category>
            <category><![CDATA[pinner-experience]]></category>
            <category><![CDATA[machine-learning]]></category>
            <dc:creator><![CDATA[Pinterest Engineering]]></dc:creator>
            <pubDate>Fri, 05 Dec 2025 20:02:12 GMT</pubDate>
            <atom:updated>2025-12-05T20:02:12.974Z</atom:updated>
            <content:encoded><![CDATA[<p>Rudraksh Kapil | Machine Learning Engineer I; Michal Giemza | Senior Machine Learning Engineer; Devan Srinivasan | Machine Learning Engineering Intern; Leif Sigerson | Senior Data Scientist; Stephanie Chen | Staff Quantitative Product Researcher; Wendy Matheny | Senior Lead Public Policy Manager; Jianjin Dong | Engineering Manager II; Qinglong Zeng | Senior Engineering Manager</p><h3>Introductory Summary</h3><p>In 2023 Pinterest became the Founding Signatory of the Inspired Internet Pledgeâ€Šâ€”â€Špublicly stating our vision to adhere to three principles: (1) tuning for wellbeing, (2) listening to and acting on what we hear from users, and (3) sharing what we learn about making the internet a safer and healthier place for all, especially teens.</p><p>Now, at the end of the second full year of our work with the Inspired Internet Pledge, weâ€™re pleased to share more about what weâ€™re learning from Pinner (a.k.a. user) surveys and how we incorporate their feedback to improve content quality on our platform.</p><p>While much of this blog will get into the weeds of how we design surveys, interpret the data we get from them, and technical details on how we train a machine learning model to understand generic Pinner perception, at the heart of all of this is Pinterestâ€™s commitment to â€œPut Pinners First.â€ Our work demonstrates a win/win for both Pinners and the business, and allows us to do good while doingÂ well.</p><h3>Background</h3><p>At Pinterest, we want people to discover high-quality contentâ€Šâ€”â€Šcontent that makes them feel good when they see it, inspires them to keep exploring, and ultimately drives fulfilling, long-term engagement. But itâ€™s challenging to construct guidelines on what exactly â€œhigh qualityâ€ content is without understanding the average Pinnerâ€™s notion of quality. We know that when we only show content thatâ€™s engaging, it tends to promote low-quality â€œclickbaitâ€ with limited long-term engagement.</p><p>Unsurprisingly, an effective way to understand Pinnersâ€™ perception of quality is to ask them directly. Surveys are a way for Pinners to tell us exactly what they think and for us to <a href="https://medium.com/pinterest-engineering/healthier-personalization-with-surveys-65177cf9bea8">build that intentionality directly into our platform</a>. If we use Pinner surveys to teach our recommendation systems to promote highly-rated content, we expect people will react positivelyâ€Šâ€”â€Šwith both good feelings and as a result, with action, repins, and saves. Experts in the industry have also called for using surveys more for training recommendation systems [<a href="https://arxiv.org/pdf/2207.10192">Stray et. al., 2022</a>, <a href="https://arxiv.org/abs/2402.06831">Cunningham et. al., 2024</a>], rather than optimizing purely for engagement. <a href="https://medium.com/pinterest-engineering/the-field-guide-to-non-engagement-signals-a4dd9089a176">Not all engagement is good</a>, so the latter approach can mislead the system into promoting low-quality or even harmful content. Surveys provide an excellent avenue for de-biasing the system, allowing us to understand content quality and ensure that the engagement we reward comes from high quality content. In this blog post weâ€™ll discuss the success weâ€™ve found with incorporating a machine learning model trained on survey data into all three of our major surfaces: Homefeed, Related Pins, and Search. This aligns with one of our companyâ€™s core values, to Put Pinners First, by optimizing our recommendations according to their feedback.</p><h3>Survey Data Collection</h3><p>We launched an in-app survey campaign where we asked Pinners to rate images on a scale of 1â€“5 for visual appeal. The exact question wording was, â€œHow visually pleasing or displeasing is this Pin?â€ [Figure 1]. We intentionally left the wording somewhat vague to encourage Pinners to respond based on their own notion ofÂ quality.</p><p>Although it would be great to get survey responses for each of the billions of images in our corpus, we have a policy of not bombarding Pinners with constant survey requests. For this survey, we limited ourselves to collecting responses for just 5k Pins. We sampled 1k Pins (weighted by impressions) from each of our top five L1 interest verticals:<em> Art</em>,<em> Beauty</em>,<em> DIY &amp; Crafts</em>,<em> Home Decor</em>,<em> </em>and<em> Womenâ€™s Fashion</em>.<em> â€œ</em>L1â€ here refers to Pinterestâ€™s top level <a href="https://medium.com/pinterest-engineering/pin2interest-a-scalable-system-for-content-classification-41a586675ee7">taxonomy of interests</a>. We only asked Pinners to rate mid-to-high quality images rather than exposing them to low quality images just for the purposes of thisÂ survey.</p><p>Itâ€™s important to remember that the question of visual quality is in the â€œsweet spotâ€ for subjectivity. For things that are relatively objective (e.g., policy violating content), a company should pay reliable human reviewers and not burden users or decrease their experience. For things that are extremely subjective (e.g., personal relevance), we wouldnâ€™t be able to get a reliable Pin-level signal because the score would be a function of the Pin, the context, and the individual rater. Since our goal is to understand visual quality for the average Pinner, a survey is well suited for data collection.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*GfdvfoeLgZaQ9uIZNHMmeg.png" /><figcaption><strong>Figure 1. </strong><em>In-app survey UI. â€œVery visually displeasingâ€ is assigned a score of 1, while â€œVery visually pleasingâ€ is assigned a score ofÂ 5.</em></figcaption></figure><p>Importantly, we collected at least 10 responses for each image to reduce subjectivity and alleviate noise. By computing each imageâ€™s average rating, we can get an idea of what the average Pinner would rate the image, which acts as a proxy for the â€œobjectiveâ€ rating of the image. Asking multiple Pinners to rate each image also leaves a buffer for misclicks; if each image was rated just once, we would have run the risk of ending up with a noisy and unreliable dataset.</p><p>The data from this survey allowed us to get thoughtful reflections directly from Pinners on what they consider to be quality content, which enables us to build more â€œgoodâ€ into the product and weed out the â€œbad.â€ For example, the highest average ratings were received by generally appealing images across wide-ranging topics like makeup, grooming styles, maximalist home decor, landscapes, sunsets, and baby animals. There were also subtle differences in ratings between the different L1 interests in our survey. <em>Home decor</em> Pins were in general rated higher than the rest [Figure 2]. The highest variance in responses was for <em>Art </em>Pins, which isnâ€™t too surprising considering the subjective nature of art, and they make up the majority of both the top and bottom 200 images [FigureÂ 3].</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*X7B74VYpxz1fvdI7p1mTQw.png" /><figcaption><strong>Figure 2.<em> </em></strong><em>Distribution of Pinner ratings in each interest vertical.</em></figcaption></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*fqlaD7dHXcrV2P6ubWM7GA.png" /><figcaption><strong>Figure 3. </strong><em>Proportion of images from each L1 interest vertical in the top and bottom 100 images. Art and Home Decor dominate the top 100, whereas the bottom 100 is more evenly distributed with Art images having the highest representation onceÂ again.</em></figcaption></figure><h3>Machine Learning Modelling</h3><p>We leveraged the survey data to train a machine learning model for learning the average Pinnerâ€™s perception of visual quality. Given embedding features for an image, the modelâ€™s task is to map to a single score between 0 and 1, with higher scores indicating higher visual quality as perceived by the average Pinner. These in-house embedding features each represent some aspect of the Pin image, such as the relationships between the image and boards itâ€™s saved to along with its visual and textual information. We opted for a simple fully-connected neural network with 92k parameters to learn this mapping. Not only does this help prevent the model from overfitting to the relatively small dataset of 5k Pins, but it also makes inference at scale quicker and cheaper. The model architecture is depicted in FigureÂ 4.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*n4VBZyOsu8gvdLG_exfBdg.png" /><figcaption><strong>Figure 4. </strong><em>Machine learning model architecture. We utilized a fully-connected network to map content embedding features to a single score representing visualÂ quality.</em></figcaption></figure><p>The problem was formulated as a <strong>pairwise ranking</strong> rather than a classification or regression problemâ€Šâ€”â€Ša well established machine learning technique to find the relative ranking of items instead of a composite score for each independent item. The idea can be simplified as: Given two images, we ask the model to predict which one the average Pinner would agree is â€œbetter,â€ rather than trying to predict the actual mean response score for a single image. To define whatâ€™s â€œbetterâ€ quality, we take the mean of the 10 responses per image as its ground truth quality score. So the task is formulated as a two-item ranking problem, where the model needs to determine which image has the higher mean response.</p><p>We only compare images belonging to the same L1 when training the model. This is done to force the model to focus on visual quality differences between images, rather than semantic ones. During training, the model outputs a score for each image. Then, to account for differences between L1s, we separate the images into their L1 interest verticals. To introduce stochasticity for more effective training, we further randomly split the images into smaller groups or â€œsub-group.â€ We compute the <a href="https://docs.pytorch.org/docs/stable/generated/torch.nn.MarginRankingLoss.html">pairwise margin ranking loss</a> within each sub-group and optimize the model weights to reduce this loss. As the name indicates, the loss function is computed between all pairs within a group and summed. The training process is summarized in Figure 5Â below.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*DvpYZwLPWWwsGVHA0vJ54w.png" /><figcaption><strong>Figure 5.<em> </em></strong><em>Grouped pairwise ranking approach. Within each group we compute pairwise margin rankingÂ loss.</em></figcaption></figure><p>Although we collected multiple responses for each image, our dataset may still be somewhat noisy. We incorporate the variance in responses for images by using a variable margin for the loss function. Briefly, the margin is the minimum difference we want the model to enforce between the scores of the two images. If the better imageâ€™s score isnâ€™t higher by at least this margin, the model is penalized. The margin in this loss function is typically fixed. We instead vary the margin, using higher values for images whose response mean is more certain, and lower values otherwise.</p><p>Our ranking-based approach can help tackle the data sparsity issue. Although we only collected responses for 5k images due to limited survey bandwidth, the pairwise ranking approach effectively expands the dataset size to 5 * 1000C2 = ~2.5M pairs, and the model is able to learn nuances between images to understand why one may be perceived as higher quality thanÂ another.</p><p>Moreover, this ranking approach also makes it more suitable for adoption in Pinterestâ€™s downstream surface recommendation systems, where the absolute score matters much less for these ranking models than the relative differences between different Pins.</p><h3>Offline Results</h3><p>Offline evaluation on a holdout test set showed that our model can correctly distinguish between higher and lower content quality, hinting early on that it could serve as an informative feature if incorporated into the surface recommenders. As expected, images that were assigned high scores by our model looked similar to those that received high ratings in our survey. From Figure 6, we can see that the model-predicted scores align well with the Pinner ratings for our holdout test set, with over 90% of predictions being within one standard deviation. The kernel density estimate plots in Figure 7 further show that our model can distinguish between high and low qualityÂ images.</p><p>Quantitatively, we looked at two key metrics. The first is pairwise ranking accuracy, which measures whether the model can correctly predict which of two given images is higher quality. The second is NDCG@20, or Normalized Discounted Cumulative Gain, which measures how correctly a list of 20 images are ranked by the model scores, with more importance on the top of the list. The results are summarized in Table 1. Overall the model performs well, with better performance on some verticals (<em>Art</em>) than others (<em>Womenâ€™s Fashion</em>).</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*UQifN2bDINcC9xbGueO3uA.png" /><figcaption><strong>Table 1. </strong><em>Offline evaluation results on test set. The model performs better for some verticals like Art and Beauty thanÂ others.</em></figcaption></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*ypfksD-HHRn1yW0UxPYoVQ.png" /><figcaption><strong>Figure 6</strong>. <em>Distribution of model predicted scores (blue) and the â€œtrueâ€ Pinner mean ratings (green). Outliers whose predictions fall outside one standard deviation of the mean responses are marked inÂ red.</em></figcaption></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*FCan_30oPR2FW7aFnsX7vg.png" /><figcaption><strong>Figure 7.</strong> Kernel density estimate (KDE) plot of model predicted scores for the top and bottom 100 Pins by true Pinner ratings. The peaks for the true and predicted distributions for both sets are roughly aligned, with our model predicted scores being more spread out due to the varying nature of responses.</figcaption></figure><h3>Impact</h3><p>The recommender systems for major surfaces at Pinterest such as Homefeed determine the content that makes it to a Pinnerâ€™s feed, using a combination of objective content features (e.g., the L1 interest, the Pin title and description, etc.) and subjective user features (e.g., what other Pins the Pinner has interacted with, search queries,Â etc.).</p><p>Online A/B experiment results showed that the visual quality signal we built is a win/win for both Pinners and the business <strong>across all three major user-facing surfaces at Pinterest</strong>: Homefeed, Search, and Related Pins. We saw significant reductions in â€œlow qualityâ€ sessions (i.e., sessions where Pinners encounter low quality content) and increases in â€œsuccessfulâ€ sessions (i.e., sessions where Pinners are able to find what theyâ€™re looking for) showing that the overall Pinner experience is improved. We also see an increase in individual engagement metrics like repins of organic content and long click-throughs on shopping content to name a few. Taken together, these suggest weâ€™re delivering better content that Pinners want to interactÂ with.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*yD_QBIsPNy4bRxhmb0LOvw.png" /><figcaption><strong>Table 2. </strong><em>Examples of some of our numerous online metrics wins across all three major ranking surfaces. Although we split the table into two sets of wins for clarity, we strongly feel that any wins for Pinners are also a win for our business, and viceÂ versa.</em></figcaption></figure><h3>Building on thisÂ Success</h3><p>At Pinterest, we strive to incorporate Pinner feedback directly into our product to keep improving and Putting Pinners First. Surveys are an excellent way to give Pinners a voice and learn what they think is high quality, and for them to tell us what kind of content theyâ€™d like to see more of on their feeds. Our work has shown that incorporating survey feedback into our ranking systems is a win-win for both Pinners and our business!</p><p>At the end of the day, Pinterest is about personalization. Pinners choose what ultimately makes it to their boards. Improving the quality of recommendations for the general audience helps us refine the â€œbestâ€ images that will eventually make it to theirÂ boards.</p><p>We plan to continue building on the success weâ€™ve found in this work. We have conducted additional similar surveys and will iterate on our model. Weâ€™ve explored leveraging state-of-the-art Visual Language Models (VLMs) that can learn more intrinsic information from the survey responses, and we are keen to productionize these in the next iteration of this signal in 2026. Moreover, we also monitor Pinner perception of content quality via ongoing tracking surveys, and we are working on expanding our survey-based signals to include other types of content besidesÂ images.</p><h3>Acknowledgements</h3><p>This was a cross-functional effort that would not have been possible without excellent support from our partner teams. The Content Quality Team would sincerely like toÂ thank</p><ul><li>Survey SupportÂ Team</li><li>Anket Team</li><li>Experience Framework Team</li><li>Related Pins RankingÂ Team</li><li>Search RankingÂ Team</li><li>Homefeed Team</li></ul><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=eebca8a52652" width="1" height="1" alt=""><hr><p><a href="https://medium.com/pinterest-engineering/improving-quality-of-recommended-content-through-pinner-surveys-eebca8a52652">Improving Quality of Recommended Content through Pinner Surveys</a> was originally published in <a href="https://medium.com/pinterest-engineering">Pinterest Engineering Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[On the (re)-prioritization of open-source AI]]></title>
            <link>https://medium.com/pinterest-engineering/on-the-re-prioritization-of-open-source-ai-86f7279481e3?source=rss-ef81ef829bcb------2</link>
            <guid isPermaLink="false">https://medium.com/p/86f7279481e3</guid>
            <category><![CDATA[foundation-models]]></category>
            <category><![CDATA[open-source]]></category>
            <category><![CDATA[pinterest]]></category>
            <category><![CDATA[ai]]></category>
            <category><![CDATA[engineering]]></category>
            <dc:creator><![CDATA[Pinterest Engineering]]></dc:creator>
            <pubDate>Thu, 04 Dec 2025 17:02:14 GMT</pubDate>
            <atom:updated>2025-12-04T17:02:14.646Z</atom:updated>
            <content:encoded><![CDATA[<p>Dmitry Kislyuk | Director, Machine Learning; Ryan Galgon | Director, Product Management; Chuck Rosenberg | Vice President, Engineering; Matt Madrigal | Chief Technology Officer</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*wDWMYbMluZy-MzQ0wNbPhw.jpeg" /></figure><p><strong>Foreword from Bill Ready,Â CEO</strong></p><blockquote><strong><em>The AI landscape is undergoing a fundamental shift, and itâ€™s not the one you think. The competitive frontier isnâ€™t only about building the largest proprietary models. There are two other major trends emerging that havenâ€™t had enough discussion:</em></strong></blockquote><blockquote><strong><em>Open-source models have made tremendous strides, especially on cost relative to performance.</em></strong></blockquote><blockquote><strong><em>Compact, fit-for-purpose models can meaningfully out-perform general purpose LLMs on specific tasks and do so at dramatically lowerÂ cost.</em></strong></blockquote><blockquote><strong><em>Our Chief Technology Officer and AI team share how we are using open-source AI models at Pinterest to achieve similar performance at less than 10% of the cost of leading, proprietary AI models. They also share how Pinterest has built in-house, fit-for-purpose models that are able to significantly outperform leading, proprietary general purposeÂ models.</em></strong></blockquote><blockquote><strong><em>The race to build the largest, most powerful models is profound and meaningful. If you want to see a thriving ecosystem of innovation in an AI-driven world, you should also want to see a thriving open-source AI community that creates democratization and transparency. Itâ€™s a good thing for us all that open source is in theÂ race.</em></strong></blockquote><blockquote><strong><em>For our part, weâ€™ll continue to share our findings in leveraging open-source AI so that more companies and builders can benefit from the democratizing effect of open-source AI.</em></strong></blockquote><p>Pinterest helps users worldwide to search, save, and shop for the best ideas powered by our visual AI capabilities. These are powered by a mix of models operating across different modalities; a recent development is that for applications requiring LLMs and VLMsÂ¹, we have found significant advantages in adapting open-source models with Pinterestâ€™s unique data and existing technologies. As a result, Pinterest has been shifting more of our AI investments towards fine-tuned open-source models, achieving similar quality at a fraction of the cost, particularly for visual and multimodal tasks. This shift reflects a broader industry trend: core LLM architectures are commoditizing, while differentiation increasingly comes from domain-specific data, personalization, and product integration.</p><p>It is worth taking a closer look at the technical strategy behind foundation models at Pinterest. Just because it can be built in-house does not mean every capability should or must be. The build, buy, adapt set of tradeoffs are a well understood concept in the industry, and AI models are no different. At Pinterest, we structure our thinking about this question by looking at the primary modality over which the foundation model is optimized for:</p><ol><li><strong>Users</strong> (recommendation systems). User modeling systems are typically deeply coupled with the specific product that they are optimized for and are thus nearly always built internally for large search and social platforms. Given its scale, Pinterest has published extensive work on utilizing long-term sequences of user actions and universally compatible user representations for recommendation systems, relying on an <a href="https://arxiv.org/abs/2504.17811">image-board-user graph</a> consisting of hundreds of billions of nodes to build these capabilities. These systems include both representation learning approaches (e.g. <a href="https://arxiv.org/abs/2507.12704">PinFM</a>) and generative recommendation models (e.g.Â <a href="https://arxiv.org/abs/2504.10507">PinRec</a>).</li><li><strong>Visual</strong> (encoders and diffusion models)Â². As a visual platform, Pinterest has consistently invested in building frontier image understanding models. Although strong open-source models are available, we have found that the rich visual datasets curated through our visual search product and visual board collections enable the large-scale weakly-supervised pretraining that modern visual AI systems require. Thus, we largely default to training these models internally from scratch asÂ well.</li><li><strong>Text</strong> (LLMs)Â³. The remarkable model progress on text modeling and more abstract capabilities (e.g. reasoning) has been empirically connected to training with enormous amounts of compute and internet-scale text data, and Pinterest has largely relied on both open-source and third-party proprietary LLMs to build the best possible products for our users in recentÂ years.</li></ol><p>What we are observing now is that the capabilities of open-source multimodal LLM architectures have begun to level the playing field of model capabilities. Critically, across many product categories at Pinterest, the core differentiation in capabilities is shifting to the ability to fine-tune models with domain-specific data, and investing in end-to-end optimization and integration.</p><p>The trend toward domain-specific data and deep product integration as a core differentiator can be seen as a reversion to a common trend in the ML industry. In the first decade of the AlexNet era, core architectures were routinely commoditized, and either fine-tuning open-source models or training models on specific web-scale datasets was the most common form of development. We saw this first-hand with our development of various visual encoders (e.g. <a href="https://medium.com/pinterest-engineering/unifying-visual-embeddings-for-visual-search-at-pinterest-74ea7ea103f0">UVE</a>, <a href="https://www.pinterestcareers.com/media/eoqd5wcs/pinclip.pdf">PinCLIP</a>), where training embedding models from scratch on Pinterest image and visual search data has yielded meaningful retrieval gains over off-the-shelf embedding modelsâ´. Recently, weâ€™ve also seen this with <a href="https://medium.com/pinterest-engineering/building-pinterest-canvas-a-text-to-image-foundation-model-aa34965e84d9">Pinterest Canvas</a>, our image generation foundation model, where tuning an internally-trained diffusion model for specific image editing and enhancement use-cases with Pinterest data has thus far yielded better results than using larger but more general-purpose visual generative models.</p><p>Our most recent data point in this trend comes from the beta launch of <a href="https://newsroom.pinterest.com/news/pinterest-assistant-revolutionizing-the-way-you-shop-online/">Pinterest Assistant</a> in October of this year. We can think of the Pinterest Assistant as being broken down into two sets of ML technologies. First, there is an underlying set of multimodal retrieval systems, recommendation services, and specialized generative models (including other LLMs) that serve as tools for an agentic LLM to invoke. These tools are predominantly Pinterest-native and rely on our user and visual foundation models.</p><p>And second, there is the core multimodal LLM itself, which oversees the agentic loop and is responsible for query understanding, query planning, and effective tool calling. The key factor with this LLM is that it acts as an intelligent router that recursively delegates much of the recommendation and agentic capabilities to the aforementioned Pinterest-native tools. In this design, the biggest lever we have for product improvements is scaling the quality of the tools, and scaling test-time-compute (e.g. breaking down the call into more advanced steps), as opposed to focusing solely on using the largest core LLM possible. Indeed, as comparisons of LLMs start showing small or negligible differences, we have observed that open-source solutions meet our product needs; weâ€™re getting more value by focusing on building out more domain-specific tools, fine-tuning for product-specific use-cases, optimizing for latency, etc. There are some benefits we are particularly excited about as we adopt more open-source multimodal LLMs at Pinterest:</p><ol><li><strong>Cost</strong>. Pinterestâ€™s visual-first AI systems have heavy image understanding requirements, often dealing with 10s of images in each conversation turn. In these situations, we are currently observing that self-hosted, fine-tuned open-source models allow for an order of magnitude reduction in inference costs. With further engineering investment in inference optimizations like disaggregated serving and smarter cache routing, we expect that the cost and throughput via internal model hosting will become even more favorable.</li><li><strong>Personalization</strong>. Personalization in a proprietary LLM can typically occur by passing in relevant context about a user as text, and there are many situations where this is sufficient. However, the visual nature of Pinterest means that user representations encode many more stylistic and visual preferences that do not translate well to text, and the ability to fine-tune an LLM to natively interpolate with internal embeddings, session context, and other user signals is highly desirable to produce relevantÂ results.</li><li><strong>Capabilities</strong>. Beyond personalization, leveraging precomputed internal content embeddings (e.g. PinCLIP) projected to internal LLMs allows for more efficient computation of long visual contexts. For Pinterest product development, this is an important consideration, since board and collage objects are a natural target for conversations, and they may consist of dozens or hundreds of Pins. On the output side, another capability that we have already seen progress on is the ability to fine-tune for specific tool calling (e.g. refining multimodal queries), especially in ways that enable future extensibility to novel tools without needing end-to-end fine-tuning of all capabilities jointly.</li><li><strong>Brand Values</strong>. There are significant advantages in being able to tune and update a model directly to align with Pinterestâ€™s investments in Inclusive AI commitments and our Community Guidelines, as opposed to layering this on as an additional module on top of a third party APIÂ call.</li></ol><p>Looking ahead, ML and AI capabilities at Pinterest will continue to be powered by a mix of internally-developed foundation models, fine-tuned open-source models, and licensed third party models. In addition, third party AI platforms are widely used by Pinterest engineering teams for coding tools, internal productivity, and rapid prototyping. However, the scalability advantages and capability gains from all forms of internally hosted models, whether they are trained from scratch or fine-tuned, are leading to a change in technology defaults at Pinterest. Furthermore, the development of model families that provide generative capabilities across a variety of latency and throughput requirements have allowed for a development pattern where product teams can prototype and iterate with third party models, while the ML teams develop more scalable and personalized internal models for the relevant capability.</p><p>How long will this open-source trend hold? We can only make an educated guess. The large-scale buildout of AI data centers may result in more step-function jumps in quality and emergent capabilities for proprietary models. In parallel, the supply-side growth in chip production may drive down fine-tuned open-source inference costs even further. Either way, our strategy at Pinterest to bring inspiration to all of our users will remain the same: leverage our visual, graph, and recommendation data to build the best and most efficient models we can, and address any capability gaps by partnering with third-party providers, alongside regular research &amp; development from <a href="https://www.pinterestlabs.com/">Pinterest Labs</a>.</p><p>Â¹We use â€œLLMâ€ to refer to both text-only models, and multimodal visual LLMs, which contain an image encoder, which are sometimes referred to as Visual Language Models (VLMs). Most applications of generative models at Pinterest require visual inputs, so internally we assume multimodal capabilities as aÂ default.</p><p>Â²Visual models are commonly trained with text supervision via contrastive learning or other forms of conditioning. But the dominant training signal for the model remains the raw visualÂ input.</p><p>Â³Most LLMs benefit from a mix of modalities, with VLMs designed explicitly for this purpose. However, pre-training remains focused on an autoregressive text token prediction task, which is why we characterize them as text-dominant models.</p><p>â´For example, we have seen our <a href="https://www.pinterestcareers.com/media/eoqd5wcs/pinclip.pdf">PinCLIP</a> system outperform state of the art open-source multimodal embeddings by more than 30% on core retrieval tasks.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=86f7279481e3" width="1" height="1" alt=""><hr><p><a href="https://medium.com/pinterest-engineering/on-the-re-prioritization-of-open-source-ai-86f7279481e3">On the (re)-prioritization of open-source AI</a> was originally published in <a href="https://medium.com/pinterest-engineering">Pinterest Engineering Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Autonomous Observability at Pinterest (Part 1 of 2)]]></title>
            <link>https://medium.com/pinterest-engineering/autonomous-observability-at-pinterest-part-1-of-2-eb0adae830ba?source=rss-ef81ef829bcb------2</link>
            <guid isPermaLink="false">https://medium.com/p/eb0adae830ba</guid>
            <category><![CDATA[pinterest]]></category>
            <category><![CDATA[engineering]]></category>
            <category><![CDATA[infrastructure]]></category>
            <dc:creator><![CDATA[Pinterest Engineering]]></dc:creator>
            <pubDate>Wed, 03 Dec 2025 17:02:13 GMT</pubDate>
            <atom:updated>2025-12-03T19:38:21.235Z</atom:updated>
            <content:encoded><![CDATA[<p>Marcel Mateos Salles | Software Engineer Intern; Jorge Chavez | Sr. Software Engineer; Khashayar Kamran | Software Engineer II; Andres Almeida | Software Engineer; Peter Kim | Manager II; Ajay Jha | Sr.Â Manager</p><p><em>At Pinterest, inspiration isnâ€™t just for our usersâ€Šâ€”â€Šit shapes how we build and care for our platform. Until recently, our own observability (o11y) tools told a fragmented story: logs over here, traces over there, and metrics somewhere else. Weâ€™ve always excelled at collecting signals: time-series metrics, traces, logs, and change related events. But without the seamless context and unity now promised by open standards like OpenTelemetry (OTel), we were missing out on the â€œbig pictureâ€: the full narrative behind every anomaly and alert. While modern observability standards like OpenTelemetry (OTel) promise a unified world of correlated data, the reality for many mature, large-scale infrastructures is far more fragmented. These systems, often predating the widespread adoption of such standards, are composed of powerful but disconnected data silos. We solved the problem with a pragmatic solution to this common challenge by leveraging AI agents through a centralized Model Context Protocol (MCP) server to bridge these gaps without mandating a complete infrastructure overhaul.</em></p><p><em>The Pinterest Observability team is charting a new course that meets the moment. Weâ€™re working both left and right: â€œshift-leftâ€ practices to bake better logging and instrumentation into the heart of our code, and â€œshift-rightâ€ strategies to keep production observability robust and responsive. Still, we know that tools alone arenâ€™t enough. The real breakthrough comes with bringing more intelligence and context into the mix. We are embracing the new era of AI, and at its core is the Model Context Protocol (MCP), Agent2Agent(A2A) and context engineering, a new way to bring all our observability signals together and feed them into intelligent agents. Beginning with the MCP server, we attempt to make every major pillar of observability data available in a unified, contextual stream.</em></p><p><em>Observability analysis systems can dig deep, asking the right questions. Following clues across logs, metrics, traces, and change events, and iteratively building insight much like a Pinterest board comes together, piece by piece. The result? Faster, clearer root-cause analysis, and actionable guidance for our engineers, right where they need it. This isnâ€™t just about connecting yesterdayâ€™s silos, itâ€™s about creating new frontiers for discovery and problem-solving, empowering every Pinterest team to build their own context-aware tools and shape observability that grows withÂ us.</em></p><h3>A Fragmented State</h3><p>The field of observability (o11y) faces major turning points every couple of years, with a major shift a couple of years back when <a href="http://opentelemetry.io/">OpenTelemetry</a> (OTel) and similar services came into the picture. These tools facilitate the o11y process by enabling context propagation across the different pillars of o11y data while remaining vendor and language agnostic. For example, under a single SDK, you have the ability to generate metrics, logs, and traces with an ID that allows for correlation and connections between those unique dataÂ pillars.</p><p>However, our o11y infrastructure was set up before conventions and tools like OTel were available, and it is not feasible to overturn our entire o11y infrastructure in order to incorporate them into our stack. This means that we suffer from a lack of the virtues they provide. We had to individually implement separate tools and pipelines for ingesting logs, metrics, and traces from our services. This resulted in a strong, yet fragmented system where each individual pillar is constrained to its own domain with no clear matching across datapoints. As a result, an on-call engineer must jump around multiple unique interfaces when root causing an issue, leading to the potential loss of valuable time. A steep learning curve for the current tools unique to each pillar further extends this loss of time for newer engineers. Consequently, advanced o11y analysis by leveraging machine learning or other techniques that can holistically understand the health of our systems creates non-trivial problems for the o11yÂ team.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*fjQuRqkJV5hvTHXUgOxBPg.png" /><figcaption>Figure 1: Fragmented Signals for Observability</figcaption></figure><h3>Sidestepping theÂ Problem</h3><p>Knowing these limitations, the o11y team here at Pinterest is committed to overcoming these gaps by what we call â€œshifting-leftâ€ and â€œshifting-right.â€ When â€œshifting-left,â€ we have prioritized the integration and standardization of o11y practices and tools, which facilitates the proactive identification and resolution of issues. Meanwhile, when â€œshifting-right,â€ we focus on maintaining system visibility in production through the use of our alerting and health inferencing systems.</p><p>This means that we have to continue to innovate and connect the dots across our pillars while ensuring teams can continue to monitor the health of their services and quickly solve problems when theyÂ arise.</p><p>Enter the era of AI and Agents. What if our limitations didnâ€™t truly matter? We could just provide our data to Large Language Models (LLMs) acting as agents and have them connect the dots for us, find correlations, return meaningful information to our users in a single interface, facilitate the root-causing process, and in the future lead to a system where we can autonomously solve issues as they arise. We are working towards that future and are excited to share work we have taken up in thatÂ regard.</p><h3>Context Engineering</h3><p>An AI agent is only as good as the information that it has access to, so we knew that we had to build a system that would be able to provide our o11y agents with as much relevant data as possible. LLMs are impressive on their own, but with some real <a href="https://www.philschmid.de/context-engineering">context engineering</a> behind them, they become so impressive that you begin to feel like you are living in the advanced future from your favorite Sci-Fi movies andÂ shows.</p><p>Different techniques have sprung up recently to facilitate the sharing of context with an agent. However, the most prominent and widely accepted is that of the <a href="https://modelcontextprotocol.io/overview">Model Context Protocol</a> (MCP), which was <a href="https://www.anthropic.com/news/model-context-protocol">released by Anthropic</a> in late 2024. This protocol has become the new standard and a staple of agentic projects for companies and enthusiasts alike. In short, it provides an agent different tools that it can utilize when working to resolve a request, allowing it the flexibility to <strong><em>choose</em></strong><em> </em>what to use (if it wants to call anything at all) as it organically works through a task with its reasoning and newfound information. MCP was the perfect fit to help us sidestep our limitations and begin to drive Pinterest o11y into a new era as it grants the following:</p><ul><li><strong>Unity of Disparate Signals: </strong>By building an MCP server, we can empower an agent to simultaneously interact with time-series metrics, logs, traces, changefeed events (deployments, experiments, etc.), alerts, and more. This allows it to find connections and build hypotheses from patterns it sees within and across our data despite the lack of a thread connecting it together.</li><li><strong>Fine-Grained Context Control: </strong>As the developers of the MCP server, we get to decide what information and ability agents interacting with our teamâ€™s data actually get, so developing the MCP server ensures that we maintain full control of our services and data. If not, other teams could have independently developed them on their own, incorrectly accessing our data or giving their agents the ability to alter data that should not be changed. By providing the MCP server as an interface to our data, we prevent the agents access to everything, allowing us to maintain tighter safety and privacy controls. Furthermore, we can also guide agents in the right direction, providing relevant subsets of data by combining what we know with what the agent hasÂ learned.</li><li><strong>Plug-and-Play Extensibility: </strong>In a practical sense, an MCP server is a service that provides an AI agent with a toolbox to better fulfill its job. The tools within can easily be removed, replaced, and expanded upon without changing the overall system. The agent will connect and interact with it the same, only changing what it can achieve and discover with the tools provided. This means that our server can easily grow and change with our team, becoming more advanced as we provide more tools overÂ time.</li><li><strong>Hub for Agentic o11y Experience: </strong>We plan for this to only be the beginning of our teamâ€™s GenAI tooling. It creates the perfect infrastructure to allow for advanced agents and creates a hub for engineering teams at Pinterest to be able to access our data for their own agentic needs (recently, we hosted our company-wide hackathon where multiple teams developed projects that depended on our MCP server, including the team that bagged the firstÂ place!).</li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*ss4Y8MR7jmBjNQl758NCDA.png" /><figcaption><em>Figure 2: Before and After experience with Tricorder Agent with MCP ServerÂ tools</em></figcaption></figure><h3>MCP Server For Observability</h3><p>And so, the o11y teamâ€™s very own MCP server was born. It is now available internally for Pinterest engineers to use and is a central part of our move towards autonomous o11y. Currently, it provides models with tooling for accessing the following data:</p><ul><li><strong>ChangeFeed Events:</strong> Finds events related to the service of interest; for instance: deploys, feature flag activations, or experiment rollouts</li><li><strong>Metrics:</strong> Queries metrics from our time seriesÂ database</li><li><strong>Post Mortem DocumentsÂ </strong>: After having previously ingested incident Post-Mortem documents into a database, fetches them for analysis whenÂ relevant</li><li><strong>Logs</strong>Â : Fetches logs related to the service of interest for a relevant timeÂ range</li><li><strong>Traces</strong>Â : Fetches traces related to the service of interest for a relevant timeÂ range</li><li><strong>Alert Information:</strong> From a triggered alert, fetches information such as relevant metrics, service identifiers, and time range ofÂ interest</li><li><strong>Dependency GraphsÂ </strong>: Finds dependencies for a service of interest, both downstream andÂ upstream</li></ul><p>Its development was a great experience and allowed us to learn a very important lesson about applied AI. It is partially a consequence of our data but something that anyone who wants to do something with agents should consider as a limitation: the model context size. Going in, we overestimated the amount of information that a model could take while also underestimating the amount of data that we own as a team. The o11y team processes around 3 billion data points per minute, 12 billion keys (tag key/value combinations) per minute, 7 TB of logs per day, and 7 TB of traces per dayâ€Šâ€”â€Šno small amount of data! If we allow an agent to organically look through this data, it would end up querying for too much at a time (even if it was only querying for a 15 min window), breaking its context window and causing itself to crash. We came up with two main solutions to prevent this from happening, the first being short-term while we test theÂ other:</p><ol><li><strong>Link Generation:</strong> The first use case we planned to test our agents on was the collection of relevant data for an on-call engineer. We just wanted it to collect the relevant information related to an alert to facilitate the engineers job, streamlining resolution and reducing mean time to resolution (MTTR). For this use case, the agent does not need to parse the raw data. Instead, it only needs to know of relevant time periods and services. This allowed us to have the agent generate links to the dashboards containing that information (already filtered to the correct time periods and relevant services), saving an on-call engineer the time spent jumping around all our interfaces and filtering. All their time can now be spent on the act of resolving conflicts.</li><li><strong>More Specific Tool Documentation: </strong>Knowing the previous solution would not work for all use cases, especially the most advanced ones where we would want agents to be able to find connections between the data, we kept looking for solutions. We noticed that we were overcomplicating the situation trying to come up with complex solutions. The tools a MCP server gives an agent come with a lot of metadata explaining their functionality so that an agent can reason about whether to use it. This means that in this metadata, we could include instructions to only query for a very small period and to call it again until the wanted time period isÂ covered.</li></ol><p>We are also currently working on and testing another solution with the Spark team within Pinterest. They are looking at building a similar agent, where we leverage an additional LLM within the server (with a fresh context) to summarize the data. This allows us to only return a summary to the agent connected to the MCP server which, in theory, would conserve a lot of context space. We just need to verify that these summaries donâ€™t drastically decrease the agentâ€™s performance.</p><p>Our MCP Server agent is called Tricorder Agent. It is designed to assist engineers in quickly analyzing problems and resolving conflicts. The agent is part of a broader suite of new tools under development by the o11y team, collectively known as the Tricorder. The engineer can provide the Tricorder with their alert link/number and sit back while it gathers the relevant information for their investigation. Before, this would have been extremely time consuming as engineers wouldâ€™ve had to switch between all our interfaces and apply filters to find relevant data. Additionally, Tricorder queries our services directly to understand what is going on and hypothesize a cause, providing suggestions and next steps as it gains more information. Throughout this process, there have been many times where we have been pleasantly surprised by the Tricorder. For example, a lot of information is unlocked when a dependency graph becomes available. The agents use tools on multiple parts of the graph, exploring all the incoming and outgoing dependencies to check for the overall health of connections with no specific prompting to do so. Additionally, when generating links and narrowing down to relevant services, they include the services in the dependency graph, knowing the problems could be stemming fromÂ them.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=eb0adae830ba" width="1" height="1" alt=""><hr><p><a href="https://medium.com/pinterest-engineering/autonomous-observability-at-pinterest-part-1-of-2-eb0adae830ba">Autonomous Observability at Pinterest (Part 1 of 2)</a> was originally published in <a href="https://medium.com/pinterest-engineering">Pinterest Engineering Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Slashing CI Wait Times: How Pinterest Cut Android Testing Build Times by 36%+]]></title>
            <link>https://medium.com/pinterest-engineering/slashing-ci-wait-times-how-pinterest-cut-android-testing-build-times-by-36-feb6ff121d91?source=rss-ef81ef829bcb------2</link>
            <guid isPermaLink="false">https://medium.com/p/feb6ff121d91</guid>
            <category><![CDATA[pinterest]]></category>
            <category><![CDATA[infrastructure]]></category>
            <category><![CDATA[developer-velocity]]></category>
            <category><![CDATA[engineering]]></category>
            <category><![CDATA[testing]]></category>
            <dc:creator><![CDATA[Pinterest Engineering]]></dc:creator>
            <pubDate>Mon, 10 Nov 2025 23:02:29 GMT</pubDate>
            <atom:updated>2025-11-10T23:02:29.528Z</atom:updated>
            <content:encoded><![CDATA[<p>George Kandalaft | Software Engineer II, Test Tools; Alice Yang | Staff Software Engineer, TestÂ Tools</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*WoYM36s5IE8NP4FWrnN0Vg.png" /></figure><h3>TL;DR</h3><p><strong>Problem Statement</strong></p><p>Our Android end-to-end testing builds in CI were slow and flaky because the test shards were unbalanced and the entire build had to wait for the single slowest shard on a third-party testing platform.</p><p><strong>Solution</strong></p><p>We built a runtime-aware sharding mechanism that uses historical test duration and stability data to pack tests greedily in an in-house testing platform with EC2-hosted emulators, ensuring all shards have a similar totalÂ runtime.</p><p><strong>Impact</strong></p><p>The end-to-end build time was reduced by 9 minutes (a 36% improvement), decreased the slowest shardâ€™s runtime by 55%, and compressed the time difference between the fastest and slowest shards from 597 seconds to just 130Â seconds.</p><h3>Introduction</h3><p>At Pinterest, one of the Test Tools team missions is to save engineers time across development, testing, and shipping. Delivering on that mission requires fast, reliable feedback for our end-to-end (E2E) testing pipelines. As our app and test suites have grown, we faced a familiar challenge: how do we keep E2E tests fast and reliable as test volume and complexity rise inÂ CI?</p><p>Our Android E2E tests run on emulators and devices that exercise real user flows. In CI, we traditionally split these suites into shards by package names. The test run and parallelism were managed by Firebase Test Lab (FTL) based on the packaged sharding. This type of sharding created imbalance: some shards finished quickly while others dragged onâ€Šâ€”â€Šso the slowest shard gated theÂ build.</p><p>To make feedback predictable and fast, we moved to time-based sharding with a new in-house testing platform and infrastructure. Using historical runtimes, we assign tests so each shard takes roughly the same time, not merely the same count of tests. This post explains how we built in-house time-based sharding for Android at Pinterest, how we handle flaky tests, and how the change reduced build latency and improved developer velocity.</p><p>Key terms weâ€™llÂ use</p><ul><li>Emulator: a virtual Android device used to run tests inÂ CI.</li><li>Shard: a batch of tests that runs in parallel with otherÂ shards.</li><li>Time sharding: assigning tests to shards using historical runtimes so all shards finish around the sameÂ time.</li><li>Flaky test: a failed test passes in retries without codeÂ changes.</li></ul><h3>Why Build an In-House Testing Platform?</h3><h3>The Challenges Encountered withÂ FTL</h3><p>Before we could optimize our testing sharding, we needed to re-evaluate our testing execution environment. With FTL, there were some challenges. Each test run incurred a baseline setup time of five to six minutes. As our test suites grew and flaky tests required retries, this setup overhead became a significant portion (&gt; 50%) of the total build duration.</p><p>Additionally, FTL infrastructure flakiness and instability started early 2023 blocked developer productivity one to two times per week on average, with each outage causing three to four hours of downtime and halting all code merges. There were no near term solutions provided byÂ Google.</p><h3>Evaluating the Market for an Alternative</h3><p>One of the first steps we took was to evaluate the vendor market for an alternative solution. The goal was to find a third-party solution that offers high reliability, low setup overhead, and massive parallelism on both emulators and physical devices. Emulator support is especially important as it offers much better cost and performance efficiencies for our large-scale testÂ suite.</p><p>However, the vendor evaluation showed that the available solutions did not fully align with our requirements for large-scale, native emulator support. We found out that even with an alternative vendor, we would still need to build and maintain our own custom test orchestration and sharding mechanism. Below is a list of the critical acceptance criteria we had during the evaluation.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*NBC2_8_hNqBUp5WT5ygWsQ.png" /></figure><h3>The Decision to Build A Path to Deeper Innovation</h3><p>Based on the analysis, we concluded that building an in-house testing platform was the most direct path to solve the root cause of the problem to improve developer velocity and experience. Through conversations with peer companies who had successfully built their own Android testing platform, and an internal proof-of-concept, weâ€™ve seen feasibility and promising performance gains.</p><p>This led to the creation of PinTestLab, Pinterestâ€™s in-house Android E2E testing solution that runs on EC2-hosted Emulators. This gave us direct control over the entire testing stack, from docker images, emulator configurations, error logging, and sharding.</p><h3>Infrastructure and Resource Allocation</h3><p>A smart sharding algorithm is only as effective as the infrastructure it runs on. Building PinTestLab required a stable and efficient foundation. This involves a solid proof-of-concept (POC) to identify the right instance type, followed by iterative optimization to maximize resource utilization.</p><p>Our initial POC explored running Android emulators on standard, virtualized EC2 instances (x86 architecture) inside a docker image. It revealed significant performance bottlenecks. A switch to bare-metal instances immediately resolved these issues, as shown by the drastic improvements in emulator boot time and testÂ time.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*RkKrkEkVqhRj4_-ht9sUGQ.png" /></figure><p>We also briefly investigated using ARM-base bare-metal instances but encountered significant tooling friction and kernel panics during emulation.</p><p>Based on these findings, we selected c7i.metal-24xlarge instances for all test executions. Our initial rollout established a baseline, which we later improved through resource allocation optimization to our docker image. This enhancement, combined with the improved load distribution from time-based sharding, allowed us to better utilize host resources and increase peak memory usage toÂ 80%.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*0MECQbqAMuRii0-5o0P1Sw.png" /></figure><h3>How We Ran Tests Before, and Why WeÂ Changed</h3><p>In December 2024 we launched PinTestLab minimum viable product (MVP). We wanted to cut external flakiness and regain control over device images, scheduling, and retriesâ€Šâ€”â€Šwhile keeping engineersâ€™ CI/CD flow in Buildkite.</p><p>For the MVP, we kept orchestration simple. Buildkite YAML defined the endâ€‘toâ€‘end test run. A Python orchestrator spun up one emulator per shard, ran a preâ€‘assigned test list, uploaded artifacts, and then shut the emulator down. Sharding used a uniform roundâ€‘robin split by test count, with packageâ€‘based splits as a baseline. We recorded perâ€‘shard timings and outcomes as Buildkite artifacts and sent events to an observability platform to spot skew and outliers, and we added conservative shardâ€‘level retries. This gave us stable runs under ourÂ control.</p><p>In FTL we split tests by package: all tests within a single package were grouped together and put into one shard. This was slow, and some packages containing long running or resource intensive tests took too long to finish, increasing total build time. We decided to move away from this toward a more balanced approach.</p><p>The first version kept things simple: scan the repo, divide the same total number of tests into shards roundâ€‘robin by count, and generate Buildkite steps to execute them. It worked, but shard durations were uneven: across 17 shards we saw P90 runtimes from roughly 8.8 to 13 minutes, which meant one long shard often gated the entire build. Our goal was to tighten that spread so that the â€œtailâ€ stopped owning the overallÂ time.</p><h3>The Goal: Equal Wallâ€‘Time, Not Equal Number ofÂ Tests</h3><p>Two practical constraints shape how we doÂ this:</p><ul><li><strong>Multiple emulators per shard:</strong> Each shard runs several emulators in parallel, and each emulator runs a list of tests assigned in sequence. Some emulators may finish their job minutes before the others depending on the distributed tests.</li><li><strong>Test Flakiness and retries:</strong> Which add time that isnâ€™t accounted for.</li></ul><p>Thatâ€™s why equal test counts per shard can still be slow. In the example, Shard 1 bunches three tests on one emulator while two emulators sit idle, so the shardâ€™s wallâ€‘time is dragged out by that single busy emulator and any retry there makes it worse. Shard 2 spreads work across emulators, so it finishes sooner even with the same number ofÂ tests.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*ZZWEuIzshyaU_nHcOMISYQ.png" /></figure><h3>Why â€œSort + Roundâ€‘Robinâ€ StillÂ Fails</h3><p>For PinTestLab, we kept orchestration simple at first, so we split tests with a roundâ€‘robin approach. It was better than the old packageâ€‘based splits, but still not ideal. This six test example made itÂ obvious.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*ApqtnGRFyPK4dJAqhGbyfw.png" /></figure><p>You get Shard_1 = [test_1, test_3, test_5] and Shard_2 = [test_2, test_4, test_6]. Inside the shard, the scheduler runs tests on two emulators; Shard_1 ends up with one emulator running for 10=6+4 minutes, while the other finishes a single 3 minute test and sits idle. Shard_1 takes 10 minutes, and Shard_2 takes 4 minutes. Even though we â€œbalancedâ€ by test count and total time looks fine on paper, one emulator in one shard becomes the straggler, causing a total build time of at least 10Â minutes.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*W6eR-3MfZfDXLSG9edt8jg.png" /></figure><h4>Takeaway One</h4><p>Even with equal test counts per shard, one emulator in shard_1 becomes the straggler, so total build time is â‰¥ 10 minutes. Roundâ€‘robin balancesÂ lists.</p><p><strong>Attempt 2: Sort, then roundâ€‘robin</strong></p><p>Sorted tests by expected time: [6, 4, 3, 2, 2, 2] â†’ [test_1, test_5, test_3, test_2, test_4, test_6] Shards (roundâ€‘robin afterÂ sort):</p><ul><li>Shard 1: [test_1, test_4,Â test_3]</li><li>Shard 2: [test_5, test_6,Â test_2]</li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*_78Pd90vhJcQpI3l295CeQ.png" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*Fb40ACiBiK_aipA6mg1GHw.png" /></figure><h4>Takeaway Two</h4><p>Sorting reduces the skew (8 vs. 10 minutes) but still leaves idle time on the second emulator in each shard and a longer shard that gates theÂ build.</p><h3>The Insight We Shipped: Runtimeâ€‘Aware Sharding</h3><p>We decided to develop a runtime-aware sharding, and we had a promising first run. The numbers speak for themselves. Under round-robin sharding, our slowest shard took 522 seconds, while the fastest finished in just 246 seconds. A 276-second spread meant developers waited for the tail everyÂ time.</p><p>With time-based sharding using p90 duration as our metric, we compressed that spread to just 28 seconds (5.6 vs 5.1 minutes). In practice, this meant cutting typical CI feedback from nearly 9 minutes down to about 5.5 minutes; this is a significant boost to developer velocity.</p><h4>Round-robin</h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*QN0FHcFHo8bQcUL8Kk7JWA.png" /></figure><h4>Runtime-aware Sharding</h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*SAssWT8_Pkb91OQhTw2zrw.png" /></figure><p>To address the timing imbalance, we leveraged <strong>historical runtime</strong> data stored in Metro, Pinterestâ€™s test management system. Metro tracks test results, trends, ownership, and build history, while allowing engineers to debug test failures and take actions, such as quarantine flaky tests when necessary.</p><p>Using this rich historical data, we developed a greedy, runtime-aware allocation algorithm. The approach is straightforward: we sort tests by a robust performance metric (such as the average duration), add a small per-test overhead to account for setup costs, then pack tests into shards using a greedy strategy. Each test gets assigned to whichever emulator is projected to finish earliest, keeping all execution emulators busy and ensuring emulators within each shard complete at roughly the same time. This approach reduces tail latency without adding complexity to our CI orchestration.</p><p>Our initial implementation created a minimal runtime-aware sorter that ran entirely within Buildkite. We added a step that fetched historical data from Metro for the current test suite, calculated average runtimes based on the past x runs from the pipeline, then used a min-heap data structure to continuously assign the next test to the shard expected to finishÂ first.</p><p>We schedule tests using a simple, fast heuristic. Let N be the number of tests and M be the number of emulators. We first sort tests by descending average runtime, then assign each test to the currently least loaded emulator using a min-heap. This is the classic Longest Processing Time (LPT) rule for identical machines: it isnâ€™t guaranteed to be optimal, but itâ€™s well suited to our CI. Computing the exact optimum is NP-hard and impractical at our scale, while LPT runs in O(N log N + N log M), scales cleanly across shardsÃ—emulators, and delivers nearâ€‘optimal finish times in practice. Given runtime estimates and the need for predictable, lowâ€‘overhead scheduling, we favor effectiveness, simplicity, and operational reliability over theoretical optimality.</p><p>Why this works well forÂ us:</p><ul><li>Predictable runtime with a small implementation surface</li><li>Fast enough to run on every commit and resilient to minor runtimeÂ noise</li><li>Easy to reason about and monitor thanks to a simple heap invariant</li></ul><p>How the algorithm works (step-by-step):</p><ul><li>Estimate runtime per test from recentÂ history</li><li>Sort tests by adjusted runtime, descending</li><li>Initialize the min-heap with k entries (finish_time=0, shard_id)</li><li>For each test:<br>- Pop the shard with the earliest projected finish_time<br>- Assign the test to that shard<br>- Update finish_time += test_runtime and push back into the heap<br>- The maximum finish_time across shards approximates total suiteÂ duration</li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*mrxB3OEldR7Q_eDJQ9d9HA.png" /></figure><p>Why isÂ this?</p><ul><li>Predictable runtime and small implementation surface</li><li>Fast enough to run on every commit, robust to minor runtimeÂ noise</li><li>Easy to reason about and monitor (simple heap invariant)</li></ul><h3>Expanding Beyond PostÂ Commit</h3><p>To scale beyond post-commit, we decoupled our sharding algorithm from the pipeline. Because we already had rich historical data in the Metro, we built the solution there to be reusable for any testing pipelines. Given a list of tests plus the desired number of shards and emulators, the system computes an optimal sharding plan and estimates the fastest and slowest shard, as well as the fastest and slowest emulator if applicable. It also tracks prediction accuracy over time, so we can both forecast performance and measure how well those forecasts holdÂ up.</p><pre>ALGORITHM: Runtime-Aware Test Allocation<br><br>INPUT:<br>    - tests: List of test cases<br>    - num_shards: Number of available shards<br>    - num_of_emulators: Number of available emulators<br>    - per_test_overhead: Setup cost per test (constant)<br>OUTPUT:<br>    - shard_assignments: Map of shard_id -&gt; List of tests</pre><p>This allowed us to use Metro as a brain given that we already have the tests duration in Metro. In addition, it also provided us with a rough estimate for the slowest and fastest shard giving us insights on the overall run time of theÂ build.</p><h3>Other Options Considered: Onâ€‘Demand Sharding</h3><p>We also explored a second path: onâ€‘demand sharding driven by a message queue. This would let emulators pull work dynamically rather than running a precomputed test list. Itâ€™s promising, but it requires a separate proof of concept with third party dependencies evaluation.</p><h3>How It Would Work at a HighÂ Level</h3><p>We introduce an SQS layer to dispatch individual tests and create logical queues keyed by build_id and device_type so each build and device family is isolated.</p><p>Each shard spins up N emulators; every emulator becomes an SQS consumer. Instead of a static list, emulators fetch test messages on demand. For each message, the consumer asks Metro whether the test is already done (passed) or has failed out by hitting the retry limit. If neither, the emulator runs the test. On success, the consumer deletes the message from SQS. On failure, it reâ€‘enqueues the test with retry metadata (attempt count, last error, backoff hints) so another consumer can pick it upÂ later.</p><p>This pushâ€‘pull model keeps emulators busy and naturally shifts capacity toward longer or flaky tests during theÂ run.</p><h3>Tradeâ€‘Offs andÂ Decision</h3><p>We chose timeâ€‘based sharding for the best ROI at the moment, as it already serves the purpose and meets (and exceeds) our expectations: tightening tail latency with minimal orchestration changes. It offers lower uncertainty and an existing prototype, reducing implementation risk and operational overhead. Onâ€‘demand sharding remains a viable next step: if our test mix becomes more skewed, or we need finerâ€‘grained elasticity and targeted retries at very large scale, we can integrate Metro with SQS and pilot dynamic dispatch behind a feature flag. For now, runtimeâ€‘aware sharding provides predictable, fast feedback with a smaller blast radius and simpler operations.</p><h3>Reflections</h3><h3>Wins</h3><p>To measure impact, we ran two builds in parallel to capture upâ€‘toâ€‘date results. Compared with sorted countâ€‘based sharding, runtimeâ€‘aware sharding reduced the slowest shard from 863s to 392s (âˆ’54.6%), lowered average shard time from 400.1s to 303s (âˆ’24.3%), and cut total build time by about nine minutes (âˆ’36%). Bottom line: runtimeâ€‘aware sharding shortens endâ€‘toâ€‘end build time by roughly nine minutes while cutting tail latency by aboutÂ 55%.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*46Wus1tSSXk_tDX04FiHNw.png" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*-ij1CN1_jEszfJiheIwG6A.png" /></figure><h3>Learns</h3><ul><li>Always ship with a graceful fallback (roundâ€‘robin) to preserve developer trust. During Metro outages, the sharding mechanism automatically falls back to a roundâ€‘robin algorithm based on the tests retrieved from the repository within the testing pipeline.</li><li>Budget for newly added tests and account for silenced tests in planning. When a test has no historical data, assume the worst caseâ€Šâ€”â€Šfor example, assign it the maximum expected runtimeâ€Šâ€”â€Šuntil sufficient data is collected.</li><li>When planning, prefer averages to p90s because flakiness inflates tail metrics and misbalances shards.</li></ul><h3>Future Enhancements and Expansion</h3><ul><li>Explore on demand sharding for more balanced execution times</li><li>Expand the solution to other testing pipelines reporting toÂ Metro</li><li>Dynamic infrastructure resources allocation based on the predicted execution runÂ time</li></ul><h3>Acknowledgement</h3><p>Kudos to everyone involved! AmongÂ them:</p><ul><li>David Chang for Android infrastructure and general Android domain consultation, Jesus Antonio Huerta, Blake Martinez for their code contributions and codeÂ reviews.</li><li>Darsh Patel, Krishna Pandian for their timely support and prioritization with Buildkite infrastructure capacity and stability.</li><li>Eric Kalkanger for validating the instance type and ensuring the availability of EC2 instances.</li><li>Sha Sha Chu, Vani Kumar, Manuel Nakamurakare, Jiawei Shi and Roger Wang for all the support of theÂ project.</li><li>David Chang, Wei Hou and Jiawei Shi for reviewing the blogÂ post.</li></ul><h3>Join the Conversation</h3><p>Building a fast and reliable CI/CD pipeline is a continuous journey. What challenges have you faced with test parallelization in your organization? Share your thoughts or questions in the commentsÂ below!</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=feb6ff121d91" width="1" height="1" alt=""><hr><p><a href="https://medium.com/pinterest-engineering/slashing-ci-wait-times-how-pinterest-cut-android-testing-build-times-by-36-feb6ff121d91">Slashing CI Wait Times: How Pinterest Cut Android Testing Build Times by 36%+</a> was originally published in <a href="https://medium.com/pinterest-engineering">Pinterest Engineering Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[A Decade of AI Platform at Pinterest]]></title>
            <link>https://medium.com/pinterest-engineering/a-decade-of-ai-platform-at-pinterest-4e3b37c0f758?source=rss-ef81ef829bcb------2</link>
            <guid isPermaLink="false">https://medium.com/p/4e3b37c0f758</guid>
            <category><![CDATA[engineering]]></category>
            <category><![CDATA[platform-engineering]]></category>
            <category><![CDATA[engineering-leadership]]></category>
            <category><![CDATA[pinterest]]></category>
            <category><![CDATA[artificial-intelligence]]></category>
            <dc:creator><![CDATA[Pinterest Engineering]]></dc:creator>
            <pubDate>Tue, 04 Nov 2025 18:01:56 GMT</pubDate>
            <atom:updated>2025-11-04T18:01:56.992Z</atom:updated>
            <content:encoded><![CDATA[<p>Lessons on building platforms, driving adoption, and evolving foundations</p><p>David Liu â€¢ Senior Director of Engineering, MLÂ Platform</p><p>AI at Pinterest has been a proving ground for building platforms. Over the past decade, we went from ad-hoc machine learning stacks cobbled together by individual teams to a unified AI Platform that powers every major surface, spanning from recommendation and ranking models to emerging foundation and generative models.</p><p>The rapid pace of ML and AI brought new capabilities, but also new limits. What appeared to be purely technical choices often turned out to hinge on organizational structures and timing. This retrospective looks back at lessons we learned that we hope resonate with other companies on a similarÂ journey.</p><ol><li><strong>Adoption follows alignment. </strong>Teams adopted infra only when it was organizationally incentivized, for example by product goals, leadership priorities, or industryÂ timing.</li><li><strong>Foundations are layered, bottom-up, and temporary. </strong>Each stable layer enables the next, but no foundation is permanentâ€Šâ€”â€Ševery wave (DNNs, GPUs, LLMs) eventually forces aÂ rebuild.</li><li><strong>Local innovations prove possibility, but decay without shared foundations. </strong>Cutting-edge experiments tend to be coupled to local context and often need to be rebuilt to generalize.</li><li><strong>Enablement, efficiency, and velocity multiply each other. </strong>Increasingly, theyâ€™re shaped by both modeling and platform advances working inÂ tandem.</li></ol><p>I saw these dynamics from multiple seats: cobbling together the first Related Pins ML recommendation systems, proposing our first unified inference service, then starting and growing the ML Platform teamâ€Šâ€”â€Šthe backbone of Pinterestâ€™s AI infrastructure today. This platform now serves hundreds of millions of inferences per second, each user request completing thousands of model evaluations in under 100 milliseconds. Behind that, thousands of GPUs power hybrid CPU/GPU clusters that balance latency, cost, and utilization at globalÂ scale.</p><p>Iâ€™ll organize Pinterestâ€™s ML Platform story into fiveÂ eras.</p><ul><li><strong>Individual Team Stacks (2014â€“2015) and Early Unification (2016â€“2017):</strong> Fragmentation drove the first attempts at unification with bottom-up layering.</li><li><strong>Scrappy ML Platform Team (2018â€“2019):</strong> A tiny two-eng team tried to unify much bigger teamsâ€™ stacks and learned that incentives, not complaints, determine adoption.</li><li><strong>Transition Years (2019â€“2020):</strong> Product teams built a bridge to DNNs, but struggled to generalize their solution; we needed to rebuild our data foundation.</li><li><strong>Broader Alignment (2021â€“2022):</strong> With exec sponsorship, standardization accelerated when tech maturity, org mandate, and industry momentumÂ aligned.</li><li><strong>Scaling the Frontier (2022â€“2025):</strong> Transformer models, GPUs, and foundation models reset infra again; efficiency became the limiter, demanding deeper modeling and platform partnership.</li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*XovzZwe30rklWyWGT6vvMQ.png" /><figcaption><em>A timeline of Pinterest AI PlatformÂ eras.</em></figcaption></figure><p>There were many more stories than I have space to share, but the projects I selected highlight the themes that shaped adoption in each period. This timeline below maps the overview of the projects discussed and their part in the ML lifecycle at theÂ time.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*g-hYizf_40LyTlSvrJBPTA.png" /><figcaption><em>Evolution of Pinterestâ€™s ML Platform (2014â€“2025), featuring major projects discussed in thisÂ essay.</em></figcaption></figure><h3>Individual Team Stacks (2014â€“2015) and Early Unification (2016â€“2017)</h3><p>Pinterest ML began with excitement and fragmentation. Product teams like Home Feed, Related Pins, and Ads were all eager to use ML, but each team built its own stack with different solutions for data, training, and serving. The theme of this era was discovering that unification has to happen bottom-up, one stable layer at aÂ time.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*eSiG0qfKpT8HtBLqE0spVg.png" /><figcaption>We built abstractions one layer at a time to unify MLÂ stacks.</figcaption></figure><h4>Initial Fragmentation</h4><p>Models at the time consisted of hand-engineered features extracted with offline Hadoop jobs and fed to training in a variety of classical ML frameworks: scikit-learn, xgboost, LightGBM, and Vowpal Wabbit. Inference was often a parallel implementation of the feature extraction and inference written in the serving systems, which spanned Java, C++, and Go. But with each team building for its immediate context, we reinvented solutions repeatedly with slight variations.</p><p>With no shared abstractions and separate training and serving codepaths, one of the biggest headaches was <strong>training-serving skew</strong>: subtle differences between how features were generated offline and online that could silently tank model performance.</p><h4>Unifying Features and Modeling: Linchpin DSLÂ (2015)</h4><p>A senior staff engineer in Home Feed proposed a domain-specific language (DSL) called <strong>Linchpin</strong>. The idea was to define feature transformations once and run them in both training and serving. Linchpin programs consumed our Thrift data structures describing Pins and users and returned numeric feature vectors for ML training. The trained models (linear models and decision forests) would also be implemented in Linchpin for serving. Linchpin eliminated a whole class of training-serving mismatches and quickly became the canonical way to express feature transformations andÂ models.</p><h4>Unifying Serving: Scorpion (2016â€“2017)</h4><p>Now that we had a common approach for writing features and models, we aimed to unify the infrastructure across Home Feed, Related Pins, and Ads to fetch features and rank items. We designed <strong>Scorpion</strong>, a C++ inference service that co-located compute with cached Pin data, letting teams score thousands of Pins per request efficiently. Paired with Linchpin, it was Pinterestâ€™s first company-wide online inference engine.</p><h4>Linchpinâ€™s Later Challenges</h4><p>Building Linchpin was technically complex and fascinating, but a custom language had real tradeoffs that we discovered years later as we wanted to use more complex data sources, feature transformations, and models. Debugging was painful with minimal tooling, and new transformations often required implementing new language elements in C++, defeating the purpose of the abstraction. Later, engineers even built Python wrappers to generate Linchpin programs programmatically (â€œPynchpinâ€), creating a Rube Goldberg machine of generating, parsing, and interpreting Linchpin.</p><h4>Lessons</h4><p>Linchpin or Scorpion both had sharp edges, but they were our first experience with <em>layered unification</em>. Linchpin was a common interface for features and models that unlocked a unified servingÂ system.</p><p>Any unification is temporaryâ€Šâ€”â€Šthe future is always unknowable, and todayâ€™s stable layer will eventually give way to new abstractions. In 2015, TensorFlow 1.0 hadnâ€™t even been released and features and models were simple, so Linchpin seemed a viable solution. But when the industry shifted to Python-native frameworks and DNNs, its design turned brittle. Scorpion targeted a more stable problem (scoring with high fanout) and lasted longer, until GPUs altered the architecture.</p><h3>ML Platformâ€™s Scrappy Era (2018â€“2019)</h3><p>In late 2017, Pinterest funded a two-engineer ML Platform team. It was tiny compared to the dozens of engineers on the product teams. Our job was simple in theory: prove enough value to justify further investment. We quickly discovered how uneven the groundÂ was.</p><p>We chased pain points where we could, but were surprised to find that solving the loudest complaints didnâ€™t guarantee quick adoption. We learned that <em>perceived pain</em> of a problem is one ingredient for adoption, but <em>organizational incentive</em> is another: teams are under immense pressure to move engagement or revenue metrics now, and with high divergence across the company at this time, it was hard for product team engineers to justify significant investment for unificationâ€™s sake.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*SveZ1w0dIgB2dQNqvP90zg.png" /><figcaption><em>Most infra work in this era sat in the â€œpainfulâ€ quadrantâ€Šâ€”â€Šengineers felt the friction, but adoption lagged without aligned incentives.</em></figcaption></figure><h4>Training Orchestration: EzFlowÂ (2018)</h4><p>Iteration velocity was a common pain point. Ads teams struggled with brittle orchestration: sprawling graphs of jobs, deep inheritance chains, and growing config flags where a base-class change could silently break downstream workflows.</p><p>We built <strong>EzFlow</strong> to untangle this. It was code-first, emphasizing programmatic DAG creation rather than configuring monolithic workflows with flags. It was also lineage-based, addressing output data by hashing its input specifications, enabling caching and deduplication.</p><p>EzFlow improved the technical foundation, but adoption lagged. Under pressure to ship, Ads engineers resisted migrating to an unfamiliar system built by a two-person infra team. Sticking with the old stack felt safer than spending weeks on cleaner workflows with no immediate revenueÂ gain.</p><p>Over time, with new workflows as well as softening some design ideals, EzFlow did eventually become the dominant training orchestrator at Pinterest, significantly reducing theÂ chaos.</p><p>Many years later, as Spark and Ray made individual jobs more powerful and easier to manage, orchestration needs shrank. It was then that EzFlow finally became less relevant, and we shifted to the off-the-shelf Airflow.</p><h4>Seed BetsÂ (2019)</h4><p>Alongside EzFlow, we placed several early bets. Some took years to pay off, others shifted ownership, but many became lasting foundations.</p><p>We introduced <strong>PySpark</strong>, which later transitioned to a dedicated Data Engineering team and became widely adopted across Pinterest. We built the initial version of our <strong>Training Compute Platform</strong>, bringing TensorFlow and hyperparameter tuning onto Kubernetes. It was lightweight at the time, but soon supported much larger-scale GPU training and PyTorch. We also launched early <strong>model management</strong> through ModelHub, later replaced by MLflow and more recently Weights &amp;Â Biases.</p><p>Another significant seed bet was <strong>Galaxy</strong>, a unified signal platform we inherited when the Signal Platform team merged into ML Platform. Galaxy turned a tangle of monolithic Hadoop jobs into modular signals about users, boards, and Pins, each owned by individual teams. Galaxy was a difficult migration that required coordinating with dozens of teams and carrying large parts of it ourselves. It took years, but it later became the foundation for our unified ML feature store. Like with EzFlow, long-term value accumulated slowly until reaching a tippingÂ point.</p><h4>Lessons from the ScrappyÂ Era</h4><p>Much infra work in this era lived in the <strong>â€œpainful but not criticalâ€ quadrant</strong>. ICs tolerated brittle workflows because their roadmaps rewarded product metrics, not infra migrations. Leaders voiced support for unification, but the visible priorities remained engagement and revenue. Without aligned incentives, even strong technical solutions spreadÂ slowly.</p><p>Yet the era also planted durable seeds for a larger foundation. EzFlow eventually stabilized orchestration. Galaxyâ€™s modular signals became the backbone for our feature store. Our early Training Compute Platform paved the way for later scale-out training. A small infra team starting out faces many challenges, but their value can still compound overÂ time.</p><p>Later, in the Broader Alignment era, as technical, org, and industry factors converged, infra issues moved into the <strong>critical quadrant</strong> and these seeds came to fruition.</p><h3>Transition Years (2019â€“2020)</h3><p>By 2018, deep neural networks were quickly becoming more popular in recommendation systems. Pinterest product teams pushed ahead by building their own solutions to use DNNs, delivering big breakthroughs. But when those approaches spread beyond their original contexts, they exposed brittle foundations that needed stronger standardization.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*w0aQ4kwJEgipryAoSOJL5A.png" /></figure><h4>AutoML</h4><p>The Home Feed team was building DNNs in Linchpin, with heavy manual setup to wire inputs, generate features, and define networks by hand. They built <strong>AutoML</strong> (unrelated to Googleâ€™s) to automate this growing work. It solved two problems: generating standardized inputs for DNNs, and defining the networks with a library of TensorFlow layers configured byÂ flags.</p><p>For Home Feed, it was a breakthrough: engagement surged and new DNNs went into production quickly. AutoML demonstrated the potential of deep learning and gave executives confidence to push further. Under that pressure, it was adapted for Ads as well. But the system was tightly coupled to Home Feedâ€™s data structures, and Adsâ€™ differences forced forks. Config-driven layers ballooned into sprawling flag lists. Under the hood, AutoML relied on annotated Thrift structures parsed by a custom compiler.</p><p>This complexity was the logical outcome of local optimization and made it impossible to generalize. The deeper issue was the nature of the input data: each teamâ€™s freeform Thrift structures. Linchpin had been built to tame them, and AutoML to route around Linchpin, but the approach was becoming unmanageable.</p><h4>Rethinking the Foundation: Unified Feature Representation (UFR)</h4><p>Meanwhile, ML Platform was extending the Galaxy signal platform into a unified feature store. We introduced the Unified Feature Representation (UFR), a single container convertible into tensors for frameworks like TensorFlow and PyTorch. With these expressive frameworks, feature transforms could now move into the modelÂ itself.</p><p>UFR resembled industry peers like Twitterâ€™s DataRecord or Facebookâ€™s FBLearner schemas, and supported both hand-engineered features and raw inputs. Over time, it became the basis for Pinterestâ€™s feature store and enabled Linchpinâ€™s deprecation.</p><h4>Lessons from the Transition Years</h4><p>In these years we saw both the power and limits of local innovation. AutoML, built by a product team, rapidly unlocked DNNs in production and delivered huge engagement gains. But local teamsâ€™ incentives pushed them to patch local pain quickly, not to rebuild fundamentals. UFR was a longer-term effort to reset the foundation, which would later enable further advances to scale across Pinterest.</p><h3>Broader Alignment and Standardization (2021â€“2022)</h3><p>By 2021, ML was both Pinterestâ€™s biggest lever for growth and its biggest bottleneck. Fragmented stacks slowed iteration, and VPs in Ads and Core now saw that ML velocity directly shaped engagement and revenue. ML infra work had finally reached org-level visibility, and priorities shifted accordingly.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*jWDarcpvtS6FESqoTmpZ3A.png" /><figcaption><em>Adoption accelerates when both incentives and pain rise. Incentives increase through leadership mandates, technical necessity, and external convergence. Pain rises as scaling limits, fragility, or usability barriersÂ mount.</em></figcaption></figure><h4>Org Alignment and Exec Sponsorship</h4><p>Ads and Core orgs had internally formed efforts to provide additional ML infra. The Ads ML Infra team had grown to nearly the same scale as ML Platform, focused on urgent revenue needs. Core ML teams were building infra within their product groups to support a wide range of use cases. And ML Platform concentrated on general-purpose foundations. Each effort was valuable, but the parallel growth created duplication andÂ drift.</p><p>Leaders recognized that these streams needed to be harnessed together. The result was ML Foundations, a cross-org partnership that coordinated investmentsâ€Šâ€”â€Šenabling teams to reuse components and build up shared layers, specializing where necessary but compounding progress wherever possible.</p><p>At the same time, executives elevated ML infra to an org-level priority within Ads and Core. We introduced the ML Scorecard, grading major ML products on production readiness across data, training, deployment, and monitoring. For the first time, ML velocity was explicitly tracked at the org level, creating real pressure to raise the floor everywhere.</p><h4>MLEnv: Standardizing the TrainerÂ (2021)</h4><p>Although all training now ran on the shared Training Compute Platform, the workloads inside were chaotic: 10+ frameworks, custom CI/CD pipelines, and brittle dependencies. Important advances like distributed training, mixed precision, and GPU training were nearly impossible to roll out consistently.</p><p><strong>MLEnv</strong> began in ATG (Advanced Technologies Group, Pinterestâ€™s advanced ML team) to accelerate their PyTorch-based deep learning work, but it promised to solve company-wide pain. ATG partnered with ML Platform to expand it into a shared build and runtime stack: a monorepo with Docker, CI/CD, drivers, frameworks, and standardized tooling. It supported both TensorFlow and PyTorch to ease the transition. Later, PyTorch had clearly pulled ahead in momentum and developer experience, and Pinterest formally decided to standardize onÂ PyTorch.</p><p>Combined with the new exec sponsorship for ML Foundations, adoption jumped from &lt;5% to ~95% of jobs in a year. <a href="https://medium.com/pinterest-engineering/mlenv-standardizing-ml-at-pinterest-under-one-ml-engine-to-accelerate-innovation-e2b30b2f6768">MLEnv</a> turned a patchwork of training practices into a unified stack running reliably onÂ TCP.</p><h4>TabularML and the ML Dataset Store: Standardizing Data (2021â€“2022)</h4><p>While MLEnv unified training, data remained fragmented. UFR had freed us from Linchpin-era Thrift structs, but training datasets were still stored in ad-hoc containers that fragmented tooling.</p><p>We built <strong>TabularML</strong> to unify the dataset format, introducing a columnar Parquet format: each row was a training example and the columns were UFR. That shift halved storage costs and doubled feature backfill speeds. With Presto integration, hours of debugging could be replaced with minutes of SQLÂ queries.</p><p>On top of this we built the <strong>ML Dataset Store</strong>, a central repository managing those TabularML datasets. It provided common tools like feature backfilling, label iteration, cost tracking, and access control. Instead of every team hand-rolling ingestion jobs, they now had a consistent Python-centric API with direct Spark, Presto, and MLEnv integration.</p><p>Together, these efforts created a layered foundation: <strong>UFR</strong> standardized feature representation and <strong>Galaxy Feature Store</strong> managed their storage, production, and consumption. <strong>TabularML</strong> standardized training dataset format and the <strong>ML Dataset Store</strong> similarly managed their storage, production, and consumption. With this layering, features and datasets became reliable, reusable building blocks acrossÂ teams.</p><h4>Lessons from the Broader Alignment Era</h4><p>In this era, a number of forces converged to increase the organizational incentive to fix infra issues and amplify the perceived pain of leaving it as-is. This led to rapidly accelerating adoption.</p><p><strong>Increasing Incentive</strong></p><ul><li><strong>Direct value.</strong> In 2018, infra solved developer pain but didnâ€™t map to team business goals. By 2021, ML velocity was directly tied to engagement and revenue, when we found that most top-line wins were coming from ML experiments.</li><li><strong>Mandates.</strong> VPs sponsorship in Ads and Core made ML velocity and operational health a leadership priority, and the ML Scorecard made gaps moreÂ visible.</li><li><strong>Confidence.</strong> New infra was tested on at least one major surface before broader rollout. TabularML was co-developed with Ads, aligning initial adoption with the team that felt the most urgent need. Increasing adoption across components snowballed the belief that unification was achievable and the company would stay committed toÂ it.</li></ul><p><strong>Increasing Pain</strong></p><ul><li><strong>Technical limits.</strong> DNN and GPU-based models created bottlenecks that could no longer be easily patched locally. Distributed training, mixed precision, and GPU workflows demanded company-wide solutions likeÂ MLEnv.</li><li><strong>Process friction and usability.</strong> Teams struggled with bespoke CI/CD, dependency management, and brittle environments. Standardization cut this overhead and made training more reliable and usable acrossÂ teams.</li></ul><h3>Scaling the Frontier (2022â€“Present)</h3><p>From 2022 on, the feedback loop between modeling and infrastructure tightened, as new modeling ideas pushed infra limits. For years, ML Platformâ€™s north star was <strong>velocity</strong>: helping engineers ship more experiments. But as models grew more complex and compute-intensive, two adjacent goals became equally important: <strong>enablement</strong>, since many new architectures couldnâ€™t ship without platform support, and <strong>efficiency</strong>, since large models were only viable if we could run them at scale at reasonable cost.</p><p>The diagram below shows how these goals interlock.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*CLkQ4rDqfdlBNXbQye7hww.png" /></figure><h4>Breaking through the GPUÂ Frontier</h4><p>Our initial DNN recommenders were served on CPUs, relying on pre-computed representations of Pins and users. User representations eventually included compute-heavy transformer models that we would run on GPUs in offline batch processes. The next logical step to improve accuracy was to bring those transformers online and ingest recent user activity in real-time. Analysis indicated that these real-time models would outperform every prior generation, but latency and cost would explode by orders of magnitude onÂ CPU.</p><p>Our initial attempts at GPU inference were bottlenecked on low GPU utilization. We rebuilt the Scorpion serving stack around one goal: keep the GPU busy. We consolidated work onto fewer GPU hosts with SSD caches, minimized CPU-GPU handoffs, dynamic batched work from multiple user requests, and created custom CUDA ops and adopted half-precision inference. This <a href="https://medium.com/@Pinterest_Engineering/gpu-accelerated-ml-inference-at-pinterest-ad1b6a03a16d">efficiency<strong> </strong>work enabled Pinterestâ€™s first GPU-served production model</a>: a <a href="https://medium.com/pinterest-engineering/how-pinterest-leverages-realtime-user-actions-in-recommendation-to-boost-homefeed-engagement-volume-165ae2e8cde8">transformer model that used recent user actions</a> in online inference. It boosted Homefeed engagement by 16% in one launch, and GPU-based models now supported 100Ã— larger architectures without increasing cost and latency. Without this efficiency work, the model wouldnâ€™t have shipped: efficiency gated enablement of this new architecture.</p><h4>Ripple Effects of the GPUÂ Shift</h4><p>GPU serving spread quickly, leading to new cost efficiency and dev velocity concerns and spurring further changes to our system architecture.</p><p>The initial GPU serving launch relied on a delicate balance of CPU- and GPU-bound work. We increased efficiency and flexibility with <strong>remote inference,</strong> splitting CPU/memory-bound tasks (data fetching &amp; caching) from GPU computation into separate hosts. We could then scale CPU capacity independently of the much more expensive GPU hosts. Multiple CPU/cache hosts could now feed large batched requests to GPUÂ workers.</p><p>A similar pattern emerged on training to keep GPUs fed with data. GPUs were often bottlenecked on CPU work to download, parse and preprocess training data. We rebuilt the â€œlast mileâ€ of training data pipelines with the <strong>Ray distributed computing framework</strong>: data could be <a href="https://medium.com/pinterest-engineering/last-mile-data-processing-with-ray-629affbf34ff">preprocessed just-in-time</a> on a cluster of CPU hosts and streamed into GPU trainers. Now training, too, could independently scale CPU and GPU resources asÂ needed.</p><p>We adopted Ray for efficiency initially, but it also <strong>improved velocity and enabled new capabilities</strong>. Previously, training data had to be fully materialized via heavy Spark jobs and then stored. A new feature set or labeling scheme often meant days of re-processing and terabytes of increased storage. With just-in-time preprocessing, modelers could skip this step, saving time and storage cost. They could also now <a href="https://medium.com/pinterest-engineering/scaling-pinterest-ml-infrastructure-with-ray-from-training-to-end-to-end-ml-pipelines-4038b9e837a0">try modeling directions that were previously too costly and slow to attempt</a>, like generating user histories of different lengths or filtering inputs by context on theÂ fly.</p><p>As GPU serving scaled, we also encountered new <strong>operational velocity challenges</strong>: our architecture was historically designed for small CPU models, mirroring all models across the full cluster. GPU-based models were much larger, and soon we could not fit all experimental models concurrently in the GPU memory of a single machine. <strong>Model Farm</strong> enabled hosts to serve different subsets of models, while still presenting a unified cluster interface that can route to the correct host. We also began moving workloads to Kubernetes to spin up new clusters more easily. Although conceptually simple, it was a significant architectural change. It also improved batch consolidation and therefore efficiency.</p><p>These ripple effects revealed how connected our three goals had become: efficiency enabled transformers, which created new scaling issues and demanded further efficiency and velocity solutions. Ray improved efficiency, unlocking velocity and enabling new modeling capabilities. Model Farm improved velocity and then also efficiency.</p><h4>Modeling and Infra Work Fuse, AdvancesÂ Compound</h4><p>From 2023â€“2025, modeling ideas and infrastructure advances compounded together. By 2025, foundation ranking models represented the culmination: extending earlier modeling approaches while drawing on infrastructure built across multipleÂ efforts.</p><h4>Scaling to Large Embedding Tables for Memorization (2023)</h4><p>As recommender models grew, teams reached limits in how much user and content behavior they could represent with pre-trained content embeddings derived from each Pinâ€™s image, text, and graph connections. These embeddings captured broad semantic relationships but smoothed over item-specific nuances. To model fine-grained behavioral patternsâ€Šâ€”â€Šhow individual users interact with specific Pins and adsâ€Šâ€”â€Šteams adopted <strong>large ID embedding tables</strong>: billions of parameters that directly memorize these patterns and are updated continuously during training rather than frozen after pre-training.</p><p>These tables created immediate infrastructure challenges. Single-GPU memory limits made training and serving infeasible without architectural changes. We extended our stack to support <strong>distributed model-parallel training</strong>, sharding embedding tables across multiple GPUs, and introduced a <strong>hybrid CPU/GPU serving architecture</strong> that stored embeddings on high-memory CPU hosts while executing model inference on GPUs. At serving, we also added <strong>INT4 quantization</strong> and other optimizations, cutting memory and latency overhead while maintaining modelÂ quality.</p><p><a href="https://arxiv.org/pdf/2508.18700">Large embedding table models</a> and their resulting memorization capabilities were soon adopted across Related Pins, Home Feed, and Ads, improving engagement and revenue. Our platform gained a foundational capability: training and serving models too large for singleÂ GPUs.</p><h4>Scaling to Long User Sequences (2024)</h4><p>Separately, teams scaled transformer models from short sequences (hundreds of recent actions) to <a href="https://medium.com/pinterest-engineering/next-level-personalization-how-16k-lifelong-user-actions-supercharge-pinterests-recommendations-bd5989f8f5d3">lifelong sequences (16k+ user actions spanning years)</a>. This promised better personalization but created efficiency challenges at serving time. We worked with teams through numerous optimizations, including:</p><ul><li><strong>Request-level deduplication. </strong>Since recommenders score thousands of items per user request, we added an inference server mechanism to process request data once (i.e. the large user sequences) and share them across all the items toÂ score.</li><li><strong>Sparse tensor handling.</strong> We enabled sparse data formats to be carried all the way into the GPU during inference.</li><li><strong>Custom GPU kernels in Triton.</strong> We fused multiple GPU steps into one, tailored to this specific model. The ability to easily write custom GPU kernels was a breakthrough for this and future model architectures.</li></ul><p>This effort was part of a trend of building infrastructure specialized for specific model architectures. Platform engineers needed to understand transformer internalsâ€Šâ€”â€Šattention mechanisms, sequence processing patternsâ€Šâ€”â€Što optimize effectively.</p><h4>Foundation Ranking ModelsÂ (2025)</h4><p>The ATG team built <a href="https://arxiv.org/pdf/2507.12704">foundation ranking models</a> that extended the large embedding table approach from 2023, now pre-training a single shared model across user activities on all surfaces before fine-tuning for each specific use caseâ€Šâ€”â€Ša foundation-style approach similar to modernÂ LLMs.</p><p>This modeling advance compounded previous modeling and infrastructure work. We worked with ATG to leverage the hybrid CPU/GPU architectures from 2023 and many optimizations from 2024, while also adding new compression and quantization techniques. Together, previous infra and modeling capabilities formed a ladder for each subsequent step.</p><h4>Emerging Frontier: Generative AI and LLMsÂ (2024â€“)</h4><p>As our highest-traffic recommender systems matured, a new frontier of generative AI models began to take shape. Weâ€™ve been developing infrastructure to help teams build image-generation models (like <a href="https://medium.com/pinterest-engineering/building-pinterest-canvas-a-text-to-image-foundation-model-aa34965e84d9">Pinterest Canvas</a>) as well as large language models. These workloads bring a different set of pressures: massive compute needs but lower QPS, KV caching needs, prompt management, dynamic control flow at inference, and much tighter research-production loops.</p><p>These models are already reshaping our priorities and we see a familiar pattern starting: rapid exploration followed by consolidation into new foundations. The next phase of AI at Pinterest will depend on how quickly we can evolve efficiency, velocity, and enablement for this new class ofÂ models.</p><h4>Lesson of the FrontierÂ Era</h4><p><strong>Efficiency, velocity, and enablement are closely interlinked.</strong> Online transformers only became viable once GPU optimizations improved efficiency by orders of magnitude. The shift to GPU spurred further efficiency and velocity concerns; their solutions, like Ray, themselves unlocked new velocity and enablement.</p><p><strong>Modeling and platform fuse and capabilities compound.</strong> In this era, modeling is infra-bound: the cutting edge of modeling is now closely tied to infra capabilities. At the same time, we are seeing accelerating compounding of both modeling and infra advances in each new generation ofÂ models.</p><h3>Closing Reflections</h3><p>A rhythm emerged over the last decade: local breakthroughs showed what was possible, but endured only when rebuilt on shared foundations. Each wave, from DNNs to GPU inference to foundation models, forced us to re-examine assumptions and rebuild the stack around new realities.</p><p>For other companies on similar journeys, the takeaway isnâ€™t about specific technologiesâ€Šâ€”â€ŠLinchpin, Scorpion, and MLEnv will all be rebuilt in time. Itâ€™s about sensing when to unify and when to explore. Unify too early and you ossify around the wrong abstractions; too late and youâ€™re paralyzed by fragmentation. The art is in theÂ timing.</p><p>Modeling and platform work are now intertwined, shaping the next phase of Pinterestâ€™s AI infrastructure. The abstractions will keep changing, but the rhythm will stay theÂ same.</p><h3>Acknowledgements</h3><p>This essay reflects my vantage point on work done by teams across the company, including ML Platform, Ads ML Infra, Core ML Infra, ATG, Data Platforms Science, Data Engineering. Our customer teams have been co-creators throughout, including Home Feed, Related Pins, Search, Ads Ranking &amp; Retrieval, Data Science, and manyÂ more.</p><p>Thanks to Pong Eksombatchai and Saurabh Vishwas Joshi for reviewing drafts and helping refine the narrative.</p><p>Special thanks to Dave Burgess, whose guidance and support were crucial in building organizational alignment and growing ML Platform, and to Chunyan Wang for continued support as weâ€™ve scaled to the frontier. To the engineers who built these systemsâ€Šâ€”â€Šfrom the early foundations to todayâ€™s massive scaleâ€Šâ€”â€Šand to the product and program managers who guided direction and drove adoption across teams, this story belongs to all ofÂ you.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=4e3b37c0f758" width="1" height="1" alt=""><hr><p><a href="https://medium.com/pinterest-engineering/a-decade-of-ai-platform-at-pinterest-4e3b37c0f758">A Decade of AI Platform at Pinterest</a> was originally published in <a href="https://medium.com/pinterest-engineering">Pinterest Engineering Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Identify User Journeys at Pinterest]]></title>
            <link>https://medium.com/pinterest-engineering/identify-user-journeys-at-pinterest-b517f6275b42?source=rss-ef81ef829bcb------2</link>
            <guid isPermaLink="false">https://medium.com/p/b517f6275b42</guid>
            <category><![CDATA[pinner-experience]]></category>
            <category><![CDATA[engineering]]></category>
            <category><![CDATA[recommender-systems]]></category>
            <category><![CDATA[pinterest]]></category>
            <category><![CDATA[notifications]]></category>
            <dc:creator><![CDATA[Pinterest Engineering]]></dc:creator>
            <pubDate>Tue, 21 Oct 2025 21:42:27 GMT</pubDate>
            <atom:updated>2025-10-21T21:42:27.185Z</atom:updated>
            <content:encoded><![CDATA[<p>Lin Zhu | Sr. Staff Machine Learning Engineer<br>Jaewon Yang | Principal Machine Learning Engineer<br>Ravi Kiran Holur Vijay | Director, Machine Learning Engineering</p><p>Pinterest has always been a go-to destination for inspiration, a place where users explore everything from daily meal ideas to major life events like planning a wedding or renovating a home. Our core mission is to be an inspiration-to-realization platform. To fulfill this, we recognized a critical challenge: we needed to move beyond understanding immediate interests and comprehend the underlying, long-term goals of our users. Therefore, we introduce user journeys as the foundation for recommendations.</p><p>We define a journey as <strong>the intersection of a userâ€™s interests, intent, </strong>and<strong> context at a specific point in time</strong>. A user journey is a sequence of user-item interactions, often spanning multiple sessions, that centers on a particular interest and reveals a clear intentâ€Šâ€”â€Šsuch as exploring trends or making a purchase. For example, a journey might involve an interest in â€œsummer dresses,â€ an intent to â€œlearn whatâ€™s in style,â€ and a context of being â€œready to buy.â€ Users can have multiple, sometimes overlapping, journeys occurring simultaneously as their interests and goalsÂ evolve.</p><p>Inferring user journeys goes beyond understanding immediate interests, it allows us to comprehend the underlying, long-term goals of our users. By identifying user journeys, we can move from simple content recommendations to becoming a platform that assists users in achieving their goals, whether itâ€™s planning a wedding, renovating a kitchen, or learning a new skill. This aligns with Pinterestâ€™s mission to be an inspiration-to-realization platform, and provides the foundation for journey-aware recommendations.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*E8Pphp5RrtKw4ajYVm3nEA.png" /><figcaption>Figure 1: Example of notifications based on userÂ journey</figcaption></figure><h3>Our Solution Philosophy</h3><p>From the outset, we knew we were building a new product without large amounts of training data. This constraint shaped our engineering philosophy for thisÂ project:</p><ul><li><strong>Be Lean:</strong> Minimize the development of new components where no dataÂ exists.</li><li><strong>Start Small:</strong> Begin with a small, high-quality dataset of a few hundred human-annotated examples.</li><li><strong>Leverage Foundation Models:</strong> Utilize pretrained models, like pretrained <a href="https://arxiv.org/abs/2404.16260">SearchSage</a> for keyword embeddings, to maximize cost efficiency and effectiveness.</li><li><strong>Make it Extensible:</strong> Design a system that supports more complex models as we collect more data, with a clear path to incorporating more advanced ML and LLM techniques.</li></ul><h3>System Architecture: A Walkthrough</h3><p>To identify these journeys, we evaluated two primary approaches:</p><ol><li><strong>Predefined Journey Taxonomy:</strong> Building a fixed set of journeys and mapping users to them. While this offers consistency, it risks overlapping with existing systems, requiring significant maintenance, and being slow to adapt to newÂ trends.</li><li><strong>Dynamic Keyword Extraction:</strong> Directly extracting journeys from a userâ€™s activities, representing each journey as a cluster of keywords (queries, annotations, interests, etc.).</li></ol><p>We chose the <strong>Dynamic Extraction</strong> approach to generate journeys based on the userâ€™s information. It offered greater <strong>flexibility, personalization, and adaptability</strong>, allowing the system to respond to emerging trends and unique user behaviors. This method also allowed us to leverage existing infrastructure and simplify the modeling process by focusing on clustering activities for individual users.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*MUa4ynApKMdwwuL6wpni3w.png" /><figcaption>Figure 2: High-level journey aware notification systemÂ design</figcaption></figure><p>At a high level, we extract keywords from multiple sources and employ hierarchical clustering to generate keyword clusters; each cluster is a journey candidate. We then build specialized models for journey ranking, stage prediction, naming, and expansion. This inference pipeline runs on a streaming system, allowing us to run full inference if thereâ€™s algorithm change, or daily incremental inference for recent active users so the journeys respond quickly to a userâ€™s most recent activities.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*xPQt3cUCLU1bqlEpxXcWcQ.png" /><figcaption>Figure 3: User journey inference pipeline via Streaming system</figcaption></figure><p>Letâ€™s break down the key components of this innovative system:</p><h3>1. User Journey Extraction and Clustering</h3><p>This foundational component is designed to generate fresh, personalized journeys for eachÂ user.</p><ul><li><strong>Input Data:</strong> We leverage a rich set of user data, including:<br>â€Šâ€”â€Š<strong>User search history:</strong> Aggregated queries and timestamps.<br>â€Šâ€”â€Š<strong>User activity history:</strong> Interactions like Pin closeups, repins, and clickthroughs, extract the annotations and interests from the engaged Pins.<br>â€Šâ€”â€Š<strong>Userâ€™s boards:</strong> Extract the annotations and interests from the Pins in the userâ€™sÂ boards.</li><li><strong>User Journey Clustering:</strong> We treat all the queries, annotations, and interests as keywords with metadata. Then we adopt the pretrained text embedding for the keywords to perform hierarchical clustering to form journey clusters.</li></ul><h3>2. Journey Naming &amp; Expansion</h3><p>Clear and intuitive journey names are crucial for user experience.</p><ul><li><strong>Journey Naming:</strong> The current production model is to apply a ranking model to pick the top<strong> </strong>keyword<strong> </strong>extracted from each cluster as the journey name. It balances personalization and simplicity by choosing the most relevant keywords from the cluster. We are working with scaling LLM<strong> </strong>for Journey Name<strong> </strong>Generation, which promises highly personalized and adaptable names.</li><li><strong>Journey Expansion:</strong> We leverage LLMs to generate new journey recommendations based on a userâ€™s past or ongoing journeys, with an emphasis on balancing the predictive power of LLMs and efficiently serving through pre-generated recommendations. In the initial stage, we focus on creating non-personalized, related journeys based on a given input journey. Since the total number of journeys is limited, we can use LLMs to generate this data offline and store it in a key-value store. For personalized recommendations, we will apply the journey ranking model online to rank related journeys for eachÂ user.</li></ul><h3>3. Journey Ranking &amp; Diversification</h3><p>To ensure the most relevant journeys are presented, and to prevent monotony, we built a ranking model and applied diversification afterwards.</p><h3>Journey Ranking</h3><p>Similar to traditional ranking problems, our initial approach is to build a point-wise ranking model. We get labels from user email feedback and human annotation. The model takes user features, engagement features (how frequently the user engaged on this journey through search, actions on Pins, etc.) and recency features. This provides a simple, immediate baseline.</p><h3>Journey Diversification</h3><p>To prevent the top ranked journeys from always being similar, we implement a diversifier after the journey ranking stage. The most straightforward approach is to apply a penalty if the journey is similar to the journeys that ranked higher (the similarity is measured using pretrained keyword embedding). For each journey i, score will be updated based on the formula below. Finally, we re-rank the journeys according to the updatedÂ score.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*dHiQ1v0EtujoF4eYuHdB6A.png" /></figure><p><em>Occurrence</em> is the number of similar journeys ranked before the current journey, and <em>penalty</em> is a hyperparameter to tune, usually chosen asÂ 0.95.</p><h3>4. Journey Stage Prediction</h3><p>Understanding a journeyâ€™s lifecycle helps us determine appropriate notification timing. We simplify this into two objectives:</p><ul><li><strong>Situational vs. Evergreen Classification: </strong>Journeys are categorized based on user engagement patterns and activity duration. If users engage with a journey consistently over an extended period, we classify it as â€œEvergreenâ€â€Šâ€”â€Šthese journeys remain perpetually active. In contrast, journeys with engagement limited to a shorter timeframe are classified as â€œSituational,â€ as they are expected to conclude at a certainÂ point.</li><li><strong>Journey Stage (Ongoing vs. Ended) Classification:</strong> For situational journeys, we evaluate whether the journey is still ongoing or has ended, primarily by analyzing the time since the userâ€™s last engagement. Future improvements will include incorporating user feedback and developing a supervised model for more accurate classification.</li></ul><h3>5. User JourneysÂ Output</h3><p>The user journeys could be used in downstream applications for retrieval and ranking. The desired output is a list of distinct user journeys. Each journey should ideally be represented with:</p><ul><li><strong>Journey Name:</strong> A concise and descriptive name (e.g., â€œKitchen Renovation,â€ â€œImproving Home Organization,â€ â€œEngagement Ring Selectionâ€).</li><li><strong>Keywords: </strong>List of keywords related to this journey; it could be the corresponding interests, annotations, queries, or any keywords.</li><li><strong>Stage:</strong> An indicator of where the user is within that journey (e.g., â€œinspiration,â€ â€œactionâ€); we simplified it to â€œongoingâ€ or â€œendedâ€ in the initialÂ launch.</li></ul><p><strong>Confidence Score:</strong> The confidence score for this predicted journey.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*0Nrn-ARKep8uaEvbzILYGA.png" /><figcaption>Figure 4: User journey inference examples</figcaption></figure><h3>6. Relevance Evaluation</h3><p>We aim to establish a robust evaluation and monitoring pipeline to ensure consistent and reliable quality assessment of top-k user journey predictions. Because human evaluation is costly and sometimes inconsistent, we leverage LLMs to assess the relevance of predicted user journeys. By providing user features and engagement history, we ask the LLM to generate a 5-level score with explanations. We have validated that LLM judgments closely correlate with human assessments in our use case, giving us confidence in this approach.</p><h3>Experiment Results</h3><p>We applied user journeys inference to deliver notifications related to the userâ€™s ongoing journeys. Our initial experiments demonstrate the significant impact of Journey-Aware NotificationsÂ¹:</p><ul><li>The system drove statistically significant gains in user engagements.</li><li>Compared to our existing interest-based notifications, journey-aware notifications demonstrated an <strong>88% higher email click rate</strong> and a <strong>32% higher push openÂ rate</strong>.</li><li>User surveys revealed a <strong>23% increase in positive feedback rate</strong> compared to interest-based notifications.</li></ul><h3>Ongoing Effort</h3><p>As a follow up, we are working on leveraging large language models (LLMs) to infer user journeys given user information and activities, while offering several key benefits:</p><ul><li><strong>Simplification: </strong>Many existing components of the journey inference systemâ€Šâ€”â€Šincluding keyword extraction, clustering, journey naming, and stage prediction modelsâ€Šâ€”â€Šcan be consolidated and replaced with a singleÂ LLM.</li><li><strong>Quality Improvement:</strong> By utilizing the advanced capabilities of LLMs to understand user behavior, we aim to significantly enhance the accuracy and quality of user journey predictions.</li></ul><p>We tuned our prompts and used GPT to generate ground truth labels for fine-tuning <a href="https://huggingface.co/Qwen/collections">Qwen</a>, enabling us to scale in-house LLM inference while maintaining competitive relevance. Next, we utilized <a href="https://docs.ray.io/en/latest/data/batch_inference.html">Ray</a> batch inference to improve the efficiency and scalability. Finally, we are implementing full inference for all users and incremental inference for recently active users to reduce overall inference costs. All generated journeys will go through safety checks to ensure they meet our safety standards.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*o9e7-eLg1LHGmaf0MrCHcQ.png" /><figcaption>Figure 5: User Journey inference usingÂ LLMs</figcaption></figure><h3>Acknowledgement</h3><p>Weâ€™d like to thank Kevin Che, Justin Tran, Rui Liu, Anya Trivedi, Binghui Gong, Randy Tumalle, Tianqi Wang, Fangzheng Tian, Eric Tam, Manan Kalra, Mengtian Hu and Mengying Yang for their contribution!</p><p>Thanks Jeanette Mukai, Darien Boyd, Samuel Owens, Justin Pangilinan, Blake Weber, Gloria Lee, Jess Adamiak for the product insights!</p><p>Thanks Tingting Zhu, Shivani Rao, Dimitra Tsiaousi, Ye Tian, Vishwakarma Singh, Shipeng Yu, Rajat Raina and Randall Keller for theÂ support!</p><p>Â¹Pinterest Internal Data, USA, April-May 2025</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=b517f6275b42" width="1" height="1" alt=""><hr><p><a href="https://medium.com/pinterest-engineering/identify-user-journeys-at-pinterest-b517f6275b42">Identify User Journeys at Pinterest</a> was originally published in <a href="https://medium.com/pinterest-engineering">Pinterest Engineering Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Tracking Down Mysterious ML Training Stalls]]></title>
            <link>https://medium.com/@Pinterest_Engineering/tracking-down-mysterious-ml-training-stalls-5290bb19be6d?source=rss-ef81ef829bcb------2</link>
            <guid isPermaLink="false">https://medium.com/p/5290bb19be6d</guid>
            <category><![CDATA[machine-learning]]></category>
            <category><![CDATA[pinterest]]></category>
            <category><![CDATA[pytorch]]></category>
            <category><![CDATA[problem-solving]]></category>
            <category><![CDATA[engineering]]></category>
            <dc:creator><![CDATA[Pinterest Engineering]]></dc:creator>
            <pubDate>Fri, 17 Oct 2025 16:01:48 GMT</pubDate>
            <atom:updated>2025-10-17T17:35:34.781Z</atom:updated>
            <content:encoded><![CDATA[<p>Chen Yang, Andrew Yu, Shunyao Li, Pong Eksombatchai, MarkÂ Molinaro</p><h3>Introduction</h3><p>Pinterestâ€™s ML training platform, MLEnv, is a collection of external software dependencies and in-house built high performance ML library centered around Meta AIâ€™s PyTorch. Upgrading the PyTorch version involves the upgrade of a web of dependencies together with necessary code changes. In most cases, this is just mundane workâ€¦ until itÂ isnâ€™t.</p><p>During Pinterestâ€™s latest effort to upgrade PyTorchâ€™s version, instead of seeing neutrality or improvements of training throughput, we saw a sharp drop (&gt;50%) in performance. Debugging the root cause turns out to be an interesting journey, from observing the throughput impact all the way down to identifying the culprit low-level linux kernels. This blog post documents our debugging process. We hope it can help a broader audience for their futureÂ work.</p><h3>Background</h3><p>Before we start, let us briefly review the ML model training process. A distributed data parallel ML trainer generally contains these steps: data loading, model forward and backward pass, all-reduce synchronization among GPU ranks, and an optimizer that applies the gradients to model weights. Depending on the model architecture, there may be additional synchronization points during the forward and backward pass. At Pinterest, we also leverage Anyscaleâ€™s Ray to horizontally scale the data loader (see <a href="https://medium.com/pinterest-engineering/ray-infrastructure-at-pinterest-0248efe4fd52">here</a>), where the bulk of data transforms are put in a remote CPU worker node, and only a lightweight data loader is kept within the main training process. With such a setup, we not only eliminated the data loading bottleneck, but also unlocked many interesting opportunities (such as <a href="https://medium.com/pinterest-engineering/last-mile-data-processing-with-ray-629affbf34ff">Ray Last-mile Data Processing</a>).</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*ENfRNMfnMaSJ-JPY5DHnyQ.png" /><figcaption><em>Figure 1: Pinterest ML Trainer Architecture</em></figcaption></figure><p>To keep our ML infrastructure modern and empower our MLEs with the best tools, it is paramount that we upgrade PyTorch to its latest versions. A large part of GPU efficiency gains comes from the adoption of optimized GPU kernels and the adoption of torch.compile; both are quickly iterated and improved upon each PyTorchÂ upgrade.</p><h3>The Problem</h3><p>Unfortunately, our latest PyTorch upgrade effort resulted in a 50% drop in training throughput in terms of examples per second. Something was obviously wrong, but given that a web of dependencies were also changed together with a fairly complex training process setup, it was a daunting task to root cause the issue. What caused theÂ stalls?</p><h3>Fix the GPUÂ Roofline</h3><p>To narrow down the problem space, the first thing we checked was the <em>GPU roofline throughput</em>, a metric that measures the training throughput when we remove the data loader from the trainer by caching a batch on the GPU and reusing it. If the data loader is not a bottleneck, then the GPU roofline throughput provides an upper bound estimation; otherwise, one should focus on data loader optimization in order to improve the end-to-end throughput.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*MCrJxvLgiQ3RrYMLMDRbmA.png" /><figcaption><em>Figure 2: GPU Roofline Throughput Measurement Architecture</em></figcaption></figure><p>It turned out that for the GPU roofline measurement, the upgraded PyTorch version already resulted in a -20% in throughput. Without fixing this, there was no way we could achieve neutral performance.</p><h3>Breakdown theÂ Model</h3><p>Thereâ€™s no need to worry about data loading since we focused on the model. Nothing stood out during our initial examination of the PyTorch profiler trace. It seemed as if it was generally slowed down at every module. Or isÂ it?</p><p><strong><em>Were there specific modules that contributed to the slow down the most?</em></strong> To answer this question, we took the modules out of the model one by one until the throughputs were identical between versions. We were able to identify one of our most expensive transformer modules (letâ€™s call it <em>module A</em>) to be the culprit. Close examination of the PyTorch profiler trace of this module revealed something interesting: in the upgraded PyTorch version, we no longer saw the CompiledFunctions (Figure 3). We further verified that turning off the `torch.compile` indeed resulted in identical throughput.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*AvxTuAPyw7ptfec__d8Jng.png" /><figcaption><em>Figure 3: CompiledFunction PyTorch Profiler Trace for a NormalÂ Trainer</em></figcaption></figure><h3>Debug torch.compile</h3><p><strong>Was </strong><strong>torch.compile broken in the new PyTorch version for this module?</strong> We turned on TORCH_LOGS=dynamo,inductor and reran the training, which showed that the problematic module was successfully compiled, but one line of log caught ourÂ eyes:</p><p>skipping: &lt;operator_name&gt; (<strong>reason: non-infra torch dispatch mode present</strong>, this is not supported today in torch.compile)</p><p>It indicates that the CompiledFunction was somehow skipped duringÂ runtime.</p><p>To isolate the issue, we built a series of minimal reproducible scripts to check if CompiledFunction would disappear in theÂ trace:</p><ul><li>#1, only initializes module A and runs forward/backward in a simple trainingÂ loop</li><li>#2, initializes a simple model, i.e., our prod model, but removes everything else except module A, and runs it in a simple trainingÂ loop</li><li>#3, initializes the simple model and runs our trainer classâ€™s trainingÂ loop</li></ul><p>Only #3 reproduced the issue, i.e., the disappearance of CompiledFunction in the trace. It became very clear that our trainer class interferes with torch.compile.</p><h3>Pinpoint theÂ Issue</h3><p>Having narrowed down the issue to the trainer class, we had a much smaller code search space. We again built a series of repro cases by removing components of the trainer class, and finally noticed that the forward and backward pass were wrapped by a contextÂ manager:</p><pre>with FlopCountMode():<br>  ...</pre><p>It was added by our infrastructure engineers to track the flops during training, and it was enabled by default for every model. We recognized it as a PyTorch dispatch mode and connected it to the earlier suspicious log, so we disabled this mode, and it worked! The CompiledFunction reappeared in our regular training trace. For some unknown reason it only adversely affected the newer PyTorch version. We decided it is not worth our time to dig deeper, as there were more urgent issues to resolve. We will discuss theseÂ next.</p><h3>Fix the End-to-End Throughput</h3><p>Unfortunately, fixing torch.compile did not improve end-to-end throughput atÂ all.</p><p>But now it has to be data loading, right? No. In the last section, when we measured the GPU roofline throughput, we took Ray completely out of the picture by launching the training as a native PyTorch application. When we launched the training as a Ray application, the GPU roofline throughput still suffered. This helped us rule out ray.data as the potential culprit.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*c65n8c6DTghYrvm8_6pc-A.png" /><figcaption><em>Figure 4: GPU Roofline Throughput Measurement Architecture for a Ray Application</em></figcaption></figure><h3>Observations</h3><p>Here are a few initial observations from various logs and a profilerÂ trace:</p><ul><li>O1.1: The slow down is caused by slow iterations: roughly every three seconds there is one slow iteration, while every other iteration has identical latency compared to a normalÂ run.</li><li>O1.2: During the slow iteration, we observed a straggler effect: at synchronization points (usually during a NCCL kernel), one GPU rank was slow while every other rank needed to wait for it to catch up. Whatâ€™s more interesting was that it seemed each GPU rank took turns in a round robin fashion to be the slowÂ rank.</li><li>O1.3: The slow rank could seemingly be stuck at any random Python function that is usuallyÂ trivial.</li><li>O1.4: Enabling Nvidiaâ€™s Nsight Systems profiler somehow eliminated the slowness: this was the weirdest observation, yet it was very consistent and reproducible.</li><li>O1.5: Distributed training wasnâ€™t the culprit either, since with a single GPU we could reproduce the slownessÂ too.</li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*vFxTDdUu4cm9h-y7lb93tw.png" /><figcaption><em>Figure 5: The Straggler Effect in the Slow Iteration</em></figcaption></figure><p>O1.1 led us to do a code search of three-second timers in the Ray repository. We found a few, but changing them made no differences. O1.3 made us suspect Pythonâ€™s Global Interpreter Lock (GIL). Both Ray and PyTorch make extensive use of C++ extensions. Maybe in some other thread the GIL wasnâ€™t properly released causing the main thread to be stuck? However it turned out to be challenging to verify this hypothesis since the PyTorch profiler doesnâ€™t capture GIL events, and we cannot rely on the Nsight System due to O1.4, which itself was quite a mystery. O1.5 allowed us to focus on a single GPU rank without spending effort to find which rankÂ stalled.</p><p><strong>Is it torch.compileâ€¦ again?</strong> We tried turning off torch.compile completely in the Ray setup, and it indeed resulted in identical throughputs between old and new PyTorch versions. In order to recoup the efficiency gains from torch.compile, we started to explore fine-grained compilation by only compiling certain modules, and we were able to achieve a better end-to-end throughput than with no compilation at all. Although we could not settle on such a solution, we learned one more observation: <strong>it was the modules with the most extensive graph breaks that caused the slowÂ downs.</strong></p><p>Given this observation, we came up with a minimal reproducible model that contained a single module where we artificially exploded the number of graph breaks. We used it to significantly speed up the debug iteration.</p><h3>Diagnosis</h3><p>Using the minimal reproducible model, we had the following observations:</p><ul><li>O2.1: Regular iterationsâ€™ latency was 5ms, while slow iterations wereÂ 25ms.</li><li>O2.2: Slow iterations still happened roughly three secondsÂ apart.</li><li>O2.3: Occasionally there were some 500ms slow iterations.</li><li>O2.4: Enabling Nsight System increased the regular latency to 12ms, but we also observed 19ms slower iterations.</li></ul><p>O2.4 was especially critical, as the collected Nsight System trace on the minimal reproducible model became useful again. From the trace, it was very clear that no matter how long the slow iteration was (~25ms, or ~500ms), the main training thread held the GIL. This invalidated the hypothesis that GIL was the culprit. Closely examining the big pause, i.e., the 566ms in this example, we realized that it was due to the Python garbage collection. While it was a severe slow down for the minimal repro model, we did not have such observations in the regular training. Hence we believed that those small pauses, i.e., ~25ms iterations, were the true culprit for our training.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*ebMMwrQXXVBEh46cgrFliw.png" /><figcaption><em>Figure 6: Nsight System Trace for the BigÂ Pause</em></figcaption></figure><p>For the small pauses, however, we no longer saw the CPU backtrace and could not peak into what the thread was doing. It was holding the GIL and not making any progress, yet the CPU utilization was 100%. This led us to think that maybe it was other processes that were starving the training process by occupying the CPU. Thus far, we had been exclusively focusing on the training process. It was time to expand the searchÂ scope.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*DaXYgFPRRtsS1-8O-Aw0vQ.png" /><figcaption><em>Figure 7: Nsight System Trace for the SmallÂ Pause</em></figcaption></figure><h3>Finding the RootÂ Cause</h3><p>This time we resorted to the linux perf tool, we used the -a argument to collect callstacks for all the processes during a period. We knew the slow down would happen roughly every three seconds, so collecting a four second trace should be more than enough to captureÂ it.</p><p>A proper visualization tool is also needed to examine the collected trace. Tools like Flamegraph arenâ€™t suitable for this task, as it aggregates the callstacks, making the small pauses like 6ms impossible to find in a trace of seconds long for dozens of processes. We needed a time series representation, and chrome://tracing came to mind. Yet a format conversion tool wasnâ€™t readily available. We turned to GenAI and asked it to generate a conversion script. With a few manual tweaks, the conversion script worked perfectly, and we were able to visualize the events as shown in the following graph.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*L_lPD9yzO3MjWqex37lLPQ.png" /><figcaption><em>Figure 8: Time Series Callstack for the Main TrainingÂ Process</em></figcaption></figure><p>Identifying the correct small pause in the trace wasnâ€™t hard, we just needed to find the gaps where the training thread made no progress and the duration roughly fit the observation.</p><p>When we scrolled down to examine other processes, one of the Python processes looked suspicious: it was seemingly doing some expensive computing. Within the callstack, we noticed a call named smap_gather_stats, which is a linux kernel call that gathers virtual memory statistics.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*pc8MPcT0hhfO6Csm3M40Yg.png" /><figcaption><em>Figure 9: Time Series Callstack for the Suspicious PythonÂ Process</em></figcaption></figure><p>Zooming out and collecting a 10 second trace convinced us that this was very likely the culprit. This exact same callstack pattern happened roughly every three seconds, and we could always find a gap in the main training process within its callÂ period.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*zT644P1_Z34Ck53WVUE-3g.png" /><figcaption><em>Figure 10: Zoom Out Time Series Callstack for the Suspicious PythonÂ Process</em></figcaption></figure><p>Unfortunately the perf callstack did not show us the Python stacktrace, so we had to use some tricks to identify the process. We recollected a trace while pausing the program at the very end, identified the pid of this Python process through searching the kernel name â€œsmap_gather_statsâ€, and then went back to the dev machine and did a ps aux | grep &lt;pid&gt;.Â Voila!</p><h3>The Culprit andÂ Solution</h3><p>The culprit: the Ray monitoring process ray/dashboard/agent.py. In particular, one of its tasks periodically reports Ray worker processesâ€™ memory statistics, and in addition to the high level statistics, it also uses <a href="https://psutil.readthedocs.io/en/latest/#psutil.Process.memory_full_info">psutils.memory_full_info</a> to collect detailed information.</p><p>The finding explains our observations.</p><ul><li>â€œIt reoccurs roughly every three secnodsâ€: in fact, the task is scheduled to run every 2.5 seconds. This also explains why our code search of three seconds timer was not effective.</li><li>â€œThe training thread is seemingly stuckâ€: smap_gather_stats would put a lock on the page table while walking the target processesâ€™ memory space and collecting statistics. The target process would have to wait for the lockÂ release.</li><li>â€œA round-robin-like straggler effectâ€: the memory statistics collection for each worker happens in a for loop of worker processes. In each iteration, when the collection happens, that particular rank wouldÂ stall.</li><li>â€œEnabling Nsight System eliminates the slownessâ€: agent.py treats rayletâ€™s (Rayâ€™s backend process) child processes as its worker processes. When enabling Nsight System, the nsys wrapper becomes the child process, which apparently uses way less memory than the trainingÂ process.</li></ul><p>We hardly need such detailed memory statistics in our every day training runs, so we decided to remove it in our custom Ray build. Removing the <a href="https://psutil.readthedocs.io/en/latest/#psutil.Process.memory_full_info">psutils.memory_full_info</a> fixed our end-to-end training throughput. With the newer PyTorch version, instead of seeing a 50% drop in throughput, we now have a 20% speed upâ€Šâ€”â€Ša 2.4x improvement!</p><h3>Summary</h3><p>We successfully identified the two root causes that significantly slowed down our training throughput. When tackling such complicated issues, breaking down the problem by building increasingly small reproducible test cases and adopting the proper tools for the problem at hand turns out to be extremely effective.</p><p>Throughout this journey, GenAI played a vital supporting role. We used it for rapid research, to understand unfamiliar system calls, and to quickly craft tools when off-the-shelf solutions werenâ€™t available. Yet experienced engineers are essential for steering the investigation toward the right directions. We believe that when thoughtfully integrated, AI models substantially amplify engineering productivity.</p><p>Overall, we hope this blog post may inspire you for your present or future profiling and optimization efforts.</p><p>P.S. Interestingly, this exact same kernel call caused another performance issue in our serving systems, as detailed in this <a href="https://medium.com/pinterest-engineering/debugging-the-one-in-a-million-failure-migrating-pinterests-search-infrastructure-to-kubernetes-bef9af9dabf4">blog</a>. While system monitoring is essential, it can introduce significant overhead; we should always be mindful of its potential impact.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=5290bb19be6d" width="1" height="1" alt="">]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Next Gen Data Processing at Massive Scale At Pinterest With Moka (Part 2 of 2)]]></title>
            <link>https://medium.com/pinterest-engineering/next-gen-data-processing-at-massive-scale-at-pinterest-with-moka-part-2-of-2-d0210ded34e0?source=rss-ef81ef829bcb------2</link>
            <guid isPermaLink="false">https://medium.com/p/d0210ded34e0</guid>
            <category><![CDATA[pinterest]]></category>
            <category><![CDATA[engineering]]></category>
            <category><![CDATA[aws]]></category>
            <category><![CDATA[kubernetes]]></category>
            <category><![CDATA[infrastructure]]></category>
            <dc:creator><![CDATA[Pinterest Engineering]]></dc:creator>
            <pubDate>Wed, 10 Sep 2025 16:01:44 GMT</pubDate>
            <atom:updated>2025-09-10T18:59:57.131Z</atom:updated>
            <content:encoded><![CDATA[<p><em>Soam Acharya: Principal Engineer Â· Rainie Li: Manager, Data Processing Infrastructure Â· William Tom: Senior Staff Software Engineer Â· Ang Zhang: Sr. Director, Big DataÂ Platform</em></p><p>As Pinterestâ€™s data processing needs grow and as our current <a href="https://hadoop.apache.org/docs/r2.10.0/">Hadoop</a>-based platform (Monarch) ages, the Big Data Platform (BDP) team within Pinterest Data Engineering started considering alternatives for our next generation massive scale data processing platform. In <a href="https://medium.com/pinterest-engineering/next-gen-data-processing-at-massive-scale-at-pinterest-with-moka-part-1-of-2-39a36d5e82c4">part one</a> we shared the overall design of Moka, our new next gen data processing platform, and detailed its application focused components. In part two of our series, we spotlight the infrastructure focused aspects of our platform: how we deploy Moka using AWS Elastic Kubernetes Service (EKS), our approach to logging and observability, image management, and how we built a UI for Moka. We conclude with our learnings and future direction.</p><h3>Deploying EKS at Pinterest</h3><p>We have standardized on four cluster environments at Pinterestâ€Šâ€”â€Štest, dev, staging, and production:</p><ul><li>test: intended for cluster or other infrastructure level development, e.g. <a href="https://github.com/kubeflow/spark-operator">Spark Operator</a> experimentation</li><li>dev: higher platform level development, e.g. Archer additions</li><li>staging: integration testing and <a href="http://spark.apache.org/">Spark</a> job validation</li><li>production: user workload execution</li></ul><p>Each environment has different levels of access and isolation. For example, it is not possible to write production data from a dev environment. We deploy our EKS clusters within each environment using <a href="https://developer.hashicorp.com/terraform">Terraform</a> augmented by a collection of AWS originated modules and <a href="https://helm.sh/">Helm</a> charts. All of our Terraform root modules live in a single internal git repository, <em>terraform-aws-eks-live</em>. They use the following reusableÂ modules:</p><ol><li><em>terraform-aws-common-eks</em>: contains resources common across all Pinterest EKS projects. In addition to Spark, this includes <a href="https://medium.com/pinterest-engineering/tidb-adoption-at-pinterest-1130ab787a10">TiDB</a>, <a href="https://medium.com/pinterest-engineering/last-mile-data-processing-with-ray-629affbf34ff">Ray</a> (in development), and others. This module also incorporates <a href="https://github.com/pinterest/terraform-aws-eks/tree/pinterest-main"><em>terraform-aws-eks</em></a>, our Pinterest public fork from open source, which creates an EKS cluster &amp; EKS managedÂ addons.</li><li><em>terraform-aws-doeks</em>: this is a project specific module forked from open source for Moka/Spark on EKS specific resources.</li><li>Other kubernetes resource modules forked from <a href="https://aws-ia.github.io/terraform-aws-eks-blueprints/v4-to-v5/motivation/">AWS EKS Blueprints v5</a>:<br>- <em>terraform-aws-eks-blueprints-addons</em>: Forked from open source, contains a select set of addons supported by EKS Blueprints<br>- <em>terraform-aws-eks-pinterest-addons</em>: Forked from eks-blueprints-addons, contains a select set of addons supported by Pinterest<br>- <em>eks-blueprints-addon</em>: Forked from open source, creates a Terraform-based â€œaddonâ€ by installing a Helm chart. Used by eks-blueprints-addons and eks-pinterest-addons.</li></ol><p><em>terraform-aws-eks-data-addons</em>: Forked from open source, contains addons for data on EKS. Used by doeks for Spark Operator.</p><p>Figure 1 illustrates how these modules are interlinked.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*Jp0zHUcO0vCQL7KLfPLPLA.png" /><figcaption>Figure 1: Terraform Module Dependency Tree</figcaption></figure><p>Our deployment process remains an area of active development. For example, we expect to move the items from #3 above away from Terraform to a separate deploy-focused pipeline in theÂ future.</p><h3>Logging Infrastructure</h3><p>Effective management of logs output by cluster components and Spark applications is critical to determining how well they are running, identifying issues, and performing post mortem analysis. Our users expect this as a matter of course when running jobs, thus it was important for us to find an effective alternative to the functionality provided by Hadoop. Broadly categorizing, a logging solution for Moka would have to consider: 1) Amazon EKS control plane logs, 2) Spark Application logs, and 3) System pod logs. The following figure illustrates the various log categories.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*Nt6MgREhDjzyN8z3BdNIFQ.png" /><figcaption>Figure 2: Amazon EKS log categories</figcaption></figure><p>Control plane logs are those generated by the components that constitute the Amazon EKS control plane. These include the K8s API server, audit system, scheduler, and authenticator. The latter is unique to AWS and represents the control plane component that EKS uses for K8s Role-Based Access Control (RBAC) authentication using <a href="https://aws.amazon.com/iam/">AWS Identity and Access Management (IAM)</a> credentials.</p><p>Because the control plane is managed by Amazon EKS, logs for these components are not available directly. Instead, Amazon EKS exports logs for each of the components to <a href="https://aws.amazon.com/cloudwatch/">Amazon CloudWatch</a>. Each log listed previously can be enabled/disabled independently. Once the logs are ingested into CloudWatch, they can be analyzed in-place using <a href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/AnalyzingLogData.html">CloudWatch Log Insights</a>. Because a solution for collecting these logs already exists, we instead focus in the remainder of this section on how best to collect system pod and spark application logs.</p><p>Spark applications generate a variety of logs depending on the application component:</p><ul><li><strong>Driver</strong>: These logs are generated by the Spark driver and contain information about the driverâ€™s activities, such as task submission, task scheduling, and data shuffling.</li><li><strong>Executor</strong>: These logs are generated by the Spark executors and contain information about the tasks executed by the executors, such as task start, completion, and failures.</li><li><strong>Event Logs</strong>: These logs are also generated by the Spark driver during execution and contain information about the internal operations of Spark, such as the allocation of resources, the scheduling of tasks, and the execution of stages. The event logs provide a comprehensive view of the operations performed by Spark and are useful for performance tuning, debugging, and profiling.</li></ul><p>We also felt it would be crucial to persist logs from non Spark system-critical pods in order to diagnose failures that may occur under heavy load. In particular, this is due to the transient nature of pods in K8s and the logs they produce as well as our initial lack of familiarity with operating Amazon EKS atÂ scale.</p><p>Taken together, a logging solution for Moka would need to meet the following requirements:</p><ul><li>Spark application logs for a single job have to be grouped together in one location in Amazon S3 such that individual logs for drivers/executors for that job can be retrieved.</li><li>Upload Spark event logs to Amazon S3 in a way that can be consumed by Spark History Server. Spark is able to upload event logs to Amazon S3 but, by default, the driver buffers the logs on the local disk and only uploads once the main job completes. In the event of job errors or driver crashes, the event log is not uploaded. Spark 3.x <a href="https://spark.apache.org/docs/latest/monitoring.html#applying-compaction-on-rolling-event-log-files">introduced a feature (rolling event logs)</a> that uploads Amazon S3 event logs in increments. However, the minimum increment is 10 MB, which means we would effectively suffer the same problem for small applications.</li><li>System pod logs have to be uploaded to individual locations in AmazonÂ S3.</li><li>YuniKorn pod logs, in addition to being uploaded to Amazon S3, also need to be filtered to collect Spark application resource usage summaries that would be placed in another Amazon S3 location so that it could be processed by our cluster usage analysis workflows.</li></ul><p>We explored a number of possible solutions and ultimately settled on <a href="https://fluentbit.io/">Fluent Bit</a>, a Cloud Native Computing Foundation (CNCF) graduated project, and a well-known solution for handling K8s logs. Fluent Bit is able to filter, forward, and augment logs, and it can be extended through plugins. In particular, <a href="https://docs.fluentbit.io/manual/pipeline/outputs/s3">an Amazon S3 plugin</a> allows a Fluent Bit agent to directly upload files to AmazonÂ S3.</p><p>We collaborated with members of the AWS Solution Architects team to deploy Fluent Bit on our EKS clusters as a DaemonSet, making sure each node has a Fluent Bit pod running. We configured Fluent Bit to perform the following tasks:</p><ul><li>System pods running in their own namespaces are uploaded to unique locations in AmazonÂ S3.</li><li>When submitting to Amazon EKS, Archer makes sure the driver and executor pods of a Spark job have the same unique prefix (Archer unique ID). Fluent Bit is configured to make sure logs from Spark pods are uploaded under this unique ID in Amazon S3. This makes sure logs from the same Spark application are grouped together in one location.</li><li>The driver of a Spark application outputs events to a single uniquely named log file to a central location on a host. Fluent Bit uploads this file in chunks to Amazon S3 in a layout that mimics the rolling event log format. It uses filtering to create additional files necessary for Spark History Server to recognize event logs files in AmazonÂ S3.</li><li>Filtering is also used to extract specific strings corresponding to resource summaries from YuniKorn logs and to upload to a separate location.</li></ul><p>The following figure illustrates the various log flows performed by Fluent Bit on a singleÂ node.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*cjhHBlgWXYVQsui8EDcewQ.png" /><figcaption>Figure 3: Fluent Bit log uploadÂ flow</figcaption></figure><p>Once Spark application logs are uploaded to S3, Archer is able to retrieve and piece sections of the logs together on demand (recall that Fluent Bit uploads all logs to S3 in chunks). For more details on our logging setup, please refer to our joint blog post series with AWS: <a href="https://aws.amazon.com/blogs/containers/inside-pinterests-custom-spark-job-logging-and-monitoring-on-amazon-eks-using-aws-for-fluent-bit-amazon-s3-and-adot/">Inside Pinterestâ€™s Custom Spark Job logging and monitoring on Amazon EKS: Using AWS for Fluent Bit, Amazon S3, and ADOT PartÂ I</a>.</p><h3>Metrics and Observability</h3><p>In order to operate a K8s platform efficiently, storing metrics in a queryable, displayable format is critical to overall platform stability, performance/efficiency, and, ultimately, operating costs. Prometheus formatted metrics are the standard for K8s ecosystem tools. Observability frameworks such as <a href="https://prometheus.io/">Prometheus</a> (the project, not the format), OTEL, and other CNCF projects continue to see increases in activity year overÂ year.</p><p>At Pinterest, the current observability framework, <a href="https://medium.com/pinterest-engineering/analyzing-time-series-for-pinterest-observability-95f8cc0c5885">Statsboard</a>, is TSDB-based and ingests metrics via a sidecar metrics-agent that runs on every host. Systems typically use TSDB libraries to write to their local metrics-agent, which passes the metrics on to Kafka clusters, after which they are ingested into <a href="https://medium.com/pinterest-engineering/goku-building-a-scalable-and-high-performant-time-series-database-system-a8ff5758a181">Goku</a>, Pinterestâ€™s custom TSDB implementation, and made available in Statsboard dashboards. In contrast, the Prometheus-styled frameworks involve systems exposing their metrics for scraping by agents. Unfortunately, support for TSDB as a metrics destination within the open source Cloud Native Computing Foundation (CNCF)/K8s ecosystem is inactive. To address this gap, the Cloud Runtime team at Pinterest has developed <em>kubemetricsexporter</em>, a K8s sidecar container that can periodically scrape Prometheus endpoints in a pod and write the scraped metrics to the local metrics-agent. Because Amazon EKS pods can be in a different network than the host, the batch processing platform team at Pinterest worked with the Cloud Runtime team to extend <em>kubemetricexporter</em> so that it could be configured to use the host IP address instead of localhost. The following figure shows the deployment pattern.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*xC_iGuGnbfyCyymdeQ0eGQ.png" /><figcaption>Figure 4: Using kubemetricexporter with Prometheus MetricsÂ Source</figcaption></figure><p>After exploring a variety of options and configurations, we ultimately decided to use a combination of OTEL for extracting detailed insights from our EKS clusters and <a href="https://github.com/kubernetes/kube-state-metrics">kube-state-metrics</a>, an open source K8s tool, for providing a broader overview of the Amazon EKS control plane. In contrast with Prometheus, the OTEL framework only focuses on metrics collection and pre-processing, leaving metrics storage to other solutions. A key portion of the framework is the <a href="https://github.com/open-telemetry/opentelemetry-collector/tree/main">OpenTelemetry Collector</a>, which is an executable binary that can extract telemetry data, optionally process it, and export it further. The Collector supports several popular open source protocols for receiving and sending telemetry data, as well as offering a pluggable architecture for adding more protocols.</p><p>Data receiving, processing, and exporting in OTEL is done using <a href="https://github.com/open-telemetry/opentelemetry-collector/blob/main/docs/internal-architecture.md">Pipelines</a>. The Collector can be configured to have one or more pipelines. Each pipeline includes:</p><ul><li>A set of <a href="https://github.com/open-telemetry/opentelemetry-collector/blob/main/docs/scraping-receivers.md">Receivers</a> that receive theÂ data</li><li>A series of optional <a href="https://github.com/open-telemetry/opentelemetry-collector/tree/main/processor">Processors</a> that get the data from receivers and processÂ it</li><li>A set of <a href="https://github.com/open-telemetry/opentelemetry-collector/tree/main/exporter">Exporters</a> that get the data from processors and send it further outside the Collector</li></ul><p>After extensive experimentation, we ended up with a pipeline consisting of a <a href="https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/receiver/prometheusreceiver">Prometheus receiver</a>, <a href="https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/processor/attributesprocessor">Attributes processor</a>, and a <a href="https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/exporter/prometheusexporter#prometheus-exporter">Prometheus exporter</a>. Our OTEL metrics pipeline looks like the following:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*etHyO0qSn8iPS-BZMzay6A.png" /><figcaption>Figure 5: OTEL pipeline for Moka observability</figcaption></figure><p>For more information on our observability infrastructure for EKS, please visit the <a href="https://aws.amazon.com/blogs/containers/inside-pinterests-custom-spark-job-logging-and-monitoring-on-amazon-eks-using-aws-for-fluent-bit-amazon-s3-and-adot-2/">second part</a> of our joint blog with AWS: <a href="https://aws.amazon.com/blogs/containers/inside-pinterests-custom-spark-job-logging-and-monitoring-on-amazon-eks-using-aws-for-fluent-bit-amazon-s3-and-adot-2/">Inside Pinterestâ€™s Custom Spark Job logging and monitoring on Amazon EKS: Using AWS for Fluent Bit, Amazon S3, and ADOT PartÂ II</a>.</p><h3>Image Management</h3><p>Containerization is a key difference between how Pinterest runs Spark applications on Monarch compared to how they run on Moka. On Monarch, Spark drivers and executors were containerized only from the resource perspective but still shared a common environment including things like Hadoop, Spark, and Java versions. Furthermore, containers running on non-Kerberized Monarch clusters had full access to any other container running on that host. In Moka, we get the full isolation benefits of containerization (cgroups and namespaces) by default with Kubernetes. Given our previous operating model, we did not have a structured system in place for defining, building, deploying, and maintaining container images, nor did we have any support for ARM builds. We wanted applications running on Moka to be architecture agnostic, so not only did we have to build our image generation pipelines from scratch, but we had to ensure that they supported both Intel and ARM from the beginning.</p><p>Our images needed to mirror the base environment that each Spark application was accustomed to when running on Monarch, with the main requirements being Java, Hadoop libraries, Spark, and in the case of PySpark, both Python and PythonÂ modules.</p><p>We built three main pipelines:</p><ul><li>Hadoop Debian Package: Generates a multi-architecture debian package of Hadoop 2.10 to be used by both Monarch andÂ Moka</li><li>Spark Debian Package: Generates a multi-architecture debian package of Spark 3.2 to be used by both Monarch andÂ Moka</li><li>Spark image builder: Using Corretto Java 11 as a base image, installs the two previous multi-architecture packages, standard libraries, compression libraries, and commonly used static jars fromÂ S3</li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*nPyu7v2WRvyMDc8tahPRPQ.png" /><figcaption>Figure 6: Moka Image Management</figcaption></figure><h3>Accessing Spark Components InÂ Moka</h3><h4>Spark Live UIÂ Access</h4><p>Each driver for a running Spark application serves a dedicated UI showing the status of various aspects of the job. Because driver pods can run anywhere on a K8s cluster, setting up a dynamic ingress solution per live application can be tricky. Our ingress solution is built using an ingress-nginx controller, AWS LoadBalancer controller, and an AWS Network Load Balancer (NLB) with IP-based routing for each cluster. The AWS LoadBalancer manages the NLB, which configures the user facing NLB for the ingress controller. The Spark on K8s Operator has a <em>uiService</em> component that provisions a Service resource and Ingress Resource for a Spark application. The Service resource is of type ClusterIP, which points to the UI port (4045) on the driver pod. The Ingress resource has the following mappings:</p><ul><li>host: NLBÂ address</li><li>path: AppID</li><li>backend: ServiceÂ object.</li></ul><p>In the example below, the ingress resource for App-A would be configured with host: Moka-NLB.elb.us-east1.amazonaws.com, path: /App-A, and backend: App-A-ui-svc. Users access the actual link to each running application from the <a href="https://docs.google.com/document/d/1FICDrO9-iVKuY-olohL23Tn1euBcsPwnaZ5LSgtc344/edit#heading=h.o9s6dya7w9cz">Moka UI</a>. Figure 7 visualizes the resulting workflow.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*OBJ1mFYm_UiLOjNV8lUxaA.png" /><figcaption>Figure 8: Spark Live UI Architecture</figcaption></figure><h4>Spark HistoryÂ Server</h4><p>In Moka, there is one Spark History Server (SHS) cluster (consisting of one or more nodes) <em>per environment</em>. This is a shift from the layout in Monarch, our Hadoop setup, which had one more Spark History Servers<em> per cluster.</em> The rationale behind this change from per cluster to per environment is to simplify the overhead in managing many Moka clusters.</p><p>Users access SHS through dedicated moka-&lt;environment&gt;-spark-historyserver ingress endpoints, which routes the traffic to the corresponding cluster and performs load-balancing across the history servers in the cluster. Weâ€™ve made modifications to SHS for faster parsing and management of event logs, as they are now uploaded to S3 by our logging infrastructure.</p><h3>Moka UI</h3><p>One of the main components that we had to build from scratch was an interface to provide both a comprehensive overview of the platform and detailed information about a Spark application. Coming from Hadoop YARN, many users were accustomed to the native interface that the Hadoop project provided. The YARN interface had existing proxy support for Spark applications, which was seamless to the user. However, one downside to the YARN UI was that it is per-cluster, meaning that users had to have knowledge about the different Hadoop clusters underlying the data platform.</p><p>When designing Moka, one of our goals was to abstract away individual clusters from the user and have them interact with it as a monolithic platform. To build the interface, we chose to use Internal Tools Platform (ITP), which is a Typescript React-based internal framework for building internal tooling. The first interface we built is our Application Explorer, which aggregates applications running on different clusters and exposes basic information to theÂ user.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*AeV5zxxnQlj8tsgiIzdA0Q.png" /><figcaption>Figure 9: Application Explorer part of the MokaÂ UI</figcaption></figure><p>The second UI we built was the Moka Application UI, which gives users information about their Spark application. It surfaces commonly used pieces of information such as identity of the client that submitted the job,the identity of the job itself, job run location, and current job state. The UI also surfaces dynamic links such as those to the driver log or Spark UI. These dynamic links redirect based on the state of the underlying Spark application. For example, while the application is running, the log links will return logs fetched from the Kubernetes control plane, which allows users to debug and track their application in real time. After the application completes or if the user requests logs that have already been rotated from the control plane, Archer will coalesce the chunked logs located in S3 and serve them back to theÂ user.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*pyFZjbKiyfODnTC0NysJCQ.png" /><figcaption>Figure 10: Moka Application UI</figcaption></figure><h3>Current Status and Learnings</h3><p>Pinterestâ€™s transition from Monarch to Moka has marked a significant advancement for infrastructure at Pinterest beyond just batch data processing. Spark on EKS is resource intensive beyond just CPUsâ€Šâ€”â€Šit has bursty AWS API requirements and requires a significant number of IP addresses. Consequently, supporting the Spark on EKS use case has catalyzed infrastructure modernization efforts at Pinterest including:</p><ul><li>Moving to AWS multi-account</li><li>Rethinking our networking topology (see our joint publications with AWS on Spark on EKS networking, parts <a href="https://aws.amazon.com/blogs/containers/spark-on-amazon-eks-networking-part-2/">1</a> and <a href="https://aws.amazon.com/blogs/containers/spark-on-amazon-eks-networking-part-2/">2</a>, for more details on thisÂ topic).</li><li>Support for pod level identities, credentials, and accessÂ control.</li><li>Extending our internal logging system, <a href="http://interference">Singer</a>, so that it can take over more of the logging duties from FluentÂ Bit.</li></ul><p>Finally, Moka has opened the doors for EKS adoption by other use cases at Pinterest Data Engineering, particularly those that require access to the Kubernetes API. These include both TiDB on EKS for online systems use cases and Flink for our Stream processing platform. Weâ€™re currently working on adopting Ray and PyTorch on EKS and are particularly excited about the possibility of commingling CPU and GPU focused workloads.</p><h3>Acknowledgements</h3><p>Moka was a massive project that necessitated and continues to require extensive cross functional collaboration between teams at multiple organizations at Pinterest and elsewhere. Hereâ€™s an incomplete list of folks and teams that helped us build our first set of production Moka clusters:</p><p>Data Processing Infra: Aria Wang, Bhavin Pathak, Hengzhe Guo, Royce Poon, BogdanÂ Pisica</p><p>Big Data Query Platform: Zaheen Aziz, Sophia Hetts, AshishÂ Singh</p><p>Batch Processing Platform: Nan Zhu, Yongjun Zhang, Zirui Li, Frida Pulido, ChenÂ Qin</p><p>SRE: Ashim Shrestha, Samuel Bahr, Ihor Chaban, Byron Benitez, Juan Pablo DanielÂ Borgna</p><p>TPM: Ping-Huai Jen, Svetlana Vaz Menezes Pereira, HannahÂ Chen</p><p>Cloud Architecture: James Fogel, Sekou Doumbouya</p><p>Traffic: James Fish, Scott Beardsley</p><p>Security: Henry Luo, Jeremy Krach, Ali Yousefi, Victor Savu, Vedant Radhakrishnan, CedricÂ Staub</p><p>Continuous Integration Platform: AnhÂ Nguyen</p><p>Infra Provisioning: Su Song, MatthewÂ Tejo</p><p>Cloud Runtime: David Westbrook, Quentin Miao, Yi Li, MingÂ Zong</p><p>Workflow Platform: Dinghang Yu, Yulei Li, Jin HyukÂ Chang</p><p>ML platform: Se Won Jang, AndersonÂ Lam</p><p>AWS Team: Doug Youd, Alan Tyson, Vara Bonthu, Aaron Miller, Sahas Maheswari, Vipul Viswanath, Raj Gajjar, NirmalÂ Mehta</p><p>Leadership: Chunyan Wang, Dave Burgess, David Chaiken, Madhuri Racherla, Jooseong Kim, Anthony Suarez, Amine Kamel, Rizwan Tanoli, Alvaro LopezÂ Ortega</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=d0210ded34e0" width="1" height="1" alt=""><hr><p><a href="https://medium.com/pinterest-engineering/next-gen-data-processing-at-massive-scale-at-pinterest-with-moka-part-2-of-2-d0210ded34e0">Next Gen Data Processing at Massive Scale At Pinterest With Moka (Part 2 of 2)</a> was originally published in <a href="https://medium.com/pinterest-engineering">Pinterest Engineering Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
    </channel>
</rss>