<?xml version="1.0" encoding="utf-8"?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://search.yahoo.com/mrss/" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0" xml:base="https://news.mit.edu">
  <channel>
    <title>MIT News - Computer Science and Artificial Intelligence Laboratory (CSAIL)</title>
    <link>https://news.mit.edu/topic/mitcomputers-rss.xml</link>
    <atom:link href="https://news.mit.edu/topic/mitcomputers-rss.xml" rel="self" type="application/rss+xml"/>
    <description>MIT news feed about: Computer Science and Artificial Intelligence Laboratory (CSAIL)</description>
    <language>en</language>
    
    <lastBuildDate>Fri, 12 Dec 2025 05:00:00 +0000</lastBuildDate>
    <item>
  <title>Guided learning lets “untrainable” neural networks realize their potential</title>
  <link>https://news.mit.edu/2025/guided-learning-lets-untrainable-neural-networks-realize-their-potential-1218</link>
  <description>CSAIL researchers find even “untrainable” neural nets can learn effectively when guided by another network’s built-in biases using their guidance method.</description>
  <pubDate>Thu, 18 Dec 2025 16:20:00 -0500</pubDate>
    <guid isPermaLink="true">https://news.mit.edu/2025/guided-learning-lets-untrainable-neural-networks-realize-their-potential-1218</guid>
        <dc:creator>Rachel Gordon | MIT CSAIL</dc:creator>
  <content:encoded>&lt;p dir="ltr" id="docs-internal-guid-fb3bf34a-7fff-7c39-06e2-626dc3313469"&gt;Even networks long considered “untrainable” can learn effectively with a bit of a helping hand. Researchers at MIT’s Computer Science and Artificial Intelligence Laboratory (CSAIL) have shown that a brief period of alignment between neural networks, a method they call guidance, can dramatically improve the performance of architectures previously thought unsuitable for modern tasks.&lt;/p&gt;&lt;p dir="ltr"&gt;Their findings suggest that many so-called “ineffective” networks may simply start from less-than-ideal starting points, and that short-term guidance can place them in a spot that makes learning easier for the network.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;The team’s guidance method works by encouraging a target network to match the internal representations of a guide network during training. Unlike traditional methods like knowledge distillation, which focus on mimicking a teacher’s outputs, guidance transfers structural knowledge directly from one network to another. This means the target learns how the guide organizes information within each layer, rather than simply copying its behavior. Remarkably, even untrained networks contain architectural biases that can be transferred, while trained guides additionally convey learned patterns.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;“We found these results pretty surprising,” says Vighnesh Subramaniam ’23, MEng ’24, MIT Department of Electrical Engineering and Computer Science (EECS) PhD student and CSAIL researcher, who is a lead author on a&amp;nbsp;&lt;a href="https://arxiv.org/abs/2410.20035"&gt;paper&lt;/a&gt; presenting these findings. “It’s impressive that we could use representational similarity to make these traditionally ‘crappy’ networks actually work.”&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;strong&gt;Guide-ian angel&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;&lt;p dir="ltr"&gt;A central question was whether guidance must continue throughout training, or if its primary effect is to provide a better initialization. To explore this, the researchers performed an experiment with deep fully connected networks (FCNs). Before training on the real problem, the network spent a few steps practicing with another network using random noise, like stretching before exercise. The results were striking: Networks that typically overfit immediately remained stable, achieved lower training loss, and avoided the classic performance degradation seen in something called standard FCNs. This alignment acted like a helpful warmup for the network, showing that even a short practice session can have lasting benefits without needing constant guidance.&lt;/p&gt;&lt;p dir="ltr"&gt;The study also compared guidance to knowledge distillation, a popular approach in which a student network attempts to mimic a teacher’s outputs. When the teacher network was untrained, distillation failed completely, since the outputs contained no meaningful signal. Guidance, by contrast, still produced strong improvements because it leverages internal representations rather than final predictions. This result underscores a key insight: Untrained networks already encode valuable architectural biases that can steer other networks toward effective learning.&lt;/p&gt;&lt;p dir="ltr"&gt;Beyond the experimental results, the findings have broad implications for understanding neural network architecture. The researchers suggest that success — or failure — often depends less on task-specific data, and more on the network’s position in parameter space. By aligning with a guide network, it’s possible to separate the contributions of architectural biases from those of learned knowledge. This allows scientists to identify which features of a network’s design support effective learning, and which challenges stem simply from poor initialization.&lt;/p&gt;&lt;p dir="ltr"&gt;Guidance also opens new avenues for studying relationships between architectures. By measuring how easily one network can guide another, researchers can probe distances between functional designs and reexamine theories of neural network optimization. Since the method relies on representational similarity, it may reveal previously hidden structures in network design, helping to identify which components contribute most to learning and which do not.&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;strong&gt;Salvaging the hopeless&lt;/strong&gt;&lt;/p&gt;&lt;p dir="ltr"&gt;Ultimately, the work shows that so-called “untrainable” networks are not inherently doomed. With guidance, failure modes can be eliminated, overfitting avoided, and previously ineffective architectures brought into line with modern performance standards. The CSAIL team plans to explore which architectural elements are most responsible for these improvements and how these insights can influence future network design. By revealing the hidden potential of even the most stubborn networks, guidance provides a powerful new tool for understanding — and hopefully shaping — the foundations of machine learning.&lt;/p&gt;&lt;p dir="ltr"&gt;“It’s generally assumed that different neural network architectures have particular strengths and weaknesses,” says Leyla Isik, Johns Hopkins University assistant professor of cognitive science, who wasn’t involved in the research. “This exciting research shows that one type of network can inherit the advantages of another architecture, without losing its original capabilities. Remarkably, the authors show this can be done using small, untrained ‘guide’ networks. This paper introduces a novel and concrete way to add different inductive biases into neural networks, which is critical for developing more efficient and human-aligned AI.”&lt;/p&gt;&lt;p dir="ltr"&gt;Subramaniam wrote the paper with CSAIL colleagues: Research Scientist Brian Cheung; PhD student David Mayo ’18, MEng ’19; Research Associate Colin Conwell; principal investigators Boris Katz, a CSAIL principal research scientist, and Tomaso Poggio, an MIT professor in brain and cognitive sciences; and former CSAIL research scientist Andrei Barbu. Their work was supported, in part, by the Center for Brains, Minds, and Machines, the National Science Foundation, the MIT CSAIL Machine Learning Applications Initiative, the MIT-IBM Watson AI Lab, the U.S. Defense Advanced Research Projects Agency (DARPA), the U.S. Department of the Air Force Artificial Intelligence Accelerator, and the U.S. Air Force Office of Scientific Research.&lt;br&gt;&lt;br&gt;Their work was recently presented at the Conference and Workshop on Neural Information Processing Systems (NeurIPS).&lt;/p&gt;</content:encoded>
      <media:content url="https://news.mit.edu/sites/default/files/styles/news_article__cover_image__original/public/images/202512/mit-csail-Untrainable-networks.jpg?itok=_xsMAwVf" medium="image" type="image/jpeg" width="390" height="260">
              <media:description type="plain">MIT researchers found that many so-called “ineffective” networks may simply start from less-than-ideal starting points, and that short-term guidance can strengthen their performance.  </media:description>
              <media:credit>Image: Alex Shipps/MIT CSAIL</media:credit>
      </media:content>
        <category domain="https://news.mit.edu/topic/research">Research</category>
      <category domain="https://news.mit.edu/topic/data">Data</category>
      <category domain="https://news.mit.edu/topic/artificial-intelligence2">Artificial intelligence</category>
      <category domain="https://news.mit.edu/topic/algorithms">Algorithms</category>
      <category domain="https://news.mit.edu/topic/machine-learning">Machine learning</category>
      <category domain="https://news.mit.edu/topic/networks">Networks</category>
      <category domain="https://news.mit.edu/topic/neuroscience">Neuroscience</category>
      <category domain="https://news.mit.edu/topic/computers">Computer science and technology</category>
      <category domain="https://news.mit.edu/topic/electrical-engineering-computer-science-eecs">Electrical engineering and computer science (EECS)</category>
      <category domain="https://news.mit.edu/topic/center-brains-minds-and-machines">Center for Brains Minds and Machines</category>
      <category domain="https://news.mit.edu/topic/computer-science-and-artificial-intelligence-laboratory-csail">Computer Science and Artificial Intelligence Laboratory (CSAIL)</category>
      <category domain="https://news.mit.edu/topic/mit-ibm-watson-ai-lab">MIT-IBM Watson AI Lab</category>
      <category domain="https://news.mit.edu/topic/mit-schwarzman-college-computing">MIT Schwarzman College of Computing</category>
      <category domain="https://news.mit.edu/topic/school-engineering">School of Engineering</category>
      <category domain="https://news.mit.edu/topic/nsf">National Science Foundation (NSF)</category>
      <category domain="https://news.mit.edu/topic/darpa">Defense Advanced Research Projects Agency (DARPA)</category>
    </item>
<item>
  <title>A new way to increase the capabilities of large language models</title>
  <link>https://news.mit.edu/2025/new-way-to-increase-large-language-model-capabilities-1217</link>
  <description>MIT-IBM Watson AI Lab researchers developed an expressive architecture that provides better state tracking and sequential reasoning in LLMs over long texts.</description>
  <pubDate>Wed, 17 Dec 2025 23:10:00 -0500</pubDate>
    <guid isPermaLink="true">https://news.mit.edu/2025/new-way-to-increase-large-language-model-capabilities-1217</guid>
        <dc:creator>Lauren Hinkel | MIT-IBM Watson AI Lab</dc:creator>
  <content:encoded>&lt;p&gt;Most languages use word position and sentence structure to extract meaning. For example, “The cat sat on the box,” is not the same as “The box was on the cat.” Over a long text, like a financial document or a novel, the syntax of these words likely evolves.&amp;nbsp;&lt;/p&gt;&lt;p&gt;Similarly, a person might be tracking variables in a piece of code or following instructions that have conditional actions. These are examples of state changes and sequential reasoning that we expect state-of-the-art artificial intelligence systems to excel at; however, the existing, cutting-edge attention mechanism within transformers — the primarily architecture used in large language models (LLMs) for determining the importance of words — has theoretical and empirical limitations when it comes to such capabilities.&lt;/p&gt;&lt;p&gt;An attention mechanism allows an LLM to look back at earlier parts of a query or document and, based on its training, determine which details and words matter most; however, this mechanism alone does not understand word order. It “sees” all of the input words, a.k.a. tokens, at the same time and handles them in the order that they’re presented, so researchers have developed techniques to encode position information. This is key for domains that are highly structured, like language. But the predominant position-encoding method, called rotary position encoding (RoPE), only takes into account the relative distance between tokens in a sequence and is independent of the input data. This means that, for example, words that are four positions apart, like “cat” and “box” in the example above, will all receive the same fixed mathematical rotation specific to that relative distance.&amp;nbsp;&lt;/p&gt;&lt;p&gt;Now research led by MIT and the MIT-IBM Watson AI Lab has produced an encoding technique known as “PaTH Attention” that makes positional information adaptive and context-aware rather than static, as with RoPE.&lt;/p&gt;&lt;p&gt;“Transformers enable accurate and scalable modeling of many domains, but they have these limitations vis-a-vis state tracking, a class of phenomena that is thought to underlie important capabilities that we want in our AI systems. So, the important question is: How can we maintain the scalability and efficiency of transformers, while enabling state tracking?” says the paper’s senior author Yoon Kim, an associate professor in the Department of Electrical Engineering and Computer Science (EECS), a member of the Computer Science and Artificial Intelligence Laboratory (CSAIL), and a researcher with the MIT-IBM Watson AI Lab.&lt;/p&gt;&lt;p&gt;A new paper on this work was presented earlier this month at the Conference on Neural Information Processing Systems (NeurIPS). Kim’s co-authors include lead author Songlin Yang, an EECS graduate student and former MIT-IBM Watson AI Lab Summer Program intern; Kaiyue Wen of Stanford University; Liliang Ren of Microsoft; and Yikang Shen, Shawn Tan, Mayank Mishra, and Rameswar Panda of IBM Research and the MIT-IBM Watson AI Lab.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Path to understanding&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Instead of assigning every word a fixed rotation based on relative distance between tokens, as RoPE does, PaTH Attention is flexible, treating the in-between words as a path made up of small, data-dependent transformations. Each transformation, based on a mathematical operation called a Householder reflection, acts like a tiny mirror that adjusts depending on the content of each token it passes. Each step in a sequence can influence how the model interprets information later on. The cumulative effect lets the system model how the meaning changes along the path between words, not just how far apart they are. This approach allows transformers to keep track of how entities and relationships change over time, giving it a sense of “positional memory.” Think of this as walking a path while experiencing your environment and how it affects you. Further, the team also developed a hardware-efficient algorithm to more efficiently compute attention scores between every pair of tokens so that the cumulative mathematical transformation from PaTH Attention is compressed and broken down into smaller computations so that it’s compatible with fast processing on GPUs.&lt;/p&gt;&lt;p&gt;The MIT-IBM researchers then explored PaTH Attention’s performance on synthetic and real-world tasks, including reasoning, long-context benchmarks, and full LLM training to see whether it improved a model’s ability to track information over time. The team tested its ability to follow the most recent “write” command despite many distracting steps and multi-step recall tests, tasks that are difficult for standard positional encoding methods like RoPE. The researchers also trained mid-size LLMs and compared them against other methods. PaTH Attention improved perplexity and outcompeted other methods on reasoning benchmarks it wasn’t trained on. They also evaluated retrieval, reasoning, and stability with inputs of tens of thousands of tokens. PaTH Attention consistently proved capable of content-awareness.&lt;/p&gt;&lt;p&gt;“We found that both on diagnostic tasks that are designed to test the limitations of transformers and on real-world language modeling tasks, our new approach was able to outperform existing attention mechanisms, while maintaining their efficiency,” says Kim. Further, “I’d be excited to see whether these types of data-dependent position encodings, like PATH, improve the performance of transformers on structured domains like biology, in [analyzing] proteins or DNA.”&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Thinking bigger and more efficiently&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;The researchers then investigated how the PaTH Attention mechanism would perform if it more similarly mimicked human cognition, where we ignore old or less-relevant information when making decisions. To do this, they combined PaTH Attention with another position encoding scheme known as the Forgetting Transformer (FoX), which allows models to selectively “forget.” The resulting PaTH-FoX system adds a way to down-weight information in a data-dependent way, achieving strong results across reasoning, long-context understanding, and language modeling benchmarks. In this way, PaTH Attention extends the expressive power of transformer architectures.&amp;nbsp;&lt;/p&gt;&lt;p&gt;Kim says research like this is part of a broader effort to develop the “next big thing” in AI. He explains that a major driver of both the deep learning and generative AI revolutions has been the creation of “general-purpose building blocks that can be applied to wide domains,” such as “convolution layers, RNN [recurrent neural network] layers,” and, most recently, transformers. Looking ahead, Kim notes that considerations like accuracy, expressivity, flexibility, and hardware scalability have been and will be essential. As he puts it, “the core enterprise of modern architecture research is trying to come up with these new primitives that maintain or improve the expressivity, while also being scalable.”&lt;/p&gt;&lt;p&gt;This work was supported, in part, by the MIT-IBM Watson AI Lab and the AI2050 program at Schmidt Sciences.&lt;/p&gt;</content:encoded>
      <media:content url="https://news.mit.edu/sites/default/files/styles/news_article__cover_image__original/public/images/202512/mit-watson-cats-in-a-box.jpg?itok=ytH_wQHS" medium="image" type="image/jpeg" width="390" height="260">
              <media:description type="plain">Large language models struggle with state changes that are common in long texts, like how a cat might interact with a box over time and how the box might break down. Now, work from MIT-IBM Watson AI Lab researchers can outperform the current state-of-the-art method, RoPE.</media:description>
              <media:credit>Image: AdobeStock</media:credit>
      </media:content>
        <category domain="https://news.mit.edu/topic/research">Research</category>
      <category domain="https://news.mit.edu/topic/artificial-intelligence2">Artificial intelligence</category>
      <category domain="https://news.mit.edu/topic/machine-learning">Machine learning</category>
      <category domain="https://news.mit.edu/topic/algorithms">Algorithms</category>
      <category domain="https://news.mit.edu/topic/computers">Computer science and technology</category>
      <category domain="https://news.mit.edu/topic/data">Data</category>
      <category domain="https://news.mit.edu/topic/mit-ibm-watson-ai-lab">MIT-IBM Watson AI Lab</category>
      <category domain="https://news.mit.edu/topic/electrical-engineering-computer-science-eecs">Electrical engineering and computer science (EECS)</category>
      <category domain="https://news.mit.edu/topic/computer-science-and-artificial-intelligence-laboratory-csail">Computer Science and Artificial Intelligence Laboratory (CSAIL)</category>
      <category domain="https://news.mit.edu/topic/mit-schwarzman-college-computing">MIT Schwarzman College of Computing</category>
      <category domain="https://news.mit.edu/topic/school-engineering">School of Engineering</category>
    </item>
<item>
  <title>Enabling small language models to solve complex reasoning tasks</title>
  <link>https://news.mit.edu/2025/enabling-small-language-models-solve-complex-reasoning-tasks-1212</link>
  <description>The “self-steering” DisCIPL system directs small models to work together on tasks with constraints, like itinerary planning and budgeting.</description>
  <pubDate>Fri, 12 Dec 2025 15:30:00 -0500</pubDate>
    <guid isPermaLink="true">https://news.mit.edu/2025/enabling-small-language-models-solve-complex-reasoning-tasks-1212</guid>
        <dc:creator>Alex Shipps | MIT CSAIL</dc:creator>
  <content:encoded>&lt;p dir="ltr" id="docs-internal-guid-7a6dc935-7fff-5776-c2a2-b03a9816a853"&gt;As language models (LMs) improve at tasks like image generation, trivia questions, and simple math, you might think that human-like reasoning is around the corner. In reality, they still trail us by a wide margin on complex tasks. Try playing Sudoku with one, for instance, where you fill in numbers one through nine in such a way that each appears only once across the columns, rows, and sections of a nine-by-nine grid. Your AI opponent will either fail to fill in boxes on its own or do so inefficiently, although it can verify if you’ve filled yours out correctly.&lt;/p&gt;&lt;p dir="ltr"&gt;Whether an LM is trying to solve advanced puzzles, design molecules, or write math proofs, the system struggles to answer open-ended requests that have strict rules to follow. The model is better at telling users how to approach these challenges than attempting them itself. Moreover, hands-on problem-solving requires LMs to consider a wide range of options while following constraints. Small LMs can’t do this reliably on their own; large language models (LLMs) sometimes can, particularly if they’re optimized for reasoning tasks, but they take a while to respond, and they use a lot of computing power.&lt;/p&gt;&lt;p dir="ltr"&gt;This predicament led researchers from MIT’s Computer Science and Artificial Intelligence Laboratory (CSAIL) to develop a collaborative approach where an LLM does the planning, then divvies up the legwork of that strategy among smaller ones. Their method helps small LMs provide more accurate responses than leading LLMs like OpenAI’s&amp;nbsp;&lt;a href="https://openai.com/index/hello-gpt-4o/"&gt;GPT-4o&lt;/a&gt;, and approach the precision of top reasoning systems such as&amp;nbsp;&lt;a href="https://openai.com/o1/"&gt;o1&lt;/a&gt;, while being more efficient than both. Their framework, called “Distributional Constraints by Inference Programming with Language Models” (or “DisCIPL”), has a large model steer smaller “follower” models toward precise responses when writing things like text blurbs, grocery lists with budgets, and travel itineraries.&lt;br&gt;&lt;br&gt;The inner workings of DisCIPL are much like contracting a company for a particular job. You provide a “boss” model with a request, and it carefully considers how to go about doing that project. Then, the LLM relays these instructions and guidelines in a clear way to smaller models. It corrects follower LMs’ outputs where needed — for example, replacing one model’s phrasing that doesn’t fit in a poem with a better option from another.&lt;/p&gt;&lt;p dir="ltr"&gt;The LLM communicates with its followers using a language they all understand — that is, a programming language for controlling LMs called&amp;nbsp;&lt;a href="https://arxiv.org/abs/2306.03081"&gt;“LLaMPPL.”&lt;/a&gt;&amp;nbsp;Developed by MIT's Probabilistic Computing Project in 2023, this program allows users to encode specific rules that steer a model toward a desired result. For example, LLaMPPL can be used to produce&amp;nbsp;&lt;a href="https://news.mit.edu/2025/making-ai-generated-code-more-accurate-0418"&gt;error-free code&lt;/a&gt; by incorporating the rules of a particular language within its instructions. Directions like “write eight lines of poetry where each line has exactly eight words” are encoded in LLaMPPL, queuing smaller models to contribute to different parts of the answer.&lt;/p&gt;&lt;p dir="ltr"&gt;MIT PhD student Gabriel Grand, who is the lead author on a&amp;nbsp;&lt;a href="https://openreview.net/pdf?id=XvCBtm5PgF" target="_blank"&gt;paper&lt;/a&gt; presenting this work, says that DisCIPL allows LMs to guide each other toward the best responses, which improves their overall efficiency. “We’re working toward improving LMs’ inference efficiency, particularly on the many modern applications of these models that involve generating outputs subject to constraints,” adds Grand, who is also a CSAIL researcher. “Language models are consuming more energy as people use them more, which means we need models that can provide accurate answers while using minimal computing power.”&lt;/p&gt;&lt;p dir="ltr"&gt;“It's really exciting to see new alternatives to standard language model inference,” says University of California at Berkeley Assistant Professor Alane Suhr, who wasn’t involved in the research. “This work invites new approaches to language modeling and LLMs that significantly reduce inference latency via parallelization, require significantly fewer parameters than current LLMs, and even improve task performance over standard serialized inference. The work also presents opportunities to explore transparency, interpretability, and controllability of model outputs, which is still a huge open problem in the deployment of these technologies.”&lt;br&gt;&lt;br&gt;&lt;strong&gt;An underdog story&lt;/strong&gt;&lt;br&gt;&lt;br&gt;You may think that larger-scale LMs are “better” at complex prompts than smaller ones when it comes to accuracy and efficiency. DisCIPL suggests a surprising counterpoint for these tasks: If you can combine the strengths of smaller models instead, you may just see an efficiency bump with similar results.&lt;/p&gt;&lt;p dir="ltr"&gt;The researchers note that, in theory, you can plug in dozens of LMs to work together in the DisCIPL framework, regardless of size. In writing and reasoning experiments, they went with GPT-4o as their “planner LM,” which is one of the models that helps ChatGPT generate responses. It brainstormed a plan for several&amp;nbsp;&lt;a href="https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/"&gt;“Llama-3.2-1B”&lt;/a&gt; models (smaller systems developed by Meta), in which those LMs filled in each word (or token) of the response.&lt;/p&gt;&lt;p dir="ltr"&gt;This collective approach competed against three comparable ones: a follower-only baseline powered by Llama-3.2-1B, GPT-4o working on its own, and the industry-leading o1 reasoning system that helps ChatGPT figure out more complex questions, such as coding requests and math problems.&lt;br&gt;&lt;br&gt;DisCIPL first presented an ability to write sentences and paragraphs that follow explicit rules. The models were given very specific prompts — for example, writing a sentence that has exactly 18 words, where the fourth word must be “Glasgow,” the eighth should be “in”, and the 11th must be “and.” The system was remarkably adept at handling this request, crafting coherent outputs while achieving accuracy and coherence similar to o1.&lt;br&gt;&lt;br&gt;&lt;strong&gt;Faster, cheaper, better&lt;/strong&gt;&lt;/p&gt;&lt;p dir="ltr"&gt;This experiment also revealed that key components of DisCIPL were much cheaper than state-of-the-art systems. For instance, whereas existing reasoning models like OpenAI’s o1 perform reasoning in text, DisCIPL “reasons” by writing Python code, which is more compact. In practice, the researchers found that DisCIPL led to 40.1 percent shorter reasoning and 80.2 percent cost savings over o1.&lt;/p&gt;&lt;p dir="ltr"&gt;DisCIPL’s efficiency gains stem partly from using small Llama models as followers, which are 1,000 to 10,000 times cheaper per token than comparable reasoning models. This means that DisCIPL is more “scalable” — the researchers were able to run dozens of Llama models in parallel for a fraction of the cost.&lt;br&gt;&lt;br&gt;Those weren’t the only surprising findings, according to CSAIL researchers. Their system also performed well against o1 on real-world tasks, such as making ingredient lists, planning out a travel itinerary, and writing grant proposals with word limits. Meanwhile, GPT-4o struggled with these requests, and with writing tests, it often couldn’t place keywords in the correct parts of sentences. The follower-only baseline essentially finished in last place across the board, as it had difficulties with following instructions.&lt;/p&gt;&lt;p dir="ltr"&gt;“Over the last several years, we’ve seen some impressive results from approaches that use language models to&amp;nbsp;‘&lt;a href="https://news.mit.edu/2024/natural-language-boosts-llm-performance-coding-planning-robotics-0501" target="_blank"&gt;auto-formalize&lt;/a&gt;’ problems in math and robotics by representing them with code,” says senior author Jacob Andreas, who is an MIT electrical engineering and computer science associate professor and CSAIL principal investigator. “What I find most exciting about this paper is the fact that we can now use LMs to auto-formalize text generation itself, enabling the same kinds of efficiency gains and guarantees that we’ve seen in these other domains.”&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;In the future, the researchers plan on expanding this framework into a more fully-recursive approach, where you can use the same model as both the leader and followers. Grand adds that DisCIPL could be extended to mathematical reasoning tasks, where answers are harder to verify. They also intend to test the system on its ability to meet users’ fuzzy preferences, as opposed to following hard constraints, which can’t be outlined in code so explicitly. Thinking even bigger, the team hopes to use the largest possible models available, although they note that such experiments are computationally expensive.&lt;/p&gt;&lt;p dir="ltr"&gt;Grand and Andreas wrote the paper alongside CSAIL principal investigator and MIT Professor Joshua Tenenbaum, as well as MIT Department of Brain and Cognitive Sciences Principal Research Scientist Vikash Mansinghka and Yale University Assistant Professor Alex Lew SM ’20 PhD ’25. CSAIL researchers presented the work at the Conference on Language Modeling in October and IVADO’s “Deploying Autonomous Agents: Lessons, Risks and Real-World Impact” workshop in November.&lt;br&gt;&lt;br&gt;Their work was supported, in part, by the MIT Quest for Intelligence, Siegel Family Foundation, the MIT-IBM Watson AI Lab, a Sloan Research Fellowship, Intel, the Air Force Office of Scientific Research, the Defense Advanced Research Projects Agency, the Office of Naval Research, and the National Science Foundation.&lt;/p&gt;</content:encoded>
      <media:content url="https://news.mit.edu/sites/default/files/styles/news_article__cover_image__original/public/images/202512/self-steering-model-mit-csail-00_0.png?itok=v7ciylOd" medium="image" type="image/jpeg" width="390" height="260">
              <media:description type="plain">A new approach developed by MIT CSAIL researchers uses an LLM to plan how to answer complex reasoning tasks, then divides the legwork of that strategy among smaller language models. Their method helps LMs provide more accurate responses than leading LLMs and approach the precision of top reasoning systems, while being more efficient than both.</media:description>
              <media:credit>Image: Alex Shipps/MIT CSAIL</media:credit>
      </media:content>
        <category domain="https://news.mit.edu/topic/research">Research</category>
      <category domain="https://news.mit.edu/topic/brain-cognitive">Brain and cognitive sciences</category>
      <category domain="https://news.mit.edu/topic/algorithms">Algorithms</category>
      <category domain="https://news.mit.edu/topic/quest-intelligence">MIT Siegel Family Quest for Intelligence</category>
      <category domain="https://news.mit.edu/topic/computers">Computer science and technology</category>
      <category domain="https://news.mit.edu/topic/artificial-intelligence2">Artificial intelligence</category>
      <category domain="https://news.mit.edu/topic/machine-learning">Machine learning</category>
      <category domain="https://news.mit.edu/topic/mit-ibm-watson-ai-lab">MIT-IBM Watson AI Lab</category>
      <category domain="https://news.mit.edu/topic/electrical-engineering-computer-science-eecs">Electrical engineering and computer science (EECS)</category>
      <category domain="https://news.mit.edu/topic/computer-science-and-artificial-intelligence-laboratory-csail">Computer Science and Artificial Intelligence Laboratory (CSAIL)</category>
      <category domain="https://news.mit.edu/topic/school-engineering">School of Engineering</category>
      <category domain="https://news.mit.edu/topic/mit-schwarzman-college-computing">MIT Schwarzman College of Computing</category>
      <category domain="https://news.mit.edu/topic/school-science">School of Science</category>
      <category domain="https://news.mit.edu/topic/nsf">National Science Foundation (NSF)</category>
      <category domain="https://news.mit.edu/topic/darpa">Defense Advanced Research Projects Agency (DARPA)</category>
    </item>
<item>
  <title>New method improves the reliability of statistical estimations</title>
  <link>https://news.mit.edu/2025/new-method-improves-reliability-statistical-estimations-1212</link>
  <description>The technique can help scientists in economics, public health, and other fields understand whether to trust the results of their experiments.</description>
  <pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
    <guid isPermaLink="true">https://news.mit.edu/2025/new-method-improves-reliability-statistical-estimations-1212</guid>
        <dc:creator>Adam Zewe | MIT News</dc:creator>
  <content:encoded>&lt;p&gt;Let’s say an environmental scientist is studying whether exposure to air pollution is associated with lower birth weights in a particular county.&lt;/p&gt;&lt;p&gt;They might train a machine-learning model to estimate the magnitude of this association, since machine-learning methods are especially good at learning complex relationships.&lt;/p&gt;&lt;p&gt;Standard machine-learning methods excel at making predictions and sometimes provide uncertainties, like confidence intervals, for these predictions. However, they generally don’t provide estimates or confidence intervals when determining whether two variables are related. Other methods have been developed specifically to address this association problem and provide confidence intervals. But, in spatial settings, MIT researchers found these confidence intervals can be completely off the mark.&lt;/p&gt;&lt;p&gt;When variables like air pollution levels or precipitation change across different locations, common methods for generating confidence intervals may claim a high level of confidence when, in fact, the estimation completely failed to capture the actual value. These faulty confidence intervals can mislead the user into trusting a model that failed.&lt;/p&gt;&lt;p&gt;After identifying this shortfall, the researchers developed a new method designed to generate valid confidence intervals for problems involving data that vary across space. In simulations and experiments with real data, their method was the only technique that consistently generated accurate confidence intervals.&lt;/p&gt;&lt;p&gt;This work could help researchers in fields like environmental science, economics, and epidemiology better understand when to trust the results of certain experiments.&lt;/p&gt;&lt;p&gt;“There are so many problems where people are interested in understanding phenomena over space, like weather or forest management. We’ve shown that, for this broad class of problems, there are more appropriate methods that can get us better performance, a better understanding of what is going on, and results that are more trustworthy,” says Tamara Broderick, an associate professor in MIT’s Department of Electrical Engineering and Computer Science (EECS), a member of the Laboratory for Information and Decision Systems (LIDS) and the Institute for Data, Systems, and Society, an affiliate of the Computer Science and Artificial Intelligence Laboratory (CSAIL), and senior author of this &lt;a href="https://arxiv.org/pdf/2502.06067" target="_blank"&gt;study&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;Broderick is joined on the paper by co-lead authors David R. Burt, a postdoc, and Renato Berlinghieri, an EECS graduate student; and Stephen Bates an assistant professor in EECS and member of LIDS. The research was recently presented at the Conference on Neural Information Processing Systems.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Invalid assumptions&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Spatial association involves studying how a variable and a certain outcome are related over a geographic area. For instance, one might want to study how tree cover in the United States relates to elevation.&lt;/p&gt;&lt;p&gt;To solve this type of problem, a scientist could gather observational data from many locations and use it to estimate the association at a different location where they do not have data.&lt;/p&gt;&lt;p&gt;The MIT researchers realized that, in this case, existing methods often generate confidence intervals that are completely wrong. A model might say it is 95 percent confident its estimation captures the true relationship between tree cover and elevation, when it didn’t capture that relationship at all.&lt;/p&gt;&lt;p&gt;After exploring this problem, the researchers determined that the assumptions these confidence interval methods rely on don’t hold up when data vary spatially.&lt;/p&gt;&lt;p&gt;Assumptions are like rules that must be followed to ensure results of a statistical analysis are valid. Common methods for generating confidence intervals operate under various assumptions.&lt;/p&gt;&lt;p&gt;First, they assume that the source data, which is the observational data one gathered to train the model, is independent and identically distributed. This assumption implies that the chance of including one location in the data has no bearing on whether another is included. But, for example, U.S. Environmental Protection Agency (EPA) air sensors are placed with other air sensor locations in mind.&lt;/p&gt;&lt;p&gt;Second, existing methods often assume that the model is perfectly correct, but this assumption is never true in practice. Finally, they assume the source data are similar to the target data where one wants to estimate.&lt;/p&gt;&lt;p&gt;But in spatial settings, the source data can be fundamentally different from the target data because the target data are in a different location than where the source data were gathered.&lt;/p&gt;&lt;p&gt;For instance, a scientist might use data from EPA pollution monitors to train a machine-learning model that can predict health outcomes in a rural area where there are no monitors. But the EPA pollution monitors are likely placed in urban areas, where there is more traffic and heavy industry, so the air quality data will be much different than the air quality data in the rural area.&lt;/p&gt;&lt;p&gt;In this case, estimates of association using the urban data suffer from bias because the target data are systematically different from the source data.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;A smooth solution&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;The new method for generating confidence intervals explicitly accounts for this potential bias.&lt;/p&gt;&lt;p&gt;Instead of assuming the source and target data are similar, the researchers assume the data vary smoothly over space.&lt;/p&gt;&lt;p&gt;For instance, with fine particulate air pollution, one wouldn’t expect the pollution level on one city block to be starkly different than the pollution level on the next city block. Instead, pollution levels would smoothly taper off as one moves away from a pollution source.&lt;/p&gt;&lt;p&gt;“For these types of problems, this spatial smoothness assumption is more appropriate. It is a better match for what is actually going on in the data,” Broderick says.&lt;/p&gt;&lt;p&gt;When they compared their method to other common techniques, they found it was the only one that could consistently produce reliable confidence intervals for spatial analyses. In addition, their method remains reliable even when the observational data are distorted by random errors.&lt;/p&gt;&lt;p&gt;In the future, the researchers want to apply this analysis to different types of variables and explore other applications where it could provide more reliable results.&lt;/p&gt;&lt;p&gt;This research was funded, in part, by an MIT Social and Ethical Responsibilities of Computing (SERC) seed grant, the Office of Naval Research, Generali, Microsoft, and the National Science Foundation (NSF).&lt;/p&gt;</content:encoded>
      <media:content url="https://news.mit.edu/sites/default/files/styles/news_article__cover_image__original/public/images/202512/MIT-SmoothSailing-01-press.jpg?itok=7_MdZ9Pf" medium="image" type="image/jpeg" width="390" height="260">
              <media:description type="plain">Researchers developed a new method designed to generate valid confidence intervals for problems involving data that vary across space. Instead of assuming the source and target data are similar, the researchers assume the data vary smoothly over space.</media:description>
              <media:credit>Image: MIT News; iStock</media:credit>
      </media:content>
        <category domain="https://news.mit.edu/topic/research">Research</category>
      <category domain="https://news.mit.edu/topic/data">Data</category>
      <category domain="https://news.mit.edu/topic/mathematics">Mathematics</category>
      <category domain="https://news.mit.edu/topic/computers">Computer science and technology</category>
      <category domain="https://news.mit.edu/topic/computer-science-and-artificial-intelligence-laboratory-csail">Computer Science and Artificial Intelligence Laboratory (CSAIL)</category>
      <category domain="https://news.mit.edu/topic/artificial-intelligence2">Artificial intelligence</category>
      <category domain="https://news.mit.edu/topic/machine-learning">Machine learning</category>
      <category domain="https://news.mit.edu/topic/lids">Laboratory for Information and Decision Systems (LIDS)</category>
      <category domain="https://news.mit.edu/topic/idss">IDSS</category>
      <category domain="https://news.mit.edu/topic/electrical-engineering-computer-science-eecs">Electrical engineering and computer science (EECS)</category>
      <category domain="https://news.mit.edu/topic/school-engineering">School of Engineering</category>
      <category domain="https://news.mit.edu/topic/mit-schwarzman-college-computing">MIT Schwarzman College of Computing</category>
      <category domain="https://news.mit.edu/topic/nsf">National Science Foundation (NSF)</category>
    </item>
<item>
  <title>New control system teaches soft robots the art of staying safe</title>
  <link>https://news.mit.edu/2025/new-control-system-teaches-soft-robots-art-staying-safe-1202</link>
  <description>MIT CSAIL and LIDS researchers developed a mathematically grounded system that lets soft robots deform, adapt, and interact with people and objects, without violating safety limits. </description>
  <pubDate>Tue, 02 Dec 2025 14:00:00 -0500</pubDate>
    <guid isPermaLink="true">https://news.mit.edu/2025/new-control-system-teaches-soft-robots-art-staying-safe-1202</guid>
        <dc:creator>Rachel Gordon | MIT CSAIL</dc:creator>
  <content:encoded>&lt;p dir="ltr" id="docs-internal-guid-8adfd4f6-7fff-d047-727d-cc029b78b3ad"&gt;Imagine having a continuum soft robotic arm bend around a bunch of grapes or broccoli, adjusting its grip in real time as it lifts the object. Unlike traditional rigid robots that generally aim to avoid contact with the environment as much as possible and stay far away from humans for safety reasons, this arm senses subtle forces, stretching and flexing in ways that mimic more of the compliance of a human hand. Its every motion is calculated to avoid excessive force while achieving the task efficiently. In MIT Computer Science and Artificial Intelligence Laboratory (CSAIL) and Laboratory for Information and Decisions Systems (LIDS) labs, these seemingly simple movements are the culmination of complex mathematics, careful engineering, and a vision for robots that can safely interact with humans and delicate objects.&lt;/p&gt;&lt;p dir="ltr"&gt;Soft robots, with their deformable bodies, promise a future where machines move more seamlessly alongside people, assist in caregiving, or handle delicate items in industrial settings. Yet that very flexibility makes them difficult to control. Small bends or twists can produce unpredictable forces, raising the risk of damage or injury. This motivates the need for safe control strategies for soft robots.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;“Inspired by advances in safe control and formal methods for rigid robots, we aim to adapt these ideas to soft robotics — modeling their complex behavior and embracing, rather than avoiding, contact — to enable higher-performance designs (e.g., greater payload and precision) without sacrificing safety or embodied intelligence,” says lead senior author and MIT Assistant Professor Gioele Zardini, who is a principal investigator in LIDS and the Department of Civil and Environmental Engineering, and an affiliate faculty with the Institute for Data, Systems, and Society (IDSS). “This vision is shared by recent and parallel work from other groups.”&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;strong&gt;Safety first&lt;/strong&gt;&lt;/p&gt;&lt;p dir="ltr"&gt;The team developed a new framework that blends nonlinear control theory (controlling systems that involve highly complex dynamics) with advanced physical modeling techniques and efficient real-time optimization to produce what they call “contact-aware safety.” At the heart of the approach are high-order control barrier functions (HOCBFs) and high-order control Lyapunov functions (HOCLFs). HOCBFs define safe operating boundaries, ensuring the robot doesn’t exert unsafe forces. HOCLFs guide the robot efficiently toward its task objectives, balancing safety with performance.&lt;/p&gt;&lt;p dir="ltr"&gt;“Essentially, we’re teaching the robot to know its own limits when interacting with the environment while still achieving its goals,” says MIT Department of Mechanical Engineering PhD student Kiwan Wong, the lead author of a new paper describing the framework. “The approach involves some complex derivation of soft robot dynamics, contact models, and control constraints, but the specification of control objectives and safety barriers is rather straightforward for the practitioner, and the outcomes are very tangible, as you see the robot moving smoothly, reacting to contact, and never causing unsafe situations.”&lt;/p&gt;&lt;p dir="ltr"&gt;“Compared with traditional kinematic CBFs — where forward-invariant safe sets are hard to specify — the HOCBF framework simplifies barrier design, and its optimization formulation accounts for system dynamics (e.g., inertia), ensuring the soft robot stops early enough to avoid unsafe contact forces,” says Worcester Polytechnic Institute Assistant Professor and former CSAIL postdoc Wei Xiao.&lt;/p&gt;&lt;p dir="ltr"&gt;“Since soft robots emerged, the field has highlighted their embodied intelligence and greater inherent safety relative to rigid robots, thanks to passive material and structural compliance. Yet their “cognitive” intelligence — especially safety systems — has lagged behind that of rigid serial-link manipulators,” says co-lead author Maximilian Stölzle, a research intern at Disney Research and formerly a Delft University of Technology PhD student and visiting researcher at MIT LIDS and CSAIL. “This work helps close that gap by adapting proven algorithms to soft robots and tailoring them for safe contact and soft-continuum dynamics.”&lt;/p&gt;&lt;p dir="ltr"&gt;The LIDS and CSAIL team tested the system on a series of experiments designed to challenge the robot’s safety and adaptability. In one test, the arm pressed gently against a compliant surface, maintaining a precise force without overshooting. In another, it traced the contours of a curved object, adjusting its grip to avoid slippage. In yet another demonstration, the robot manipulated fragile items alongside a human operator, reacting in real time to unexpected nudges or shifts. “These experiments show that our framework is able to generalize to diverse tasks and objectives, and the robot can sense, adapt, and act in complex scenarios while always respecting clearly defined safety limits,” says Zardini.&lt;/p&gt;&lt;p dir="ltr"&gt;Soft robots with contact-aware safety could be a real value-add in high-stakes places, of course. In health care, they could assist in surgeries, providing precise manipulation while reducing risk to patients. In industry, they might handle fragile goods without constant supervision. In domestic settings, robots could help with chores or caregiving tasks, interacting safely with children or the elderly — a key step toward making soft robots reliable partners in real-world environments.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;“Soft robots have incredible potential,” says co-lead senior author Daniela Rus, director of CSAIL and a professor in the Department of Electrical Engineering and Computer Science. “But ensuring safety and encoding motion tasks via relatively simple objectives has always been a central challenge. We wanted to create a system where the robot can remain flexible and responsive while mathematically guaranteeing it won’t exceed safe force limits.”&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;strong&gt;Combining soft robot models, differentiable simulation, and control theory&lt;/strong&gt;&lt;/p&gt;&lt;p dir="ltr"&gt;Underlying the control strategy is a differentiable implementation of something called the Piecewise Cosserat-Segment (PCS) dynamics model, which predicts how a soft robot deforms and where forces accumulate. This model allows the system to anticipate how the robot’s body will respond to actuation and complex interactions with the environment. “The aspect that I most like about this work is the blend of integration of new and old tools coming from different fields like advanced soft robot models, differentiable simulation, Lyapunov theory, convex optimization, and injury-severity–based safety constraints. All of this is nicely blended into a real-time controller fully grounded in first principles,” says co-author Cosimo Della Santina, who is an associate professor at Delft University of Technology.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;Complementing this is the Differentiable Conservative Separating Axis Theorem (DCSAT), which estimates distances between the soft robot and obstacles in the environment that can be approximated with a chain of convex polygons in a differentiable manner. “Earlier differentiable distance metrics for convex polygons either couldn’t compute penetration depth — essential for estimating contact forces — or yielded non-conservative estimates that could compromise safety,” says Wong. “Instead, the DCSAT metric returns strictly conservative, and therefore safe, estimates while simultaneously allowing for fast and differentiable computation.” Together, PCS and DCSAT give the robot a predictive sense of its environment for more proactive, safe interactions.&lt;/p&gt;&lt;p dir="ltr"&gt;Looking ahead, the team plans to extend their methods to three-dimensional soft robots and explore integration with learning-based strategies. By combining contact-aware safety with adaptive learning, soft robots could handle even more complex, unpredictable environments.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;“This is what makes our work exciting,” says Rus. “You can see the robot behaving in a human-like, careful manner, but behind that grace is a rigorous control framework ensuring it never oversteps its bounds.”&lt;/p&gt;&lt;p dir="ltr"&gt;“Soft robots are generally safer to interact with than rigid-bodied robots by design, due to the compliance and energy-absorbing properties of their bodies,” says University of Michigan Assistant Professor Daniel Bruder, who wasn’t involved in the research. “However, as soft robots become faster, stronger, and more capable, that may no longer be enough to ensure safety. This work takes a crucial step towards ensuring soft robots can operate safely by offering a method to limit contact forces across their entire bodies.”&lt;br&gt;&lt;br&gt;The team’s work was supported, in part, by The Hong Kong Jockey Club Scholarships, the European Union’s Horizon Europe Program, Cultuurfonds Wetenschapsbeurzen, and the Rudge (1948) and Nancy Allen Chair. Their work was published earlier this month in the Institute of Electrical and Electronics Engineers’ &lt;em&gt;Robotics and Automation Letters&lt;/em&gt;.&lt;/p&gt;</content:encoded>
      <media:content url="https://news.mit.edu/sites/default/files/styles/news_article__cover_image__original/public/images/202511/mit-csail-Contact-Aware.jpg?itok=B1-c14Yo" medium="image" type="image/jpeg" width="390" height="260">
              <media:description type="plain">MIT researchers are teaching robots to understand their own limits while still achieving their goals, ensuring the machines move safely and never overextend themselves.</media:description>
              <media:credit>Image: Maximilian Stölzle and Joey Impoza Roberts</media:credit>
      </media:content>
        <category domain="https://news.mit.edu/topic/school-engineering">School of Engineering</category>
      <category domain="https://news.mit.edu/topic/mit-schwarzman-college-computing">MIT Schwarzman College of Computing</category>
      <category domain="https://news.mit.edu/topic/civil-engineering">Civil and environmental engineering</category>
      <category domain="https://news.mit.edu/topic/electrical-engineering-computer-science-eecs">Electrical engineering and computer science (EECS)</category>
      <category domain="https://news.mit.edu/topic/mechanical-engineering">Mechanical engineering</category>
      <category domain="https://news.mit.edu/topic/computer-science-and-artificial-intelligence-laboratory-csail">Computer Science and Artificial Intelligence Laboratory (CSAIL)</category>
      <category domain="https://news.mit.edu/topic/idss">IDSS</category>
      <category domain="https://news.mit.edu/topic/lids">Laboratory for Information and Decision Systems (LIDS)</category>
      <category domain="https://news.mit.edu/topic/computers">Computer science and technology</category>
      <category domain="https://news.mit.edu/topic/robotics">Robotics</category>
      <category domain="https://news.mit.edu/topic/artificial-intelligence2">Artificial intelligence</category>
      <category domain="https://news.mit.edu/topic/machine-learning">Machine learning</category>
      <category domain="https://news.mit.edu/topic/safety">Safety</category>
      <category domain="https://news.mit.edu/topic/research">Research</category>
    </item>
<item>
  <title>MIT scientists debut a generative AI model that could create molecules addressing hard-to-treat diseases</title>
  <link>https://news.mit.edu/2025/mit-scientists-debut-generative-ai-model-that-could-create-molecules-addressing-hard-to-treat-diseases-1125</link>
  <description>BoltzGen generates protein binders for any biological target from scratch, expanding AI’s reach from understanding biology toward engineering it.</description>
  <pubDate>Tue, 25 Nov 2025 16:25:00 -0500</pubDate>
    <guid isPermaLink="true">https://news.mit.edu/2025/mit-scientists-debut-generative-ai-model-that-could-create-molecules-addressing-hard-to-treat-diseases-1125</guid>
        <dc:creator>Alex Ouyang | Abdul Latif Jameel Clinic for Machine Learning in Health</dc:creator>
  <content:encoded>&lt;p dir="ltr"&gt;More than 300 people across academia and industry spilled into an auditorium to attend a&amp;nbsp;&lt;a href="https://jclinic.mit.edu/events/boltzgen/"&gt;BoltzGen seminar&lt;/a&gt; on Thursday, Oct. 30, hosted by the&amp;nbsp;&lt;a href="https://jclinic.mit.edu/"&gt;Abdul Latif Jameel Clinic for Machine Learning in Health&lt;/a&gt; (MIT Jameel Clinic). Headlining the event was MIT PhD student and BoltzGen’s first author Hannes Stärk, who had announced BoltzGen just a few days prior.&lt;/p&gt;&lt;p dir="ltr"&gt;Building upon&amp;nbsp;&lt;a href="https://www.biorxiv.org/content/10.1101/2025.06.14.659707v1"&gt;Boltz-2&lt;/a&gt;, an open-source biomolecular structure prediction model predicting protein binding affinity that made waves over the summer,&amp;nbsp;&lt;a href="https://www.biorxiv.org/content/10.1101/2025.11.20.689494v1"&gt;BoltzGen&lt;/a&gt; (officially released on Sunday, Oct. 26.) is the first model of its kind to go a step further by generating novel protein binders that are ready to enter the drug discovery pipeline.&lt;/p&gt;&lt;p dir="ltr"&gt;Three key innovations make this possible: first, BoltzGen’s ability to carry out a variety of tasks, unifying protein design and structure prediction while maintaining state-of-the-art performance. Next, BoltzGen’s built-in constraints are designed with feedback from wetlab collaborators to ensure the model creates functional proteins that don’t defy the laws of physics or chemistry. Lastly, a rigorous evaluation process tests the model on “undruggable” disease targets, pushing the limits of BoltzGen’s binder generation capabilities.&lt;/p&gt;&lt;p dir="ltr"&gt;Most models used in industry or academia are capable of either structure prediction or protein design. Moreover, they’re limited to generating certain types of proteins that bind successfully to easy “targets.” Much like students responding to a test question that looks like their homework, as long as the training data looks similar to the target during binder design, the models often work. But existing methods are nearly always evaluated on targets for which structures with binders already exist, and end up faltering in performance when used on more challenging targets.&lt;/p&gt;&lt;p dir="ltr"&gt;“There have been models trying to tackle binder design, but the problem is that these models are modality-specific,” Stärk points out. “A general model does not only mean that we can address more tasks. Additionally, we obtain a better model for the individual task since emulating physics is learned by example, and with a more general training scheme, we provide more such examples containing generalizable physical patterns.”&lt;/p&gt;&lt;p dir="ltr"&gt;The BoltzGen researchers went out of their way to test BoltzGen on 26 targets, ranging from therapeutically relevant cases to ones explicitly chosen for their dissimilarity to the training data.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;This comprehensive validation process, which took place in eight wetlabs across academia and industry, demonstrates the model’s breadth and potential for breakthrough drug development.&lt;/p&gt;&lt;p dir="ltr"&gt;Parabilis Medicines, one of the industry collaborators that tested BoltzGen in a wetlab setting, praised BoltzGen’s potential:&amp;nbsp;“we feel that adopting BoltzGen into our existing Helicon peptide computational platform capabilities promises to accelerate our progress to deliver transformational drugs against major human diseases.”&lt;/p&gt;&lt;p dir="ltr"&gt;While the open-source releases of Boltz-1, Boltz-2, and now BoltzGen (which was previewed at the&amp;nbsp;&lt;a href="https://www.moml.mit.edu/"&gt;7th Molecular Machine Learning Conference&lt;/a&gt; on Oct. 22) bring new opportunities and transparency in drug development, they also signal that biotech and pharmaceutical industries may need to reevaluate their offerings.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;Amid the buzz for BoltzGen on the social media platform X, Justin Grace, a principal machine learning scientist at LabGenius, raised a question. “The private-to-open performance time lag for chat AI systems is [seven] months and falling,” Grace wrote in&amp;nbsp;&lt;a href="https://x.com/jusjosgra/status/1982763802920927252"&gt;a post&lt;/a&gt;. “It looks to be even shorter in the protein space. How will binder-as-a-service co’s be able to [recoup] investment when we can just wait a few months for the free version?”&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;For those in academia, BoltzGen represents an expansion and acceleration of scientific possibility.&amp;nbsp;“A question that my students often ask me is, ‘where can AI change the therapeutics game?’” says senior co-author and MIT Professor Regina Barzilay, AI faculty lead for the Jameel Clinic and an affiliate of the Computer Science and Artificial Intelligence Laboratory (CSAIL). “Unless we identify undruggable targets and propose a solution, we won’t be changing the game,” she adds. “The emphasis here is on unsolved problems, which distinguishes Hannes’ work from others in the field.”&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;Senior co-author Tommi Jaakkola, the Thomas Siebel Professor of Electrical Engineering and Computer Science who is affiliated with the Jameel Clinic and CSAIL, notes that "models such as BoltzGen that are released fully open-source enable broader community-wide efforts to accelerate drug design capabilities.”&lt;/p&gt;&lt;p&gt;Looking ahead, Stärk believes that the future of biomolecular design will be upended by AI models.&amp;nbsp;“I want to build tools that help us manipulate biology to solve disease, or perform tasks with molecular machines that we have not even imagined yet,” he says. “I want to provide these tools and enable biologists to imagine things that they have not even thought of before.”&lt;/p&gt;</content:encoded>
      <media:content url="https://news.mit.edu/sites/default/files/styles/news_article__cover_image__original/public/images/202511/mit-BoltzGen.jpg?itok=iwyOhi0f" medium="image" type="image/jpeg" width="390" height="260">
              <media:description type="plain">More than 300 people attended a BoltzGen seminar on Oct. 30, just days after its initial release.</media:description>
              <media:credit>Photo: Ethan Wu, Will Stokes</media:credit>
      </media:content>
        <category domain="https://news.mit.edu/topic/research">Research</category>
      <category domain="https://news.mit.edu/topic/special-events">Special events and guest speakers</category>
      <category domain="https://news.mit.edu/topic/computers">Computer science and technology</category>
      <category domain="https://news.mit.edu/topic/health">Health sciences and technology</category>
      <category domain="https://news.mit.edu/topic/artificial-intelligence2">Artificial intelligence</category>
      <category domain="https://news.mit.edu/topic/machine-learning">Machine learning</category>
      <category domain="https://news.mit.edu/topic/drug-development">Drug development</category>
      <category domain="https://news.mit.edu/topic/proteins">Proteins</category>
      <category domain="https://news.mit.edu/topic/medicine">Medicine</category>
      <category domain="https://news.mit.edu/topic/pharmaceuticals">Pharmaceuticals</category>
      <category domain="https://news.mit.edu/topic/jameel-clinic">Jameel Clinic</category>
      <category domain="https://news.mit.edu/topic/electrical-engineering-computer-science-eecs">Electrical engineering and computer science (EECS)</category>
      <category domain="https://news.mit.edu/topic/computer-science-and-artificial-intelligence-laboratory-csail">Computer Science and Artificial Intelligence Laboratory (CSAIL)</category>
      <category domain="https://news.mit.edu/topic/school-engineering">School of Engineering</category>
      <category domain="https://news.mit.edu/topic/mit-schwarzman-college-computing">MIT Schwarzman College of Computing</category>
    </item>
<item>
  <title>Understanding the nuances of human-like intelligence</title>
  <link>https://news.mit.edu/2025/understanding-nuances-human-intelligence-phillip-isola-1111</link>
  <description>Associate Professor Phillip Isola studies the ways in which intelligent machines “think,” in an effort to safely integrate AI into human society. </description>
  <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
    <guid isPermaLink="true">https://news.mit.edu/2025/understanding-nuances-human-intelligence-phillip-isola-1111</guid>
        <dc:creator>Adam Zewe | MIT News</dc:creator>
  <content:encoded>&lt;p&gt;What can we learn about human intelligence by studying how machines “think?” Can we better understand ourselves if we better understand the artificial intelligence systems that are becoming a more significant part of our everyday lives?&lt;/p&gt;&lt;p&gt;These questions may be deeply philosophical, but for Phillip Isola, finding the answers is as much about computation as it is about cogitation.&lt;/p&gt;&lt;p&gt;Isola, the newly tenured associate professor in the Department of Electrical Engineering and Computer Science (EECS), studies the fundamental mechanisms involved in human-like intelligence from a computational perspective.&lt;/p&gt;&lt;p&gt;While understanding intelligence is the overarching goal, his work focuses mainly on computer vision and machine learning. Isola is particularly interested in exploring how intelligence emerges in AI models, how these models learn to represent the world around them, and what their “brains” share with the brains of their human creators.&lt;/p&gt;&lt;p&gt;“I see all the different kinds of intelligence as having a lot of commonalities, and I’d like to understand those commonalities. What is it that all animals, humans, and AIs have in common?” says Isola, who is also a member of the Computer Science and Artificial Intelligence Laboratory (CSAIL).&lt;/p&gt;&lt;p&gt;To Isola, a better scientific understanding of the intelligence that AI agents possess will help the world integrate them safely and effectively into society, maximizing their potential to benefit humanity.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Asking questions&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Isola began pondering scientific questions at a young age.&lt;/p&gt;&lt;p&gt;While growing up in San Francisco, he and his father frequently went hiking along the northern California coastline or camping around Point Reyes and in the hills of Marin County.&lt;/p&gt;&lt;p&gt;He was fascinated by geological processes and often wondered what made the natural world work. In school, Isola was driven by an insatiable curiosity, and while he gravitated toward technical subjects like math and science, there was no limit to what he wanted to learn.&lt;/p&gt;&lt;p&gt;Not entirely sure what to study as an undergraduate at Yale University, Isola dabbled until he came upon cognitive sciences.&lt;/p&gt;&lt;p&gt;“My earlier interest had been with nature — how the world works. But then I realized that the brain was even more interesting, and more complex than even the formation of the planets. Now, I wanted to know what makes us tick,” he says.&lt;/p&gt;&lt;p&gt;As a first-year student, he started working in the lab of his cognitive sciences professor and soon-to-be mentor, Brian Scholl, a member of the Yale Department of Psychology. He remained in that lab throughout his time as an undergraduate.&lt;/p&gt;&lt;p&gt;After spending a gap year working with some childhood friends at an indie video game company, Isola was ready to dive back into the complex world of the human brain. He enrolled in the graduate program in brain and cognitive sciences at MIT.&lt;/p&gt;&lt;p&gt;“Grad school was where I felt like I finally found my place. I had a lot of great experiences at Yale and in other phases of my life, but when I got to MIT, I realized this was the work I really loved and these are the people who think similarly to me,” he says.&lt;/p&gt;&lt;p&gt;Isola credits his PhD advisor, Ted Adelson, the John and Dorothy Wilson Professor of Vision Science, as a major influence on his future path. He was inspired by Adelson’s focus on understanding fundamental principles, rather than only chasing new engineering benchmarks, which are formalized tests used to measure the performance of a system.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;A computational perspective&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;At MIT, Isola’s research drifted toward computer science and artificial intelligence.&lt;/p&gt;&lt;p&gt;“I still loved all those questions from cognitive sciences, but I felt I could make more progress on some of those questions if I came at it from a purely computational perspective,” he says.&lt;/p&gt;&lt;p&gt;His thesis was focused on perceptual grouping, which involves the mechanisms people and machines use to organize discrete parts of an image as a single, coherent object.&lt;/p&gt;&lt;p&gt;If machines can learn perceptual groupings on their own, that could enable AI systems to recognize objects without human intervention. This type of self-supervised learning has applications in areas such autonomous vehicles, medical imaging, robotics, and automatic language translation.&lt;/p&gt;&lt;p&gt;After graduating from MIT, Isola completed a postdoc at the University of California at Berkeley so he could broaden his perspectives by working in a lab solely focused on computer science.&lt;/p&gt;&lt;p&gt;“That experience helped my work become a lot more impactful because I learned to balance understanding fundamental, abstract principles of intelligence with the pursuit of some more concrete benchmarks,” Isola recalls.&lt;/p&gt;&lt;p&gt;At Berkeley, he developed image-to-image translation frameworks, an early form of generative AI model that could turn a sketch into a photographic image, for instance, or turn a black-and-white photo into a color one.&lt;/p&gt;&lt;p&gt;He entered the academic job market and accepted a faculty position at MIT, but Isola deferred for a year to work at a then-small startup called OpenAI.&lt;/p&gt;&lt;p&gt;“It was a nonprofit, and I liked the idealistic mission at that time. They were really good at reinforcement learning, and I thought that seemed like an important topic to learn more about,” he says.&lt;/p&gt;&lt;p&gt;He enjoyed working in a lab with so much scientific freedom, but after a year Isola was ready to return to MIT and start his own research group.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Studying human-like intelligence&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Running a research lab instantly appealed to him.&lt;/p&gt;&lt;p&gt;“I really love the early stage of an idea. I feel like I am a sort of startup incubator where I am constantly able to do new things and learn new things,” he says.&lt;/p&gt;&lt;p&gt;Building on his interest in cognitive sciences and desire to understand the human brain, his group studies the fundamental computations involved in the human-like intelligence that emerges in machines.&lt;/p&gt;&lt;p&gt;One primary focus is representation learning, or the ability of humans and machines to represent and perceive the sensory world around them.&lt;/p&gt;&lt;p&gt;In recent work, he and his collaborators observed that the many varied types of machine-learning models, from LLMs to computer vision models to audio models, seem to represent the world in similar ways.&lt;/p&gt;&lt;p&gt;These models are designed to do vastly different tasks, but there are many similarities in their architectures. And as they get bigger and are trained on more data, their internal structures become more alike.&lt;/p&gt;&lt;p&gt;This led Isola and his team to introduce the Platonic Representation Hypothesis (drawing its name from the Greek philosopher Plato) which says that the representations all these models learn are converging toward a shared, underlying representation of reality.&lt;/p&gt;&lt;p&gt;“Language, images, sound — all of these are different shadows on the wall from which you can infer that there is some kind of underlying physical process — some kind of causal reality — out there. If you train models on all these different types of data, they should converge on that world model in the end,” Isola says.&lt;/p&gt;&lt;p&gt;A related area his team studies is self-supervised learning. This involves the ways in which AI models learn to group related pixels in an image or words in a sentence without having labeled examples to learn from.&lt;/p&gt;&lt;p&gt;Because data are expensive and labels are limited, using only labeled data to train models could hold back the capabilities of AI systems. With self-supervised learning, the goal is to develop models that can come up with an accurate internal representation of the world on their own.&lt;/p&gt;&lt;p&gt;“If you can come up with a good representation of the world, that should make subsequent problem solving easier,” he explains.&lt;/p&gt;&lt;p&gt;The focus of Isola’s research is more about finding something new and surprising than about building complex systems that can outdo the latest machine-learning benchmarks.&lt;/p&gt;&lt;p&gt;While this approach has yielded much success in uncovering innovative techniques and architectures, it means the work sometimes lacks a concrete end goal, which can lead to challenges.&lt;/p&gt;&lt;p&gt;For instance, keeping a team aligned and the funding flowing can be difficult when the lab is focused on searching for unexpected results, he says.&lt;/p&gt;&lt;p&gt;“In a sense, we are always working in the dark. It is high-risk and high-reward work. Every once in while, we find some kernel of truth that is new and surprising,” he says.&lt;/p&gt;&lt;p&gt;In addition to pursuing knowledge, Isola is passionate about imparting knowledge to the next generation of scientists and engineers. Among his favorite courses to teach is 6.7960 (Deep Learning), which he and several other MIT faculty members launched four years ago.&lt;/p&gt;&lt;p&gt;The class has seen exponential growth, from 30 students in its initial offering to more than 700 this fall.&lt;/p&gt;&lt;p&gt;And while the popularity of AI means there is no shortage of interested students, the speed at which the field moves can make it difficult to separate the hype from truly significant advances.&lt;/p&gt;&lt;p&gt;“I tell the students they have to take everything we say in the class with a grain of salt. Maybe in a few years we’ll tell them something different. We are really on the edge of knowledge with this course,” he says.&lt;/p&gt;&lt;p&gt;But Isola also emphasizes to students that, for all the hype surrounding the latest AI models, intelligent machines are far simpler than most people suspect.&lt;/p&gt;&lt;p&gt;“Human ingenuity, creativity, and emotions — many people believe these can never be modeled. That might turn out to be true, but I think intelligence is fairly simple once we understand it,” he says.&lt;/p&gt;&lt;p&gt;Even though his current work focuses on deep-learning models, Isola is still fascinated by the complexity of the human brain and continues to collaborate with researchers who study cognitive sciences.&lt;/p&gt;&lt;p&gt;All the while, he has remained captivated by the beauty of the natural world that inspired his first interest in science.&lt;/p&gt;&lt;p&gt;Although he has less time for hobbies these days, Isola enjoys hiking and backpacking in the mountains or on Cape Cod, skiing and kayaking, or finding scenic places to spend time when he travels for scientific conferences.&lt;/p&gt;&lt;p&gt;And while he looks forward to exploring new questions in his lab at MIT, Isola can’t help but contemplate how the role of intelligent machines might change the course of his work.&lt;/p&gt;&lt;p&gt;He believes that artificial general intelligence (AGI), or the point where machines can learn and apply their knowledge as well as humans can, is not that far off.&lt;/p&gt;&lt;p&gt;“I don’t think AIs will just do everything for us and we’ll go and enjoy life at the beach. I think there is going to be this coexistence between smart machines and humans who still have a lot of agency and control. Now, I’m thinking about the interesting questions and applications once that happens. How can I help the world in this post-AGI future? I don’t have any answers yet, but it’s on my mind,” he says.&lt;/p&gt;</content:encoded>
      <media:content url="https://news.mit.edu/sites/default/files/styles/news_article__cover_image__original/public/images/202511/MIT-Phillip-Isola-01-press.jpg?itok=az-LV8-Q" medium="image" type="image/jpeg" width="390" height="260">
              <media:description type="plain">“I see all the different kinds of intelligence as having a lot of commonalities, and I’d like to understand those commonalities. What is it that all animals, humans, and AIs have in common?” says Phillip Isola.</media:description>
              <media:credit>Photo: Jake Belcher</media:credit>
      </media:content>
        <category domain="https://news.mit.edu/topic/profile">Profile</category>
      <category domain="https://news.mit.edu/topic/faculty">Faculty</category>
      <category domain="https://news.mit.edu/topic/computers">Computer science and technology</category>
      <category domain="https://news.mit.edu/topic/artificial-intelligence2">Artificial intelligence</category>
      <category domain="https://news.mit.edu/topic/machine-learning">Machine learning</category>
      <category domain="https://news.mit.edu/topic/brain-cognitive">Brain and cognitive sciences</category>
      <category domain="https://news.mit.edu/topic/computer-science-and-artificial-intelligence-laboratory-csail">Computer Science and Artificial Intelligence Laboratory (CSAIL)</category>
      <category domain="https://news.mit.edu/topic/electrical-engineering-computer-science-eecs">Electrical engineering and computer science (EECS)</category>
      <category domain="https://news.mit.edu/topic/school-engineering">School of Engineering</category>
      <category domain="https://news.mit.edu/topic/mit-schwarzman-college-computing">MIT Schwarzman College of Computing</category>
    </item>
<item>
  <title>Charting the future of AI, from safer answers to faster thinking</title>
  <link>https://news.mit.edu/2025/charting-the-future-of-ai-from-safer-answers-to-faster-thinking-1106</link>
  <description>MIT PhD students who interned with the MIT-IBM Watson AI Lab Summer Program are pushing AI tools to be more flexible, efficient, and grounded in truth.</description>
  <pubDate>Thu, 06 Nov 2025 16:40:00 -0500</pubDate>
    <guid isPermaLink="true">https://news.mit.edu/2025/charting-the-future-of-ai-from-safer-answers-to-faster-thinking-1106</guid>
        <dc:creator>Lauren Hinkel | MIT-IBM Watson AI Lab</dc:creator>
  <content:encoded>&lt;p&gt;Adoption of new tools and technologies occurs when users largely perceive them as reliable, accessible, and an improvement over the available methods and workflows for the cost. Five PhD students from the inaugural class of the MIT-IBM Watson AI Lab Summer Program are utilizing state-of-the-art resources, alleviating AI pain points, and creating new features and capabilities to promote AI usefulness and deployment — from learning when to trust a model that predicts another’s accuracy to more effectively reasoning over knowledge bases. Together, the efforts from the students and their mentors form a through-line, where practical and technically rigorous research leads to more dependable and valuable models across domains.&lt;/p&gt;&lt;p&gt;Building probes, routers, new attention mechanisms, synthetic datasets, and program-synthesis pipelines, the students’ work spans safety, inference efficiency, multimodal data, and knowledge-grounded reasoning. Their techniques emphasize scaling and integration, with impact always in sight.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Learning to trust, and when&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;MIT math graduate student Andrey Bryutkin’s research prioritizes the trustworthiness of models. He seeks out internal structures within problems, such as equations governing a system and conservation laws, to understand how to leverage them to produce more dependable and robust solutions. Armed with this and working with the lab, Bryutkin developed a method to peer into the nature of large learning models (LLMs) behaviors. Together with the lab’s Veronika Thost of IBM Research and Marzyeh Ghassemi — associate professor and the Germeshausen Career Development Professor in the MIT Department of Electrical Engineering and Computer Science (EECS) and a member of the Institute of Medical Engineering Sciences and the Laboratory for Information and Decision Systems — Bryutkin explored the “uncertainty of uncertainty” of LLMs.&amp;nbsp;&lt;/p&gt;&lt;p&gt;Classically, tiny feed-forward neural networks two-to-three layers deep, called probes, are trained alongside LLMs and employed to flag untrustworthy answers from the larger model to developers; however, these classifiers can also produce false negatives and only provide point estimates, which don’t offer much information about when the LLM is failing. Investigating safe/unsafe prompts and question-answer tasks, the MIT-IBM team used prompt-label pairs, as well as the hidden states like activation vectors and last tokens from an LLM, to measure gradient scores, sensitivity to prompts, and out-of-distribution data to determine how reliable the probe was and learn areas of data that are difficult to predict. Their method also helps identify potential labeling noise. This is a critical function, as the trustworthiness of AI systems depends entirely on the quality and accuracy of the labeled data&amp;nbsp;they are built upon.&amp;nbsp;More accurate and consistent probes are especially important for domains with critical data in applications like IBM’s Granite Guardian family of models.&lt;/p&gt;&lt;p&gt;Another way to ensure trustworthy responses to queries from an LLM is to augment them with external, trusted knowledge bases to eliminate hallucinations. For structured data, such as social media connections, financial transactions, or corporate databases, knowledge graphs (KG) are natural fits; however, communications between the LLM and KGs often use fixed, multi-agent pipelines that are computationally inefficient and expensive. Addressing this, physics graduate student Jinyeop Song, along with lab researchers Yada Zhu of IBM Research and EECS Associate Professor Julian Shun created a single-agent, multi-turn, reinforcement learning framework that streamlines this process. Here, the group designed an API server hosting Freebase and Wikidata KGs, which consist of general web-based knowledge data, and a LLM agent that issues targeted retrieval actions to fetch pertinent information from the server. Then, through continuous back-and-forth, the agent appends the gathered data from the KGs to the context and responds to the query. Crucially, the system uses reinforcement learning to train itself to deliver answers that strike a balance between accuracy and completeness. The framework pairs an API server with a single reinforcement learning agent to orchestrate data-grounded reasoning with improved accuracy, transparency, efficiency, and transferability.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Spending computation wisely&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;The timeliness and completeness of a model’s response carry similar weight to the importance of its accuracy. This is especially true for handling long input texts and those where elements, like the subject of a story, evolve over time, so EECS graduate student Songlin Yang is re-engineering what models can handle at each step of inference. Focusing on transformer limitations, like those in LLMs, the lab’s Rameswar Panda of IBM Research and Yoon Kim, the NBX Professor and associate professor in EECS, joined Yang to develop next-generation language model architectures beyond transformers.&lt;/p&gt;&lt;p&gt;Transformers face two key limitations: high computational complexity in long-sequence modeling due to the softmax attention mechanism, and limited expressivity resulting from the weak inductive bias of RoPE (rotary positional encoding). This means that as the input length doubles, the computational cost quadruples. RoPE allows transformers to understand the sequence order of tokens (i.e., words); however, it does not do a good job capturing internal state changes over time, like variable values, and is limited to the sequence lengths seen during training.&lt;/p&gt;&lt;p&gt;To address this, the MIT-IBM team explored theoretically grounded yet hardware-efficient algorithms. As an alternative to softmax attention, they adopted linear attention, reducing the quadratic complexity that limits the feasible sequence length. They also investigated hybrid architectures that combine softmax and linear attention to strike a better balance between computational efficiency and performance.&lt;/p&gt;&lt;p&gt;Increasing expressivity, they replaced RoPE with a dynamic reflective positional encoding based on the Householder transform. This approach enables richer positional interactions for deeper understanding of sequential information, while maintaining fast and efficient computation. The MIT-IBM team’s advancement reduces the need for transformers to break problems into many steps, instead enabling them to handle more complex subproblems with fewer inference tokens.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Visions anew&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Visual data contain multitudes that the human brain can quickly parse, internalize, and then imitate. Using vision-language models (VLMs), two graduate students are exploring ways to do this through code.&lt;/p&gt;&lt;p&gt;Over the past two summers and under the advisement of Aude Oliva, MIT director of the MIT-IBM Watson AI Lab and a senior research scientist in the Computer Science and Artificial Intelligence Laboratory; and IBM Research’s Rogerio Feris, Dan Gutfreund, and Leonid Karlinsky (now at Xero), Jovana Kondic of EECS has explored visual document understanding, specifically charts. These contain elements, such as data points, legends, and axes labels, that require optical character recognition and numerical reasoning, which models still struggle with. In order to facilitate the performance on tasks such as these, Kondic’s group set out to create a large, open-source, synthetic chart dataset from code that could be used for training and benchmarking.&amp;nbsp;&lt;/p&gt;&lt;p&gt;With their prototype, ChartGen, the researchers created a pipeline that passes seed chart images through a VLM, which is prompted to read the chart and generate a Python script that was likely used to create the chart in the first place. The LLM component of the framework then iteratively augments the code from many charts to ultimately produce over 200,000 unique pairs of charts and their codes, spanning nearly 30 chart types, as well as supporting data and annotation like descriptions and question-answer pairs about the charts. The team is further expanding their dataset, helping to enable critical multimodal understanding to data visualizations for enterprise applications like financial and scientific reports, blogs, and more.&lt;/p&gt;&lt;p&gt;Instead of charts, EECS graduate student Leonardo Hernandez Cano has his eyes on digital design, specifically visual texture generation for CAD applications and the goal of discovering efficient ways to enable to capabilities in VLMs. Teaming up with the lab groups led by Armando Solar-Lezama, EECS professor and Distinguished Professor of Computing in the MIT Schwarzman College of Computing, and IBM Research’s Nathan Fulton, Hernandez Cano created a program synthesis system that learns to refine code on its own. The system starts with a texture description given by a user in the form of an image. It then generates an initial Python program, which produces visual textures, and iteratively refines the code with the goal of finding a program that produces a texture that matches the target description, learning to search for new programs from the data that the system itself produces. Through these refinements, the novel program can create visualizations with the desired luminosity, color, iridescence, etc., mimicking real materials.&lt;/p&gt;&lt;p&gt;When viewed together, these projects, and the people behind them, are making a cohesive push toward more robust and practical artificial intelligence. By tackling the core challenges of reliability, efficiency, and multimodal reasoning, the work paves the way for AI systems that are not only more powerful, but also more dependable and cost-effective, for real-world enterprise and scientific applications.&lt;/p&gt;</content:encoded>
      <media:content url="https://news.mit.edu/sites/default/files/styles/news_article__cover_image__original/public/images/202510/MIT-IBM-summer-interns-2025.jpg?itok=gxiU5iCw" medium="image" type="image/jpeg" width="390" height="260">
              <media:description type="plain">Top row, from left to right: Andrey Bryutkin, Jovana Kondic, and Songlin Yang. Bottom row: Jinyeop Song (left) and Leo Hernandez Cano</media:description>
              <media:credit>Photos courtesy of the students.</media:credit>
      </media:content>
        <category domain="https://news.mit.edu/topic/classes-and-programs">Classes and programs</category>
      <category domain="https://news.mit.edu/topic/computers">Computer science and technology</category>
      <category domain="https://news.mit.edu/topic/artificial-intelligence2">Artificial intelligence</category>
      <category domain="https://news.mit.edu/topic/algorithms">Algorithms</category>
      <category domain="https://news.mit.edu/topic/machine-learning">Machine learning</category>
      <category domain="https://news.mit.edu/topic/data">Data</category>
      <category domain="https://news.mit.edu/topic/students">Students</category>
      <category domain="https://news.mit.edu/topic/graduate">Graduate, postdoctoral</category>
      <category domain="https://news.mit.edu/topic/research">Research</category>
      <category domain="https://news.mit.edu/topic/electrical-engineering-computer-science-eecs">Electrical engineering and computer science (EECS)</category>
      <category domain="https://news.mit.edu/topic/lids">Laboratory for Information and Decision Systems (LIDS)</category>
      <category domain="https://news.mit.edu/topic/mit-ibm-watson-ai-lab">MIT-IBM Watson AI Lab</category>
      <category domain="https://news.mit.edu/topic/school-engineering">School of Engineering</category>
      <category domain="https://news.mit.edu/topic/institute-medical-engineering-and-science-imes-0">Institute for Medical Engineering and Science (IMES)</category>
      <category domain="https://news.mit.edu/topic/mit-schwarzman-college-computing">MIT Schwarzman College of Computing</category>
      <category domain="https://news.mit.edu/topic/computer-science-and-artificial-intelligence-laboratory-csail">Computer Science and Artificial Intelligence Laboratory (CSAIL)</category>
    </item>
<item>
  <title>MIT researchers propose a new model for legible, modular software</title>
  <link>https://news.mit.edu/2025/mit-researchers-propose-new-model-for-legible-modular-software-1106</link>
  <description>The coding framework uses modular concepts and simple synchronization rules to make software clearer, safer, and easier for LLMs to generate.</description>
  <pubDate>Thu, 06 Nov 2025 08:00:00 -0500</pubDate>
    <guid isPermaLink="true">https://news.mit.edu/2025/mit-researchers-propose-new-model-for-legible-modular-software-1106</guid>
        <dc:creator>Rachel Gordon | MIT CSAIL</dc:creator>
  <content:encoded>&lt;p dir="ltr" id="docs-internal-guid-8faef2c5-7fff-2ff6-d2b4-9dd6f3d78997"&gt;Coding with large language models (LLMs) holds huge promise, but it also exposes some long-standing flaws in software: code that’s messy, hard to change safely, and often opaque about what’s really happening under the hood. Researchers at MIT’s Computer Science and Artificial Intelligence Laboratory (CSAIL) are charting a more “modular” path ahead.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr" id="docs-internal-guid-8faef2c5-7fff-2ff6-d2b4-9dd6f3d78997"&gt;Their new approach breaks systems into “concepts,” separate pieces of a system, each designed to do one job well, and “synchronizations,” explicit rules that describe exactly how those pieces fit together. The result is software that’s more modular, transparent, and easier to understand. A small domain-specific language (DSL) makes it possible to express synchronizations simply, in a form that LLMs can reliably generate. In a real-world case study, the team showed how this method can bring together features that would otherwise be scattered across multiple services.&lt;/p&gt;&lt;p dir="ltr"&gt;The team, including Daniel Jackson, an MIT professor of electrical engineering and computer science (EECS) and CSAIL associate director, and Eagon Meng, an EECS PhD student, CSAIL affiliate, and designer of the new synchronization DSL, explore this approach in their paper “&lt;a href="https://arxiv.org/abs/2508.14511" target="_blank"&gt;What You See Is What It Does: A Structural Pattern for Legible Software&lt;/a&gt;,” which they presented at the Splash Conference in Singapore in October.&amp;nbsp;The challenge, they explain, is that in most modern systems, a single feature is never fully self-contained. Adding a “share” button to a social platform like Instagram, for example, doesn’t live in just one service. Its functionality is split across code that handles posting, notification, authenticating users, and more. All these pieces, despite being scattered across the code, must be carefully aligned, and any change risks unintended side effects elsewhere.&lt;/p&gt;&lt;p dir="ltr"&gt;Jackson calls this “feature fragmentation,” a central obstacle to software reliability. “The way we build software today, the functionality is not localized. You want to understand how ‘sharing’ works, but you have to hunt for it in three or four different places, and when you find it, the connections are buried in low-level code,” says Jackson.&lt;/p&gt;&lt;p dir="ltr"&gt;Concepts and synchronizations are meant to tackle this problem. A concept bundles up a single, coherent piece of functionality, like sharing, liking, or following, along with its state and the actions it can take. Synchronizations, on the other hand, describe at a higher level how those concepts interact. Rather than writing messy low-level integration code, developers can use a small domain-specific language to spell out these connections directly. In this DSL, the rules are simple and clear: one concept’s action can trigger another, so that a change in one piece of state can be kept in sync with another.&lt;/p&gt;&lt;p dir="ltr"&gt;“Think of concepts as modules that are completely clean and independent. Synchronizations then act like contracts — they say exactly how concepts are supposed to interact. That’s powerful because it makes the system both easier for humans to understand and easier for tools like LLMs to generate correctly,” says Jackson. “Why can’t we read code like a book? We believe that software should be legible and written in terms of our understanding: our hope is that concepts map to familiar phenomena, and synchronizations represent our intuition about what happens when they come together,” says Meng.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;The benefits extend beyond clarity. Because synchronizations are explicit and declarative, they can be analyzed, verified, and of course generated by an LLM. This opens the door to safer, more automated software development, where AI assistants can propose new features without introducing hidden side effects.&lt;/p&gt;&lt;p dir="ltr"&gt;In their case study, the researchers assigned features like liking, commenting, and sharing each to a single concept — like a microservices architecture, but more modular. Without this pattern, these features were spread across many services, making them hard to locate and test. Using the concepts-and-synchronizations approach, each feature became centralized and legible, while the synchronizations spelled out exactly how the concepts interacted.&lt;/p&gt;&lt;p dir="ltr"&gt;The study also showed how synchronizations can factor out common concerns like error handling, response formatting, or persistent storage. Instead of embedding these details in every service, synchronization can handle them once, ensuring consistency across the system.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;More advanced directions are also possible. Synchronizations could coordinate distributed systems, keeping replicas on different servers in step, or allow shared databases to interact cleanly. Weakening synchronization semantics could enable eventual consistency while still preserving clarity at the architectural level.&lt;/p&gt;&lt;p dir="ltr"&gt;Jackson sees potential for a broader cultural shift in software development. One idea is the creation of “concept catalogs,” shared libraries of well-tested, domain-specific concepts. Application development could then become less about stitching code together from scratch and more about selecting the right concepts and writing the synchronizations between them. “Concepts could become a new kind of high-level programming language, with synchronizations as the programs written in that language.”&lt;/p&gt;&lt;p dir="ltr"&gt;“It’s a way of making the connections in software visible,” says Jackson. “Today, we hide those connections in code. But if you can see them explicitly, you can reason about the software at a much higher level. You still have to deal with the inherent complexity of features interacting. But now it’s out in the open, not scattered and obscured.”&lt;/p&gt;&lt;p dir="ltr"&gt;“Building software for human use on abstractions from underlying computing machines has burdened the world with software that is all too often costly, frustrating, even dangerous, to understand and use,” says University of Virginia Associate Professor Kevin Sullivan, who wasn’t involved in the research. “The impacts (such as in health care) have been devastating. Meng and Jackson flip the script and insist on building interactive software on abstractions from human understanding, which they call ‘concepts.’ They combine expressive mathematical logic and natural language to specify such purposeful abstractions, providing a basis for verifying their meanings, composing them into systems, and refining them into programs fit for human use. It’s a new and important direction in the theory and practice of software design that bears watching.”&lt;br&gt;&lt;br&gt;"It’s been clear for many years that we need better ways to describe and specify what we want software to do,” adds Thomas Ball, Lancaster University honorary professor and University of Washington affiliate faculty, who also wasn’t involved in the research. “LLMs’ ability to generate code has only added fuel to the specification fire. Meng and Jackson’s work on concept design provides a promising way to describe what we want from software in a modular manner. Their concepts and specifications are well-suited to be paired with LLMs to achieve the designer's intent.”&lt;/p&gt;&lt;p&gt;Looking ahead, the researchers hope their work can influence how both industry and academia think about software architecture in the age of AI. “If software is to become more trustworthy, we need ways of writing it that make its intentions transparent,” says Jackson. “Concepts and synchronizations are one step toward that goal.”&lt;br&gt;&lt;br&gt;This work was partially funded by the Machine Learning Applications (MLA) Initiative of CSAIL Alliances. At the time of funding, the initiative board was British Telecom, Cisco, and Ernst and Young.&amp;nbsp;&lt;/p&gt;</content:encoded>
      <media:content url="https://news.mit.edu/sites/default/files/styles/news_article__cover_image__original/public/images/202510/MIT-csail-AI-software.jpg?itok=t1zpwViL" medium="image" type="image/jpeg" width="390" height="260">
              <media:description type="plain">MIT researchers propose breaking software systems down into “concepts” (pieces that each do a specific job) and “synchronizations” (rules that outline how the pieces fit together), potentially opening the door to safer, more automated software development.</media:description>
              <media:credit>Image: Alex Shipps/MIT CSAIL, using assets from Pexels</media:credit>
      </media:content>
        <category domain="https://news.mit.edu/topic/research">Research</category>
      <category domain="https://news.mit.edu/topic/human-computer-interaction">Human-computer interaction</category>
      <category domain="https://news.mit.edu/topic/programming">Programming</category>
      <category domain="https://news.mit.edu/topic/machine-learning">Machine learning</category>
      <category domain="https://news.mit.edu/topic/software">Software</category>
      <category domain="https://news.mit.edu/topic/artificial-intelligence2">Artificial intelligence</category>
      <category domain="https://news.mit.edu/topic/computers">Computer science and technology</category>
      <category domain="https://news.mit.edu/topic/programming-languages">Programming languages</category>
      <category domain="https://news.mit.edu/topic/electrical-engineering-computer-science-eecs">Electrical engineering and computer science (EECS)</category>
      <category domain="https://news.mit.edu/topic/computer-science-and-artificial-intelligence-laboratory-csail">Computer Science and Artificial Intelligence Laboratory (CSAIL)</category>
      <category domain="https://news.mit.edu/topic/school-engineering">School of Engineering</category>
      <category domain="https://news.mit.edu/topic/mit-schwarzman-college-computing">MIT Schwarzman College of Computing</category>
    </item>
<item>
  <title> 3 Questions: How AI is helping us monitor and support vulnerable ecosystems</title>
  <link>https://news.mit.edu/2025/3q-how-ai-is-helping-monitor-support-vulnerable-ecosystems-1103</link>
  <description>MIT PhD student and CSAIL researcher Justin Kay describes his work combining AI and computer vision systems to monitor the ecosystems that support our planet.</description>
  <pubDate>Mon, 03 Nov 2025 15:55:00 -0500</pubDate>
    <guid isPermaLink="true">https://news.mit.edu/2025/3q-how-ai-is-helping-monitor-support-vulnerable-ecosystems-1103</guid>
        <dc:creator>Alex Shipps | MIT CSAIL</dc:creator>
  <content:encoded>&lt;p dir="ltr" id="docs-internal-guid-7b6e7d1c-7fff-6bcd-4c2a-61dd2e419bca"&gt;&lt;em&gt;A recent&amp;nbsp;&lt;/em&gt;&lt;a href="https://academic.oup.com/bioscience/article-lookup/doi/10.1093/biosci/biaf059"&gt;&lt;em&gt;study&lt;/em&gt;&lt;/a&gt;&lt;em&gt; from Oregon State University estimated that more than 3,500 animal species are at risk of extinction because of factors including habitat alterations, natural resources being overexploited, and climate change.&lt;/em&gt;&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;em&gt;To better understand these changes and protect vulnerable wildlife, conservationists like MIT PhD student and Computer Science and Artificial Intelligence Laboratory (CSAIL) researcher Justin Kay are developing computer vision algorithms that carefully monitor animal populations. A member of the lab of MIT Department of Electrical Engineering and Computer Science assistant professor and CSAIL principal investigator Sara Beery, Kay is currently working on tracking salmon in the Pacific Northwest, where they provide crucial nutrients to predators like birds and bears, while managing the population of prey, like bugs.&lt;/em&gt;&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;em&gt;With all that wildlife data, though, researchers have lots of information to sort through and many AI models to choose from to analyze it all. Kay and his colleagues at CSAIL and the University of Massachusetts Amherst are developing AI methods that make this data-crunching process much more efficient, including a new approach called “consensus-driven active model selection” (or “CODA”) that helps conservationists choose which AI model to use. Their&amp;nbsp;&lt;/em&gt;&lt;a href="https://www.arxiv.org/abs/2507.23771"&gt;&lt;em&gt;work&lt;/em&gt;&lt;/a&gt;&lt;em&gt; was named a Highlight Paper at the International Conference on Computer Vision (ICCV) in October.&lt;/em&gt;&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;em&gt;That research was supported, in part, by the National Science Foundation, Natural Sciences and Engineering Research Council of Canada, and Abdul Latif Jameel Water and Food Systems Lab (J-WAFS). Here, Kay discusses this project, among other conservation efforts.&lt;/em&gt;&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;strong&gt;Q:&lt;/strong&gt; In your paper, you pose the question of which AI models will perform the best on a particular dataset. With as many as 1.9 million pre-trained models available in the HuggingFace Models repository alone, how does CODA help us address that challenge?&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;strong&gt;A:&lt;/strong&gt; Until recently, using AI for data analysis has typically meant training your own model. This requires significant effort to collect and annotate a representative training dataset, as well as iteratively train and validate models. You also need a certain technical skill set to run and modify AI training code. The way people interact with AI is changing, though — in particular, there are now millions of publicly available pre-trained models that can perform a variety of predictive tasks very well. This potentially enables people to use AI to analyze their data without developing their own model, simply by downloading an existing model with the capabilities they need. But this poses a new challenge: Which model, of the millions available, should they use to analyze their data?&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;Typically, answering this model selection question also requires you to spend a lot of time collecting and annotating a large dataset, albeit for testing models rather than training them. This is especially true for real applications where user needs are specific, data distributions are imbalanced and constantly changing, and model performance may be inconsistent across samples. Our goal with CODA was to substantially reduce this effort. We do this by making the data annotation process “active.” Instead of requiring users to bulk-annotate a large test dataset all at once, in active model selection we make the process interactive, guiding users to annotate the most informative data points in their raw data. This is remarkably effective, often requiring users to annotate as few as 25 examples to identify the best model from their set of candidates.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;We’re very excited about CODA offering a new perspective on how to best utilize human effort in the development and deployment of machine-learning (ML) systems. As AI models become more commonplace, our work emphasizes the value of focusing effort on robust evaluation pipelines, rather than solely on training.&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;strong&gt;Q:&lt;/strong&gt; You applied the CODA method to classifying wildlife in images. Why did it perform so well, and what role can systems like this have in monitoring ecosystems in the future?&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;strong&gt;A:&lt;/strong&gt; One key insight was that when considering a collection of candidate AI models, the consensus of all of their predictions is more informative than any individual model’s predictions. This can be seen as a sort of “wisdom of the crowd:” On average, pooling the votes of all models gives you a decent prior over what the labels of individual data points in your raw dataset should be. Our approach with CODA is based on estimating a “confusion matrix” for each AI model — given the true label for some data point is class X, what is the probability that an individual model predicts class X, Y, or Z? This creates informative dependencies between all of the candidate models, the categories you want to label, and the unlabeled points in your dataset.&lt;/p&gt;&lt;p dir="ltr"&gt;Consider an example application where you are a wildlife ecologist who has just collected a dataset containing potentially hundreds of thousands of images from cameras deployed in the wild. You want to know what species are in these images, a time-consuming task that computer vision classifiers can help automate. You are trying to decide which species classification model to run on your data. If you have labeled 50 images of tigers so far, and some model has performed well on those 50 images, you can be pretty confident it will perform well on the remainder of the (currently unlabeled) images of tigers in your raw dataset as well. You also know that when that model predicts some image contains a tiger, it is likely to be correct, and therefore that any model that predicts a different label for that image is more likely to be wrong. You can use all these interdependencies to construct probabilistic estimates of each model’s confusion matrix, as well as a probability distribution over which model has the highest accuracy on the overall dataset. These design choices allow us to make more informed choices over which data points to label and ultimately are the reason why CODA performs model selection much more efficiently than past work.&lt;/p&gt;&lt;p dir="ltr"&gt;There are also a lot of exciting possibilities for building on top of our work. We think there may be even better ways of constructing informative priors for model selection based on domain expertise — for instance, if it is already known that one model performs exceptionally well on some subset of classes or poorly on others. There are also opportunities to extend the framework to support more complex machine-learning tasks and more sophisticated probabilistic models of performance. We hope our work can provide inspiration and a starting point for other researchers to keep pushing the state of the art.&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;strong&gt;Q:&lt;/strong&gt; You work in the Beerylab, led by Sara Beery, where researchers are combining the pattern-recognition capabilities of machine-learning algorithms with computer vision technology to monitor wildlife. What are some other ways your team is tracking and analyzing the natural world, beyond CODA?&lt;br&gt;&lt;br&gt;&lt;strong&gt;A:&lt;/strong&gt; The lab is a really exciting place to work, and new projects are emerging all the time. We have ongoing projects monitoring coral reefs with drones, re-identifying individual elephants over time, and fusing multi-modal Earth observation data from satellites and in-situ cameras, just to name a few. Broadly, we look at emerging technologies for biodiversity monitoring and try to understand where the data analysis bottlenecks are, and develop new computer vision and machine-learning approaches that address those problems in a widely applicable way. It’s an exciting way of approaching problems that sort of targets the “meta-questions” underlying particular data challenges we face.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;The computer vision algorithms I’ve worked on that count migrating salmon in underwater sonar video are examples of that work. We often deal with shifting data distributions, even as we try to construct the most diverse training datasets we can. We always encounter something new when we deploy a new camera, and this tends to degrade the performance of computer vision algorithms. This is one instance of a general problem in machine learning called domain adaptation, but when we tried to apply existing domain adaptation algorithms to our fisheries data we realized there were serious limitations in how existing algorithms were trained and evaluated. We were able to develop a new domain adaptation framework,&amp;nbsp;&lt;a href="https://arxiv.org/abs/2403.12029"&gt;published&lt;/a&gt; earlier this year in&amp;nbsp;&lt;em&gt;Transactions on Machine Learning Research&lt;/em&gt;, that addressed these limitations and led to advancements in fish counting, and even self-driving and spacecraft analysis.&lt;/p&gt;&lt;p dir="ltr"&gt;One line of work that I’m particularly excited about is understanding how to better develop and analyze the performance of predictive ML algorithms in the context of what they are actually used for. Usually, the outputs from some computer vision algorithm — say, bounding boxes around animals in images — are not actually the thing that people care about, but rather a means to an end to answer a larger problem — say, what species live here, and how is that changing over time? We have been working on methods to analyze predictive performance in this context and reconsider the ways that we input human expertise into ML systems with this in mind. CODA was one example of this, where we showed that we could actually consider the ML models themselves as fixed and build a statistical framework to understand their performance very efficiently. We have been working recently on similar integrated analyses combining ML predictions with multi-stage prediction pipelines, as well as ecological statistical models.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;The natural world is changing at unprecedented rates and scales, and being able to quickly move from scientific hypotheses or management questions to data-driven answers is more important than ever for protecting ecosystems and the communities that depend on them. Advancements in AI can play an important role, but we need to think critically about the ways that we design, train, and evaluate algorithms in the context of these very real challenges.&lt;/p&gt;</content:encoded>
      <media:content url="https://news.mit.edu/sites/default/files/styles/news_article__cover_image__original/public/images/202511/MIT-CSAIL-Justin-Kay.jpg?itok=ZGfXAG2c" medium="image" type="image/jpeg" width="390" height="260">
              <media:description type="plain">"We look at emerging technologies for biodiversity monitoring and try to understand where the data analysis bottlenecks are, and develop new computer vision and machine-learning approaches that address those problems," says MIT doctoral student Justin Kay.</media:description>
              <media:credit>Photo courtesy of Justin Kay.</media:credit>
      </media:content>
        <category domain="https://news.mit.edu/topic/interview">Interview</category>
      <category domain="https://news.mit.edu/topic/students">Students</category>
      <category domain="https://news.mit.edu/topic/graduate">Graduate, postdoctoral</category>
      <category domain="https://news.mit.edu/topic/research">Research</category>
      <category domain="https://news.mit.edu/topic/ecology">Ecology</category>
      <category domain="https://news.mit.edu/topic/environment">Environment</category>
      <category domain="https://news.mit.edu/topic/data">Data</category>
      <category domain="https://news.mit.edu/topic/animals">Animals</category>
      <category domain="https://news.mit.edu/topic/computers">Computer science and technology</category>
      <category domain="https://news.mit.edu/topic/computer-vision">Computer vision</category>
      <category domain="https://news.mit.edu/topic/artificial-intelligence2">Artificial intelligence</category>
      <category domain="https://news.mit.edu/topic/machine-learning">Machine learning</category>
      <category domain="https://news.mit.edu/topic/algorithms">Algorithms</category>
      <category domain="https://news.mit.edu/topic/pollution">Pollution</category>
      <category domain="https://news.mit.edu/topic/sustainability">Sustainability</category>
      <category domain="https://news.mit.edu/topic/school-engineering">School of Engineering</category>
      <category domain="https://news.mit.edu/topic/computer-science-and-artificial-intelligence-laboratory-csail">Computer Science and Artificial Intelligence Laboratory (CSAIL)</category>
      <category domain="https://news.mit.edu/topic/abdul-latif-jameel-water-and-food-systems-lab-j-wafs">Abdul Latif Jameel Water and Food Systems Lab (J-WAFS)</category>
      <category domain="https://news.mit.edu/topic/electrical-engineering-computer-science-eecs">Electrical engineering and computer science (EECS)</category>
      <category domain="https://news.mit.edu/topic/mit-schwarzman-college-computing">MIT Schwarzman College of Computing</category>
      <category domain="https://news.mit.edu/topic/nsf">National Science Foundation (NSF)</category>
    </item>
<item>
  <title>Five with MIT ties elected to National Academy of Medicine for 2025</title>
  <link>https://news.mit.edu/2025/mit-affiliates-elected-national-academy-medicine-1022</link>
  <description>Professors Facundo Batista and Dina Katabi, along with three additional MIT alumni, are honored for their outstanding professional achievement and commitment to service.</description>
  <pubDate>Wed, 22 Oct 2025 15:25:00 -0400</pubDate>
    <guid isPermaLink="true">https://news.mit.edu/2025/mit-affiliates-elected-national-academy-medicine-1022</guid>
        <dc:creator>Lillian Eden | Jane Halpern | Department of Biology | Department of Electrical Engineering and Computer Science</dc:creator>
  <content:encoded>&lt;p&gt;On Oct. 20 during its annual meeting, the National Academy of Medicine &lt;a href="https://nam.edu/news-and-insights/100-new-members-elected-2025/"&gt;announced&lt;/a&gt; the election of 100 new members, including MIT faculty members Dina Katabi and Facundo Batista, along with three additional MIT alumni.&lt;/p&gt;&lt;p&gt;Election to the National Academy of Medicine (NAM) is considered one of the highest honors in the fields of health and medicine, recognizing individuals who have demonstrated outstanding professional achievement and commitment to service.&lt;/p&gt;&lt;p&gt;&lt;a href="https://biology.mit.edu/profile/facundo-batista/"&gt;Facundo Batista&lt;/a&gt; is the associate director and scientific director of the Ragon Institute of MGH, MIT and Harvard, as well as the first Phillip T. and Susan M. Ragon Professor in the MIT Department of Biology. The National Academy of Medicine recognized Batista for “his work unraveling the biology of antibody-producing B cells to better understand how our body’s immune systems responds to infectious disease.” More recently, Facundo’s research has advanced preclinical vaccine and therapeutic development for globally important diseases including HIV, malaria, and influenza.&lt;/p&gt;&lt;p&gt;Batista earned a PhD from the International School of Advanced Studies and established his lab in 2002 as a member of the Francis Crick Institute (formerly the London Research Institute), simultaneously holding a professorship at Imperial College London. In 2016, he joined the Ragon Institute to pursue a new research program applying his expertise in B cells and antibody responses to vaccine development, and preclinical vaccinology for diseases including SARS-CoV-2 and HIV. Batista is an elected fellow or member of the U.K. Academy of Medical Sciences, the American Academy of Microbiology, the Academia de Ciencias de América Latina, and the European Molecular Biology Organization, and he is chief editor of &lt;em&gt;The EMBO Journal&lt;/em&gt;.&lt;/p&gt;&lt;p&gt;&lt;a href="https://groups.csail.mit.edu/netmit/lab/people"&gt;Dina Katabi&lt;/a&gt; SM ’99, PhD ’03 is the Thuan (1990) and Nicole Pham Professor in the Department of Electrical Engineering and Computer Science at MIT. Her research spans digital health, wireless sensing, mobile computing, machine learning, and computer vision. Katabi’s contributions include efficient communication protocols for the internet, advanced contactless biosensors, and novel AI models that interpret physiological signals. The NAM recognized Katabi for “pioneering digital health technology that enables non-invasive, off-body remote health monitoring via AI and wireless signals, and for developing digital biomarkers for Parkinson’s progression and detection. She has translated this technology to advance objective, sensitive measures of disease trajectory and treatment response in clinical trials.”&lt;/p&gt;&lt;p&gt;Katabi is director of the MIT Center for Wireless Networks and Mobile Computing. She is also a member of the Computer Science and Artificial Intelligence Laboratory (CSAIL), where she leads the Networks at MIT Research Group. Katabi received a bachelor’s degree from the University of Damascus and MS and PhD degrees in computer science from MIT. She is a MacArthur Fellow; a member of the American Academy of Arts and Sciences, National Academy of Sciences, and National Academy of Engineering; and a recipient of the ACM Computing Prize.&amp;nbsp;&lt;/p&gt;&lt;p&gt;Additional MIT alumni who were elected to the NAM for 2025 are:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Christopher S. Chen SM ’93, PhD ’97, an alumnus of the Department of Mechanical Engineering and the Harvard-MIT Program in Health Sciences and Technology;&lt;br&gt;&amp;nbsp;&lt;/li&gt;&lt;li&gt;Michael E. Matheny SM ’06, an alumnus of the Harvard-MIT Program in Health Sciences and Technology; and&lt;br&gt;&amp;nbsp;&lt;/li&gt;&lt;li&gt;Rebecca R. Richards-Kortum SM ’87, PhD ’90, and alumna of the Department of Physics and the Harvard-MIT Program in Health Sciences and Technology.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Established originally as the Institute of Medicine in 1970 by the National Academy of Sciences, the National Academy of Medicine addresses critical issues in health, science, medicine, and related policy, and inspires positive actions across sectors.&lt;/p&gt;&lt;p&gt;“I am deeply honored to welcome these extraordinary health and medicine leaders and researchers into the National Academy of Medicine,” says NAM President Victor J. Dzau. “Their demonstrated excellence in tackling public health challenges, leading major discoveries, improving health care, advancing health policy, and addressing health equity will critically strengthen our collective ability to tackle the most pressing health challenges of our time.”&amp;nbsp;&lt;/p&gt;</content:encoded>
      <media:content url="https://news.mit.edu/sites/default/files/styles/news_article__cover_image__original/public/images/202510/batista-katabi-nam-2025_1.png?itok=YwA9Mj5W" medium="image" type="image/jpeg" width="390" height="260">
              <media:description type="plain">MIT professors Facundo Batista (left) and Dina Katabi have been elected to the National Academy of Medicine, along with three additional MIT alumni.</media:description>
              <media:credit>Photos courtesy of the departments of Biology and EECS.</media:credit>
      </media:content>
        <category domain="https://news.mit.edu/topic/awards">Awards, honors and fellowships</category>
      <category domain="https://news.mit.edu/topic/faculty">Faculty</category>
      <category domain="https://news.mit.edu/topic/alumni">Alumni/ae</category>
      <category domain="https://news.mit.edu/topic/biology">Biology</category>
      <category domain="https://news.mit.edu/topic/health">Health sciences and technology</category>
      <category domain="https://news.mit.edu/topic/disease">Disease</category>
      <category domain="https://news.mit.edu/topic/artificial-intelligence2">Artificial intelligence</category>
      <category domain="https://news.mit.edu/topic/vaccines">Vaccines</category>
      <category domain="https://news.mit.edu/topic/health-care">Health care</category>
      <category domain="https://news.mit.edu/topic/wireless">Wireless</category>
      <category domain="https://news.mit.edu/topic/mechanical-engineering">Mechanical engineering</category>
      <category domain="https://news.mit.edu/topic/physics">Physics</category>
      <category domain="https://news.mit.edu/topic/electrical-engineering-computer-science-eecs">Electrical engineering and computer science (EECS)</category>
      <category domain="https://news.mit.edu/topic/computer-science-and-artificial-intelligence-laboratory-csail">Computer Science and Artificial Intelligence Laboratory (CSAIL)</category>
      <category domain="https://news.mit.edu/topic/koch-institute-0">Koch Institute</category>
      <category domain="https://news.mit.edu/topic/ragon-institute">Ragon Institute</category>
      <category domain="https://news.mit.edu/topic/school-engineering">School of Engineering</category>
      <category domain="https://news.mit.edu/topic/school-science">School of Science</category>
      <category domain="https://news.mit.edu/topic/mit-schwarzman-college-computing">MIT Schwarzman College of Computing</category>
      <category domain="https://news.mit.edu/topic/harvard-mit-health-sciences-and-technology">Harvard-MIT Health Sciences and Technology</category>
    </item>
<item>
  <title>Charts can be social artifacts that communicate more than just data </title>
  <link>https://news.mit.edu/2025/charts-can-be-social-artifacts-communicate-more-than-data-1022</link>
  <description>Researchers find that design elements of data visualizations influence viewers’ assumptions about the source of the information and its trustworthiness.</description>
  <pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
    <guid isPermaLink="true">https://news.mit.edu/2025/charts-can-be-social-artifacts-communicate-more-than-data-1022</guid>
        <dc:creator>Adam Zewe | MIT News</dc:creator>
  <content:encoded>&lt;p&gt;The degree to which someone trusts the information depicted in a chart can depend on their assumptions about who made the data visualization, according to a pair of studies by MIT researchers.&lt;/p&gt;&lt;p&gt;For instance, if someone infers that a graph about a controversial topic like gun violence was produced by an organization they feel is in opposition with their beliefs or political views, they may discredit the information or dismiss the visualization all together.&lt;/p&gt;&lt;p&gt;The researchers found that even the clearest visualizations often communicate more than the data they explicitly depict, and can elicit strong judgments from viewers about the social contexts, identities, and characteristics of those who made the chart.&lt;/p&gt;&lt;p&gt;Readers make these assessments about the social context of a visualization primarily from its design features, like the color palette or the way information is arranged, rather than the underlying data. Often, these inferences are unintended by the designers.&lt;/p&gt;&lt;p&gt;Qualitative and quantitative studies revealed that these social inferences aren’t restricted to certain subgroups, nor are they caused by limited data literacy.&lt;/p&gt;&lt;p&gt;The researchers consolidate their findings into a framework that scientists and communicators can use to think critically about how design choices might affect these social assumptions. Ultimately, they hope this work leads to better strategies for scientific communication.&lt;/p&gt;&lt;p&gt;“If you are scrolling through social media and you see a chart, and you immediately dismiss it as something an influencer has produced just to get attention, that shapes your entire experience with the chart before you even dig into the data. We’ve shown in these papers that visualizations do more than just communicate the data they are depicting — they also communicate other social signals,” says Arvind Satyanarayan, an associate professor in the MIT Department of Electrical Engineering and Computer Science (EECS) and member of the Computer Science and Artificial Intelligence Laboratory (CSAIL) and co-senior author of this research.&lt;/p&gt;&lt;p&gt;He is joined on the paper by co-lead authors Amy Rae Fox, a former CSAIL postdoc, and Michelle Morgenstern, a current postdoc in MIT’s anthropology program; and co-senior author Graham M. Jones, professor of anthropology. Two related papers on this research will be presented at the IEEE Visualization Conference.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Charts as social artifacts&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;During the height of the Covid-19 pandemic, social media was awash in charts from organizations like the World Health Organization and Centers for Disease Control and Prevention, which were designed to convey information about the spread of disease.&lt;/p&gt;&lt;p&gt;The MIT researchers studied how these visualizations were being used to discuss the pandemic. They found that some citizen scientists were using the underlying data to make visualizations of their own, challenging the findings of mainstream science.&lt;/p&gt;&lt;p&gt;“This was an unexpected discovery as, previously, citizen scientists were typically aligned with mainstream scientists. It took us a few years to figure out how to study this phenomenon more deeply,” Satyanarayan says.&lt;/p&gt;&lt;p&gt;Most research into data visualization studies how charts communicate data. Instead, the researchers wanted to explore visualizations from a social and linguistic perspective to assess the information they convey beyond the data.&lt;/p&gt;&lt;p&gt;Linguistic anthropologists have found that, while language allows people to communicate ideas, it also holds social meaning beyond the words people use. For instance, an accent or dialect can indicate that someone is part of a particular community.&lt;/p&gt;&lt;p&gt;By “pointing” to certain social meanings, identities, and characteristics, language serves what is known as a socio-indexical function.&lt;/p&gt;&lt;p&gt;“We wanted to see if things in the visual language of data communication might point to certain institutions, or the kinds of people in those institutions, that carry a meaning that could be unintended by the makers of the visualization,” Jones says.&lt;/p&gt;&lt;p&gt;To do this, the researchers conducted an initial, qualitative study of users on the social media platform Tumblr. During one-on-one interviews, the researchers showed users a variety of real visualizations from online sources, as well as modified visualizations where they removed the textual information, like titles and axes labels.&lt;/p&gt;&lt;p&gt;Stripping out the textual information was particularly important, since it mimics the way people often interact with online visualizations.&lt;/p&gt;&lt;p&gt;“Our engagement with social media is a few quick seconds. People aren’t taking the time to read the title of a chart or look at the data very carefully,” Satyanarayan says.&lt;/p&gt;&lt;p&gt;The interviews revealed that users made detailed inferences about the people or organizations who created the visualizations based on what they called “vibes,” design elements, like colors or the use of certain graphics. These inferences in turn impacted their trust in the data.&lt;/p&gt;&lt;p&gt;For instance, after seeing a chart with the flags of Georgia and Texas and a graph with two lines in red and black, but no text, one user said, “This kind of looks like something a Texas Republican (legislator) would put on Twitter or on their website, or as part of a campaign presentation.”&lt;/p&gt;&lt;p&gt;&lt;strong&gt;A quantitative approach&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Building on this initial work, the researchers used the same methodology in three quantitative studies involving surveys sent to larger groups of people from a variety of backgrounds.&lt;/p&gt;&lt;p&gt;They found the same phenomenon: People make inferences about the social context of a visualization based on its design, which can lead to misunderstandings about, and mistrust in, the data it depicts.&lt;/p&gt;&lt;p&gt;For instance, users felt some visualizations were so neatly arranged they believed them to be advertisements, and therefore not trustworthy. In another example, one user dismissed a chart by a Pulitzer-prize winning designer because they felt the hand-drawn graphical style indicated it was made by “some female Instagram influencer who is just trying to look for attention.”&lt;/p&gt;&lt;p&gt;“If that is the first reaction someone has to a chart, it is going to massively impact the degree to which they trust it,” Satyanarayan says.&lt;/p&gt;&lt;p&gt;Moreover, when the researchers reintroduced text in the visualizations from which it had been removed, users still made these social inferences.&lt;/p&gt;&lt;p&gt;Typically, in data visualization, the solution to such a problem would be to create clearer charts or educate people about data literacy. But this research points to a completely different kind of data literacy, Jones says.&lt;/p&gt;&lt;p&gt;“It is not erroneous for people to be drawing these inferences. It requires a lot of cultural knowledge about where visualizations come from, how they are made, and how they circulate. Drawing these inferences is a feature, not a bug, of the way we use signs,” he says.&lt;/p&gt;&lt;p&gt;From these results, they created a classification framework to organize the social inferences users made and the design elements that contributed to them. They hope the typology serves as a tool designers can use to develop more effective visualizations, as well as a starting point for additional studies.&lt;/p&gt;&lt;p&gt;Moving forward, the researchers want to continue exploring the role of data visualizations as social artifacts, perhaps by drilling down on each design feature they identified in the typology. They also want to expand the scope of their study to include visualizations in research papers and scientific journals.&lt;/p&gt;&lt;p&gt;“Part of the value of this work is a methodological contribution to render a set of phenomena amenable to experimental study. But this work is also important because it showcases an interdisciplinary cross-pollination that is powerful and unique to MIT,” Jones says.&lt;/p&gt;&lt;p&gt;This work was supported, in part, by MIT METEOR and PFPFEE fellowships, an Amar G. Bose Fellowship, an Alfred P. Sloan Fellowship, and the National Science Foundation.&lt;/p&gt;</content:encoded>
      <media:content url="https://news.mit.edu/sites/default/files/styles/news_article__cover_image__original/public/images/202510/MIT-VisVibes-01-press.jpg?itok=I0b4-rEk" medium="image" type="image/jpeg" width="390" height="260">
              <media:description type="plain">“We’ve shown in these papers that visualizations do more than just communicate the data they are depicting — they also communicate other social signals,” says Arvind Satyanarayan.</media:description>
              <media:credit>Credit: MIT News; iStock</media:credit>
      </media:content>
        <category domain="https://news.mit.edu/topic/research">Research</category>
      <category domain="https://news.mit.edu/topic/human-computer-interaction">Human-computer interaction</category>
      <category domain="https://news.mit.edu/topic/computers">Computer science and technology</category>
      <category domain="https://news.mit.edu/topic/data">Data</category>
      <category domain="https://news.mit.edu/topic/design">Design</category>
      <category domain="https://news.mit.edu/topic/linguistics">Linguistics</category>
      <category domain="https://news.mit.edu/topic/social-media">Social media</category>
      <category domain="https://news.mit.edu/topic/social-networks">Social networks</category>
      <category domain="https://news.mit.edu/topic/science-communications">Science communications</category>
      <category domain="https://news.mit.edu/topic/computer-science-and-artificial-intelligence-laboratory-csail">Computer Science and Artificial Intelligence Laboratory (CSAIL)</category>
      <category domain="https://news.mit.edu/topic/electrical-engineering-computer-science-eecs">Electrical engineering and computer science (EECS)</category>
      <category domain="https://news.mit.edu/topic/anthropology">Anthropology</category>
      <category domain="https://news.mit.edu/topic/school-engineering">School of Engineering</category>
      <category domain="https://news.mit.edu/topic/school-humanities-arts-and-social-sciences">School of Humanities Arts and Social Sciences</category>
      <category domain="https://news.mit.edu/topic/mit-schwarzman-college-computing">MIT Schwarzman College of Computing</category>
      <category domain="https://news.mit.edu/topic/nsf">National Science Foundation (NSF)</category>
    </item>
<item>
  <title>Creating AI that matters</title>
  <link>https://news.mit.edu/2025/creating-ai-that-matters-1021</link>
  <description>How the MIT-IBM Watson AI Lab is shaping AI-sociotechnical systems for the future.</description>
  <pubDate>Tue, 21 Oct 2025 16:10:00 -0400</pubDate>
    <guid isPermaLink="true">https://news.mit.edu/2025/creating-ai-that-matters-1021</guid>
        <dc:creator>Lauren Hinkel | MIT-IBM Watson AI Lab</dc:creator>
  <content:encoded>&lt;p&gt;When it comes to artificial intelligence, MIT and IBM were there at the beginning: laying foundational work and creating some of the first programs — AI predecessors — and theorizing how machine “intelligence” might come to be.&lt;/p&gt;&lt;p&gt;Today, collaborations like the MIT-IBM Watson AI Lab, which launched eight years ago, are continuing to deliver expertise for the promise of tomorrow’s AI technology. This is critical for industries and the labor force that stand to benefit, particularly in the short term: from $3-4 trillion of forecast global economic benefits and 80 percent productivity gains for knowledge workers and creative tasks, to significant incorporations of generative AI into business processes (80 percent) and software applications (70 percent) in the next three years.&lt;/p&gt;&lt;p&gt;While industry has seen a boom in notable models, chiefly in the past year, &lt;a href="https://hai.stanford.edu/ai-index"&gt;academia continues to drive the innovation&lt;/a&gt;, contributing most of the highly cited research. At the MIT-IBM Watson AI Lab, success takes the form of 54 patent disclosures, an excess of 128,000 citations with an h-index of 162, and more than 50 industry-driven use cases. Some of the lab’s many achievements include improved stent placement with AI imaging techniques, slashing computational overhead, shrinking models while maintaining performance, and modeling of interatomic potential for silicate chemistry.&lt;/p&gt;&lt;p&gt;“The lab is uniquely positioned to identify the ‘right’ problems to solve, setting us apart from other entities,” says Aude Oliva, lab MIT director and director of strategic industry engagement in the MIT Schwarzman College of Computing. “Further, the experience our students gain from working on these challenges for enterprise AI translates to their competitiveness in the job market and the promotion of a competitive industry.”&lt;/p&gt;&lt;p&gt;“The MIT-IBM Watson AI Lab has had tremendous impact by bringing together a rich set of collaborations between IBM and MIT’s researchers and students,” says Provost Anantha Chandrakasan, who is the lab’s MIT co-chair and the Vannevar Bush Professor of Electrical Engineering and Computer Science. “By supporting cross-cutting research at the intersection of AI and many other disciplines, the lab is advancing foundational work and accelerating the development of transformative solutions for our nation and the world.”&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Long-horizon work&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;As AI continues to garner interest, many organizations struggle to channel the technology into meaningful outcomes. A &lt;a href="https://www.gartner.com/en/newsroom/press-releases/2024-07-29-gartner-predicts-30-percent-of-generative-ai-projects-will-be-abandoned-after-proof-of-concept-by-end-of-2025"&gt;2024 Gartner study&lt;/a&gt; finds that, “at least 30% of generative AI projects will be abandoned after proof of concept by the end of 2025,” demonstrating ambition and widespread hunger for AI, but a lack of knowledge for how to develop and apply it to create immediate value.&lt;/p&gt;&lt;p&gt;Here, the lab shines, bridging research and deployment. The majority of the lab’s current-year research portfolio is aligned to use and develop new features, capacities, or products for IBM, the lab’s corporate members, or real-world applications. The last of these comprise large language models, AI hardware, and foundation models, including multi-modal, bio-medical, and geo-spatial ones. Inquiry-driven students and interns are invaluable in this pursuit, offering enthusiasm and new perspectives while accumulating domain knowledge to help derive and engineer advancements in the field, as well as opening up new frontiers for exploration with AI as a tool.&lt;/p&gt;&lt;p&gt;Findings from the &lt;a href="https://aaai.org/wp-content/uploads/2025/03/AAAI-2025-PresPanel-Report-Digital-3.7.25.pdf"&gt;AAAI 2025 Presidential panel on the Future of AI Research&lt;/a&gt; support the need for contributions from academia-industry collaborations like the lab in the AI arena: “Academics have a role to play in providing independent advice and interpretations of these results [from industry] and their consequences. The private sector focuses more on the short term, and universities and society more on a longer-term perspective.”&lt;/p&gt;&lt;p&gt;Bringing these strengths together, along with the push for open sourcing and open science, can spark innovation that neither could achieve alone. History shows that embracing these principles, and sharing code and making research accessible, has long-term benefits for both the sector and society. In line with IBM and MIT’s missions, the lab contributes technologies, findings, governance, and standards to the public sphere through this collaboration, thereby enhancing transparency, accelerating reproducibility, and ensuring trustworthy advances.&lt;/p&gt;&lt;p&gt;The lab was created to merge MIT’s deep research expertise with IBM’s industrial R&amp;amp;D capacity, aiming for breakthroughs in core AI methods and hardware, as well as new applications in areas like health care, chemistry, finance, cybersecurity, and robust planning and decision-making for business.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Bigger isn't always better&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Today, large foundation models are giving way to smaller, more task-specific models yielding better performance. Contributions from lab members like Song Han, associate professor in the MIT Department of Electrical Engineering and Computer Science (EECS), and IBM Research’s Chuang Gan help make this possible, through work such as &lt;a href="https://openreview.net/pdf?id=HylxE1HKwS"&gt;once-for-all&lt;/a&gt; and &lt;a href="https://proceedings.mlsys.org/paper_files/paper/2024/file/42a452cbafa9dd64e9ba4aa95cc1ef21-Paper-Conference.pdf"&gt;AWQ&lt;/a&gt;. Innovations such as these improve efficiency with better architectures, algorithm shrinking, and activation-aware weight quantization, letting models like language processing run on edge devices at faster speeds and reduced latency.&lt;/p&gt;&lt;p&gt;Consequently, foundation, vision, multimodal, and large language models have seen benefits, allowing for the lab research groups of Oliva, MIT EECS Associate Professor Yoon Kim, and IBM Research members Rameswar Panda, Yang Zhang, and Rogerio Feris to build on the work. This includes techniques to &lt;a href="https://news.mit.edu/2024/enhancing-llm-collaboration-smarter-more-efficient-solutions-0916"&gt;imbue models with external knowledge&lt;/a&gt; and the development of linear attention transformer methods for higher throughput, compared to other state-of-the-art systems.&amp;nbsp;&lt;/p&gt;&lt;p&gt;Understanding and reasoning in vision and multimodal systems has also seen a boon.&amp;nbsp;Works like “&lt;a href="https://openaccess.thecvf.com/content/CVPR2022/html/Mishra_Task2Sim_Towards_Effective_Pre-Training_and_Transfer_From_Synthetic_Data_CVPR_2022_paper.html"&gt;Task2Sim&lt;/a&gt;” and “&lt;a href="https://arxiv.org/pdf/2102.05775.pdf"&gt;AdaFuse&lt;/a&gt;” demonstrate improved vision model performance if pre-training takes place on synthetic data, and how video action recognition can be boosted by fusing channels from past and current feature maps.&lt;/p&gt;&lt;p&gt;As part of a commitment to leaner AI, the lab teams of Gregory Wornell, the MIT EECS Sumitomo Electric Industries Professor in Engineering, IBM Research’s Chuang Gan, and David Cox, VP for foundational AI at IBM Research and the lab’s IBM director, have shown that model adaptability and data efficiency can go hand in hand. Two approaches, &lt;a href="https://arxiv.org/pdf/2505.23604"&gt;EvoScale&lt;/a&gt; and &lt;a href="https://arxiv.org/pdf/2502.02508"&gt;Chain-of-Action-Thought reasoning&lt;/a&gt; (COAT), enable language models to make the most of limited data and computation by improving on prior generation attempts through structured iteration, narrowing in on a better response.&amp;nbsp;COAT uses a meta-action framework and reinforcement learning to tackle reasoning-intensive tasks via self-correction, while EvoScale brings a similar philosophy to code generation, evolving high-quality candidate solutions. These techniques help to enable resource-conscious, targeted, real-world deployment.&lt;/p&gt;&lt;p&gt;“The impact of MIT-IBM research on our large language model development efforts cannot be overstated,” says Cox. “We’re seeing that smaller, more specialized models and tools are having an outsized impact, especially when they are combined. Innovations from the MIT-IBM Watson AI Lab help shape these technical directions and influence the strategy we are taking in the market through platforms like watsonx.”&lt;/p&gt;&lt;p&gt;For example, numerous lab projects have contributed features, capabilities, and uses to IBM’s &lt;a href="https://arxiv.org/pdf/2502.09927"&gt;Granite Vision&lt;/a&gt;, which provides impressive computer vision designed for document understanding, despite its compact size. This comes at a time when there’s a growing need for extraction, interpretation, and trustworthy summarization of information and data contained in long formats for enterprise purposes.&lt;/p&gt;&lt;p&gt;Other achievements that extend beyond direct research on AI and across disciplines are not only beneficial, but necessary for advancing the technology and lifting up society, concludes the 2025 AAAI panel.&lt;/p&gt;&lt;p&gt;Work from the lab’s Caroline Uhler and Devavrat Shah — both Andrew (1956) and Erna Viterbi Professors in EECS and the Institute for Data, Systems, and Society (IDSS) — along with IBM Research’s Kristjan Greenewald, transcends specializations. They are developing causal discovery methods to uncover how interventions affect outcomes, and identify which ones achieve desired results. The studies include developing a framework that can both elucidate how “treatments” for different sub-populations may play out, like on an ecommerce platform or mobility restrictions on morbidity outcomes. Findings from this body of work could influence the fields of marketing and medicine to education and risk management.&lt;/p&gt;&lt;p&gt;“Advances in AI and other areas of computing are influencing how people formulate and tackle challenges in nearly every discipline. At the MIT-IBM Watson AI Lab, researchers recognize this cross-cutting nature of their work and its impact, interrogating problems from multiple viewpoints and bringing real-world problems from industry, in order to develop novel solutions,” says Dan Huttenlocher, MIT lab co-chair, dean of the MIT Schwarzman College of Computing, and the Henry Ellis Warren (1894) Professor of Electrical Engineering and Computer Science.&lt;/p&gt;&lt;p&gt;A significant piece of what makes this research ecosystem thrive is the steady influx of student talent and their contributions through MIT’s Undergraduate Research Opportunities Program (&lt;a href="https://urop.mit.edu/"&gt;UROP&lt;/a&gt;), &lt;a href="https://www.eecs.mit.edu/home/eecs-alliance-2/6a/"&gt;MIT EECS 6A Program&lt;/a&gt;,&amp;nbsp;and the new MIT-IBM Watson AI Lab Internship Program. Altogether, more than 70 young researchers have not only accelerated their technical skill development, but, through guidance and support by the lab’s mentors, gained knowledge in AI domains to become emerging practitioners themselves. This is why the lab continually seeks to identify promising students at all stages in their exploration of AI’s potential.&lt;/p&gt;&lt;p&gt;“In order to unlock the full economic and societal potential of AI, we need to foster ‘useful and efficient intelligence,’” says Sriram Raghavan, IBM Research VP for AI and IBM chair of the lab. “To translate AI promise into progress, it’s crucial that we continue to focus on innovations to develop efficient, optimized, and fit-for-purpose models that can easily be adapted to specific domains and use cases. Academic-industry collaborations, such as the MIT-IBM Watson AI Lab, help drive the breakthroughs that make this possible.”&lt;/p&gt;</content:encoded>
      <media:content url="https://news.mit.edu/sites/default/files/styles/news_article__cover_image__original/public/images/202510/mit-ibm-watson-AI-that-Matters.jpg?itok=lL-9hhzs" medium="image" type="image/jpeg" width="390" height="260">
              <media:description type="plain">While industry has seen a boom in notable AI models, academia continues to drive the innovation, contributing most of the highly cited research.</media:description>
              <media:credit>Image: AdobeStock</media:credit>
      </media:content>
        <category domain="https://news.mit.edu/topic/research">Research</category>
      <category domain="https://news.mit.edu/topic/mit-ibm-watson-ai-lab">MIT-IBM Watson AI Lab</category>
      <category domain="https://news.mit.edu/topic/collaboration">Collaboration</category>
      <category domain="https://news.mit.edu/topic/industry">Industry</category>
      <category domain="https://news.mit.edu/topic/computers">Computer science and technology</category>
      <category domain="https://news.mit.edu/topic/artificial-intelligence2">Artificial intelligence</category>
      <category domain="https://news.mit.edu/topic/machine-learning">Machine learning</category>
      <category domain="https://news.mit.edu/topic/algorithms">Algorithms</category>
      <category domain="https://news.mit.edu/topic/data">Data</category>
      <category domain="https://news.mit.edu/topic/business">Business and management</category>
      <category domain="https://news.mit.edu/topic/technology-society">Technology and society</category>
      <category domain="https://news.mit.edu/topic/electrical-engineering-computer-science-eecs">Electrical engineering and computer science (EECS)</category>
      <category domain="https://news.mit.edu/topic/computer-science-and-artificial-intelligence-laboratory-csail">Computer Science and Artificial Intelligence Laboratory (CSAIL)</category>
      <category domain="https://news.mit.edu/topic/idss">IDSS</category>
      <category domain="https://news.mit.edu/topic/mit-schwarzman-college-computing">MIT Schwarzman College of Computing</category>
      <category domain="https://news.mit.edu/topic/school-engineering">School of Engineering</category>
    </item>
<item>
  <title>School of Engineering welcomes new faculty in 2024-25</title>
  <link>https://news.mit.edu/2025/school-engineering-welcomes-eight-new-faculty-1017</link>
  <description>The newest MIT engineering faculty are conducting research across a diverse range of subject areas.</description>
  <pubDate>Fri, 17 Oct 2025 15:55:00 -0400</pubDate>
    <guid isPermaLink="true">https://news.mit.edu/2025/school-engineering-welcomes-eight-new-faculty-1017</guid>
        <dc:creator>Jordan Silva | School of Engineering</dc:creator>
  <content:encoded>&lt;p&gt;The MIT School of Engineering welcomes new faculty members across six of its academic units. This new cohort of faculty members, who have recently started their roles at MIT, conduct research across a diverse range of disciplines.&lt;/p&gt;&lt;p&gt;“We are thrilled to welcome these accomplished scholars to the School of Engineering,” says Maria C. Yang, interim dean of engineering and William E. Leonhard (1940) Professor in the Department of Mechanical Engineering. “Each brings unique expertise across a wide range of fields and is advancing knowledge with real-world impact. They all share a deep commitment to research excellence and a passion for teaching and mentorship.”&lt;/p&gt;&lt;p&gt;Faculty with appointments in the Department of Electrical Engineering and Computer Science (EECS) and the Institute for Data, Systems, and Society (IDSS) report into both the School of Engineering and the MIT Stephen A. Schwarzman College of Computing.&lt;/p&gt;&lt;p&gt;The new engineering faculty include:&lt;/p&gt;&lt;p&gt;&lt;a href="https://aeroastro.mit.edu/people/masha-folk/"&gt;Masha Folk&lt;/a&gt; joined the Department of Aeronautics and Astronautics as an assistant professor in July 2024 and is the Charles Stark Draper Career Development Professor. Her research focuses on sustainable aerospace technology driven by a deep desire to accelerate carbon-neutral aviation. She previously worked as an aerodynamics specialist for Rolls-Royce. Folk received her BS in aerospace engineering from Ohio State University, her MS in aerospace engineering from Purdue University, and her PhD in energy, fluids, and turbomachinery from the University of Cambridge.&lt;/p&gt;&lt;p&gt;&lt;a href="https://nse.mit.edu/people/5666/"&gt;Sophia Henneberg&lt;/a&gt;, the Norman C. Rasmussen Career Development Professor, joined the Department of Nuclear Science and Engineering (NSE) as an assistant professor in September. Her&amp;nbsp;research focuses on developing, utilizing, and extending optimization tools to identify new, promising stellarator designs, which are a promising path toward&amp;nbsp;fusion energy. Previously, she was the principal investigator of EUROfusion’s Stellarator Optimization Theory, Simulation, Validation, and Verification group. Henneberg received a BS in physics at the Goethe-Universität, an MA in physics at the University of Wisconsin at Madison, and a PhD in physics at the University of York.&lt;/p&gt;&lt;p&gt;&lt;a href="https://omarkhattab.com/"&gt;Omar Khattab&lt;/a&gt;, the TIBCO Founders Career Development Professor, joined the Department of Electrical Engineering and Computer Science as an assistant professor in July. He is also affiliated with the Computer Science and Artificial Intelligence Laboratory (CSAIL). His research develops new algorithms and abstractions for declarative AI programming and for composing retrieval and reasoning. Khattab previously worked as a research scientist at Databricks. He received a BS in computer science from Carnegie Mellon University and a PhD in computer science from Stanford University.&lt;/p&gt;&lt;p&gt;&lt;a href="https://dmse.mit.edu/people/faculty/tania-lopez-silva/"&gt;Tania Lopez-Silva&lt;/a&gt; joined the Department of Materials Science and Engineering as an assistant professor and the Institute for Medical Engineering and Science (IMES) as a core faculty member in July.&amp;nbsp;Her research focuses on supramolecular hydrogels — soft materials made from self-assembling molecules, primarily peptides. Previously, she served as a postdoc at the National Cancer Institute. Lopez-Silva earned her BS in chemistry from Tecnológico de Monterrey and her MA and PhD in chemistry from Rice University.&lt;/p&gt;&lt;p&gt;&lt;a href="https://nse.mit.edu/people/ethan-peterson/"&gt;Ethan Peterson&lt;/a&gt; ’13 joined the Department of Nuclear Science and Engineering as an assistant professor in July 2024. As the Class of 1956 Career Development Professor, his research focuses on improving radiation transport and transmutation methods for the design of fusion technologies, as well as whole-facility modeling for fusion power plants. Previously, he worked as a research scientist at MIT’s Plasma Science and Fusion Center. Peterson received his BS in nuclear engineering and physics from MIT and his PhD in plasma physics from the University of Wisconsin at Madison.&lt;/p&gt;&lt;p&gt;&lt;a href="https://nse.mit.edu/people/dean-price/"&gt;Dean Price&lt;/a&gt; joined the Department of Nuclear Science and Engineering as the Atlantic Richfield Career Development Professor in Energy Studies and an assistant professor in September. His work focuses on the simulation and control of advanced reactors, with expertise in uncertainty quantification, scientific machine learning, and artificial intelligence for nuclear applications. Previously, he was the Russell L. Heath Distinguished Postdoctoral Fellow at Idaho National Laboratory. He earned his&amp;nbsp;BS in nuclear engineering from the University of Illinois and his PhD in nuclear engineering from the University of Michigan.&lt;/p&gt;&lt;p&gt;&lt;a href="https://aeroastro.mit.edu/people/daniel-varon/"&gt;Daniel Varon&lt;/a&gt; joined the Department of Aeronautics and Astronautics as the Boeing Career Development Assistant Professor in Aeronautics and Astronautics, holding an MIT Schwarzman College of Computing shared position with IDSS, in July. Varon’s research focuses on using satellite observations of atmospheric composition to better understand human impacts on the environment and identify opportunities to reduce them. Previously, he held a visiting postdoctoral fellowship at the Princeton School of Public and International Affairs. Varon earned a BS in physics and a BA in English literature from McGill University, and an MS in applied mathematics and PhD in atmospheric chemistry from Harvard University.&lt;/p&gt;&lt;p&gt;&lt;a href="https://meche.mit.edu/people/faculty/raphz@mit.edu"&gt;Raphael Zufferey&lt;/a&gt; joined the Department of Mechanical Engineering as an assistant professor and the William I. Koch Career Development Professor in January. He&amp;nbsp;studies bioinspired methods and unconventional designs to solve seamless aerial and aquatic locomotion for applications in ocean sciences. Zufferey previously worked as a Marie Curie postdoc at the École Polytechnique Fédérale de Lausanne (EPFL). He received his BA in micro-engineering and MS in robotics from EPFL and a PhD in robotics and aeronautics from Imperial College London.&lt;br&gt;&lt;br&gt;The School of Engineering is also welcoming a number of faculty in the Department of EECS and the IDSS who hold shared positions with the MIT Schwarzman College of Computing and other departments. These include: Bailey Flanigan, Brian Hedden, Yunha Hwang, Benjamin Lindquist, Paris Smaragdis, Pu “Paul" Liang, Mariana Popescu, and Daniel Varon. For more information about these faculty members, read the &lt;a href="https://news.mit.edu/2025/mit-schwarzman-college-computing-welcomes-11-new-faculty-2025-1009"&gt;Schwarzman College of Computing’s recent article&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;Additionally, the School of Engineering has adopted the shared faculty search model to hire its first shared faculty member: Mark Rau. For more information, read&amp;nbsp;&lt;a href="https://news.mit.edu/2025/mit-shass-welcomes-new-faculty-0731"&gt;the School of Humanities, Arts, and Social Sciences recent article&lt;/a&gt;.&lt;/p&gt;</content:encoded>
      <media:content url="https://news.mit.edu/sites/default/files/styles/news_article__cover_image__original/public/images/202510/mit-soe-faculty-2025.jpg?itok=LfuaAQEd" medium="image" type="image/jpeg" width="390" height="260">
              <media:description type="plain">Top row, left to right: Masha Folk, Sophia Henneberg, Omar Khattab, and Tania Lopez Silva. Bottom row, left to right: Ethan Peterson, Daniel Varon, Dean Price, and Raphael Zufferey.</media:description>
              <media:credit>Photos: Conor McArdle</media:credit>
      </media:content>
        <category domain="https://news.mit.edu/topic/faculty">Faculty</category>
      <category domain="https://news.mit.edu/topic/alumni">Alumni/ae</category>
      <category domain="https://news.mit.edu/topic/aeronautics">Aeronautical and astronautical engineering</category>
      <category domain="https://news.mit.edu/topic/electrical-engineering-computer-science-eecs">Electrical engineering and computer science (EECS)</category>
      <category domain="https://news.mit.edu/topic/materialsscienceandengineering">Materials science and engineering</category>
      <category domain="https://news.mit.edu/topic/nuclear-engineering">Nuclear science and engineering</category>
      <category domain="https://news.mit.edu/topic/mechanical-engineering">Mechanical engineering</category>
      <category domain="https://news.mit.edu/topic/institute-medical-engineering-and-science-imes-0">Institute for Medical Engineering and Science (IMES)</category>
      <category domain="https://news.mit.edu/topic/computer-science-and-artificial-intelligence-laboratory-csail">Computer Science and Artificial Intelligence Laboratory (CSAIL)</category>
      <category domain="https://news.mit.edu/topic/idss">IDSS</category>
      <category domain="https://news.mit.edu/topic/dmse">DMSE</category>
      <category domain="https://news.mit.edu/topic/plasma-science-and-fusion-center">Plasma Science and Fusion Center</category>
      <category domain="https://news.mit.edu/topic/school-engineering">School of Engineering</category>
      <category domain="https://news.mit.edu/topic/mit-schwarzman-college-computing">MIT Schwarzman College of Computing</category>
    </item>
<item>
  <title>MIT Schwarzman College of Computing welcomes 11 new faculty for 2025</title>
  <link>https://news.mit.edu/2025/mit-schwarzman-college-computing-welcomes-new-faculty-1017</link>
  <description>The faculty members occupy core computing and shared positions, bringing varied backgrounds and expertise to the MIT community.</description>
  <pubDate>Fri, 17 Oct 2025 15:45:00 -0400</pubDate>
    <guid isPermaLink="true">https://news.mit.edu/2025/mit-schwarzman-college-computing-welcomes-new-faculty-1017</guid>
        <dc:creator>Amanda Diehl | MIT Schwarzman College of Computing</dc:creator>
  <content:encoded>&lt;p&gt;The MIT Schwarzman College of Computing welcomes 11 new faculty members in core computing and shared positions to the MIT community. They bring varied backgrounds and expertise spanning sustainable design, satellite remote sensing, decision theory, and the development of new algorithms for declarative artificial intelligence programming, among others.&lt;/p&gt;&lt;p&gt;“I warmly welcome this talented group of new faculty members. Their work lies at the forefront of computing and its broader impact in the world,” says Dan Huttenlocher, dean of the MIT Schwarzman College of Computing and the Henry Ellis Warren Professor of Electrical Engineering and Computer Science.&lt;/p&gt;&lt;p&gt;College faculty include those with appointments in the Department of Electrical Engineering and Computer Science (EECS) or in the Institute for Data, Systems, and Society (IDSS), which report into both the MIT Schwarzman College of Computing and the School of Engineering. There are also several new faculty members in shared positions between the college and other MIT departments and sections, including Political Science, Linguistics and Philosophy, History, and Architecture.&lt;/p&gt;&lt;p&gt;“Thanks to another successful year of collaborative searches, we have hired six additional faculty in shared positions, bringing the total to 20,” says Huttenlocher.&lt;/p&gt;&lt;p&gt;The new shared faculty include:&lt;/p&gt;&lt;p&gt;&lt;a href="https://sites.google.com/mit.edu/bailey-flanigan/home"&gt;Bailey Flanigan&lt;/a&gt; is an assistant professor in the Department of Political Science, holding an MIT Schwarzman College of Computing shared position with EECS. Her research combines tools from social choice theory, game theory, algorithms, statistics, and survey methods to advance political methodology and strengthen democratic participation. She is interested in sampling algorithms, opinion measurement, and the design of democratic innovations like deliberative minipublics and participatory budgeting. Flanigan was a postdoc at Harvard University’s Data Science Initiative, and she earned her PhD in computer science from Carnegie Mellon University.&lt;/p&gt;&lt;p&gt;&lt;a href="https://brianhedden.wixsite.com/home"&gt;Brian Hedden&lt;/a&gt;&lt;strong&gt; &lt;/strong&gt;PhD ’12 is a professor in the Department of Linguistics and Philosophy, holding an MIT Schwarzman College of Computing shared position with EECS. His research focuses on how we ought to form beliefs and make decisions. His works span epistemology, decision theory, and ethics, including ethics of AI. He is the author of “Reasons without Persons: Rationality, Identity, and Time” (Oxford University Press, 2015) and articles on topics such as collective action problems, legal standards of proof, algorithmic fairness, and political polarization. Prior to joining MIT, he was a faculty member at the Australian National University and the University of Sydney, and a junior research fellow at Oxford University. He received his BA from Princeton University and his PhD from MIT, both in philosophy.&lt;/p&gt;&lt;p&gt;&lt;a href="https://www.yunhahwang.com/"&gt;Yunha Hwang&lt;/a&gt;&lt;strong&gt; &lt;/strong&gt;is an assistant professor in the Department of Biology, holding an MIT Schwarzman College of Computing shared position with EECS. She is also a member of the Laboratory for Information and Decision Systems. Her research interests span machine learning for sustainable biomanufacturing, microbial evolution, and open science. She serves as the co-founder and chief scientist at Tatta Bio, a scientific nonprofit dedicated to advancing genomic AI for biological discovery. She holds a BS in computer science from Stanford University and a PhD in biology from Harvard University.&lt;/p&gt;&lt;p&gt;&lt;a href="https://history.mit.edu/people/benjamin-lindquist/"&gt;Ben Lindquist&lt;/a&gt; is an assistant professor in the History Section, holding an MIT Schwarzman College of Computing shared position with EECS. Through a historical lens, his work observes the ways that computing has circulated with ideas of religion, emotion, and divergent thinking. His book, “The Feeling Machine” (University of Chicago Press, forthcoming), follows the history of synthetic speech to examine how emotion became a subject of computer science. He was a postdoc in the Science in Human Culture Program at Northwestern University and earned his PhD in history from Princeton University.&lt;/p&gt;&lt;p&gt;&lt;a href="https://maadpope.com/about-me/"&gt;Mariana Popescu&lt;/a&gt;&lt;strong&gt; &lt;/strong&gt;is an assistant professor in the Department of Architecture, holding an MIT Schwarzman College of Computing shared position with EECS. She is also a member of the Computer Science and Artificial Intelligence Laboratory (CSAIL).&lt;strong&gt; &lt;/strong&gt;A computational architect and structural designer, Popescu has a strong interest and experience in innovative ways of approaching the fabrication process and use of materials in construction. Her area of expertise is computational and parametric design, with a focus on digital fabrication and sustainable design. Popescu earned her doctorate at ETH Zurich.&lt;/p&gt;&lt;p&gt;&lt;a href="https://www.mit.edu/~paris/"&gt;Paris Smaragdis&lt;/a&gt;&lt;strong&gt; &lt;/strong&gt;SM ’97, PhD ’01&lt;strong&gt; &lt;/strong&gt;is a professor in the Music and Theater Arts Section, holding an MIT Schwarzman College of Computing shared position with EECS. His research focus lies at the intersection of signal processing and machine learning, especially as it relates to sound and music. Prior to coming to MIT, he worked as a research scientist at Mitsubishi Electric Research Labs, a senior research scientist at Adobe Research, and an Amazon Scholar with Amazon’s AWS. He spent 15 years as a professor at the University of Illinois Urbana Champaign in the Computer Science Department, where he spearheaded the design of the CS+Music program, and served as an associate director of the School of Computer and Data Science. He holds a BMus from Berklee College of Music and earned his PhD in perceptual computing from MIT.&lt;/p&gt;&lt;p&gt;&lt;a href="https://aeroastro.mit.edu/people/daniel-varon/"&gt;Daniel Varon&lt;/a&gt; is an assistant professor in the Department of Aeronautics and Astronautics, holding an MIT Schwarzman College of Computing shared position with IDSS. His work focuses on using satellite observations of atmospheric composition to better understand human impacts on the environment and identify opportunities to reduce them. An atmospheric scientist, Varon is particularly interested in greenhouse gasses, air pollution, and satellite remote sensing. He holds an MS in applied mathematics and a PhD in atmospheric chemistry, both from Harvard University.&lt;/p&gt;&lt;p&gt;In addition, the School of Engineering has adopted the shared faculty search model to hire its first shared faculty member:&lt;/p&gt;&lt;p&gt;&lt;a href="https://ccrma.stanford.edu/~mrau/"&gt;Mark Rau&lt;/a&gt; is an assistant professor in the Music and Theater Arts Section, holding a School of Engineering shared position with EECS. He is involved in developing graduate programming focused on music technology. He has an interest in musical acoustics, vibration and acoustic measurement, audio signal processing, and physical modeling synthesis. His work focuses on musical instruments and creative audio effects. He holds an MA in music, science, and technology from Stanford, as well as a BS in physics and BMus in jazz from McGill University. He earned his PhD at Stanford’s Center for Computer Research in Music and Acoustics.&lt;/p&gt;&lt;p&gt;The new core faculty are:&lt;/p&gt;&lt;p&gt;&lt;a href="https://mitchellg.github.io/"&gt;Mitchell Gordon&lt;/a&gt; is an assistant professor in EECS. He is also a member of CSAIL. In his research, Gordon designs interactive systems and evaluation approaches that bridge principles of human-computer interaction with the realities of machine learning. His work has won awards at conferences in human-computer interaction and artificial intelligence, including a best paper award at CHI and an Oral at NeurIPS. Gordon received a BS from the University of Rochester, and MS and PhD from Stanford University, all in computer science.&lt;/p&gt;&lt;p&gt;&lt;a href="https://omarkhattab.com/"&gt;Omar Khattab&lt;/a&gt; is an assistant professor in EECS. He is also a member of CSAIL. His work focuses on natural language processing, information retrieval, and AI systems. His research includes developing new algorithms and abstractions for declarative AI programming and for composing retrieval and reasoning. He received his BS from Carnegie Mellon University and his PhD from Stanford University, both in computer science.&lt;/p&gt;&lt;p&gt;&lt;a href="https://people.csail.mit.edu/rachit/"&gt;Rachit Nigam&lt;/a&gt; will join EECS as an assistant professor in January 2026. He will also be a member of CSAIL and the Microsystems Technology Laboratories. He works on programming languages and computer architecture to address the design, verification, and usability challenges of specialized hardware. He was previously a visiting scholar at MIT. Nigam earned an MS and PhD in computer science from Cornell University.&lt;/p&gt;</content:encoded>
      <media:content url="https://news.mit.edu/sites/default/files/styles/news_article__cover_image__original/public/images/202510/mit-schwarzman-Faculty-2025.jpg?itok=8LCtOdKQ" medium="image" type="image/jpeg" width="390" height="260">
              <media:description type="plain">Top row (left to right): Bailey Flanigan, Brian Hedden, Yunha Hwang, and Ben Lindquist. Second row (left to right): Mariana Popescu, Paris Smaragdis, Daniel Varon, and Mark Rau. Third row (left to right): Mitchell Gordon, Omar Khattab, and Rachit Nigam</media:description>
              <media:credit>Photos courtesy of the MIT Schwarzman College of Computing.</media:credit>
      </media:content>
        <category domain="https://news.mit.edu/topic/faculty">Faculty</category>
      <category domain="https://news.mit.edu/topic/alumni">Alumni/ae</category>
      <category domain="https://news.mit.edu/topic/aeronautics">Aeronautical and astronautical engineering</category>
      <category domain="https://news.mit.edu/topic/architecture">Architecture</category>
      <category domain="https://news.mit.edu/topic/biology">Biology</category>
      <category domain="https://news.mit.edu/topic/electrical-engineering-computer-science-eecs">Electrical engineering and computer science (EECS)</category>
      <category domain="https://news.mit.edu/topic/history">History</category>
      <category domain="https://news.mit.edu/topic/linguistics">Linguistics</category>
      <category domain="https://news.mit.edu/topic/music-and-theater-arts">Music and theater arts</category>
      <category domain="https://news.mit.edu/topic/philosophy">Philosophy</category>
      <category domain="https://news.mit.edu/topic/political-science">Political science</category>
      <category domain="https://news.mit.edu/topic/computer-science-and-artificial-intelligence-laboratory-csail">Computer Science and Artificial Intelligence Laboratory (CSAIL)</category>
      <category domain="https://news.mit.edu/topic/idss">IDSS</category>
      <category domain="https://news.mit.edu/topic/lids">Laboratory for Information and Decision Systems (LIDS)</category>
      <category domain="https://news.mit.edu/topic/microsystems-technology-laboratories-0">Microsystems Technology Laboratories</category>
      <category domain="https://news.mit.edu/topic/school-science">School of Science</category>
      <category domain="https://news.mit.edu/topic/school-engineering">School of Engineering</category>
      <category domain="https://news.mit.edu/topic/school-architecture-and-planning">School of Architecture and Planning</category>
      <category domain="https://news.mit.edu/topic/mit-schwarzman-college-computing">MIT Schwarzman College of Computing</category>
      <category domain="https://news.mit.edu/topic/school-humanities-arts-and-social-sciences">School of Humanities Arts and Social Sciences</category>
    </item>
<item>
  <title>New software designs eco-friendly clothing that can reassemble into new items </title>
  <link>https://news.mit.edu/2025/refashion-software-designs-eco-friendly-clothing-that-can-reassemble-new-items-1017</link>
  <description>To reduce waste, the Refashion program helps users create outlines for adaptable clothing, such as pants that can be reconfigured into a dress. Each component of these pieces can be replaced, rearranged, or restyled.</description>
  <pubDate>Fri, 17 Oct 2025 14:30:00 -0400</pubDate>
    <guid isPermaLink="true">https://news.mit.edu/2025/refashion-software-designs-eco-friendly-clothing-that-can-reassemble-new-items-1017</guid>
        <dc:creator>Alex Shipps | MIT CSAIL</dc:creator>
  <content:encoded>&lt;p dir="ltr" id="docs-internal-guid-1cf4f40f-7fff-2939-9710-707696adbaca"&gt;It’s hard to keep up with the ever-changing trends of the fashion world. What’s “in” one minute is often out of style the next season, potentially causing you to re-evaluate your wardrobe.&lt;/p&gt;&lt;p dir="ltr"&gt;Staying current with the latest fashion styles can be wasteful and expensive, though. Roughly&amp;nbsp;&lt;a href="https://earth.org/statistics-about-fast-fashion-waste/"&gt;92 million tons&lt;/a&gt; of textile waste are produced annually, including the clothes we discard when they go out of style or no longer fit. But what if we could simply reassemble our clothes into whatever outfits we wanted, adapting to trends and the ways our bodies change?&lt;/p&gt;&lt;p dir="ltr"&gt;A team of researchers at MIT’s Computer Science and Artificial Intelligence Laboratory (CSAIL) and Adobe are attempting to bring eco-friendly, versatile garments to life. Their new “&lt;a href="https://rebeccayelin.github.io/refashion/"&gt;Refashion&lt;/a&gt;” software system breaks down fashion design into modules — essentially, smaller building blocks — by allowing users to draw, plan, and visualize each element of a clothing item. The tool turns fashion ideas into a blueprint that outlines how to assemble each component into reconfigurable clothing, such as a pair of pants that can be transformed into a dress.&lt;/p&gt;&lt;p dir="ltr"&gt;With Refashion, users simply draw shapes and place them together to develop an outline for adaptable fashion pieces. It’s a visual diagram that shows how to cut garments, providing a straightforward way to design things like a shirt with an attachable hood for rainy days. One could also create a skirt that can then be reconfigured into a dress for a formal dinner, or maternity wear that fits during different stages of pregnancy.&lt;br&gt;&lt;br&gt;“We wanted to create garments that consider reuse from the start,” says Rebecca Lin, MIT Department of Electrical Engineering and Computer Science (EECS) PhD student, CSAIL and Media Lab researcher, and lead author on a&amp;nbsp;&lt;a href="https://arxiv.org/abs/2510.11941"&gt;paper presenting the project&lt;/a&gt;. “Most clothes you buy today are static, and are discarded when you no longer want them. Refashion instead makes the most of our garments by helping us design items that can be easily resized, repaired, or restyled into different outfits.”&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;strong&gt;Modules à la mode&lt;/strong&gt;&lt;/p&gt;&lt;p dir="ltr"&gt;The researchers conducted a preliminary user study where both designers and novices explored Refashion and were able to create garment prototypes. Participants assembled pieces such as an asymmetric top that could be extended into a jumpsuit, or remade into a formal dress, often within 30 minutes. These results suggest that Refashion has the potential to make prototyping garments more approachable and efficient. But what features might contribute to this ease of use?&lt;/p&gt;&lt;p dir="ltr"&gt;Its interface first presents a simple grid in its “Pattern Editor” mode, where users can connect dots to outline the boundaries of a clothing item. It’s essentially drawing rectangular panels and specifying how different modules will connect to each other.&lt;/p&gt;&lt;p dir="ltr"&gt;Users can customize the shape of each component, create a straight design for garments (which might be useful for less form-fitting items, like chinos) or perhaps tinkering with one of Refashion’s templates. A user can edit pre-designed blueprints for things like a T-shirt, fitted blouse, or trousers.&lt;br&gt;&lt;br&gt;Another, more creative route is to change the design of individual modules. One can choose the “pleat” feature to fold a garment over itself, similar to an accordion, for starters. It’s a useful way to design something like a maxi dress. The “gather” option adds an artsy flourish, where a garment is crumpled together to create puffy skirts or sleeves. A user might even go with the “dart” module, which removes a triangular piece from the fabric. It allows for shaping a garment at the waist (perhaps for a pencil skirt) or tailor to the upper body (fitted shirts, for instance).&lt;/p&gt;&lt;p dir="ltr"&gt;While it might seem that each of these components needs to be sewn together, Refashion enables users to connect garments through more flexible, efficient means. Edges can be seamed together via double-sided connectors such as metal snaps (like the buttons used to close a denim jacket) or Velcro dots. A user could also fasten them in pins called brads, which have a pointed side that they stick through a hole and split into two “legs” to attach to another surface; it’s a handy way to secure, say, a picture on a poster board. Both connective methods make it easy to reconfigure modules, should they be damaged or a “fit check” calls for a new look.&lt;br&gt;&lt;br&gt;As a user designs their clothing piece, the system automatically creates a simplified diagram of how it can be assembled. The pattern is divided into numbered blocks, which is dragged onto different parts of a 2D mannequin to specify the position of each component. The user can then simulate how their sustainable clothing will look on 3D models of a range of body types (one can also upload a model).&lt;/p&gt;&lt;p dir="ltr"&gt;Finally, a digital blueprint for sustainable clothing can extend, shorten, or combine with other pieces. Thanks to Refashion, a new piece could be emblematic of a potential shift in fashion: Instead of buying new clothes every time we want a new outfit, we can simply reconfigure existing ones. Yesterday’s scarf could be today’s hat, and today’s T-shirt could be tomorrow’s jacket.&lt;/p&gt;&lt;p dir="ltr"&gt;“Rebecca’s work is at an exciting intersection between computation and art, craft, and design,” says MIT EECS professor and CSAIL principal investigator Erik Demaine, who advises Lin. “I’m excited to see how Refashion can make custom fashion design accessible to the wearer, while also making clothes more reusable and sustainable.”&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;strong&gt;Constant change&lt;/strong&gt;&lt;/p&gt;&lt;p dir="ltr"&gt;While Refashion presents a greener vision for the future of fashion, the researchers note that they’re actively improving the system. They intend to revise the interface to support more durable items, stepping beyond standard prototyping fabrics. Refashion may soon support other modules, like curved panels, as well. The CSAIL-Adobe team may also evaluate whether their system can use as few materials as possible to minimize waste, and whether it can help “remix” old store-bought outfits.&lt;/p&gt;&lt;p dir="ltr"&gt;Lin also plans to develop new computational tools that help designers create unique, personalized outfits using colors and textures. She’s exploring how to design clothing by patchwork — essentially, cutting out small pieces from materials like decorative fabrics, recycled denim, and crochet blocks and assembling them into a larger item.&lt;/p&gt;&lt;p&gt;“This is a great example of how computer-aided design can also be key in supporting more sustainable practices in the fashion industry,” says Adrien Bousseau, a senior researcher at Inria Centre at Université Côte d'Azur who wasn’t involved in the paper. “By promoting garment alteration from the ground up, they developed a novel design interface and accompanying optimization algorithm that helps designers create garments that can undergo a longer lifetime through reconfiguration. While sustainability often imposes additional constraints on industrial production, I am confident that research like the one by Lin and her colleagues will empower designers in innovating despite these constraints.”&lt;br&gt;&lt;br&gt;Lin wrote the paper with Adobe Research scientists Michal Lukáč and Mackenzie Leake, who is the paper’s senior author and a former CSAIL postdoc. Their work was supported, in part, by the MIT Morningside Academy for Design, an MIT MAKE Design-2-Making Mini-Grant, and the Natural Sciences and Engineering Research Council of Canada. The researchers presented their work recently at the ACM Symposium on User Interface Software and Technology.&lt;/p&gt;</content:encoded>
      <media:content url="https://news.mit.edu/sites/default/files/styles/news_article__cover_image__original/public/images/202510/MIT-Refashion.jpg?itok=U64zwdqx" medium="image" type="image/jpeg" width="390" height="260">
              <media:description type="plain">With Refashion, users simply draw shapes and place them together to develop an outline for adaptable fashion pieces. It’s a visual diagram that demonstrates how to cut garments, providing a straightforward way to design things like pants that can be reconfigured into a dress.</media:description>
              <media:credit>Image: Alex Shipps/MIT CSAIL and Rebecca Lin</media:credit>
      </media:content>
        <category domain="https://news.mit.edu/topic/research">Research</category>
      <category domain="https://news.mit.edu/topic/invention">Invention</category>
      <category domain="https://news.mit.edu/topic/design">Design</category>
      <category domain="https://news.mit.edu/topic/sustainability">Sustainability</category>
      <category domain="https://news.mit.edu/topic/machine-learning">Machine learning</category>
      <category domain="https://news.mit.edu/topic/artificial-intelligence2">Artificial intelligence</category>
      <category domain="https://news.mit.edu/topic/cleaner-industry">Cleaner industry</category>
      <category domain="https://news.mit.edu/topic/human-computer-interaction">Human-computer interaction</category>
      <category domain="https://news.mit.edu/topic/computers">Computer science and technology</category>
      <category domain="https://news.mit.edu/topic/electrical-engineering-computer-science-eecs">Electrical engineering and computer science (EECS)</category>
      <category domain="https://news.mit.edu/topic/media-lab-0">Media Lab</category>
      <category domain="https://news.mit.edu/topic/mit-morningside-academy-design">MIT Morningside Academy for Design</category>
      <category domain="https://news.mit.edu/topic/school-engineering">School of Engineering</category>
      <category domain="https://news.mit.edu/topic/mit-schwarzman-college-computing">MIT Schwarzman College of Computing</category>
      <category domain="https://news.mit.edu/topic/computer-science-and-artificial-intelligence-laboratory-csail">Computer Science and Artificial Intelligence Laboratory (CSAIL)</category>
    </item>
<item>
  <title>Method teaches generative AI models to locate personalized objects</title>
  <link>https://news.mit.edu/2025/method-teaches-generative-ai-models-locate-personalized-objects-1016</link>
  <description>After being trained with this technique, vision-language models can better identify a unique item in a new scene.</description>
  <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
    <guid isPermaLink="true">https://news.mit.edu/2025/method-teaches-generative-ai-models-locate-personalized-objects-1016</guid>
        <dc:creator>Adam Zewe | MIT News</dc:creator>
  <content:encoded>&lt;p&gt;Say a person takes their French Bulldog, Bowser, to the dog park. Identifying Bowser as he plays among the other canines is easy for the dog-owner to do while onsite.&lt;/p&gt;&lt;p&gt;But if someone wants to use a generative AI model like GPT-5 to monitor their pet while they are at work, the model could fail at this basic task. Vision-language models like GPT-5 often excel at recognizing general objects, like a dog, but they perform poorly at locating personalized objects, like Bowser the French Bulldog.&amp;nbsp; &amp;nbsp;&amp;nbsp;&lt;/p&gt;&lt;p&gt;To address this shortcoming, researchers from MIT, the MIT-IBM Watson AI Lab, the Weizmann Institute of Science, and elsewhere have introduced a new training method that teaches vision-language models to localize personalized objects in a scene.&lt;/p&gt;&lt;p&gt;Their method uses carefully prepared video-tracking data in which the same object is tracked across multiple frames. They designed the dataset so the model must focus on contextual clues to identify the personalized object, rather than relying on knowledge it previously memorized.&lt;/p&gt;&lt;p&gt;When given a few example images showing a personalized object, like someone’s pet, the retrained model is better able to identify the location of that same pet in a new image.&lt;/p&gt;&lt;p&gt;Models retrained with their method outperformed state-of-the-art systems at this task. Importantly, their technique leaves the rest of the model’s general abilities intact.&lt;/p&gt;&lt;p&gt;This new approach could help future AI systems track specific objects across time, like a child’s backpack, or localize objects of interest, such as a species of animal in ecological monitoring. It could also aid in the development of AI-driven assistive technologies that help visually impaired users find certain items in a room.&lt;/p&gt;&lt;p&gt;“Ultimately, we want these models to be able to learn from context, just like humans do. If a model can do this well, rather than retraining it for each new task, we could just provide a few examples and it would infer how to perform the task from that context. This is a very powerful ability,” says Jehanzeb Mirza, an MIT postdoc and senior author of a &lt;a href="https://arxiv.org/pdf/2411.13317" target="_blank"&gt;paper on this technique&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;Mirza is joined on the paper by co-lead authors Sivan Doveh, a postdoc at Stanford University who was a graduate student at Weizmann Institute of Science when this research was conducted; and Nimrod Shabtay, a researcher at IBM Research; James Glass, a senior research scientist and the head of the Spoken Language Systems Group in the MIT Computer Science and Artificial Intelligence Laboratory (CSAIL); and others. The work will be presented at the International Conference on Computer Vision.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;An unexpected shortcoming&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Researchers have found that large language models (LLMs) can excel at learning from context. If they feed an LLM a few examples of a task, like addition problems, it can learn to answer new addition problems based on the context that has been provided.&lt;/p&gt;&lt;p&gt;A vision-language model (VLM) is essentially an LLM with a visual component connected to it, so the MIT researchers thought it would inherit the LLM’s in-context learning capabilities. But this is not the case.&lt;/p&gt;&lt;p&gt;“The research community has not been able to find a black-and-white answer to this particular problem yet. The bottleneck could arise from the fact that some visual information is lost in the process of merging the two components together, but we just don’t know,” Mirza says.&lt;/p&gt;&lt;p&gt;The researchers set out to improve VLMs abilities to do in-context localization, which involves finding a specific object in a new image. They focused on the data used to retrain existing VLMs for a new task, a process called fine-tuning.&lt;/p&gt;&lt;p&gt;Typical fine-tuning data are gathered from random sources and depict collections of everyday objects. One image might contain cars parked on a street, while another includes a bouquet of flowers.&lt;/p&gt;&lt;p&gt;“There is no real coherence in these data, so the model never learns to recognize the same object in multiple images,” he says.&lt;/p&gt;&lt;p&gt;To fix this problem, the researchers developed a new dataset by curating samples from existing video-tracking data. These data are video clips showing the same object moving through a scene, like a tiger walking across a grassland.&lt;/p&gt;&lt;p&gt;They cut frames from these videos and structured the dataset so each input would consist of multiple images showing the same object in different contexts, with example questions and answers about its location.&lt;/p&gt;&lt;p&gt;“By using multiple images of the same object in different contexts, we encourage the model to consistently localize that object of interest by focusing on the context,” Mirza explains.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Forcing the focus&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;But the researchers found that VLMs tend to cheat. Instead of answering based on context clues, they will identify the object using knowledge gained during pretraining.&lt;/p&gt;&lt;p&gt;For instance, since the model already learned that an image of a tiger and the label “tiger” are correlated, it could identify the tiger crossing the grassland based on this pretrained knowledge, instead of inferring from context.&lt;/p&gt;&lt;p&gt;To solve this problem, the researchers used pseudo-names rather than actual object category names in the dataset. In this case, they changed the name of the tiger to “Charlie.”&lt;/p&gt;&lt;p&gt;“It took us a while to figure out how to prevent the model from cheating. But we changed the game for the model. The model does not know that ‘Charlie’ can be a tiger, so it is forced to look at the context,” he says.&lt;/p&gt;&lt;p&gt;The researchers also faced challenges in finding the best way to prepare the data. If the frames are too close together, the background would not change enough to provide data diversity.&lt;/p&gt;&lt;p&gt;In the end, finetuning VLMs with this new dataset improved accuracy at personalized localization by about 12 percent on average. When they included the dataset with pseudo-names, the performance gains reached 21 percent.&lt;/p&gt;&lt;p&gt;As model size increases, their technique leads to greater performance gains.&lt;/p&gt;&lt;p&gt;In the future, the researchers want to study possible reasons VLMs don’t inherit in-context learning capabilities from their base LLMs. In addition, they plan to explore additional mechanisms to improve the performance of a VLM without the need to retrain it with new data.&lt;/p&gt;&lt;p&gt;“This work reframes few-shot personalized object localization — adapting on the fly to the same object across new scenes — as an instruction-tuning problem and uses video-tracking sequences to teach VLMs to localize based on visual context rather than class priors. It also introduces the first benchmark for this setting with solid gains across open and proprietary VLMs. Given the immense significance of quick, instance-specific grounding — often without finetuning — for users of real-world workflows (such as robotics, augmented reality assistants, creative tools, etc.), the practical, data-centric recipe offered by this work can help enhance the widespread adoption of vision-language foundation models,” says Saurav Jha, a postdoc at the Mila-Quebec Artificial Intelligence Institute, who was not involved with this work.&lt;/p&gt;&lt;p&gt;Additional co-authors are Wei Lin, a research associate at Johannes Kepler University; Eli Schwartz, a research scientist at IBM Research; Hilde Kuehne, professor of computer science at&amp;nbsp;Tuebingen AI Center and an affiliated professor at the MIT-IBM Watson AI Lab; Raja Giryes, an associate professor at Tel Aviv University; Rogerio Feris, a principal scientist and manager at the MIT-IBM Watson AI Lab; Leonid Karlinsky, a principal research scientist at IBM Research; Assaf Arbelle, a senior research scientist at IBM Research; and Shimon Ullman, the Samy and Ruth Cohn Professor of Computer Science at the Weizmann Institute of Science.&lt;/p&gt;&lt;p&gt;This research was funded, in part, by the MIT-IBM Watson AI Lab.&lt;/p&gt;</content:encoded>
      <media:content url="https://news.mit.edu/sites/default/files/styles/news_article__cover_image__original/public/images/202510/MIT-VLM-local-01-press.jpg?itok=I38AF21h" medium="image" type="image/jpeg" width="390" height="260">
              <media:description type="plain">A new training method teaches vision-language generative AI models to localize a personalized object, like a cat named Snoofkin, in a new scene.</media:description>
              <media:credit>Credit: MIT News; iStock</media:credit>
      </media:content>
        <category domain="https://news.mit.edu/topic/research">Research</category>
      <category domain="https://news.mit.edu/topic/computers">Computer science and technology</category>
      <category domain="https://news.mit.edu/topic/artificial-intelligence2">Artificial intelligence</category>
      <category domain="https://news.mit.edu/topic/machine-learning">Machine learning</category>
      <category domain="https://news.mit.edu/topic/algorithms">Algorithms</category>
      <category domain="https://news.mit.edu/topic/video">Video</category>
      <category domain="https://news.mit.edu/topic/data">Data</category>
      <category domain="https://news.mit.edu/topic/computer-science-and-artificial-intelligence-laboratory-csail">Computer Science and Artificial Intelligence Laboratory (CSAIL)</category>
      <category domain="https://news.mit.edu/topic/electrical-engineering-computer-science-eecs">Electrical engineering and computer science (EECS)</category>
      <category domain="https://news.mit.edu/topic/school-engineering">School of Engineering</category>
      <category domain="https://news.mit.edu/topic/mit-schwarzman-college-computing">MIT Schwarzman College of Computing</category>
      <category domain="https://news.mit.edu/topic/mit-ibm-watson-ai-lab">MIT-IBM Watson AI Lab</category>
    </item>
<item>
  <title>Using generative AI to diversify virtual training grounds for robots</title>
  <link>https://news.mit.edu/2025/using-generative-ai-diversify-virtual-training-grounds-robots-1008</link>
  <description>New tool from MIT CSAIL creates realistic virtual kitchens and living rooms where simulated robots can interact with models of real-world objects, scaling up training data for robot foundation models.</description>
  <pubDate>Wed, 08 Oct 2025 13:45:00 -0400</pubDate>
    <guid isPermaLink="true">https://news.mit.edu/2025/using-generative-ai-diversify-virtual-training-grounds-robots-1008</guid>
        <dc:creator>Alex Shipps | MIT CSAIL</dc:creator>
  <content:encoded>&lt;p dir="ltr" id="docs-internal-guid-19f1f3f3-7fff-ddef-2547-d3fc1f5d3464"&gt;Chatbots like ChatGPT and Claude have experienced a meteoric rise in usage over the past three years because they can help you with a wide range of tasks. Whether you’re writing Shakespearean sonnets, debugging code, or need an answer to an obscure trivia question, artificial intelligence systems seem to have you covered. The source of this versatility? Billions, or even trillions, of textual data points across the internet.&lt;/p&gt;&lt;p dir="ltr"&gt;Those data aren’t enough to teach a robot to be a helpful household or factory assistant, though. To understand how to handle, stack, and place various arrangements of objects across diverse environments, robots need demonstrations. You can think of robot training data as a collection of how-to videos that walk the systems through each motion of a task. Collecting these demonstrations on real robots is time-consuming and not perfectly repeatable, so engineers have created training data by generating simulations with AI (which don’t often reflect real-world physics), or tediously handcrafting each digital environment from scratch.&lt;/p&gt;&lt;p dir="ltr"&gt;Researchers at MIT’s Computer Science and Artificial Intelligence Laboratory (CSAIL) and the Toyota Research Institute may have found a way to create the diverse, realistic training grounds robots need. Their “&lt;a href="https://steerable-scene-generation.github.io/" target="_blank"&gt;steerable scene generation&lt;/a&gt;” approach creates digital scenes of things like kitchens, living rooms, and restaurants that engineers can use to simulate lots of real-world interactions and scenarios. Trained on over 44 million 3D rooms filled with models of objects such as tables and plates, the tool places existing assets in new scenes, then refines each one into a physically accurate, lifelike environment.&lt;/p&gt;&lt;p dir="ltr"&gt;Steerable scene generation creates these 3D worlds by “steering” a diffusion model — an AI system that generates a visual from random noise — toward a scene you’d find in everyday life. The researchers used this generative system to “in-paint” an environment, filling in particular elements throughout the scene. You can imagine a blank canvas suddenly turning into a kitchen scattered with 3D objects, which are gradually rearranged into a scene that imitates real-world physics. For example, the system ensures that a fork doesn’t pass through a bowl on a table — a common glitch in 3D graphics known as “clipping,” where models overlap or intersect.&lt;br&gt;&lt;br&gt;How exactly steerable scene generation guides its creation toward realism, however, depends on the strategy you choose. Its main strategy is “Monte Carlo tree search” (MCTS), where the model creates a series of alternative scenes, filling them out in different ways toward a particular objective (like making a scene more physically realistic, or including as many edible items as possible). It’s used by the AI program AlphaGo to beat human opponents in Go (a game similar to chess), as the system considers potential sequences of moves before choosing the most advantageous one.&lt;br&gt;&lt;br&gt;“We are the first to apply MCTS to scene generation by framing the scene generation task as a sequential decision-making process,” says MIT Department of Electrical Engineering and Computer Science (EECS) PhD student Nicholas Pfaff, who is a CSAIL researcher and a lead author on a&amp;nbsp;&lt;a href="https://steerable-scene-generation.github.io/"&gt;paper&lt;/a&gt; presenting the work. “We keep building on top of partial scenes to produce better or more desired scenes over time. As a result, MCTS creates scenes that are more complex than what the diffusion model was trained on.”&lt;/p&gt;&lt;p dir="ltr"&gt;In one particularly telling experiment, MCTS added the maximum number of objects to a simple restaurant scene. It featured as many as 34 items on a table, including massive stacks of dim sum dishes, after training on scenes with only 17 objects on average.&lt;/p&gt;&lt;p dir="ltr"&gt;Steerable scene generation also allows you to generate diverse training scenarios via reinforcement learning — essentially, teaching a diffusion model to fulfill an objective by trial-and-error. After you train on the initial data, your system undergoes a second training stage, where you outline a reward (basically, a desired outcome with a score indicating how close you are to that goal). The model automatically learns to create scenes with higher scores, often producing scenarios that are quite different from those it was trained on.&lt;br&gt;&lt;br&gt;Users can also prompt the system directly by typing in specific visual descriptions (like “a kitchen with four apples and a bowl on the table”). Then, steerable scene generation can bring your requests to life with precision. For example, the tool accurately followed users’ prompts at rates of 98 percent when building scenes of pantry shelves, and 86 percent for messy breakfast tables. Both marks are at least a 10 percent improvement over comparable methods like&amp;nbsp;“&lt;a href="https://arxiv.org/abs/2405.21066"&gt;MiDiffusion&lt;/a&gt;” and “&lt;a href="https://arxiv.org/abs/2303.14207"&gt;DiffuScene&lt;/a&gt;.”&lt;br&gt;&lt;br&gt;The system can also complete specific scenes via prompting or light directions (like “come up with a different scene arrangement using the same objects”). You could ask it to place apples on several plates on a kitchen table, for instance, or put board games and books on a shelf. It’s essentially “filling in the blank” by slotting items in empty spaces, but preserving the rest of a scene.&lt;/p&gt;&lt;p dir="ltr"&gt;According to the researchers, the strength of their project lies in its ability to create many scenes that roboticists can actually use. “A key insight from our findings is that it’s OK for the scenes we pre-trained on to not exactly resemble the scenes that we actually want,” says Pfaff. “Using our steering methods, we can move beyond that broad distribution and sample from a ‘better’ one. In other words, generating the diverse, realistic, and task-aligned scenes that we actually want to train our robots in.”&lt;/p&gt;&lt;p dir="ltr"&gt;Such vast scenes became the testing grounds where they could record a virtual robot interacting with different items. The machine carefully placed forks and knives into a cutlery holder, for instance, and rearranged bread onto plates in various 3D settings. Each simulation appeared fluid and realistic, resembling the real-world, adaptable robots steerable scene generation could help train, one day.&lt;/p&gt;&lt;p dir="ltr"&gt;While the system could be an encouraging path forward in generating lots of diverse training data for robots, the researchers say their work is more of a proof of concept. In the future, they’d like to use generative AI to create entirely new objects and scenes, instead of using a fixed library of assets. They also plan to incorporate articulated objects that the robot could open or twist (like cabinets or jars filled with food) to make the scenes even more interactive.&lt;/p&gt;&lt;p dir="ltr"&gt;To make their virtual environments even more realistic, Pfaff and his colleagues may incorporate real-world objects by using a library of objects and scenes pulled from images on the internet and using their previous work on “&lt;a href="https://scalable-real2sim.github.io/"&gt;Scalable Real2Sim&lt;/a&gt;.” By expanding how diverse and lifelike AI-constructed robot testing grounds can be, the team hopes to build a community of users that’ll create lots of data, which could then be used as a massive dataset to teach dexterous robots different skills.&lt;br&gt;&lt;br&gt;“Today, creating realistic scenes for simulation can be quite a challenging endeavor; procedural generation can readily produce a large number of scenes, but they likely won’t be representative of the environments the robot would encounter in the real world. Manually creating bespoke scenes is both time-consuming and expensive,” says Jeremy Binagia, an applied scientist at Amazon Robotics who wasn’t involved in the paper. “Steerable scene generation offers a better approach: train a generative model on a large collection of pre-existing scenes and adapt it (using a strategy such as reinforcement learning) to specific downstream applications. Compared to previous works that leverage an off-the-shelf vision-language model or focus just on arranging objects in a 2D grid, this approach guarantees physical feasibility and considers full 3D translation and rotation, enabling the generation of much more interesting scenes.”&lt;/p&gt;&lt;p dir="ltr"&gt;“Steerable scene generation with post training and inference-time search provides a novel and efficient framework for automating scene generation at scale,” says Toyota Research Institute roboticist Rick Cory SM ’08, PhD ’10, who also wasn’t involved in the paper. “Moreover, it can generate ‘never-before-seen’ scenes that are deemed important for downstream tasks. In the future, combining this framework with vast internet data could unlock an important milestone towards efficient training of robots for deployment in the real world.”&lt;br&gt;&lt;br&gt;Pfaff wrote the paper with senior author Russ Tedrake, the Toyota Professor of Electrical Engineering and Computer Science, Aeronautics and Astronautics, and Mechanical Engineering at MIT; a senior vice president of large behavior models at the Toyota Research Institute; and CSAIL principal investigator. Other authors were Toyota Research Institute robotics researcher Hongkai Dai SM ’12, PhD ’16; team lead and Senior Research Scientist Sergey Zakharov; and Carnegie Mellon University PhD student Shun Iwase. Their work was supported, in part, by Amazon and the Toyota Research Institute. The researchers presented their work at the Conference on Robot Learning (CoRL) in September.&lt;/p&gt;</content:encoded>
      <media:content url="https://news.mit.edu/sites/default/files/styles/news_article__cover_image__original/public/images/202509/mit-csail-restaurant.gif?itok=fvPRFM7r" medium="image" type="image/jpeg" width="390" height="260">
              <media:description type="plain">The “steerable scene generation” system creates digital scenes of things like kitchens, living rooms, and restaurants that engineers can use to simulate lots of real-world robot interactions and scenarios.</media:description>
              <media:credit>Generative AI image, courtesy of the researchers.</media:credit>
      </media:content>
        <category domain="https://news.mit.edu/topic/research">Research</category>
      <category domain="https://news.mit.edu/topic/simulation">Simulation</category>
      <category domain="https://news.mit.edu/topic/robotics">Robotics</category>
      <category domain="https://news.mit.edu/topic/data">Data</category>
      <category domain="https://news.mit.edu/topic/artificial-intelligence2">Artificial intelligence</category>
      <category domain="https://news.mit.edu/topic/computer-modeling">Computer modeling</category>
      <category domain="https://news.mit.edu/topic/internet">Internet</category>
      <category domain="https://news.mit.edu/topic/machine-learning">Machine learning</category>
      <category domain="https://news.mit.edu/topic/computers">Computer science and technology</category>
      <category domain="https://news.mit.edu/topic/mit-schwarzman-college-computing">MIT Schwarzman College of Computing</category>
      <category domain="https://news.mit.edu/topic/computer-science-and-artificial-intelligence-laboratory-csail">Computer Science and Artificial Intelligence Laboratory (CSAIL)</category>
      <category domain="https://news.mit.edu/topic/school-engineering">School of Engineering</category>
      <category domain="https://news.mit.edu/topic/electrical-engineering-computer-science-eecs">Electrical engineering and computer science (EECS)</category>
    </item>
<item>
  <title>AI maps how a new antibiotic targets gut bacteria</title>
  <link>https://news.mit.edu/2025/ai-maps-how-new-antibiotic-targets-gut-bacteria-1003</link>
  <description>MIT CSAIL and McMaster researchers used a generative AI model to reveal how a narrow-spectrum antibiotic attacks disease-causing bacteria, speeding up a process that normally takes years.</description>
  <pubDate>Fri, 03 Oct 2025 17:00:00 -0400</pubDate>
    <guid isPermaLink="true">https://news.mit.edu/2025/ai-maps-how-new-antibiotic-targets-gut-bacteria-1003</guid>
        <dc:creator>Rachel Gordon | MIT CSAIL</dc:creator>
  <content:encoded>&lt;p dir="ltr" id="docs-internal-guid-b4723cd9-7fff-1a9e-4632-a731c7d5a6d7"&gt;For patients with inflammatory bowel disease, antibiotics can be a double-edged sword. The broad-spectrum drugs often prescribed for gut flare-ups can kill helpful microbes alongside harmful ones, sometimes worsening symptoms over time. When fighting gut inflammation, you don’t always want to bring a sledgehammer to a knife fight.&lt;/p&gt;&lt;p dir="ltr"&gt;Researchers at MIT’s Computer Science and Artificial Intelligence Laboratory (CSAIL) and McMaster University have &lt;a href="https://www.nature.com/articles/s41564-025-02142-0" target="_blank"&gt;identified a new compound&lt;/a&gt; that takes a more targeted approach. The molecule, called enterololin, suppresses a group of bacteria linked to Crohn’s disease flare-ups while leaving the rest of the microbiome largely intact. Using a generative AI model, the team mapped how the compound works, a process that usually takes years but was accelerated here to just months.&lt;/p&gt;&lt;p dir="ltr"&gt;“This discovery speaks to a central challenge in antibiotic development,” says Jon Stokes, senior author of a &lt;a href="https://www.nature.com/articles/s41564-025-02142-0"&gt;new paper on the work&lt;/a&gt;, assistant professor of biochemistry and biomedical sciences at McMaster, and research affiliate at MIT’s Abdul Latif Jameel Clinic for Machine Learning in Health. “The problem isn’t finding molecules that kill bacteria in a dish — we’ve been able to do that for a long time. A major hurdle is figuring out what those molecules actually do inside bacteria. Without that detailed understanding, you can’t develop these early-stage antibiotics into safe and effective therapies for patients.”&lt;/p&gt;&lt;p dir="ltr"&gt;Enterololin is a stride toward precision antibiotics: treatments designed to knock out only the bacteria causing trouble. In mouse models of Crohn’s-like inflammation, the drug zeroed in on&amp;nbsp;&lt;em&gt;Escherichia coli&lt;/em&gt;, a gut-dwelling bacterium that can worsen flares, while leaving most other microbial residents untouched. Mice given enterololin recovered faster and maintained a healthier microbiome than those treated with vancomycin, a common antibiotic.&lt;/p&gt;&lt;p dir="ltr"&gt;Pinning down a drug’s mechanism of action, the molecular target it binds inside bacterial cells, normally requires years of painstaking experiments. Stokes’ lab discovered enterololin using a high-throughput screening approach, but determining its target would have been the bottleneck. Here, the team turned to&amp;nbsp;&lt;a href="https://news.mit.edu/2023/speeding-drug-discovery-with-diffusion-generative-models-diffdock-0331"&gt;DiffDock&lt;/a&gt;, a generative AI model developed at CSAIL by MIT PhD student Gabriele Corso and MIT Professor Regina Barzilay.&lt;/p&gt;&lt;p dir="ltr"&gt;DiffDock was designed to predict how small molecules fit into the binding pockets of proteins, a notoriously difficult problem in structural biology. Traditional docking algorithms search through possible orientations using scoring rules, often producing noisy results. DiffDock instead frames docking as a probabilistic reasoning problem: a diffusion model iteratively refines guesses until it converges on the most likely binding mode.&lt;/p&gt;&lt;p dir="ltr"&gt;“In just a couple of minutes, the model predicted that enterololin binds to a protein complex called LolCDE, which is essential for transporting lipoproteins in certain bacteria,” says Barzilay, who also co-leads the Jameel Clinic. “That was a very concrete lead — one that could guide experiments, rather than replace them.”&lt;/p&gt;&lt;p dir="ltr"&gt;Stokes’ group then put that prediction to the test. Using DiffDock predictions as an experimental GPS, they first evolved enterololin-resistant mutants of&amp;nbsp;&lt;em&gt;E. coli&lt;/em&gt; in the lab, which revealed that changes in the mutant’s DNA mapped to lolCDE, precisely where DiffDock had predicted enterololin to bind. They also performed RNA sequencing to see which bacterial genes switched on or off when exposed to the drug, as well as used CRISPR to selectively knock down expression of the expected target. These laboratory experiments all revealed disruptions in pathways tied to lipoprotein transport, exactly what DiffDock had predicted.&lt;/p&gt;&lt;p dir="ltr"&gt;“When you see the computational model and the wet-lab data pointing to the same mechanism, that’s when you start to believe you’ve figured something out,” says Stokes.&lt;/p&gt;&lt;p dir="ltr"&gt;For Barzilay, the project highlights a shift in how AI is used in the life sciences. “A lot of AI use in drug discovery has been about searching chemical space, identifying new molecules that might be active,” she says. “What we’re showing here is that AI can also provide mechanistic explanations, which are critical for moving a molecule through the development pipeline.”&lt;/p&gt;&lt;p dir="ltr"&gt;That distinction matters because mechanism-of-action studies are often a major rate-limiting step in drug development. Traditional approaches can take 18 months to two years, or more, and cost millions of dollars. In this case, the MIT–McMaster team cut the timeline to about six months, at a fraction of the cost.&lt;/p&gt;&lt;p dir="ltr"&gt;Enterololin is still in the early stages of development, but translation is already underway. Stokes’ spinout company, Stoked Bio, has licensed the compound and is optimizing its properties for potential human use. Early work is also exploring derivatives of the molecule against other resistant pathogens, such as&amp;nbsp;&lt;em&gt;Klebsiella pneumoniae&lt;/em&gt;. If all goes well, clinical trials could begin within the next few years.&lt;/p&gt;&lt;p dir="ltr"&gt;The researchers also see broader implications. Narrow-spectrum antibiotics have long been sought as a way to treat infections without collateral damage to the microbiome, but they have been difficult to discover and validate. AI tools like DiffDock could make that process more practical, rapidly enabling a new generation of targeted antimicrobials.&lt;/p&gt;&lt;p dir="ltr"&gt;For patients with Crohn’s and other inflammatory bowel conditions, the prospect of a drug that reduces symptoms without destabilizing the microbiome could mean a meaningful improvement in quality of life. And in the bigger picture, precision antibiotics may help tackle the growing threat of antimicrobial resistance.&lt;/p&gt;&lt;p dir="ltr"&gt;“What excites me is not just this compound, but the idea that we can start thinking about the mechanism of action elucidation as something we can do more quickly, with the right combination of AI, human intuition, and laboratory experiments,” says Stokes. “That has the potential to change how we approach drug discovery for many diseases, not just Crohn’s.”&lt;/p&gt;&lt;p dir="ltr"&gt;“One of the greatest challenges to our health is the increase of antimicrobial-resistant bacteria that evade even our best antibiotics,” adds Yves Brun, professor at the University of Montreal and distinguished professor emeritus at Indiana University Bloomington, who wasn’t involved in the paper. “AI is becoming an important tool in our fight against these bacteria. This study uses a powerful and elegant combination of AI methods to determine the mechanism of action of a new antibiotic candidate, an important step in its potential development as a therapeutic.”&lt;br&gt;&lt;br&gt;Corso, Barzilay, and Stokes wrote the paper with McMaster researchers Denise B. Catacutan, Vian Tran, Jeremie Alexander, Yeganeh Yousefi, Megan Tu, Stewart McLellan, and Dominique Tertigas, and professors ​​Jakob Magolan, Michael Surette, Eric Brown, and Brian Coombes. Their research was supported, in part, by the Weston Family Foundation; the David Braley Centre for Antibiotic Discovery; the Canadian Institutes of Health Research; the Natural Sciences and Engineering Research Council of Canada; M. and M. Heersink; Canadian Institutes for Health Research; Ontario Graduate Scholarship Award; the Jameel Clinic; and the U.S. Defense Threat Reduction Agency Discovery of Medical Countermeasures Against New and Emerging Threats program.&lt;/p&gt;&lt;p dir="ltr"&gt;The researchers posted sequencing data in public repositories and released the DiffDock-L code openly on GitHub.&lt;/p&gt;</content:encoded>
      <media:content url="https://news.mit.edu/sites/default/files/styles/news_article__cover_image__original/public/images/202510/mit-csail-enterololin.jpg?itok=cVoJWo4J" medium="image" type="image/jpeg" width="390" height="260">
              <media:description type="plain">By using AI to sift through more than 10,000 molecules, researchers found enterololin (inset), a compound that blocks a key pathway in harmful gut bacteria and, in mice with IBD, eased infection without disturbing the rest of the microbiome.</media:description>
              <media:credit>Image: Alex Shipps/MIT CSAIL, using assets from the researchers and Pexels</media:credit>
      </media:content>
        <category domain="https://news.mit.edu/topic/research">Research</category>
      <category domain="https://news.mit.edu/topic/antibiotics">Antibiotics</category>
      <category domain="https://news.mit.edu/topic/artificial-intelligence2">Artificial intelligence</category>
      <category domain="https://news.mit.edu/topic/computers">Computer science and technology</category>
      <category domain="https://news.mit.edu/topic/machine-learning">Machine learning</category>
      <category domain="https://news.mit.edu/topic/algorithms">Algorithms</category>
      <category domain="https://news.mit.edu/topic/health">Health sciences and technology</category>
      <category domain="https://news.mit.edu/topic/medicine">Medicine</category>
      <category domain="https://news.mit.edu/topic/health-care">Health care</category>
      <category domain="https://news.mit.edu/topic/drug-development">Drug development</category>
      <category domain="https://news.mit.edu/topic/drug-discovery">Drug discovery</category>
      <category domain="https://news.mit.edu/topic/viruses">Viruses</category>
      <category domain="https://news.mit.edu/topic/bacteria">Bacteria</category>
      <category domain="https://news.mit.edu/topic/disease">Disease</category>
      <category domain="https://news.mit.edu/topic/jameel-clinic">Jameel Clinic</category>
      <category domain="https://news.mit.edu/topic/electrical-engineering-computer-science-eecs">Electrical engineering and computer science (EECS)</category>
      <category domain="https://news.mit.edu/topic/computer-science-and-artificial-intelligence-laboratory-csail">Computer Science and Artificial Intelligence Laboratory (CSAIL)</category>
      <category domain="https://news.mit.edu/topic/school-engineering">School of Engineering</category>
      <category domain="https://news.mit.edu/topic/mit-schwarzman-college-computing">MIT Schwarzman College of Computing</category>
    </item>
<item>
  <title>System lets people personalize online social spaces while staying connected with others</title>
  <link>https://news.mit.edu/2025/system-lets-people-personalize-online-social-spaces-while-staying-connected-1001</link>
  <description>By enabling users to easily create social apps that serve communities’ needs, the Graffiti framework aims to promote healthier online interactions.</description>
  <pubDate>Wed, 01 Oct 2025 10:00:00 -0400</pubDate>
    <guid isPermaLink="true">https://news.mit.edu/2025/system-lets-people-personalize-online-social-spaces-while-staying-connected-1001</guid>
        <dc:creator>Adam Zewe | MIT News</dc:creator>
  <content:encoded>&lt;p&gt;Say a local concert venue wants to engage its community by giving social media followers an easy way to share and comment on new music from emerging artists. Rather than working within the constraints of existing social platforms, the venue might want to create its own social app with the functionality that would be best for its community. But building a new social app from scratch involves many complicated programming steps, and even if the venue can create a customized app, the organization’s followers may be unwilling to join the new platform because it could mean leaving their connections and data behind.&lt;/p&gt;&lt;p&gt;Now, researchers from MIT have launched a framework called &lt;a href="http://graffiti.garden/" target="_blank"&gt;Graffiti&lt;/a&gt; that makes building personalized social applications easier, while allowing users to migrate between multiple applications without losing their friends or data.&lt;/p&gt;&lt;p&gt;“We want to empower people to have control over their own designs rather than having them dictated from the top down,” says electrical engineering and computer science graduate student Theia Henderson.&lt;/p&gt;&lt;p&gt;Henderson and her colleagues designed Graffiti with a flexible structure so individuals have the freedom to create a variety of customized applications, from messenger apps like WhatsApp to microblogging platforms like X to location-based social networking sites like Nextdoor, all using only front-end development tools like HTML.&lt;/p&gt;&lt;p&gt;The protocol ensures all applications can interoperate, so content posted on one application can appear on any other application, even those with disparate designs or functionality. Importantly, Graffiti users retain control of their data, which is stored on a decentralized infrastructure rather than being held by a specific application.&lt;/p&gt;&lt;p&gt;While the pros and cons of implementing Graffiti at scale remain to be fully explored, the researchers hope this new approach can someday lead to healthier online interactions.&lt;/p&gt;&lt;p&gt;“We’ve shown that you can have a rich social ecosystem where everyone owns their own data and can use whatever applications they want to interact with whoever they want in whatever way they want. And they can have their own experiences without losing connection with the people they want to stay connected with,” says David Karger, professor of EECS and a member of the Computer Science and Artificial Intelligence Laboratory (CSAIL).&lt;/p&gt;&lt;p&gt;Henderson, the lead author, and Karger are joined by MIT Research Scientist David D. Clark on a &lt;a href="https://doi.org/10.1145/3746059.3747627" target="_blank"&gt;paper about Graffiti&lt;/a&gt;, which will be presented at the ACM Symposium on User Interface Software and Technology.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Personalized, integrated applications&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;With Graffiti, the researchers had two main goals: to lower the barrier to creating personalized social applications and to enable those personalized applications to interoperate without requiring permission from developers.&lt;/p&gt;&lt;p&gt;To make the design process easier, they built a collective back-end infrastructure that all applications access to store and share content. This means developers don’t need to write any complex server code. Instead, designing a Graffiti application is more like making a website using popular tools like Vue.&lt;/p&gt;&lt;p&gt;Developers can also easily introduce new features and new types of content, giving them more freedom and fostering creativity.&lt;/p&gt;&lt;p&gt;“Graffiti is so straightforward that we used it as the infrastructure for the intro to web design class I teach, and students were able to write the front-end very easily to come up with all sorts of applications,” Karger says.&lt;/p&gt;&lt;p&gt;The open, interoperable nature of Graffiti means no one entity has the power to set a moderation policy for the entire platform. Instead, multiple competing and contradictory moderation services can operate, and people can choose the ones they like.&amp;nbsp;&lt;/p&gt;&lt;p&gt;Graffiti uses the idea of “total reification,” where every action taken in Graffiti, such as liking, sharing, or blocking a post, is represented and stored as its own piece of data. A user can configure their social application to interpret or ignore those data using its own rules.&lt;/p&gt;&lt;p&gt;For instance, if an application is designed so a certain user is a moderator, posts blocked by that user won’t appear in the application. But for an application with different rules where that person isn’t considered a moderator, other users might just see a warning or no flag at all.&lt;/p&gt;&lt;p&gt;“Theia’s system lets each person pick their own moderators, avoiding the one-sized-fits-all approach to moderation taken by the major social platforms,” Karger says.&lt;/p&gt;&lt;p&gt;But at the same time, having no central moderator means there is no one to remove content from the platform that might be offensive or illegal.&lt;/p&gt;&lt;p&gt;“We need to do more research to understand if that is going to provide real, damaging consequences or if the kind of personal moderation we created can provide the protections people need,” he adds.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Empowering social media users&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;The researchers also had to overcome a problem known as context collapse, which conflicts with their goal of interoperation.&lt;/p&gt;&lt;p&gt;For instance, context collapse would occur if a person’s Tinder profile appeared on LinkedIn, or if a post intended for one group, like close friends, would create conflict with another group, such as family members. Context collapse can lead to anxiety and have social repercussions for the user and their different communities.&lt;/p&gt;&lt;p&gt;“We realize that interoperability can sometimes be a bad thing. People have boundaries between different social contexts, and we didn’t want to violate those,” Henderson says.&lt;/p&gt;&lt;p&gt;To avoid context collapse, the researchers designed Graffiti so all content is organized into distinct channels. Channels are flexible and can represent a variety of contexts, such as people, applications, locations, etc.&lt;/p&gt;&lt;p&gt;If a user’s post appears in an application channel but not their personal channel, others using that application will see the post, but those who only follow this user will not.&lt;/p&gt;&lt;p&gt;“Individuals should have the power to choose the audience for whatever they want to say,” Karger adds.&lt;/p&gt;&lt;p&gt;The researchers created multiple Graffiti applications to showcase personalization and interoperability, including a community-specific application for a local concert venue, a text-centric microblogging platform patterned off X, a Wikipedia-like application that enables collective editing, and a real-time messaging app with multiple moderation schemes patterned off WhatsApp and Slack.&lt;/p&gt;&lt;p&gt;“It also leaves room to create so many social applications people haven’t thought of yet. I’m really excited to see what people come up with when they are given full creative freedom,” Henderson says.&lt;/p&gt;&lt;p&gt;In the future, she and her colleagues want to explore additional social applications they could build with Graffiti. They also intend to incorporate tools like graphical editors to simplify the design process. In addition, they want to strengthen Graffiti’s security and privacy.&lt;/p&gt;&lt;p&gt;And while there is still a long way to go before Graffiti could be implemented at scale, the researchers are currently &lt;a href="https://mit.co1.qualtrics.com/jfe/form/SV_9SR8Jp2DX8nOngG" target="_blank"&gt;running a user study&lt;/a&gt; as they explore the potential positive and negative impacts the system could have on the social media landscape.&amp;nbsp;&lt;/p&gt;</content:encoded>
      <media:content url="https://news.mit.edu/sites/default/files/styles/news_article__cover_image__original/public/images/202510/MIT-Interoperable-01-press.jpg?itok=Lp-eZSkT" medium="image" type="image/jpeg" width="390" height="260">
              <media:description type="plain">MIT researchers developed a new system that enables individuals to more easily create customized social applications that can seamlessly interoperate with one another.</media:description>
              <media:credit>Credit: MIT News; iStock</media:credit>
      </media:content>
        <category domain="https://news.mit.edu/topic/research">Research</category>
      <category domain="https://news.mit.edu/topic/social-media">Social media</category>
      <category domain="https://news.mit.edu/topic/computers">Computer science and technology</category>
      <category domain="https://news.mit.edu/topic/technology-society">Technology and society</category>
      <category domain="https://news.mit.edu/topic/human-computer-interaction">Human-computer interaction</category>
      <category domain="https://news.mit.edu/topic/social-networks">Social networks</category>
      <category domain="https://news.mit.edu/topic/data">Data</category>
      <category domain="https://news.mit.edu/topic/computer-science-and-artificial-intelligence-laboratory-csail">Computer Science and Artificial Intelligence Laboratory (CSAIL)</category>
      <category domain="https://news.mit.edu/topic/electrical-engineering-computer-science-eecs">Electrical engineering and computer science (EECS)</category>
      <category domain="https://news.mit.edu/topic/school-engineering">School of Engineering</category>
      <category domain="https://news.mit.edu/topic/mit-schwarzman-college-computing">MIT Schwarzman College of Computing</category>
    </item>
<item>
  <title>Responding to the climate impact of generative AI</title>
  <link>https://news.mit.edu/2025/responding-to-generative-ai-climate-impact-0930</link>
  <description>Explosive growth of AI data centers is expected to increase greenhouse gas emissions. Researchers are now seeking solutions to reduce these environmental harms.</description>
  <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
    <guid isPermaLink="true">https://news.mit.edu/2025/responding-to-generative-ai-climate-impact-0930</guid>
        <dc:creator>Adam Zewe | MIT News</dc:creator>
  <content:encoded>&lt;p&gt;&lt;em&gt;In part 2 of our two-part series on&amp;nbsp;&lt;/em&gt;&lt;a href="https://news.mit.edu/2025/explained-generative-ai-environmental-impact-0117"&gt;&lt;em&gt;generative artificial intelligence’s environmental impacts&lt;/em&gt;&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;MIT News &lt;em&gt;explores some of the ways experts are working to reduce the technology’s carbon footprint.&lt;/em&gt;&lt;/p&gt;&lt;p&gt;The energy demands of&amp;nbsp;&lt;a href="https://news.mit.edu/2023/explained-generative-ai-1109"&gt;generative AI&lt;/a&gt; are expected to continue increasing dramatically over the next decade.&lt;/p&gt;&lt;p&gt;For instance, an April 2025 report from the International Energy Agency predicts that the&amp;nbsp;&lt;a href="https://www.iea.org/news/ai-is-set-to-drive-surging-electricity-demand-from-data-centres-while-offering-the-potential-to-transform-how-the-energy-sector-works"&gt;global electricity demand from data centers&lt;/a&gt;, which house the computing infrastructure to train and deploy AI models, will more than double by 2030, to around 945 terawatt-hours. While not all operations performed in a data center are AI-related, this total amount is slightly more than the energy consumption of Japan.&lt;/p&gt;&lt;p&gt;Moreover, an August 2025 analysis from Goldman Sachs Research forecasts that about 60 percent of the increasing electricity demands from data centers will be met by burning fossil fuels, increasing&amp;nbsp;&lt;a href="https://www.goldmansachs.com/insights/articles/how-ai-is-transforming-data-centers-and-ramping-up-power-demand"&gt;global carbon emissions by about 220 million tons&lt;/a&gt;. In comparison, driving a gas-powered car for 5,000 miles produces about 1 ton of carbon dioxide.&lt;/p&gt;&lt;p&gt;These statistics are staggering, but at the same time, scientists and engineers at MIT and around the world are studying innovations and interventions to mitigate AI’s ballooning carbon footprint, from boosting the efficiency of algorithms to rethinking the design of data centers.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Considering carbon emissions&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Talk of reducing generative AI’s carbon footprint is typically centered on “operational carbon” — the emissions used by the powerful processors, known as GPUs, inside a data center. It often ignores “embodied carbon,” which are emissions created by building the data center in the first place, says Vijay Gadepally, senior scientist at MIT Lincoln Laboratory, who leads research projects in the Lincoln Laboratory Supercomputing Center.&lt;/p&gt;&lt;p&gt;Constructing and retrofitting a data center, built from tons of steel and concrete and filled with air conditioning units, computing hardware, and miles of cable, consumes a huge amount of carbon. In fact, the environmental impact of building data centers is one reason companies like&amp;nbsp;&lt;a href="https://www.constructiondive.com/news/meta-piloting-mass-timber-for-sustainable-data-center-construction-clt-green-building/756919/"&gt;Meta&lt;/a&gt; and&amp;nbsp;&lt;a href="https://www.esgdive.com/news/microsoft-swaps-concrete-steel-with-wood-in-data-centers-to-cut-emissions/731912/"&gt;Google&lt;/a&gt; are exploring more sustainable building materials. (Cost is another factor.)&lt;/p&gt;&lt;p&gt;Plus, data centers are enormous buildings — the world’s largest, the China Telecomm-Inner Mongolia Information Park, engulfs&amp;nbsp;&lt;a href="https://datacentremagazine.com/top10/top-10-biggest-data-centres"&gt;roughly 10 million square feet&lt;/a&gt; — with about 10 to 50 times the energy density of a normal office building, Gadepally adds.&amp;nbsp;&lt;/p&gt;&lt;p&gt;“The operational side is only part of the story. Some things we are working on to reduce operational emissions may lend themselves to reducing embodied carbon, too, but we need to do more on that front in the future,” he says.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Reducing operational carbon emissions&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;When it comes to reducing operational carbon emissions of AI data centers, there are many parallels with home energy-saving measures. For one, we can simply turn down the lights.&lt;/p&gt;&lt;p&gt;“Even if you have the worst lightbulbs in your house from an efficiency standpoint, turning them off or dimming them will always use less energy than leaving them running at full blast,” Gadepally says.&lt;/p&gt;&lt;p&gt;In the same fashion, research from the Supercomputing Center has shown that “turning down” the GPUs in a data center so they consume about &lt;a href="https://news.mit.edu/2025/qa-vijay-gadepally-climate-impact-generative-ai-0113"&gt;three-tenths the energy&lt;/a&gt; has minimal impacts on the performance of AI models, while also making the hardware easier to cool.&lt;/p&gt;&lt;p&gt;Another strategy is to use less energy-intensive computing hardware.&lt;/p&gt;&lt;p&gt;Demanding generative AI workloads, such as training new reasoning models like GPT-5, usually need many GPUs working simultaneously. The Goldman Sachs analysis estimates that a state-of-the-art system could soon have as many as 576 connected GPUs operating at once.&lt;/p&gt;&lt;p&gt;But engineers can sometimes achieve similar results by reducing the precision of computing hardware, perhaps by switching to less powerful processors that have been tuned to handle a specific AI workload.&lt;/p&gt;&lt;p&gt;There are also measures that boost the efficiency of training power-hungry deep-learning models before they are deployed.&lt;/p&gt;&lt;p&gt;Gadepally’s group found that about half the electricity used for training an AI model is spent to get the last 2 or 3 percentage points in accuracy. Stopping the training process early can save a lot of that energy.&lt;/p&gt;&lt;p&gt;“There might be cases where 70 percent accuracy is good enough for one particular application, like a recommender system for e-commerce,” he says.&lt;/p&gt;&lt;p&gt;Researchers can also take advantage of efficiency-boosting measures.&lt;/p&gt;&lt;p&gt;For instance, a postdoc in the Supercomputing Center realized the group might run a thousand simulations during the training process to pick the two or three best AI models for their project.&lt;/p&gt;&lt;p&gt;By building a tool that allowed them to avoid about 80 percent of those wasted computing cycles, they dramatically reduced the energy demands of training with no reduction in model accuracy, Gadepally says.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Leveraging efficiency improvements&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Constant innovation in computing hardware, such as denser arrays of transistors on semiconductor chips, is still enabling dramatic improvements in the energy efficiency of AI models.&lt;/p&gt;&lt;p&gt;Even though energy efficiency improvements have been slowing for most chips since about 2005, the amount of computation that GPUs can do per joule of energy has been improving by 50 to 60 percent each year, says Neil Thompson, director of the FutureTech Research Project at MIT’s Computer Science and Artificial Intelligence Laboratory and a principal investigator at MIT’s Initiative on the Digital Economy.&lt;/p&gt;&lt;p&gt;“The still-ongoing ‘Moore’s Law’ trend of getting more and more transistors on chip still matters for a lot of these AI systems, since running operations in parallel is still very valuable for improving efficiency,” says Thomspon.&lt;/p&gt;&lt;p&gt;Even more significant, his group’s research indicates that efficiency gains from new model architectures that can solve complex problems faster, consuming less energy to achieve the same or better results, is doubling every eight or nine months.&lt;/p&gt;&lt;p&gt;Thompson coined the term “&lt;a href="https://ide.mit.edu/insights/the-importance-of-aigorithm-efficiency-can-nflops-hold-the-key-to-efficiency-gains/"&gt;negaflop&lt;/a&gt;” to describe this effect. The same way a “negawatt” represents electricity saved due to energy-saving measures, a “negaflop” is a computing operation that doesn’t need to be performed due to algorithmic improvements.&lt;/p&gt;&lt;p&gt;These could be things like “&lt;a href="https://news.mit.edu/2023/new-techniques-efficiently-accelerate-sparse-tensors-1030"&gt;pruning&lt;/a&gt;” away unnecessary components of a neural network or employing&amp;nbsp;&lt;a href="https://news.mit.edu/2022/machine-learning-edge-microcontroller-1004"&gt;compression techniques&lt;/a&gt; that enable users to do more with less computation.&lt;/p&gt;&lt;p&gt;“If you need to use a really powerful model today to complete your task, in just a few years, you might be able to use a significantly smaller model to do the same thing, which would carry much less environmental burden. Making these models more efficient is the single-most important thing you can do to reduce the environmental costs of AI,” Thompson says.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Maximizing energy savings&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;While reducing the overall energy use of AI algorithms and computing hardware will cut greenhouse gas emissions, not all energy is the same, Gadepally adds.&lt;/p&gt;&lt;p&gt;“The amount of carbon emissions in 1 kilowatt hour varies quite significantly, even just during the day, as well as over the month and year,” he says.&lt;/p&gt;&lt;p&gt;Engineers can take advantage of these variations by leveraging the flexibility of AI workloads and data center operations to maximize emissions reductions. For instance, some generative AI workloads don’t need to be performed in their entirety at the same time.&lt;/p&gt;&lt;p&gt;Splitting computing operations so some are performed later, when more of the electricity fed into the grid is from renewable sources like solar and wind, can go a long way toward reducing a data center’s carbon footprint, says Deepjyoti Deka, a research scientist in the MIT Energy Initiative.&lt;/p&gt;&lt;p&gt;Deka and his team are also studying “smarter” data centers where the AI workloads of multiple companies using the same computing equipment are flexibly adjusted to improve energy efficiency.&lt;/p&gt;&lt;p&gt;“By looking at the system as a whole, our hope is to minimize energy use as well as dependence on fossil fuels, while still maintaining reliability standards for AI companies and users,” Deka says.&lt;/p&gt;&lt;p&gt;He and others at MITEI are building a flexibility model of a data center that considers the differing energy demands of training a deep-learning model versus deploying that model. Their hope is to uncover the best strategies for scheduling and streamlining computing operations to improve energy efficiency.&lt;/p&gt;&lt;p&gt;The researchers are also exploring the use of long-duration energy storage units at data centers, which store excess energy for times when it is needed.&lt;/p&gt;&lt;p&gt;With these systems in place, a data center could use stored energy that was generated by renewable sources during a high-demand period, or avoid the use of diesel backup generators if there are fluctuations in the grid.&lt;/p&gt;&lt;p&gt;“Long-duration energy storage could be a game-changer here because we can design operations that really change the emission mix of the system to rely more on renewable energy,” Deka says.&lt;/p&gt;&lt;p&gt;In addition, researchers at MIT and Princeton University are developing a software tool for investment planning in the power sector, called&amp;nbsp;&lt;a href="https://energy.mit.edu/genx/"&gt;GenX&lt;/a&gt;, which could be used to help companies determine the ideal place to locate a data center to minimize environmental impacts and costs.&lt;/p&gt;&lt;p&gt;Location can have a big impact on reducing a data center’s carbon footprint. For instance, Meta operates a&amp;nbsp;&lt;a href="https://datacenters.atmeta.com/wp-content/uploads/2025/02/Meta_s-Lulea-Data-Center.pdf"&gt;data center in Lulea&lt;/a&gt;, a city on the coast of northern Sweden where cooler temperatures reduce the amount of electricity needed to cool computing hardware.&lt;/p&gt;&lt;p&gt;Thinking farther outside the box (way farther), some governments are even exploring the construction of&amp;nbsp;&lt;a href="https://spectrum.ieee.org/data-center-on-the-moon"&gt;data centers on the moon&lt;/a&gt; where they could potentially be operated with nearly all renewable energy.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;AI-based solutions&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Currently, the expansion of renewable energy generation here on Earth isn’t keeping pace with the rapid growth of AI, which is one major roadblock to reducing its carbon footprint, says Jennifer Turliuk MBA ’25, a short-term lecturer, former Sloan Fellow, and former practice leader of climate and energy AI at the Martin Trust Center for MIT Entrepreneurship.&lt;/p&gt;&lt;p&gt;The local, state, and federal review processes required for a new renewable energy projects can take years.&lt;/p&gt;&lt;p&gt;Researchers at MIT and elsewhere are exploring the use of AI to speed up the process of connecting new renewable energy systems to the power grid.&lt;/p&gt;&lt;p&gt;For instance, a generative AI model could streamline interconnection studies that determine how a new project will impact the power grid, a step that often takes years to complete.&lt;/p&gt;&lt;p&gt;And when it comes to &lt;a href="https://dl.acm.org/doi/10.1145/3485128"&gt;accelerating the development and implementation of clean energy technologies&lt;/a&gt;, AI could play a major role.&lt;/p&gt;&lt;p&gt;“Machine learning is great for tackling complex situations, and the electrical grid is said to be one of the largest and most complex machines in the world,” Turliuk adds.&lt;/p&gt;&lt;p&gt;For instance, AI could help optimize the prediction of solar and wind energy generation or identify ideal locations for new facilities.&lt;/p&gt;&lt;p&gt;It could also be used to perform predictive maintenance and fault detection for solar panels or other green energy infrastructure, or to monitor the capacity of transmission wires to maximize efficiency.&lt;/p&gt;&lt;p&gt;By helping researchers gather and analyze huge amounts of data, AI could also inform targeted policy interventions aimed at getting the biggest “bang for the buck” from areas such as renewable energy, Turliuk says.&lt;/p&gt;&lt;p&gt;To help policymakers, scientists, and enterprises consider the multifaceted costs and benefits of AI systems, she and her collaborators developed the Net Climate Impact Score.&lt;/p&gt;&lt;p&gt;The score is a framework that can be used to help determine the net climate impact of AI projects, considering emissions and other environmental costs along with potential environmental benefits in the future.&lt;/p&gt;&lt;p&gt;At the end of the day, the most effective solutions will likely result from collaborations among companies, regulators, and researchers, with academia leading the way, Turliuk adds.&lt;/p&gt;&lt;p&gt;“Every day counts. We are on a path where the effects of climate change won’t be fully known until it is too late to do anything about it. This is a once-in-a-lifetime opportunity to innovate and make AI systems less carbon-intense,” she says.&lt;/p&gt;</content:encoded>
      <media:content url="https://news.mit.edu/sites/default/files/styles/news_article__cover_image__original/public/images/202509/MIT_AI-Climate-01.jpg?itok=--5jq5Ns" medium="image" type="image/jpeg" width="390" height="260">
              <media:description type="plain">“We are on a path where the effects of climate change won’t be fully known until it is too late to do anything about it,” says Jennifer Turliuk MBA ’25, who is working to help policymakers, scientists, and enterprises consider the multifaceted costs and benefits of generative AI. “This is a once-in-a-lifetime opportunity to innovate and make AI systems less carbon-intense.”</media:description>
              <media:credit>Credit: iStock</media:credit>
      </media:content>
        <category domain="https://news.mit.edu/topic/sustainable-computing">Sustainable computing</category>
      <category domain="https://news.mit.edu/topic/research">Research</category>
      <category domain="https://news.mit.edu/topic/computers">Computer science and technology</category>
      <category domain="https://news.mit.edu/topic/artificial-intelligence2">Artificial intelligence</category>
      <category domain="https://news.mit.edu/topic/machine-learning">Machine learning</category>
      <category domain="https://news.mit.edu/topic/algorithms">Algorithms</category>
      <category domain="https://news.mit.edu/topic/energy">Energy</category>
      <category domain="https://news.mit.edu/topic/climate-change">Climate change</category>
      <category domain="https://news.mit.edu/topic/supercomputing">Supercomputing</category>
      <category domain="https://news.mit.edu/topic/energy-storage">Energy storage</category>
      <category domain="https://news.mit.edu/topic/emissions">Emissions</category>
      <category domain="https://news.mit.edu/topic/infrastructure">Infrastructure</category>
      <category domain="https://news.mit.edu/topic/electrical-engineering-computer-science-eecs">Electrical engineering and computer science (EECS)</category>
      <category domain="https://news.mit.edu/topic/mit-energy-initiative">MIT Energy Initiative</category>
      <category domain="https://news.mit.edu/topic/lincoln-laboratory-0">Lincoln Laboratory</category>
      <category domain="https://news.mit.edu/topic/computer-science-and-artificial-intelligence-laboratory-csail">Computer Science and Artificial Intelligence Laboratory (CSAIL)</category>
      <category domain="https://news.mit.edu/topic/school-engineering">School of Engineering</category>
      <category domain="https://news.mit.edu/topic/mit-sloan-school-management">MIT Sloan School of Management</category>
      <category domain="https://news.mit.edu/topic/mit-schwarzman-college-computing">MIT Schwarzman College of Computing</category>
    </item>
<item>
  <title>New AI system could accelerate clinical research</title>
  <link>https://news.mit.edu/2025/new-ai-system-could-accelerate-clinical-research-0925</link>
  <description>By enabling rapid annotation of areas of interest in medical images, the tool can help scientists study new treatments or map disease progression.</description>
  <pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
    <guid isPermaLink="true">https://news.mit.edu/2025/new-ai-system-could-accelerate-clinical-research-0925</guid>
        <dc:creator>Adam Zewe | MIT News</dc:creator>
  <content:encoded>&lt;p&gt;Annotating regions of interest in medical images, a process known as segmentation, is often one of the first steps clinical researchers take when running a new study involving biomedical images.&lt;/p&gt;&lt;p&gt;For instance, to determine how the size of the brain’s hippocampus changes as patients age, the scientist first outlines each hippocampus in a series of brain scans. For many structures and image types, this is often a manual process that can be extremely time-consuming, especially if the regions being studied are challenging to delineate.&lt;/p&gt;&lt;p&gt;To streamline the process, MIT researchers developed an artificial intelligence-based system that enables a researcher to rapidly segment new biomedical imaging datasets by clicking, scribbling, and drawing boxes on the images. This new AI model uses these interactions to predict the segmentation.&lt;/p&gt;&lt;p&gt;As the user marks additional images, the number of interactions they need to perform decreases, eventually dropping to zero. The model can then segment each new image accurately without user input.&lt;/p&gt;&lt;p&gt;It can do this because the model’s architecture has been specially designed to use information from images it has already segmented to make new predictions.&lt;/p&gt;&lt;p&gt;Unlike other medical image segmentation models, this system allows the user to segment an entire dataset without repeating their work for each image.&lt;/p&gt;&lt;p&gt;In addition, the interactive tool does not require a presegmented image dataset for training, so users don’t need machine-learning expertise or extensive computational resources. They can use the system for a new segmentation task without retraining the model.&lt;/p&gt;&lt;p&gt;In the long run, this tool could accelerate studies of new treatment methods and reduce the cost of clinical trials and medical research. It could also be used by physicians to improve the efficiency of clinical applications, such as radiation treatment planning.&lt;/p&gt;&lt;p&gt;“Many scientists might only have time to segment a few images per day for their research because manual image segmentation is so time-consuming. Our hope is that this system will enable new science by allowing clinical researchers to conduct studies they were prohibited from doing before because of the lack of an efficient tool,” says Hallee Wong, an electrical engineering and computer science graduate student and lead author of a &lt;a href="https://arxiv.org/pdf/2412.15058" target="_blank"&gt;paper on this new tool&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;She is joined on the paper by Jose Javier Gonzalez Ortiz PhD ’24; John Guttag, the Dugald C. Jackson Professor of Computer Science and Electrical Engineering; and senior author Adrian Dalca, an assistant professor at Harvard Medical School and MGH, and a research scientist in the MIT Computer Science and Artificial Intelligence Laboratory (CSAIL). The research will be presented at the International Conference on Computer Vision.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Streamlining segmentation&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;There are primarily two methods researchers use to segment new sets of medical images. With interactive segmentation, they input an image into an AI system and use an interface to mark areas of interest. The model predicts the segmentation based on those interactions.&lt;/p&gt;&lt;p&gt;A tool previously developed by the MIT researchers,&amp;nbsp;&lt;a href="https://news.mit.edu/2024/scribbleprompt-helping-doctors-annotate-medical-scans-0909" target="_blank"&gt;ScribblePrompt&lt;/a&gt;, allows users to do this, but they must repeat the process for each new image.&lt;/p&gt;&lt;p&gt;Another approach is to develop a task-specific AI model to automatically segment the images. This approach requires the user to manually segment hundreds of images to create a dataset, and then train a machine-learning model. That model predicts the segmentation for a new image. But the user must start the complex, machine-learning-based process from scratch for each new task, and there is no way to correct the model if it makes a mistake.&lt;/p&gt;&lt;p&gt;This new system,&amp;nbsp;&lt;a href="https://multiverseg.csail.mit.edu/" target="_blank"&gt;MultiverSeg&lt;/a&gt;, combines the best of each approach. It predicts a segmentation for a new image based on user interactions, like scribbles, but also keeps each segmented image in a context set that it refers to later.&lt;/p&gt;&lt;p&gt;When the user uploads a new image and marks areas of interest, the model draws on the examples in its context set to make a more accurate prediction, with less user input.&lt;/p&gt;&lt;p&gt;The researchers designed the model’s architecture to use a context set of any size, so&amp;nbsp;the user doesn’t need to have a certain number of images. This gives MultiverSeg the flexibility to be used in a range of applications.&lt;/p&gt;&lt;p&gt;“At some point, for many tasks, you shouldn’t need to provide any interactions. If you have enough examples in the context set, the model can accurately predict the segmentation on its own,” Wong says.&lt;/p&gt;&lt;p&gt;The researchers carefully engineered and trained the model on a diverse collection of biomedical imaging data to ensure it had the ability to incrementally improve its predictions based on user input.&lt;/p&gt;&lt;p&gt;The user doesn’t need to retrain or customize the model for their data. To use MultiverSeg for a new task, one can upload a new medical image and start marking it.&lt;/p&gt;&lt;p&gt;When the researchers compared MultiverSeg to state-of-the-art tools for in-context and interactive image segmentation, it outperformed each baseline.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Fewer clicks, better results&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Unlike these other tools, MultiverSeg requires less user input with each image. By the ninth new image, it needed only two clicks from the user to generate a segmentation more accurate than a model designed specifically for the task.&lt;/p&gt;&lt;p&gt;For some image types, like X-rays, the user might only need to segment one or two images manually before the model becomes accurate enough to make predictions on its own.&lt;/p&gt;&lt;p&gt;The tool’s interactivity also enables the user to make corrections to the model’s prediction, iterating until it reaches the desired level of accuracy. Compared to the researchers’ previous system, MultiverSeg reached 90 percent accuracy with roughly 2/3 the number of scribbles and 3/4 the number of clicks.&lt;/p&gt;&lt;p&gt;“With MultiverSeg, users can always provide more interactions to refine the AI predictions. This still dramatically accelerates the process because it is usually faster to correct something that exists than to start from scratch,” Wong says.&lt;/p&gt;&lt;p&gt;Moving forward, the researchers want to test this tool in real-world situations with clinical collaborators and improve it based on user feedback. They also want to enable MultiverSeg to segment 3D biomedical images.&lt;/p&gt;&lt;p&gt;This work is supported, in part, by Quanta Computer, Inc. and the National Institutes of Health, with hardware support from the Massachusetts Life Sciences Center.&lt;/p&gt;</content:encoded>
      <media:content url="https://news.mit.edu/sites/default/files/styles/news_article__cover_image__original/public/images/202509/MIT-Scalable-Segmentation-01.jpg?itok=JP7l2Ede" medium="image" type="image/jpeg" width="390" height="260">
              <media:description type="plain">MIT researchers have developed a new AI-based tool that rapidly annotates areas of interest in medical images and can help in the study of new treatments or map disease progression.</media:description>
              <media:credit>Credit: iStock</media:credit>
      </media:content>
        <category domain="https://news.mit.edu/topic/research">Research</category>
      <category domain="https://news.mit.edu/topic/computers">Computer science and technology</category>
      <category domain="https://news.mit.edu/topic/artificial-intelligence2">Artificial intelligence</category>
      <category domain="https://news.mit.edu/topic/machine-learning">Machine learning</category>
      <category domain="https://news.mit.edu/topic/imaging">Imaging</category>
      <category domain="https://news.mit.edu/topic/health-care">Health care</category>
      <category domain="https://news.mit.edu/topic/health2">Health</category>
      <category domain="https://news.mit.edu/topic/medicine">Medicine</category>
      <category domain="https://news.mit.edu/topic/computer-science-and-artificial-intelligence-laboratory-csail">Computer Science and Artificial Intelligence Laboratory (CSAIL)</category>
      <category domain="https://news.mit.edu/topic/electrical-engineering-computer-science-eecs">Electrical engineering and computer science (EECS)</category>
      <category domain="https://news.mit.edu/topic/school-engineering">School of Engineering</category>
      <category domain="https://news.mit.edu/topic/mit-schwarzman-college-computing">MIT Schwarzman College of Computing</category>
      <category domain="https://news.mit.edu/topic/broad-institute">Broad Institute</category>
      <category domain="https://news.mit.edu/topic/nih">National Institutes of Health (NIH)</category>
    </item>
<item>
  <title>By attracting the world’s sharpest talent, MIT helps keep the US a step ahead </title>
  <link>https://news.mit.edu/2025/attracting-worlds-sharpest-talent-mit-helps-keep-us-step-ahead-0924</link>
  <description>MIT is a global community whose international engagement bestows benefits well beyond the Cambridge campus.</description>
  <pubDate>Wed, 24 Sep 2025 11:55:00 -0400</pubDate>
    <guid isPermaLink="true">https://news.mit.edu/2025/attracting-worlds-sharpest-talent-mit-helps-keep-us-step-ahead-0924</guid>
        <dc:creator>Lisa Capone | Office of the Vice Provost for International Activities</dc:creator>
  <content:encoded>&lt;p&gt;Just as the United States has prospered through its ability to draw talent from every corner of the globe, so too has MIT thrived as a magnet for the world’s most keen and curious minds — many of whom remain here to invent solutions, create companies, and teach future leaders, contributing to America’s success.&lt;/p&gt;&lt;p&gt;President Ronald Reagan remarked in 1989 that the United States leads the world “because, unique among nations, we draw our people — our strength — from every country and every corner of the world.&amp;nbsp;And by doing so we continuously renew and enrich our nation.” Those words ring still ring true 36 years later — and the sentiment resonates especially at MIT.&lt;/p&gt;&lt;p&gt;"To find people with&amp;nbsp;the drive, skill, and daring to see, discover, and invent things no one else can, we open ourselves to talent from every corner of the United States and from around the globe,” says MIT President Sally Kornbluth. “MIT is an American university, proudly so — but we would be gravely diminished without the students and scholars who join us from other nations."&lt;/p&gt;&lt;p&gt;MIT’s steadfast commitment to attracting the best and brightest talent from around the world has contributed to not just its own success, but also that of the nation as whole. MIT’s stature as an international hub of education and innovation adds value to the U.S. economy and competitiveness in myriad ways — from foreign-born faculty delivering breakthroughs here and founding American companies that create American jobs to international students contributing over $264 million annually to the U.S. economy during the 2023-24 school year.&lt;/p&gt;&lt;p&gt;Highlighting the extent and value of its global character, the Office of the Vice Provost for International Activities recently expanded a new video series, “&lt;a href="https://global.mit.edu/theworldatmit/"&gt;The World at MIT&lt;/a&gt;.” In it, 20 faculty members born outside the United States tell how they dreamed of coming to MIT while growing up abroad and eventually joined the MIT faculty, where they’ve helped establish and maintain global leadership in science while teaching the next generation of innovators.&amp;nbsp;A common thread running through their stories is the importance of the campus’s distinct nature as a community that is both profoundly American and deeply connected to the people, institutions, and concerns of regions and nations around the globe.&lt;/p&gt;&lt;p&gt;Joining the MIT faculty in 1980, MIT President Emeritus L. Rafael Reif knew almost instantly that he would stay.&lt;/p&gt;&lt;p&gt;“I was impressed by the richness of the variety of groups of people and cultures here,” says Reif, who moved to the United States from Venezuela and eventually served as MIT’s president from 2012 to 2022. “There is no richer place than MIT, because every point of view is here. That is what makes the place so special.”&lt;/p&gt;&lt;p&gt;The benefits of welcoming international students and researchers to campus extend well beyond MIT. More than 17,000 MIT alumni born elsewhere now call the United States home, for example, and many have founded U.S.-based companies that have generated billions of dollars in economic activity.&lt;/p&gt;&lt;p&gt;Contributing to America’s prestige internationally, one-third of MIT’s 104 Nobel laureates&amp;nbsp;— including seven of the eight&amp;nbsp;Nobel winners over the last decade — were born abroad. Drawn to MIT, they went on to make their breakthroughs in the United States. Among them is Lester Wolfe Professor of Chemistry Moungi Bawendi, who won the Nobel Prize in Chemistry in 2023 for his work in the chemical production of high-quality quantum dots.&amp;nbsp; &amp;nbsp;&lt;/p&gt;&lt;p&gt;“MIT is a great environment. It’s very collegial, very collaborative. As a result, we also have amazing students,” says Bawendi, who lived in France and Tunisia as a child before moving to the U.S. “I couldn’t have done my first three years here, which eventually got me a Nobel Prize, without having really bold, smart, adventurous graduate students.”&lt;/p&gt;&lt;p&gt;The give-and-take among MIT faculty and students also inspires electrical engineering and computer science professor Akintunde Ibitayo (Tayo) Akinwande, who grew up in Nigeria.&lt;/p&gt;&lt;p&gt;“Anytime I teach a class, I always learn something from my students’ probing questions,” Akinwande says. “It gives me new insights sometimes, and that’s always the kind of environment I like — where I’m learning something new all the time.”&lt;/p&gt;&lt;p&gt;MIT’s global vibe inspires its students to not only explore worlds of ideas in campus labs and classrooms, but to journey the world itself. Forty-three percent of undergraduates pursued international experiences during the last academic year — taking courses at foreign universities, conducting research, or interning at multinational companies. MIT students and faculty alike are regularly engaged in research outside the United States, addressing some of the world’s toughest challenges and devising solutions that can be deployed back home, as well as abroad. In so doing, they embody MIT’s motto of “mens et manus”&lt;em&gt;&amp;nbsp;&lt;/em&gt;(“mind and hand”), reflecting the educational ideals of MIT’s founders who promoted education for practical application.&lt;/p&gt;&lt;p&gt;As someone who loves exploring “lofty questions” along with the practical design of things, Nergis Mavalvala found a perfect fit at MIT and calls her position as the Marble Professor of Astrophysics and dean of the School of Science “the best job in the world.”&lt;/p&gt;&lt;p&gt;“Everybody here wants to make the world a better place and are using their intellectual gifts and their education to do so,” says Mavalvala, who emigrated from Pakistan. “And I think that’s an amazing community to be part of.”&lt;/p&gt;&lt;p&gt;Daniela Rus agrees. Now the Andrew and Erna Viterbi Professor of Electrical Engineering and Computer Science and director of MIT’s Computer Science and Artificial Intelligence Laboratory, Rus was drawn to the practical application of mathematics while still a student in her native Romania.&amp;nbsp; &amp;nbsp;&lt;/p&gt;&lt;p&gt;“And so, now here I am at MIT, essentially bringing together the world of science and math with the world of making things,” Rus says. “I’ve been here for two decades, and it’s been an extraordinary journey.”&lt;/p&gt;&lt;p&gt;The daughter of an Albert Einstein afficionado, Yukiko Yamashita grew up in Japan thinking of science not as a job, but a calling. MIT, where she is a professor of biology, is a place where people “are really open to unconventional ideas” and “intellectual freedom” thrives.&lt;/p&gt;&lt;p&gt;“There is something sacred about doing science. That’s how I grew up,” Yamashita says. “There are some distinct MIT characteristics. In a good way, people can’t let go. Every day, I am creating more mystery than I answer.”&lt;/p&gt;&lt;p&gt;For more about the paths that brought Yamashita and others to MIT and stories of how their disparate personal histories enrich the campus and wider community, visit the&amp;nbsp;&lt;a href="https://global.mit.edu/theworldatmit/"&gt;“World at MIT”&lt;/a&gt; videos website.&lt;/p&gt;&lt;p&gt;“Our global community’s multiplicity of ideas, experiences, and perspectives contributes enormously to MIT’s innovative and entrepreneurial spirit and, by extension, to the innovation and competitiveness of the U.S.,” says Vice Provost for International Activities Duane Boning, whose department developed the video series. “The bottom line is that both MIT and the U.S. grow stronger when we harness the talents of the world’s best and brightest.”&lt;/p&gt;</content:encoded>
      <media:content url="https://news.mit.edu/sites/default/files/styles/news_article__cover_image__original/public/images/202509/Beige-Aesthetic-New-Year-Vision-Board-Collage%20.jpg?itok=pORoJ6NB" medium="image" type="image/jpeg" width="390" height="260">
              <media:description type="plain">Collage of MIT faculty members</media:description>
              <media:credit>Photos: Bearwalk Cinema</media:credit>
      </media:content>
        <category domain="https://news.mit.edu/topic/global">Global</category>
      <category domain="https://news.mit.edu/topic/faculty">Faculty</category>
      <category domain="https://news.mit.edu/topic/community">Community</category>
      <category domain="https://news.mit.edu/topic/nobels">Nobel Prizes</category>
      <category domain="https://news.mit.edu/topic/immigration">Immigration</category>
      <category domain="https://news.mit.edu/topic/international-relations">International relations</category>
      <category domain="https://news.mit.edu/topic/computer-science-and-artificial-intelligence-laboratory-csail">Computer Science and Artificial Intelligence Laboratory (CSAIL)</category>
      <category domain="https://news.mit.edu/topic/school-architecture-and-planning">School of Architecture and Planning</category>
      <category domain="https://news.mit.edu/topic/school-engineering">School of Engineering</category>
      <category domain="https://news.mit.edu/topic/school-science">School of Science</category>
      <category domain="https://news.mit.edu/topic/mit-sloan-school-management">MIT Sloan School of Management</category>
      <category domain="https://news.mit.edu/topic/mit-schwarzman-college-computing">MIT Schwarzman College of Computing</category>
      <category domain="https://news.mit.edu/topic/school-humanities-arts-and-social-sciences">School of Humanities Arts and Social Sciences</category>
    </item>
<item>
  <title>What does the future hold for generative AI?</title>
  <link>https://news.mit.edu/2025/what-does-future-hold-generative-ai-0919</link>
  <description>At the inaugural MIT Generative AI Impact Consortium Symposium, researchers and business leaders discussed potential advancements centered on this powerful technology.</description>
  <pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
    <guid isPermaLink="true">https://news.mit.edu/2025/what-does-future-hold-generative-ai-0919</guid>
        <dc:creator>Adam Zewe | MIT News</dc:creator>
  <content:encoded>&lt;p&gt;When OpenAI introduced ChatGPT to the world in 2022, it brought generative artificial intelligence into the mainstream and started a snowball effect that led to its rapid integration into industry, scientific research, health care, and the everyday lives of people who use the technology.&lt;/p&gt;&lt;p&gt;What comes next for this powerful but imperfect tool?&lt;/p&gt;&lt;p&gt;With that question in mind, hundreds of researchers, business leaders, educators, and students gathered at MIT’s Kresge Auditorium for the inaugural MIT Generative AI Impact Consortium (MGAIC) Symposium on Sept. 17 to share insights and discuss the potential future of generative AI.&lt;/p&gt;&lt;p&gt;“This is a pivotal moment — generative AI is moving fast. It is our job to make sure that, as the technology keeps advancing, our collective wisdom keeps pace,” said MIT Provost Anantha Chandrakasan to kick off this first symposium of the MGAIC, a&amp;nbsp;&lt;a href="https://news.mit.edu/2025/introducing-mit-generative-ai-impact-consortium-0203" target="_blank"&gt;consortium&lt;/a&gt; of industry leaders and MIT researchers launched in February to harness the power of generative AI for the good of society.&lt;/p&gt;&lt;p&gt;Underscoring the critical need for this collaborative effort, MIT President Sally Kornbluth said that the world is counting on faculty, researchers, and business leaders like those in MGAIC to tackle the technological and ethical challenges of generative AI as the technology advances.&lt;/p&gt;&lt;p&gt;“Part of MIT’s responsibility is to keep these advances coming for the world. … How can we manage the magic [of generative AI] so that all of us can confidently rely on it for critical applications in the real world?” Kornbluth said.&lt;/p&gt;&lt;p&gt;To keynote speaker Yann LeCun, chief AI scientist at Meta, the most exciting and significant advances in generative AI will most likely not come from continued improvements or expansions of large language models like Llama, GPT, and Claude. Through training, these enormous generative models learn patterns in huge datasets to produce new outputs.&lt;/p&gt;&lt;p&gt;Instead, LuCun and others are working on the development of “world models” that learn the same way an infant does — by seeing and interacting with the world around them through sensory input.&lt;/p&gt;&lt;p&gt;“A 4-year-old has seen as much data through vision as the largest LLM. … The world model is going to become the key component of future AI systems,” he said.&lt;/p&gt;&lt;p&gt;A robot with this type of world model could learn to complete a new task on its own with no training. LeCun sees world models as the best approach for companies to make robots smart enough to be generally useful in the real world.&lt;/p&gt;&lt;p&gt;But even if future generative AI systems do get smarter and more human-like through the incorporation of world models, LeCun doesn’t worry about robots escaping from human control.&lt;/p&gt;&lt;p&gt;Scientists and engineers will need to design guardrails to keep future AI systems on track, but as a society, we have already been doing this for millennia by designing rules to align human behavior with the common good, he said.&lt;/p&gt;&lt;p&gt;“We are going to have to design these guardrails, but by construction, the system will not be able to escape those guardrails,” LeCun said.&lt;/p&gt;&lt;p&gt;Keynote speaker Tye Brady, chief technologist at Amazon Robotics, also discussed how generative AI could impact the future of robotics.&lt;/p&gt;&lt;p&gt;For instance, Amazon has already incorporated generative AI technology into many of its warehouses to optimize how robots travel and move material to streamline order processing.&lt;/p&gt;&lt;p&gt;He expects many future innovations will focus on the use of generative AI in collaborative robotics by building machines that allow humans to become more efficient.&lt;/p&gt;&lt;p&gt;“GenAI is probably the most impactful technology I have witnessed throughout my whole robotics career,” he said.&lt;/p&gt;&lt;p&gt;Other presenters and panelists discussed the impacts of generative AI in businesses, from largescale enterprises like Coca-Cola and Analog Devices to startups like health care AI company Abridge.&lt;/p&gt;&lt;p&gt;Several MIT faculty members also spoke about their latest research projects, including the use of AI to reduce noise in ecological image data, designing new AI systems that mitigate bias and hallucinations, and enabling LLMs to learn more about the visual world.&lt;/p&gt;&lt;p&gt;After a day spent exploring new generative AI technology and discussing its implications for the future, MGAIC faculty co-lead Vivek Farias, the Patrick J. McGovern Professor at MIT Sloan School of Management, said he hoped attendees left with “a sense of possibility, and urgency to make that possibility real.”&lt;/p&gt;</content:encoded>
      <media:content url="https://news.mit.edu/sites/default/files/styles/news_article__cover_image__original/public/images/202509/MIT_MGAIC_01-press.jpg?itok=RAAJ5NOt" medium="image" type="image/jpeg" width="390" height="260">
              <media:description type="plain">MIT President Sally Kornbluth said that the world is counting on faculty, researchers, and business leaders like those in MGAIC to tackle the technological and ethical challenges of generative AI as the technology advances.</media:description>
              <media:credit>Credit: Gretchen Ertl</media:credit>
      </media:content>
        <category domain="https://news.mit.edu/topic/special-events">Special events and guest speakers</category>
      <category domain="https://news.mit.edu/topic/computers">Computer science and technology</category>
      <category domain="https://news.mit.edu/topic/artificial-intelligence2">Artificial intelligence</category>
      <category domain="https://news.mit.edu/topic/machine-learning">Machine learning</category>
      <category domain="https://news.mit.edu/topic/research">Research</category>
      <category domain="https://news.mit.edu/topic/technology-society">Technology and society</category>
      <category domain="https://news.mit.edu/topic/robotics">Robotics</category>
      <category domain="https://news.mit.edu/topic/industry">Industry</category>
      <category domain="https://news.mit.edu/topic/collaboration">Collaboration</category>
      <category domain="https://news.mit.edu/topic/electrical-engineering-computer-science-eecs">Electrical engineering and computer science (EECS)</category>
      <category domain="https://news.mit.edu/topic/computer-science-and-artificial-intelligence-laboratory-csail">Computer Science and Artificial Intelligence Laboratory (CSAIL)</category>
      <category domain="https://news.mit.edu/topic/school-architecture-and-planning">School of Architecture and Planning</category>
      <category domain="https://news.mit.edu/topic/school-engineering">School of Engineering</category>
      <category domain="https://news.mit.edu/topic/school-science">School of Science</category>
      <category domain="https://news.mit.edu/topic/school-humanities-arts-and-social-sciences">School of Humanities Arts and Social Sciences</category>
      <category domain="https://news.mit.edu/topic/mit-sloan-school-management">MIT Sloan School of Management</category>
      <category domain="https://news.mit.edu/topic/mit-schwarzman-college-computing">MIT Schwarzman College of Computing</category>
      <category domain="https://news.mit.edu/topic/president-sally-kornbluth">President Sally Kornbluth</category>
      <category domain="https://news.mit.edu/topic/provost">Provost</category>
    </item>
<item>
  <title>How to build AI scaling laws for efficient LLM training and budget maximization</title>
  <link>https://news.mit.edu/2025/how-build-ai-scaling-laws-efficient-llm-training-budget-maximization-0916</link>
  <description>MIT-IBM Watson AI Lab researchers have developed a universal guide for estimating how large language models will perform based on smaller models in the same family.</description>
  <pubDate>Tue, 16 Sep 2025 11:00:00 -0400</pubDate>
    <guid isPermaLink="true">https://news.mit.edu/2025/how-build-ai-scaling-laws-efficient-llm-training-budget-maximization-0916</guid>
        <dc:creator>Lauren Hinkel | MIT-IBM Watson AI Lab</dc:creator>
  <content:encoded>&lt;p&gt;When researchers are building large language models (LLMs), they aim to maximize performance under a particular computational and financial budget. Since training a model can amount to millions of dollars, developers need to be judicious with cost-impacting decisions about, for instance, the model architecture, optimizers, and training datasets before committing to a model. To anticipate the quality and accuracy of a large model’s predictions, practitioners often turn to scaling laws: using smaller, cheaper models to try to approximate the performance of a much larger target model. The challenge, however, is that there are thousands of ways to create a scaling law.&lt;/p&gt;&lt;p&gt;New work from MIT and MIT-IBM Watson AI Lab researchers addresses this by amassing and releasing a collection of hundreds of models and metrics concerning training and performance to approximate more than a thousand scaling laws. From this, the team developed a meta-analysis and guide for how to select small models and estimate scaling laws for different LLM model families, so that the budget is optimally applied toward generating reliable performance predictions.&lt;/p&gt;&lt;p&gt;“The notion that you might want to try to build mathematical models of the training process is a couple of years old, but I think what was new here is that most of the work that people had been doing before is saying, ‘can we say something post-hoc about what happened when we trained all of these models, so that when we’re trying to figure out how to train a new large-scale model, we can make the best decisions about how to use our compute budget?’” says Jacob Andreas, associate professor in the Department of Electrical Engineering and Computer Science and principal investigator with the MIT-IBM Watson AI Lab.&lt;/p&gt;&lt;p&gt;The research was recently presented at the International Conference on Machine Learning by Andreas, along with MIT-IBM Watson AI Lab researchers Leshem Choshen and Yang Zhang of IBM Research.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Extrapolating performance&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;No matter how you slice it, developing LLMs is an expensive endeavor: from decision-making regarding the numbers of parameters and tokens, data selection and size, and training techniques to determining output accuracy and tuning to the target applications and tasks. Scaling laws offer a way to forecast model behavior by relating a large model’s loss to the performance of smaller, less-costly models from the same family, avoiding the need to fully train every candidate. Mainly, the differences between the smaller models are the number of parameters and token training size. According to Choshen, elucidating scaling laws not only enable better pre-training decisions, but also democratize the field by enabling researchers without vast resources to understand and build effective scaling laws.&lt;/p&gt;&lt;p&gt;The functional form of scaling laws is relatively simple, incorporating components from the small models that capture the number of parameters and their scaling effect, the number of training tokens and their scaling effect, and the baseline performance for the model family of interest. Together, they help researchers estimate a target large model’s performance loss; the smaller the loss, the better the target model’s outputs are likely to be.&lt;/p&gt;&lt;p&gt;These laws allow research teams to weigh trade-offs efficiently and to test how best to allocate limited resources. They’re particularly useful for evaluating scaling of a certain variable, like the number of tokens, and for A/B testing of different pre-training setups.&lt;/p&gt;&lt;p&gt;In general, scaling laws aren’t new; however, in the field of AI, they emerged as models grew and costs skyrocketed. “It’s like scaling laws just appeared at some point in the field,” says Choshen. “They started getting attention, but no one really tested how good they are and what you need to do to make a good scaling law.” Further, scaling laws were themselves also a black box, in a sense. “Whenever people have created scaling laws in the past, it has always just been one model, or one model family, and one dataset, and one developer,” says Andreas. “There hadn’t really been a lot of systematic meta-analysis, as everybody is individually training their own scaling laws. So, [we wanted to know,] are there high-level trends that you see across those things?”&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Building better&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;To investigate this, Choshen, Andreas, and Zhang created a large dataset. They collected LLMs from 40 model families, including Pythia, OPT, OLMO, LLaMA, Bloom, T5-Pile, ModuleFormer mixture-of-experts, GPT, and other families. These included 485 unique, pre-trained models, and where available, data about their training checkpoints, computational cost (FLOPs), training epochs, and the seed, along with 1.9 million performance metrics of loss and downstream tasks. The models differed in their architectures, weights, and so on. Using these models, the researchers fit over 1,000 scaling laws and compared their accuracy across architectures, model sizes, and training regimes, as well as testing how the number of models, inclusion of intermediate training checkpoints, and partial training impacted the predictive power of scaling laws to target models. They used measurements of absolute relative error (ARE); this is the difference between the scaling law’s prediction and the observed loss of a large, trained model. With this, the team compared the scaling laws, and after analysis, distilled practical recommendations for AI practitioners about what makes effective scaling laws.&lt;/p&gt;&lt;p&gt;Their shared guidelines walk the developer through steps and options to consider and expectations. First, it’s critical to decide on a compute budget and target model accuracy. The team found that 4 percent ARE is about the best achievable accuracy one could expect due to random seed noise, but up to 20 percent ARE is still useful for decision-making. The researchers identified several factors that improve predictions, like including intermediate training checkpoints, rather than relying only on final losses; this made scaling laws more reliable. However, very early training data before 10 billion tokens are noisy, reduce accuracy, and should be discarded. They recommend prioritizing training more models across a spread of sizes to improve robustness of the scaling law’s prediction, not just larger models; selecting five models provides a solid starting point.&amp;nbsp;&lt;/p&gt;&lt;p&gt;Generally, including larger models improves prediction, but costs can be saved by partially training the target model to about 30 percent of its dataset and using that for extrapolation. If the budget is considerably constrained, developers should consider training one smaller model within the target model family and borrow scaling law parameters from a model family with similar architecture; however, this may not work for encoder–decoder models. Lastly, the MIT-IBM research group found that when scaling laws were compared across model families, there was strong correlation between two sets of hyperparameters, meaning that three of the five hyperparameters explained nearly all of the variation and could likely capture the model behavior. Together, these guidelines provide a systematic approach to making scaling law estimation more efficient, reliable, and accessible for AI researchers working under varying budget constraints.&lt;/p&gt;&lt;p&gt;Several surprises arose during this work: small models partially trained are still very predictive, and further, the intermediate training stages from a fully trained model can be used (as if they are individual models) for prediction of another target model. “Basically, you don’t pay anything in the training, because you already trained the full model, so the half-trained model, for instance, is just a byproduct of what you did,” says Choshen. Another feature Andreas pointed out was that, when aggregated, the variability across model families and different experiments jumped out and was noisier than expected. Unexpectedly, the researchers found that it’s possible to utilize the scaling laws on large models to predict performance down to smaller models. Other research in the field has hypothesized that smaller models were a “different beast” compared to large ones; however, Choshen disagrees. “If they’re totally different, they should have shown totally different behavior, and they don’t.”&lt;/p&gt;&lt;p&gt;While this work focused on model training time, the researchers plan to extend their analysis to model inference. Andreas says it’s not, “how does my model get better as I add more training data or more parameters, but instead as I let it think for longer, draw more samples. I think there are definitely lessons to be learned here about how to also build predictive models of how much thinking you need to do at run time.” He says the theory of inference time scaling laws might become even more critical because, “it’s not like I'm going to train one model and then be done. [Rather,] it’s every time a user comes to me, they’re going to have a new query, and I need to figure out how hard [my model needs] to think to come up with the best answer. So, being able to build those kinds of predictive models, like we’re doing in this paper, is even more important.”&lt;/p&gt;&lt;p&gt;This research was supported, in part, by the MIT-IBM Watson AI Lab and a Sloan Research Fellowship.&amp;nbsp;&lt;/p&gt;</content:encoded>
      <media:content url="https://news.mit.edu/sites/default/files/styles/news_article__cover_image__original/public/images/202509/Increasing-LLM-size.jpg?itok=I-0gEXtr" medium="image" type="image/jpeg" width="390" height="260">
              <media:description type="plain">Scaling laws enable researchers to use smaller LLMs to predict the performance of a significantly bigger target model, thus allowing better allocation of computational power.</media:description>
              <media:credit>Image: AdobeStock</media:credit>
      </media:content>
        <category domain="https://news.mit.edu/topic/school-engineering">School of Engineering</category>
      <category domain="https://news.mit.edu/topic/mit-schwarzman-college-computing">MIT Schwarzman College of Computing</category>
      <category domain="https://news.mit.edu/topic/electrical-engineering-computer-science-eecs">Electrical engineering and computer science (EECS)</category>
      <category domain="https://news.mit.edu/topic/computer-science-and-artificial-intelligence-laboratory-csail">Computer Science and Artificial Intelligence Laboratory (CSAIL)</category>
      <category domain="https://news.mit.edu/topic/mit-ibm-watson-ai-lab">MIT-IBM Watson AI Lab</category>
      <category domain="https://news.mit.edu/topic/computers">Computer science and technology</category>
      <category domain="https://news.mit.edu/topic/artificial-intelligence2">Artificial intelligence</category>
      <category domain="https://news.mit.edu/topic/machine-learning">Machine learning</category>
      <category domain="https://news.mit.edu/topic/algorithms">Algorithms</category>
      <category domain="https://news.mit.edu/topic/data">Data</category>
      <category domain="https://news.mit.edu/topic/research">Research</category>
    </item>
<item>
  <title>Machine-learning tool gives doctors a more detailed 3D picture of fetal health</title>
  <link>https://news.mit.edu/2025/machine-learning-tool-gives-doctors-more-detailed-3d-picture-fetal-health-0915</link>
  <description>MIT CSAIL researchers developed a tool that can model the shape and movements of fetuses in 3D, potentially assisting doctors in finding abnormalities and making diagnoses.</description>
  <pubDate>Mon, 15 Sep 2025 10:00:00 -0400</pubDate>
    <guid isPermaLink="true">https://news.mit.edu/2025/machine-learning-tool-gives-doctors-more-detailed-3d-picture-fetal-health-0915</guid>
        <dc:creator>Alex Shipps | MIT CSAIL</dc:creator>
  <content:encoded>&lt;p dir="ltr" id="docs-internal-guid-9f9513be-7fff-adbc-e573-265de5f3d249"&gt;For pregnant women, ultrasounds are an informative (and sometimes necessary) procedure. They typically produce two-dimensional black-and-white scans of fetuses that can reveal key insights, including biological sex, approximate size, and abnormalities like heart issues or cleft lip. If your doctor wants a closer look, they may use magnetic resonance imaging (MRI), which uses magnetic fields to capture images that can be combined to create a 3D view of the fetus.&lt;/p&gt;&lt;p dir="ltr"&gt;MRIs aren’t a catch-all, though; the 3D scans are difficult for doctors to interpret well enough to diagnose problems because our visual system is not accustomed to processing 3D volumetric scans (in other words, a wrap-around look that also shows us the inner structures of a subject). Enter machine learning, which could help model a fetus’s development more clearly and accurately from data — although no such algorithm has been able to model their somewhat random movements and various body shapes.&lt;/p&gt;&lt;p dir="ltr"&gt;That is, until a new approach called “Fetal SMPL” from MIT’s Computer Science and Artificial Intelligence Laboratory (CSAIL), Boston Children’s Hospital (BCH), and Harvard Medical School presented clinicians with a more detailed picture of fetal health. It was adapted from “SMPL” (Skinned Multi-Person Linear model), a 3D model developed in computer graphics to capture adult body shapes and poses, as a way to represent fetal body shapes and poses accurately. Fetal SMPL was then trained on 20,000 MRI volumes to predict the location and size of a fetus and create sculpture-like 3D representations. Inside each model is a skeleton with 23 articulated joints called a “kinematic tree,” which the system uses to pose and move like the fetuses it saw during training.&lt;br&gt;&lt;br&gt;The extensive, real-world scans that Fetal SMPL learned from helped it develop pinpoint accuracy. Imagine stepping into a stranger’s footprint while blindfolded, and not only does it fit perfectly, but you correctly guess what shoe they wore — similarly, the tool closely matched the position and size of fetuses in MRI frames it hadn’t seen before. Fetal SMPL was only misaligned by an average of about 3.1 millimeters, a gap smaller than a single grain of rice.&lt;br&gt;&lt;br&gt;The approach could enable doctors to precisely measure things like the size of a baby’s head or abdomen and compare these metrics with healthy fetuses at the same age. Fetal SMPL has demonstrated its clinical potential in early tests, where it achieved accurate alignment results on a small group of real-world scans.&lt;/p&gt;&lt;p dir="ltr"&gt;“It can be challenging to estimate the shape and pose of a fetus because they’re crammed into the tight confines of the uterus,” says lead author, MIT PhD student, and CSAIL researcher Yingcheng Liu SM ’21. “Our approach overcomes this challenge using a system of interconnected bones under the surface of the 3D model, which represent the fetal body and its motions realistically. Then, it relies on a coordinate descent algorithm to make a prediction, essentially alternating between guessing pose and shape from tricky data until it finds a reliable estimate.”&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;strong&gt;In utero&lt;/strong&gt;&lt;br&gt;&lt;br&gt;Fetal SMPL was tested on shape and pose accuracy against the closest baseline the researchers could find: a system that models infant growth called&amp;nbsp;&lt;a href="https://files.is.tue.mpg.de/black/papers/miccai18.pdf"&gt;“SMIL.”&lt;/a&gt; Since babies out of the womb are larger than fetuses, the team shrank those models by 75 percent to level the playing field.&lt;br&gt;&lt;br&gt;The system outperformed this baseline on a dataset of fetal MRIs between the gestational ages of 24 and 37 weeks taken at Boston Children’s Hospital. Fetal SMPL was able to recreate real scans more precisely, as its models closely lined up with real MRIs.&lt;/p&gt;&lt;p dir="ltr"&gt;The method was efficient at lining up their models to images, only needing three iterations to arrive at a reasonable alignment. In an experiment that counted how many incorrect guesses Fetal SMPL had made before arriving at a final estimate, its accuracy plateaued from the fourth step onward.&lt;br&gt;&lt;br&gt;The researchers have just begun testing their system in the real world, where it produced similarly accurate models in initial clinical tests. While these results are promising, the team notes that they’ll need to apply their results to larger populations, different gestational ages, and a variety of disease cases to better understand the system’s capabilities.&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;strong&gt;Only skin deep&lt;/strong&gt;&lt;/p&gt;&lt;p dir="ltr"&gt;Liu also notes that their system only helps analyze what doctors can see on the surface of a fetus, since only bone-like structures lie beneath the skin of the models. To better monitor babies’ internal health, such as liver, lung, and muscle development, the team intends to make their tool volumetric, modeling the fetus’s inner anatomy from scans. Such upgrades would make the models more human-like, but the current version of Fetal SMPL already presents a precise (and unique) upgrade to 3D fetal health analysis.&lt;br&gt;&lt;br&gt;“This study introduces a method specifically designed for fetal MRI that effectively captures fetal movements, enhancing the assessment of fetal development and health,” says Kiho Im, Harvard Medical School associate professor of pediatrics and staff scientist in the Division of Newborn Medicine at BCH’s Fetal-Neonatal Neuroimaging and Developmental Science Center. Im, who was not involved with the paper, adds that this approach “will not only improve the diagnostic utility of fetal MRI, but also provide insights into the early functional development of the fetal brain in relation to body movements.”&lt;/p&gt;&lt;p dir="ltr"&gt;“This work reaches a pioneering milestone by extending parametric surface human body models for the earliest shapes of human life: fetuses,” says Sergi Pujades, an associate professor at University Grenoble Alpes, who wasn’t involved in the research. “It allows us to detangle the shape and motion of a human, which has already proven to be key in understanding how adult body shape relates to metabolic conditions and how infant motion relates to neurodevelopmental disorders. In addition, the fact that the fetal model stems from, and is compatible with, the adult (SMPL) and infant (SMIL) body models, will allow us to study human shape and pose evolution over long periods of time. This is an unprecedented opportunity to further quantify how human shape growth and motion are affected by different conditions.”&lt;/p&gt;&lt;p&gt;Liu wrote the paper with three CSAIL members: Peiqi Wang SM ’22, PhD ’25; MIT PhD student Sebastian Diaz; and senior author Polina Golland, the Sunlin and Priscilla Chou Professor of Electrical Engineering and Computer Science, a principal investigator in MIT CSAIL, and the leader of the Medical Vision Group. BCH assistant professor of pediatrics Esra Abaci Turk, Inria researcher Benjamin Billot, and Harvard Medical School professor of pediatrics and professor of radiology Patricia Ellen Grant are also authors on the paper. This work was supported, in part, by the National Institutes of Health and the MIT CSAIL-Wistron Program.&lt;br&gt;&lt;br&gt;The researchers will present their work at the International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI) in September.&lt;/p&gt;</content:encoded>
      <media:content url="https://news.mit.edu/sites/default/files/styles/news_article__cover_image__original/public/images/202508/MIT-csail-3d-fetus.jpg?itok=II5iNSeh" medium="image" type="image/jpeg" width="390" height="260">
              <media:description type="plain">Fetal SMPL was trained on 20,000 MRI volumes to predict the location and size of a fetus and create sculpture-like 3D representations. The approach could enable doctors to precisely measure things like the size of a baby’s head and compare these metrics with healthy fetuses at the same age.</media:description>
              <media:credit>Image: Alex Shipps and Yingcheng Liu/MIT CSAIL</media:credit>
      </media:content>
        <category domain="https://news.mit.edu/topic/school-engineering">School of Engineering</category>
      <category domain="https://news.mit.edu/topic/mit-schwarzman-college-computing">MIT Schwarzman College of Computing</category>
      <category domain="https://news.mit.edu/topic/electrical-engineering-computer-science-eecs">Electrical engineering and computer science (EECS)</category>
      <category domain="https://news.mit.edu/topic/computer-science-and-artificial-intelligence-laboratory-csail">Computer Science and Artificial Intelligence Laboratory (CSAIL)</category>
      <category domain="https://news.mit.edu/topic/computers">Computer science and technology</category>
      <category domain="https://news.mit.edu/topic/artificial-intelligence2">Artificial intelligence</category>
      <category domain="https://news.mit.edu/topic/machine-learning">Machine learning</category>
      <category domain="https://news.mit.edu/topic/algorithms">Algorithms</category>
      <category domain="https://news.mit.edu/topic/health">Health sciences and technology</category>
      <category domain="https://news.mit.edu/topic/health-care">Health care</category>
      <category domain="https://news.mit.edu/topic/development">Development</category>
      <category domain="https://news.mit.edu/topic/imaging">Imaging</category>
      <category domain="https://news.mit.edu/topic/3-d">3-D</category>
      <category domain="https://news.mit.edu/topic/research">Research</category>
      <category domain="https://news.mit.edu/topic/nih">National Institutes of Health (NIH)</category>
    </item>
<item>
  <title>MIT software tool turns everyday objects into animated, eye-catching displays</title>
  <link>https://news.mit.edu/2025/fabobscura-turns-everyday-objects-into-animated-displays-0910</link>
  <description>The FabObscura system helps users design and print barrier-grid animations without electronics, and can help produce dynamic household, workplace, and artistic objects.</description>
  <pubDate>Wed, 10 Sep 2025 15:15:00 -0400</pubDate>
    <guid isPermaLink="true">https://news.mit.edu/2025/fabobscura-turns-everyday-objects-into-animated-displays-0910</guid>
        <dc:creator>Alex Shipps | MIT CSAIL</dc:creator>
  <content:encoded>&lt;p dir="ltr" id="docs-internal-guid-1580d350-7fff-0c82-7e82-bf75564047ba"&gt;Whether you’re an artist, advertising specialist, or just looking to spruce up your home, turning everyday objects into dynamic displays is a great way to make them more visually engaging. For example, you could turn a kids’ book into a&amp;nbsp;&lt;a href="https://www.youtube.com/watch?v=eVG8tMW4Ctw"&gt;handheld cartoon of sorts,&lt;/a&gt; making the reading experience more immersive and memorable for a child.&lt;/p&gt;&lt;p dir="ltr"&gt;But now, thanks to MIT researchers, it’s also possible to make dynamic displays without using electronics, using&amp;nbsp;&lt;a href="https://www.pinterest.com/pin/352406739608729201/"&gt;barrier-grid animations&lt;/a&gt;, which use printed materials instead. This visual trick involves sliding a patterned sheet across an image to create the illusion of a moving image. The secret of barrier-grid animations lies in its name: An overlay called a barrier (or grid) often resembling a picket fence moves across, rotates around, or tilts toward an image to reveal frames in an animated sequence. That underlying picture is a combination of each still, sliced and interwoven to present a different snapshot depending on the overlay’s position.&lt;br&gt;&lt;br&gt;While tools exist to help artists create barrier-grid animations, they’re typically used to create barrier patterns that have straight lines. Building off of&amp;nbsp;&lt;a href="https://news.mit.edu/2022/new-twist-old-school-animation-kinecam-0721"&gt;previous work&lt;/a&gt; in creating images that appear to move, researchers at MIT’s Computer Science and Artificial Intelligence Laboratory (CSAIL) have developed a tool that allows users to explore more unconventional designs. From zigzags to circular patterns, the team’s “&lt;a href="https://vis.csail.mit.edu/pubs/fabobscura/"&gt;FabObscura&lt;/a&gt;” software turns unique concepts into printable barrier-grid animations, helping users add dynamic animations to things like pictures, toys, and decor.&lt;/p&gt;&lt;p dir="ltr"&gt;MIT Department of Electrical Engineering and Computer Science (EECS) PhD student and CSAIL researcher Ticha Sethapakdi SM ’19, a lead author on a&amp;nbsp;&lt;a href="https://vis.csail.mit.edu/pubs/fabobscura/"&gt;paper&lt;/a&gt; presenting FabObscura, says that the system is a one-size-fits-all tool for customizing barrier-grid animations. This versatility extends to unconventional, elaborate overlay designs, like pointed, angled lines to animate a picture you might put on your desk, or the swirling, hypnotic appearance of a radial pattern you could spin over an image placed on a coin or a Frisbee.&lt;/p&gt;&lt;p dir="ltr"&gt;“Our system can turn a seemingly static, abstract image into an attention-catching animation,” says Sethapakdi. “The tool lowers the barrier to entry to creating these barrier-grid animations, while helping users express a variety of designs that would’ve been very time-consuming to explore by hand.”&lt;/p&gt;&lt;p dir="ltr"&gt;Behind these novel creations is a key finding: Barrier patterns can be expressed as any continuous mathematical function — not just straight lines. Users can type these equations into a text box within the FabObscura program, and then see how it graphs out the shape and movement of a barrier pattern. If you wanted a traditional horizontal pattern, you’d enter in a constant function, where the output is the same no matter the input, much like drawing a straight line across a graph. For a wavy design, you’d use a sine function, which is smooth and resembles a mountain range when plotted out. The system’s interface includes helpful examples of these equations to guide users toward their preferred pattern.&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;strong&gt;A simple interface for elaborate ideas&lt;/strong&gt;&lt;/p&gt;&lt;p dir="ltr"&gt;FabObscura works for all known types of barrier-grid animations, supporting a variety of user interactions. The system enables the creation of a display with an appearance that changes depending on your viewpoint. FabObscura also allows you to create displays that you can animate by sliding or rotating a barrier over an image.&lt;/p&gt;&lt;p dir="ltr"&gt;To produce these designs, users can upload a folder of frames of an animation (perhaps a few stills of a horse running), or choose from a few preset sequences (like an eye blinking) and specify the angle your barrier will move. After previewing your design, you can fabricate the barrier and picture onto separate transparent sheets (or print the image on paper) using a standard 2D printer, such as an inkjet. Your image can then be placed and secured on flat, handheld items such as picture frames, phones, and books.&lt;br&gt;&lt;br&gt;You can enter separate equations if you want two sequences on one surface, which the researchers call “nested animations.” Depending on how you move the barrier, you’ll see a different story being told. For example, CSAIL researchers created a car that rotates when you move its sheet vertically, but transforms into a spinning motorcycle when you slide the grid horizontally.&lt;/p&gt;&lt;p dir="ltr"&gt;These customizations lead to unique household items, too. The researchers designed an interactive coaster that you can switch from displaying a “coffee” icon to symbols of a martini and a glass of water by pressing your fingers down on the edges of its surface. The team also spruced up a jar of sunflower seeds, producing a flower animation on the lid that blooms when twisted off.&lt;br&gt;&lt;br&gt;Artists, including graphic designers and printmakers, could also use this tool to make dynamic pieces without needing to connect any wires. The tool saves them crucial time to explore creative, low-power designs, such as a clock with a mouse that runs along as it ticks. FabObscura could produce animated food packaging, or even reconfigurable signage for places like construction sites or stores that notify people when a particular area is closed or a machine isn’t working.&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;strong&gt;Keep it crisp&lt;/strong&gt;&lt;/p&gt;&lt;p dir="ltr"&gt;FabObscura’s barrier-grid creations do come with certain trade-offs. While nested animations are novel and more dynamic than a single-layer barrier-grid animation, their visual quality isn’t as strong. The researchers wrote design guidelines to address these challenges, recommending users upload fewer frames for nested animations to keep the interlaced image simple and stick to high-contrast images for a crisper presentation.&lt;br&gt;&lt;br&gt;In the future, the researchers intend to expand what users can upload to FabObscura, like being able to drop in a video file that the program can then select the best frames from. This would lead to even more expressive barrier-grid animations.&lt;/p&gt;&lt;p dir="ltr"&gt;FabObscura might also step into a new dimension: 3D. While the system is currently optimized for flat, handheld surfaces, CSAIL researchers are considering implementing their work into larger, more complex objects, possibly using 3D printers to fabricate even more elaborate illusions.&lt;/p&gt;&lt;p dir="ltr"&gt;Sethapakdi wrote the paper with several CSAIL affiliates: Zhejiang University PhD student and visiting researcher Mingming Li; MIT EECS PhD student Maxine Perroni-Scharf; MIT postdoc Jiaji Li; MIT associate professors Arvind Satyanarayan and Justin Solomon; and senior author and MIT Associate Professor Stefanie Mueller, leader of the Human-Computer Interaction (HCI) Engineering Group at CSAIL. Their work will be presented at the ACM Symposium on User Interface Software and Technology (UIST) this month.&lt;/p&gt;</content:encoded>
      <media:content url="https://news.mit.edu/sites/default/files/styles/news_article__cover_image__original/public/images/202509/MIT-FabObScura.jpg?itok=LwX43osQ" medium="image" type="image/jpeg" width="390" height="260">
              <media:description type="plain">“Our system can turn a seemingly static, abstract image into an attention-catching animation,” says MIT PhD student Ticha Sethapakdi, a lead researcher on the FabObscura project. “The tool lowers the barrier to entry to creating these barrier-grid animations, while helping users express a variety of designs that would’ve been very time-consuming to explore by hand.” </media:description>
              <media:credit>Image courtesy of the researchers.</media:credit>
      </media:content>
        <category domain="https://news.mit.edu/topic/research">Research</category>
      <category domain="https://news.mit.edu/topic/2-d">2-D</category>
      <category domain="https://news.mit.edu/topic/arts">Arts</category>
      <category domain="https://news.mit.edu/topic/visual-arts">Visual arts</category>
      <category domain="https://news.mit.edu/topic/design">Design</category>
      <category domain="https://news.mit.edu/topic/human-computer-interaction">Human-computer interaction</category>
      <category domain="https://news.mit.edu/topic/technology-society">Technology and society</category>
      <category domain="https://news.mit.edu/topic/computers">Computer science and technology</category>
      <category domain="https://news.mit.edu/topic/electrical-engineering-computer-science-eecs">Electrical engineering and computer science (EECS)</category>
      <category domain="https://news.mit.edu/topic/computer-science-and-artificial-intelligence-laboratory-csail">Computer Science and Artificial Intelligence Laboratory (CSAIL)</category>
      <category domain="https://news.mit.edu/topic/mit-schwarzman-college-computing">MIT Schwarzman College of Computing</category>
      <category domain="https://news.mit.edu/topic/school-engineering">School of Engineering</category>
    </item>
<item>
  <title>Alzheimer’s erodes brain cells’ control of gene expression, undermining function, cognition</title>
  <link>https://news.mit.edu/2025/alzheimers-erodes-brain-cells-control-gene-expression-undermining-function-cognition-0908</link>
  <description>Study of 3.5 million cells from more than 100 human brains finds Alzheimer’s progression — and resilience to disease — depends on preserving epigenomic stability.</description>
  <pubDate>Mon, 08 Sep 2025 16:25:00 -0400</pubDate>
    <guid isPermaLink="true">https://news.mit.edu/2025/alzheimers-erodes-brain-cells-control-gene-expression-undermining-function-cognition-0908</guid>
        <dc:creator>David Orenstein | The Picower Institute for Learning and Memory</dc:creator>
  <content:encoded>&lt;p&gt;Most people recognize Alzheimer’s disease from its devastating symptoms such as memory loss, while new drugs target pathological aspects of disease manifestations, such as plaques of amyloid proteins. Now, a sweeping &lt;a href="https://www.cell.com/cell/fulltext/S0092-8674(25)00733-0"&gt;new open-access study in the Sept. 4 edition of &lt;em&gt;Cell&lt;/em&gt;&lt;/a&gt; by MIT researchers shows the importance of understanding the disease as a battle over how well brain cells control the expression of their genes. The study paints a high-resolution picture of a desperate struggle to maintain healthy gene expression and gene regulation, where the consequences of failure or success are nothing less than the loss or preservation of cell function and cognition.&lt;/p&gt;&lt;p&gt;The study presents a first-of-its-kind, multimodal atlas of combined gene expression and gene regulation spanning 3.5 million cells from six brain regions, obtained by profiling 384 post-mortem brain samples across 111 donors. The researchers profiled both the “transcriptome,” showing which genes are expressed into RNA, and the “epigenome,” the set of chromosomal modifications that establish which DNA regions are accessible and thus utilized between different cell types.&lt;/p&gt;&lt;p&gt;The resulting atlas revealed many insights showing that the progression of Alzheimer’s is characterized by two major epigenomic trends. The first is that vulnerable cells in key brain regions suffer a breakdown of the rigorous nuclear “compartments” they normally maintain to ensure some parts of the genome are open for expression but others remain locked away. The second major finding is that susceptible cells experience a loss of “epigenomic information,” meaning they lose their grip on the unique pattern of gene regulation and expression that gives them their specific identity and enables their healthy function.&lt;/p&gt;&lt;p&gt;Accompanying the evidence of compromised compartmentalization and the erosion of epigenomic information are many specific findings pinpointing molecular circuitry that breaks down by cell type, by region, and gene network. They found, for instance, that when epigenomic conditions deteriorate, that opens the door to expression of many genes associated with disease, whereas if cells manage to keep their epigenomic house in order, they can keep disease-associated genes in check. Moreover, the researchers clearly saw that when the epigenomic breakdowns were occurring people lost cognitive ability, but where epigenomic stability remained, so did cognition.&lt;/p&gt;&lt;p&gt;“To understand the circuitry, the logic responsible for gene expression changes in Alzheimer’s disease [AD], we needed to understand the regulation and upstream control of all the changes that are happening, and that’s where the epigenome comes in,” says senior author &lt;a href="https://web.mit.edu/manoli/" target="_blank" title="(opens in a new window)" data-extlink="" rel="noopener"&gt;Manolis Kellis&lt;/a&gt;, a professor in the Computer Science and Artificial Intelligence Lab and head of MIT’s &lt;a href="https://compbio.mit.edu/" target="_blank" title="(opens in a new window)" data-extlink="" rel="noopener"&gt;Computational Biology Group&lt;/a&gt;. “This is the first large-scale, single-cell, multi-region gene-regulatory atlas of AD, systematically dissecting the dynamics of epigenomic and transcriptomic programs across disease progression and resilience.”&lt;/p&gt;&lt;p&gt;By providing that detailed examination of the epigenomic mechanisms of Alzheimer’s progression, the study provides a blueprint for devising new Alzheimer’s treatments that can target factors underlying the broad erosion of epigenomic control or the specific manifestations that affect key cell types such as neurons and supporting glial cells.&lt;/p&gt;&lt;p&gt;“The key to developing new and more effective treatments for Alzheimer’s disease depends on deepening our understanding of the mechanisms that contribute to the breakdowns of cellular and network function in the brain,” says Picower Professor and co-corresponding author &lt;a href="https://live-picower-170706.pantheonsite.io/node/2"&gt;Li-Huei Tsai&lt;/a&gt;, director of The Picower Institute for Learning and Memory and a founding member of &lt;a href="https://live-picower-170706.pantheonsite.io/node/25"&gt;MIT’s Aging Brain Initiative&lt;/a&gt;, along with Kellis. “This new data advances our understanding of how epigenomic factors drive disease.”&lt;/p&gt;&lt;p&gt;Kellis Lab members Zunpeng Liu and Shanshan Zhang are the study’s co-lead authors.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Compromised compartments and eroded information&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Among the post-mortem brain samples in the study, 57 came from donors to the Religious Orders Study or the Rush Memory and Aging Project (collectively known as “ROSMAP”) who did not have AD pathology or symptoms, while 33 came from donors with early-stage pathology and 21 came from donors at a late stage. The samples therefore provided rich information about the symptoms and pathology each donor was experiencing before death.&lt;/p&gt;&lt;p&gt;In the new study, Liu and Zhang combined analyses of single-cell RNA sequencing of the samples, which measures which genes are being expressed in each cell, and ATACseq, which measures whether chromosomal regions are accessible for gene expression. Considered together, these transcriptomic and epigenomic measures enabled the researchers to understand the molecular details of how gene expression is regulated across seven broad classes of brain cells (e.g., neurons or other glial cell types) and 67 subtypes of cell types (e.g., 17 kinds of excitatory neurons or six kinds of inhibitory ones).&lt;/p&gt;&lt;p&gt;The researchers annotated more than 1 million gene-regulatory control regions that different cells employ to establish their specific identities and functionality using epigenomic marking. Then, by comparing the cells from Alzheimer’s brains to the ones without, and accounting for stage of pathology and cognitive symptoms, they could produce rigorous associations between the erosion of these epigenomic markings, and ultimately loss of function.&lt;/p&gt;&lt;p&gt;For instance, they saw that among people who advanced to late-stage AD, normally repressive compartments opened up for more expression and compartments that were normally more open during health became more repressed. Worryingly, when the normally repressive compartments of brain cells opened up, they became more afflicted with disease.&lt;/p&gt;&lt;p&gt;“For Alzheimer’s patients, repressive compartments opened up, and gene expression levels increased, which was associated with decreased cognitive function,” explains Liu.&lt;/p&gt;&lt;p&gt;But when cells managed to keep their compartments in order such that they expressed the genes they were supposed to, people remained cognitively intact.&lt;/p&gt;&lt;p&gt;Meanwhile, based on the cells’ expression of their regulatory elements, the researchers created an epigenomic information score for each cell. Generally, information declined as pathology progressed, but that was particularly notable among cells in the two brain regions affected earliest in Alzheimer’s: the entorhinal cortex and the hippocampus. The analyses also highlighted specific cell types that were especially vulnerable including microglia that play immune and other roles, oligodendrocytes that produce myelin insulation for neurons, and particular kinds of excitatory neurons.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Risk genes and “chromatin guardians”&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Detailed analyses in the paper highlighted how epigenomic regulation tracked with disease-related problems, Liu notes. The e4 variant of the APOE gene, for instance, is widely understood to be the single biggest genetic risk factor for Alzheimer’s. In APOE4 brains, microglia initially responded to the emerging disease pathology with an increase in their epigenomic information, suggesting that they were stepping up to their unique responsibility to fight off disease. But as the disease progressed, the cells exhibited a sharp drop off in information, a sign of deterioration and degeneration. This turnabout was strongest in people who had two copies of APOE4, rather than just one. The findings, Kellis said, suggest that APOE4 might destabilize the genome of microglia, causing them to burn out.&lt;/p&gt;&lt;p&gt;Another example is the fate of neurons expressing the gene RELN and its protein Reelin. Prior studies, including by &lt;a href="https://live-picower-170706.pantheonsite.io/node/1290"&gt;Kellis and Tsai&lt;/a&gt;, have shown that RELN- expressing neurons in the entorhinal cortex and hippocampus are especially vulnerable in Alzheimer’s, but promote resilience if they survive. The new study sheds new light on their fate by demonstrating that they exhibit early and severe epigenomic information loss as disease advances, but that in people who remained cognitively resilient the neurons maintained epigenomic information.&lt;/p&gt;&lt;p&gt;In yet another example, the researchers tracked what they colloquially call “chromatin guardians” because their expression sustains and regulates cells’ epigenomic programs. For instance, cells with greater epigenomic erosion and advanced AD progression displayed increased chromatin accessibility in areas that were supposed to be locked down by Polycomb repression genes or other gene expression silencers. While resilient cells expressed genes promoting neural connectivity, epigenomically eroded cells expressed genes linked to inflammation and oxidative stress.&lt;/p&gt;&lt;p&gt;“The message is clear: Alzheimer’s is not only about plaques and tangles, but about the erosion of nuclear order itself,” Kellis says. “Cognitive decline emerges when chromatin guardians lose ground to the forces of erosion, switching from resilience to vulnerability at the most fundamental level of genome regulation.&lt;/p&gt;&lt;p&gt;“And when our brain cells lose their epigenomic memory marks and epigenomic information at the lowest level deep inside our neurons and microglia, it seems that Alheimer’s patients also lose their memory and cognition at the highest level.”&lt;/p&gt;&lt;p&gt;Other authors of the paper are Benjamin T. James, Kyriaki Galani, Riley J. Mangan, Stuart Benjamin Fass, Chuqian Liang, Manoj M. Wagle, Carles A. Boix, Yosuke Tanigawa, Sukwon Yun, Yena Sung, Xushen Xiong, Na Sun, Lei Hou, Martin Wohlwend, Mufan Qiu, Xikun Han, Lei Xiong, Efthalia Preka, Lei Huang, William F. Li, Li-Lun Ho, Amy Grayson, Julio Mantero, Alexey Kozlenkov, Hansruedi Mathys, Tianlong Chen, Stella Dracheva, and David A. Bennett.&lt;/p&gt;&lt;p&gt;Funding for the research came from the National Institutes of Health, the National Science Foundation, the Cure Alzheimer’s Fund, the Freedom Together Foundation, the Robert A. and Renee E. Belfer Family Foundation, Eduardo Eurnekian, and Joseph P. DiSabato.&lt;/p&gt;</content:encoded>
      <media:content url="https://news.mit.edu/sites/default/files/styles/news_article__cover_image__original/public/images/202509/mit-csail-alzheimers.jpg?itok=SpIYcsdv" medium="image" type="image/jpeg" width="390" height="260">
              <media:description type="plain">MIT researchers analyzed a massive dataset of gene expression and regulation measures to better understand how the human brain's control of gene expression is affected by Alzheimer's disease. They found both broad trends and specific mechanisms by which the control becomes compromised and eroded.</media:description>
              <media:credit>Image: AdobeStock</media:credit>
      </media:content>
        <category domain="https://news.mit.edu/topic/research">Research</category>
      <category domain="https://news.mit.edu/topic/alzheimers">Alzheimer's</category>
      <category domain="https://news.mit.edu/topic/brain-cognitive">Brain and cognitive sciences</category>
      <category domain="https://news.mit.edu/topic/genomics">Genomics</category>
      <category domain="https://news.mit.edu/topic/neuroscience">Neuroscience</category>
      <category domain="https://news.mit.edu/topic/memory">Memory</category>
      <category domain="https://news.mit.edu/topic/electrical-engineering-computer-science-eecs">Electrical engineering and computer science (EECS)</category>
      <category domain="https://news.mit.edu/topic/picower-institute-0">Picower Institute</category>
      <category domain="https://news.mit.edu/topic/computer-science-and-artificial-intelligence-laboratory-csail">Computer Science and Artificial Intelligence Laboratory (CSAIL)</category>
      <category domain="https://news.mit.edu/topic/school-science">School of Science</category>
      <category domain="https://news.mit.edu/topic/school-engineering">School of Engineering</category>
      <category domain="https://news.mit.edu/topic/mit-schwarzman-college-computing">MIT Schwarzman College of Computing</category>
      <category domain="https://news.mit.edu/topic/nih">National Institutes of Health (NIH)</category>
      <category domain="https://news.mit.edu/topic/nsf">National Science Foundation (NSF)</category>
    </item>
<item>
  <title>A human-centered approach to data visualization</title>
  <link>https://news.mit.edu/2025/human-centered-approach-data-visualization-arvind-satyanarayan-0905</link>
  <description>Balancing automation and agency, Associate Professor Arvind Satyanarayan develops interactive data visualizations that amplify human creativity and cognition.  </description>
  <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
    <guid isPermaLink="true">https://news.mit.edu/2025/human-centered-approach-data-visualization-arvind-satyanarayan-0905</guid>
        <dc:creator>Adam Zewe | MIT News</dc:creator>
  <content:encoded>&lt;p&gt;The world is awash in data visualizations, from charts accompanying news stories on the economy to graphs tracking the weekly temperature to scatterplots showing relationships between baseball statistics.&lt;/p&gt;&lt;p&gt;At their core, data visualizations convey information, and everyone consumes that information differently. One person might scan the axes, while another may focus on an outlying data point or examine the magnitude of each colored bar.&lt;/p&gt;&lt;p&gt;But how do you consume that information if you can’t see it?&lt;/p&gt;&lt;p&gt;Making a data visualization accessible for blind and low-vision readers often involves writing a descriptive caption that captures some key points in a succinct paragraph.&lt;/p&gt;&lt;p&gt;“But that means blind and low-vision readers don’t get the ability to interpret the data for themselves. What if they had a different question about the data? Suddenly a simple caption doesn’t give them that. The core idea behind our group’s work in accessibility has been to maintain agency for blind and low-vision people,” says Arvind Satyanarayan, a newly tenured associate professor in the MIT Department of Electrical Engineering and Computer Science (EECS) and member of the Computer Science and Artificial Intelligence Laboratory (CSAIL).&lt;/p&gt;&lt;p&gt;Satyanarayan’s group has explored making data visualizations accessible for screen readers, which narrate content on a computer screen. His team created a&amp;nbsp;&lt;a href="https://news.mit.edu/2022/data-visualization-accessible-blind-0602" target="_blank"&gt;hierarchical platform&lt;/a&gt; that allows screen reader users to explore various levels of detail in a visualization with their keyboard, drilling down from high-level information to individual data points.&lt;/p&gt;&lt;p&gt;Under the umbrella of human-computer interaction (HCI) research, Satyanarayan’s Visualization Group also develops programming languages and authoring tools for visualizations, studies the sociocultural elements of visualization design, and uses visualizations to analyze machine-learning models.&lt;/p&gt;&lt;p&gt;For Satyanarayan, HCI is about promoting human agency, whether that means enabling a blind reader to interpret data trends or ensuring designers still feel in control of AI-driven visualization systems.&lt;/p&gt;&lt;p&gt;“We really take a human-centered approach to data visualization,” he says.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;An eye for technology&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Satyanarayan found the field of data visualization almost by accident.&lt;/p&gt;&lt;p&gt;As a child growing up in India, Bahrain, and Abu Dhabi, his initial interest in science sprouted from his love for tinkering.&lt;/p&gt;&lt;p&gt;Satyanarayan recalls his father bringing home a laptop, which he loaded with simple games. The internet grew up along with him, and as a teenager he became heavily engaged in the popular blogging platform Movable Type.&lt;/p&gt;&lt;p&gt;A teacher at heart even as a teenager, Satyanarayan offered tutorials on how to use the platform and ran a contest for people to style their blog. Along the way, he taught himself the skills to develop plugins and extensions.&lt;/p&gt;&lt;p&gt;He enjoyed designing eye-catching and user-friendly blogs, laying the foundation for his studies in human-computer interaction.&lt;/p&gt;&lt;p&gt;When he arrived at the University of California at San Diego for college, he was interested enough in the HCI field to take an introductory class.&lt;/p&gt;&lt;p&gt;“I’d always been a student of history, and this intro class really appealed to me because it was more about the history of user interfaces, and tracing the provenance and development of the ideas behind them,” he says.&lt;/p&gt;&lt;p&gt;Almost as an afterthought, he spoke with the professor, Jim Hollan — a pioneer of the field. Even though he hadn’t thought much about research beforehand, Satyanarayan ended up spending the summer in Hollan’s lab, studying how people interact with wall-sized displays.&lt;/p&gt;&lt;p&gt;As he prepared to pursue graduate studies (Satyanarayan split his PhD between Stanford University and the University of Washington), he was unsure whether to focus on programming languages or HCI. When it came time to choose, the human-centered focus of HCI and the interdisciplinarity of data visualization drew him in.&lt;/p&gt;&lt;p&gt;“Data visualization is deeply technical, but it also draws from cognitive science, perceptual psychology, and visual arts and aesthetics, and then it also has a big stake in civic and social responsibility,” he says.&lt;/p&gt;&lt;p&gt;He saw how visualization plays a role in civic and social responsibility through his first project with his PhD advisor, Jeffrey Heer. Satyanarayan and his collaborators built a data visualization interface for journalists at newsrooms that couldn’t afford to hire data departments. That drag-and-drop tool allowed journalists to design the visualization and all the data storytelling they wanted to do around it.&lt;/p&gt;&lt;p&gt;That project seeded many elements that became his thesis, for which he studied new programming languages for visualization and developed interactive graphical systems on top of them.&lt;/p&gt;&lt;p&gt;After earning his PhD, Satyanarayan sought a faculty job and spent an exhausting interview season crisscrossing the country, participating in 15 interviews in only two months.&lt;/p&gt;&lt;p&gt;MIT was his very last stop.&lt;/p&gt;&lt;p&gt;“I remember being exhausted and on autopilot, thinking that this is not going well. But then, the first day of my interview at MIT was filled with some of the best conversations I had. People were so eager and interested in understanding my research and how it connected to theirs,” he says.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Charting a collaborative course&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;The collaborative nature of MIT remained important as he built his research group; one of the group’s first graduate students was pursuing a PhD in MIT’s program in History, Anthropology, and Science, Technology, and Society. They continue to work closely with faculty who study anthropology, topics in the humanities, and clinical machine learning.&lt;/p&gt;&lt;p&gt;With interdisciplinary collaborators, the Visualization Group has explored the sociotechnical implications of data visualizations. For instance, charts are frequently shared, disseminated, and discussed on social media, where they are stripped of their context.&lt;/p&gt;&lt;p&gt;“What happens as a result is they can become vectors for misinformation or misunderstanding. But that is not because they are poorly designed to begin with. We spent a lot of time unpacking those details,” Satyanarayan says.&lt;/p&gt;&lt;p&gt;His group is also studying tactile graphics, which are common in museums to help blind and low-vision individuals interact with exhibits. Often, making a tactile graphic boils down to 3D-printing a chart.&lt;/p&gt;&lt;p&gt;“But a chart was designed to be read with our eyes, and our eyes work very differently than our fingers. We are now drilling into what it means to design tactile-first visualizations,” he says.&lt;/p&gt;&lt;p&gt;Co-design is a driving principle behind all his group’s accessibility work. On many projects, they work closely with Daniel Hajas, a researcher at the University College of London&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;who has been blind since the age of 16.&lt;/p&gt;&lt;p&gt;“That has been really important for us, to make sure as people who are not blind, that we are developing tools and platforms that are actually useful for blind and low-vision people,” he says.&lt;/p&gt;&lt;p&gt;His group is also studying the sociocultural implications of data visualization. For instance, during the height of the Covid-19 pandemic, data visualizations were often turned into memes and social artifacts that were used to support or contest data from experts.&lt;/p&gt;&lt;p&gt;“In reality, neither data nor visualizations are neutral. We’ve been thinking about the data you use to visualize, and the design choices behind specific visualizations, and what that is communicating besides insights about the data,” he says.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Visualizing a real-world impact&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Interdisciplinarity is also a theme of Satyanarayan’s interactive data visualization class, which he co-teaches with faculty members Sarah Williams and Catherine D'Ignazio in the Department of Urban Studies and Planning; and Crystal Lee in Comparative Media Studies/Writing, with shared appointments in the School of Arts, Humanities, and Social Sciences and the MIT Schwarzman College of Computing.&lt;/p&gt;&lt;p&gt;In the popular course, students not only learn the technical skills to make data visualizations, but they also build final projects centered on an area of social importance. For the past two years, students have focused on the housing affordability crisis in the Boston area, in partnership with the Massachusetts Area Planning Council. The students enjoy the opportunity to make a real-world impact with their work, Satyanarayan says.&lt;/p&gt;&lt;p&gt;And he enjoys the course as much as they do.&lt;/p&gt;&lt;p&gt;“I love teaching. I really enjoy getting to interact with the students. Our students are so intellectually curious and committed. It reassures me that our future is in good hands,” he says.&lt;/p&gt;&lt;p&gt;One of Satyanarayan’s personal interests is running along the Charles River Esplanade in Boston, which he does almost every day. He also enjoys cooking, especially with ingredients he has never used before.&lt;/p&gt;&lt;p&gt;Satyanarayan and his wife, who met while they were graduate students at Stanford (her PhD is in microbiology), also delight in tending their plot in the Fenway Victory Gardens, which is overflowing with lilies, lavender, lilacs, peonies, and roses.&lt;/p&gt;&lt;p&gt;Their newest addition is a miniature poodle puppy named Fen, which they got when Satyanarayan earned tenure earlier this year.&lt;/p&gt;&lt;p&gt;Thinking toward the future of his research, Satyanarayan is keen to further explore how generative AI might effectively &lt;a href="https://mit-genai.pubpub.org/pub/94y6e0f8/release/2" target="_blank"&gt;assist people in building visualizations&lt;/a&gt;, and its implications for human creativity.&lt;/p&gt;&lt;p&gt;“In the world of generative AI, this question of agency applies to all of us,” he says. “How do we make sure, for these AI-driven systems, that we haven’t lost the parts of the work we find most interesting?”&lt;/p&gt;</content:encoded>
      <media:content url="https://news.mit.edu/sites/default/files/styles/news_article__cover_image__original/public/images/202509/MIT-Arvind-Satyanarayan-01-press.jpg?itok=dXeZw_8s" medium="image" type="image/jpeg" width="390" height="260">
              <media:description type="plain">“Data visualization is deeply technical, but it also draws from cognitive science, perceptual psychology, and visual arts and aesthetics, and then it also has a big stake in civic and social responsibility,” Arvind Satyanarayan says.</media:description>
              <media:credit>Photo: Jared Charney</media:credit>
      </media:content>
        <category domain="https://news.mit.edu/topic/profile">Profile</category>
      <category domain="https://news.mit.edu/topic/faculty">Faculty</category>
      <category domain="https://news.mit.edu/topic/computers">Computer science and technology</category>
      <category domain="https://news.mit.edu/topic/human-computer-interaction">Human-computer interaction</category>
      <category domain="https://news.mit.edu/topic/data">Data</category>
      <category domain="https://news.mit.edu/topic/assistive-technology">Assistive technology</category>
      <category domain="https://news.mit.edu/topic/programming">Programming</category>
      <category domain="https://news.mit.edu/topic/computer-science-and-artificial-intelligence-laboratory-csail">Computer Science and Artificial Intelligence Laboratory (CSAIL)</category>
      <category domain="https://news.mit.edu/topic/electrical-engineering-computer-science-eecs">Electrical engineering and computer science (EECS)</category>
      <category domain="https://news.mit.edu/topic/school-engineering">School of Engineering</category>
      <category domain="https://news.mit.edu/topic/mit-schwarzman-college-computing">MIT Schwarzman College of Computing</category>
    </item>
<item>
  <title>A greener way to 3D print stronger stuff</title>
  <link>https://news.mit.edu/2025/greener-way-3d-print-stronger-stuff-0904</link>
  <description>MIT CSAIL researchers developed SustainaPrint, a system that reinforces only the weakest zones of eco-friendly 3D prints, achieving strong results with less plastic.</description>
  <pubDate>Thu, 04 Sep 2025 16:30:00 -0400</pubDate>
    <guid isPermaLink="true">https://news.mit.edu/2025/greener-way-3d-print-stronger-stuff-0904</guid>
        <dc:creator>Rachel Gordon | MIT CSAIL</dc:creator>
  <content:encoded>&lt;p dir="ltr" id="docs-internal-guid-8a16a100-7fff-866e-17d3-0c98f0ab7768"&gt;3D printing has come a long way since its invention in 1983 by Chuck Hull, who pioneered stereolithography, a technique that solidifies liquid resin into solid objects using ultraviolet lasers. Over the decades, 3D printers have evolved from experimental curiosities into tools capable of producing everything from custom prosthetics to complex food designs, architectural models, and even functioning human organs.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;But as the technology matures, its environmental footprint has become increasingly difficult to set aside. The vast majority of consumer and industrial 3D printing still relies on petroleum-based plastic filament. And while “greener” alternatives made from biodegradable or recycled materials exist, they come with a serious trade-off: they’re often not as strong. These eco-friendly filaments tend to become brittle under stress, making them ill-suited for structural applications or load-bearing parts — exactly where strength matters most.&lt;/p&gt;&lt;p dir="ltr"&gt;This trade-off between sustainability and mechanical performance prompted researchers at MIT’s Computer Science and Artificial Intelligence Laboratory (CSAIL) and the Hasso Plattner Institute to ask: Is it possible to build objects that are mostly eco-friendly, but still strong where it counts?&lt;/p&gt;&lt;p dir="ltr"&gt;Their answer is &lt;a href="https://maxineaps.github.io/sustainaprint-project-site/"&gt;SustainaPrint&lt;/a&gt;, a new software and hardware toolkit designed to help users strategically combine strong and weak filaments to get the best of both worlds. Instead of printing an entire object with high-performance plastic, the system analyzes a model through finite element analysis simulations, predicts where the object is most likely to experience stress, and then reinforces just those zones with stronger material. The rest of the part can be printed using greener, weaker filament, reducing plastic use while preserving structural integrity.&lt;/p&gt;&lt;p dir="ltr"&gt;“Our hope is that SustainaPrint can be used in industrial and distributed manufacturing settings one day, where local material stocks may vary in quality and composition,” says MIT PhD student and CSAIL researcher Maxine Perroni-Scharf, who is a lead author on a &lt;a href="https://maxineaps.github.io/sustainaprint-project-site/"&gt;paper presenting the project&lt;/a&gt;. “In these contexts, the testing toolkit could help ensure the reliability of available filaments, while the software’s reinforcement strategy could reduce overall material consumption without sacrificing function.”&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;For their experiments, the team used Polymaker’s PolyTerra PLA as the eco-friendly filament, and standard or Tough PLA from Ultimaker for reinforcement. They used a 20 percent reinforcement threshold to show that even a small amount of strong plastic goes a long way. Using this ratio, SustainaPrint was able to recover up to 70 percent of the strength of an object printed entirely with high-performance plastic.&lt;/p&gt;&lt;p dir="ltr"&gt;They printed dozens of objects, from simple mechanical shapes like rings and beams to more functional household items such as headphone stands, wall hooks, and plant pots. Each object was printed three ways: once using only eco-friendly filament, once using only strong PLA, and once with the hybrid SustainaPrint configuration. The printed parts were then mechanically tested by pulling, bending, or otherwise breaking them to measure how much force each configuration could withstand.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;In many cases, the hybrid prints held up nearly as well as the full-strength versions. For example, in one test involving a dome-like shape, the hybrid version outperformed the version printed entirely in Tough PLA. The team believes this may be due to the reinforced version’s ability to distribute stress more evenly, avoiding the brittle failure sometimes caused by excessive stiffness.&lt;/p&gt;&lt;p dir="ltr"&gt;“This indicates that in certain geometries and loading conditions, mixing materials strategically may actually outperform a single homogenous material,” says Perroni-Scharf. “It’s a reminder that real-world mechanical behavior is full of complexity, especially in 3D printing, where interlayer adhesion and tool path decisions can affect performance in unexpected ways.”&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;strong&gt;A lean, green, eco-friendly printing machine&lt;/strong&gt;&lt;/p&gt;&lt;p dir="ltr"&gt;SustainaPrint starts off by letting a user upload their 3D model into a custom interface. By selecting fixed regions and areas where forces will be applied, the software then uses an approach called “Finite Element Analysis” to simulate how the object will deform under stress. It then creates a map showing pressure distribution inside the structure, highlighting areas under compression or tension, and applies heuristics to segment the object into two categories: those that need reinforcement, and those that don’t.&lt;/p&gt;&lt;p dir="ltr"&gt;Recognizing the need for accessible and low-cost testing, the team also developed a DIY testing toolkit to help users assess strength before printing. The kit has a 3D-printable device with modules for measuring both tensile and flexural strength. Users can pair the device with common items like pull-up bars or digital scales to get rough, but reliable performance metrics. The team benchmarked their results against manufacturer data and found that their measurements consistently fell within one standard deviation, even for filaments that had undergone multiple recycling cycles.&lt;/p&gt;&lt;p dir="ltr"&gt;Although the current system is designed for dual-extrusion printers, the researchers believe that with some manual filament swapping and calibration, it could be adapted for single-extruder setups, too. In current form, the system simplifies the modeling process by allowing just one force and one fixed boundary per simulation. While this covers a wide range of common use cases, the team sees future work expanding the software to support more complex and dynamic loading conditions. The team also sees potential in using AI to infer the object’s intended use based on its geometry, which could allow for fully automated stress modeling without manual input of forces or boundaries.&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;strong&gt;3D for free&lt;/strong&gt;&lt;/p&gt;&lt;p dir="ltr"&gt;The researchers plan to release SustainaPrint open-source, making both the software and testing toolkit available for public use and modification. Another initiative they aspire to bring to life in the future: education. “In a classroom, SustainaPrint isn’t just a tool, it’s a way to teach students about material science, structural engineering, and sustainable design, all in one project,” says Perroni-Scharf. “It turns these abstract concepts into something tangible.”&lt;/p&gt;&lt;p dir="ltr"&gt;As 3D printing becomes more embedded in how we manufacture and prototype everything from consumer goods to emergency equipment, sustainability concerns will only grow. With tools like SustainaPrint, those concerns no longer need to come at the expense of performance. Instead, they can become part of the design process: built into the very geometry of the things we make.&lt;/p&gt;&lt;p dir="ltr"&gt;Co-author Patrick Baudisch, who is a professor at the Hasso Plattner Institute, adds that “the project addresses a key question: What is the point of collecting material for the purpose of recycling, when there is no plan to actually ever use that material? Maxine presents the missing link between the theoretical/abstract idea of 3D printing material recycling and what it actually takes to make this idea relevant.”&lt;/p&gt;&lt;p dir="ltr"&gt;Perroni-Scharf and Baudisch wrote the paper with CSAIL research assistant Jennifer Xiao; MIT Department of Electrical Engineering and Computer Science master’s student Cole Paulin ’24; master’s student Ray Wang SM ’25 and PhD student Ticha Sethapakdi SM ’19 (both CSAIL members); Hasso Plattner Institute PhD student Muhammad Abdullah; and Associate Professor Stefanie Mueller, lead of the Human-Computer Interaction Engineering Group at CSAIL.&lt;/p&gt;&lt;p dir="ltr"&gt;The researchers’ work was supported by a Designing for Sustainability Grant from the Designing for Sustainability MIT-HPI Research Program. Their work will be presented at the ACM Symposium on User Interface Software and Technology in September.&lt;/p&gt;</content:encoded>
      <media:content url="https://news.mit.edu/sites/default/files/styles/news_article__cover_image__original/public/images/202509/mit-csail-SustainaPrint.jpg?itok=lSAi4Ddm" medium="image" type="image/jpeg" width="390" height="260">
              <media:description type="plain">A new software and hardware toolkit called SustainaPrint can help users strategically combine strong and weak filaments to achieve the best of both worlds. Instead of printing an entire object with high-performance plastic, the system analyzes a model, predicts where the object is most likely to experience stress, and reinforces those zones with stronger material. </media:description>
              <media:credit>Image: Alex Shipps/MIT CSAIL, using assets from Pixabay and the researchers.</media:credit>
      </media:content>
        <category domain="https://news.mit.edu/topic/research">Research</category>
      <category domain="https://news.mit.edu/topic/3-d-printing">3-D printing</category>
      <category domain="https://news.mit.edu/topic/sustainability">Sustainability</category>
      <category domain="https://news.mit.edu/topic/invention">Invention</category>
      <category domain="https://news.mit.edu/topic/design">Design</category>
      <category domain="https://news.mit.edu/topic/cleaner-industry">Cleaner industry</category>
      <category domain="https://news.mit.edu/topic/computers">Computer science and technology</category>
      <category domain="https://news.mit.edu/topic/machine-learning">Machine learning</category>
      <category domain="https://news.mit.edu/topic/artificial-intelligence2">Artificial intelligence</category>
      <category domain="https://news.mit.edu/topic/recycling">Recycling</category>
      <category domain="https://news.mit.edu/topic/computer-science-and-artificial-intelligence-laboratory-csail">Computer Science and Artificial Intelligence Laboratory (CSAIL)</category>
      <category domain="https://news.mit.edu/topic/electrical-engineering-computer-science-eecs">Electrical engineering and computer science (EECS)</category>
      <category domain="https://news.mit.edu/topic/school-engineering">School of Engineering</category>
      <category domain="https://news.mit.edu/topic/mit-schwarzman-college-computing">MIT Schwarzman College of Computing</category>
    </item>
<item>
  <title>Soft materials hold onto “memories” of their past, for longer than previously thought</title>
  <link>https://news.mit.edu/2025/soft-materials-hold-past-memories-longer-previously-thought-0903</link>
  <description>New findings could help manufacturers design gels, lotions, or even paving materials that last longer and perform more predictably.</description>
  <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
    <guid isPermaLink="true">https://news.mit.edu/2025/soft-materials-hold-past-memories-longer-previously-thought-0903</guid>
        <dc:creator>Jennifer Chu | MIT News</dc:creator>
  <content:encoded>&lt;p&gt;If your hand lotion is a bit runnier&amp;nbsp;than usual coming out of the bottle, it might have something to do with the goop’s “mechanical memory.”&lt;/p&gt;&lt;p&gt;Soft gels and lotions are made by mixing ingredients until they form a stable and uniform substance. But even after a gel has set, it can hold onto “memories,” or residual stress, from the mixing process. Over time, the material can give in to these embedded stresses and slide back into its former, premixed state. Mechanical memory is, in part, why hand lotion separates and gets runny over time.&amp;nbsp;&lt;/p&gt;&lt;p&gt;Now, an MIT engineer has devised a simple way to measure the degree of residual stress in soft materials after they have been mixed, and found that common products like hair gel and shaving cream have longer mechanical memories, holding onto residual stresses for longer periods of time than manufacturers might have assumed.&lt;/p&gt;&lt;p&gt;In a study &lt;a href="https://journals.aps.org/prl/abstract/10.1103/421k-58rm" target="_blank"&gt;appearing today&lt;/a&gt; in &lt;em&gt;Physical Review Letters&lt;/em&gt;, Crystal Owens, a postdoc in MIT’s Computer Science and Artificial Intelligence Laboratory (CSAIL), presents a new protocol for measuring residual stress in soft, gel-like materials, using a standard benchtop rheometer.&lt;/p&gt;&lt;p&gt;Applying this protocol to everyday soft materials, Owens found that if a gel is made by mixing it in one direction, once it settles into a stable and uniform state, it effectively holds onto the memory of the direction in which it is mixed. Even after several days, the gel will hold some internal stress that, if released, will cause the gel to shift in the direction opposite to how it was initially mixed, reverting back to its earlier state.&lt;/p&gt;&lt;p&gt;“This is one reason different batches of cosmetics or food behave differently even if they underwent ‘identical’ manufacturing,” Owens says. “Understanding and measuring these hidden stresses during processing could help manufacturers design better products that last longer and perform more predictably.”&lt;/p&gt;&lt;p&gt;&lt;strong&gt;A soft glass&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Hand lotion, hair gel, and shaving cream all fall under the category of “soft glassy materials” — materials that exhibit properties of both solids and liquids.&lt;/p&gt;&lt;p&gt;“Anything you can pour into your hand and it forms a soft mound is going to be considered a soft glass,” Owens explains. “In materials science, it’s considered a soft version of something that has the same amorphous structure as glass.”&lt;/p&gt;&lt;p&gt;In other words, a soft glassy material is a strange amalgam of a solid and a liquid. It can be poured out like a liquid, and it can hold its shape like a solid. Once they are made, these materials exist in a delicate balance between solid and liquid. And Owens wondered: For how long?&lt;/p&gt;&lt;p&gt;“What happens to these materials after very long times? Do they finally relax or do they never relax?” Owens says. “From a physics perspective, that’s a very interesting concept: What is the essential state of these materials?”&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Twist and hold&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;In the manufacturing of soft glassy materials such as hair gel and shampoo, ingredients are first mixed into a uniform product. Quality control engineers then let a sample sit for about a minute — a period of time that they assume is enough to allow any residual stresses from the mixing process dissipate. In that time, the material should settle into a steady, stable state, ready for use.&lt;/p&gt;&lt;p&gt;But Owens suspected that the materials may hold some degree of stress from the production process long after they’ve appeared to settle.&lt;/p&gt;&lt;p&gt;“Residual stress is a low level of stress that’s trapped inside a material after it’s come to a steady state,” Owens says. “This sort of stress has not been measured in these sorts of materials.”&lt;/p&gt;&lt;p&gt;To test her hypothesis, she carried out experiments with two common soft glassy materials: hair gel and shaving cream. She made measurements of each material in a rheometer — an instrument consisting of two rotating plates that can twist and press a material together at precisely controlled pressures and forces that relate directly to the material’s internal stresses and strains.&lt;/p&gt;&lt;p&gt;In her experiments, she placed each material in the rheometer and spun the instrument’s top plate around to mix the material. Then she let the material settle, and then settle some more —&amp;nbsp;much longer than one minute. During this time, she observed the amount of force it took the rheometer to hold the material in place. She reasoned that the greater the rheometer’s force, the more it must be counteracting any stress within the material that would otherwise cause it to shift out of its current state.&lt;/p&gt;&lt;p&gt;Over multiple experiments using this new protocol, Owens found that different types of soft glassy materials held a significant amount of residual stress, long after most researchers would assume the stress had dissipated. What’s more, she found that the degree of stress that a material retained was a reflection of the direction in which it was initially mixed, and when it was mixed.&lt;/p&gt;&lt;p&gt;“The material can effectively ‘remember’ which direction it was mixed, and how long ago,” Owens says. “And it turns out they hold this memory of their past, a lot longer than we used to think.”&lt;/p&gt;&lt;p&gt;In addition to the protocol she has developed to measure residual stress, Owens has developed a model to estimate how a material will change over time, given the degree of residual stress that it holds. Using this model, she says scientists might design materials with “short-term memory,” or very little residual stress, such that they remain stable over longer periods.&lt;/p&gt;&lt;p&gt;One material where she sees room for such improvement is asphalt — a substance that is first mixed, then poured in molten form over a surface where it then cools and settles over time. She suspects that residual stresses from the mixing of asphalt may contribute to cracks forming in pavement over time. Reducing these stresses at the start of the process could lead to longer-lasting, more resilient roads.&lt;/p&gt;&lt;p&gt;“People are inventing new types of asphalt all the time to be more eco-friendly, and all of these will have different levels of residual stress that will need some control,” she says. “There’s plenty of room to explore.”&lt;/p&gt;&lt;p&gt;This research was supported, in part, by&amp;nbsp;MIT’s Postdoctoral Fellowship for Engineering Excellence and an MIT Mathworks Fellowship.&lt;/p&gt;</content:encoded>
      <media:content url="https://news.mit.edu/sites/default/files/styles/news_article__cover_image__original/public/images/202509/MIT-MaterialMemory-01-press.jpg?itok=WiCXaMQm" medium="image" type="image/jpeg" width="390" height="260">
              <media:description type="plain">Using a rheometer, MIT researchers tested the residual memory of soft materials such as hair gel, pictured here. </media:description>
              <media:credit>Credit: Courtesy of the researchers</media:credit>
      </media:content>
        <category domain="https://news.mit.edu/topic/research">Research</category>
      <category domain="https://news.mit.edu/topic/materialsscienceandengineering">Materials science and engineering</category>
      <category domain="https://news.mit.edu/topic/mechanical-engineering">Mechanical engineering</category>
      <category domain="https://news.mit.edu/topic/computer-science-and-artificial-intelligence-laboratory-csail">Computer Science and Artificial Intelligence Laboratory (CSAIL)</category>
      <category domain="https://news.mit.edu/topic/school-engineering">School of Engineering</category>
      <category domain="https://news.mit.edu/topic/mit-schwarzman-college-computing">MIT Schwarzman College of Computing</category>
    </item>
<item>
  <title>MIT researchers develop AI tool to improve flu vaccine strain selection</title>
  <link>https://news.mit.edu/2025/vaxseer-ai-tool-to-improve-flu-vaccine-strain-selection-0828</link>
  <description>VaxSeer uses machine learning to predict virus evolution and antigenicity, aiming to make vaccine selection more accurate and less reliant on guesswork.</description>
  <pubDate>Thu, 28 Aug 2025 11:50:00 -0400</pubDate>
    <guid isPermaLink="true">https://news.mit.edu/2025/vaxseer-ai-tool-to-improve-flu-vaccine-strain-selection-0828</guid>
        <dc:creator>Rachel Gordon | MIT CSAIL</dc:creator>
  <content:encoded>&lt;p dir="ltr" id="docs-internal-guid-2ef4ad7e-7fff-c28e-41c1-d61ce18592ae"&gt;Every year, global health experts are faced with a high-stakes decision: Which influenza strains should go into the next seasonal vaccine? The choice must be made months in advance, long before flu season even begins, and it can often feel like a race against the clock. If the selected strains match those that circulate, the vaccine will likely be highly effective. But if the prediction is off, protection can drop significantly, leading to (potentially preventable) illness and strain on health care systems.&lt;/p&gt;&lt;p dir="ltr"&gt;This challenge became even more familiar to scientists in the years during the Covid-19 pandemic. Think back to the time (and time and time again), when new variants emerged just as vaccines were being rolled out. Influenza behaves like a similar, rowdy cousin, mutating constantly and unpredictably. That makes it hard to stay ahead, and therefore harder to design vaccines that remain protective.&lt;/p&gt;&lt;p dir="ltr"&gt;To reduce this uncertainty, scientists at MIT’s Computer Science and Artificial Intelligence Laboratory (CSAIL) and the MIT Abdul Latif Jameel Clinic for Machine Learning in Health set out to make vaccine selection more accurate and less reliant on guesswork. They created an AI system called VaxSeer, designed to predict dominant flu strains and identify the most protective vaccine candidates, months ahead of time. The tool uses deep learning models trained on decades of viral sequences and lab test results to simulate how the flu virus might evolve and how the vaccines will respond.&lt;/p&gt;&lt;p dir="ltr"&gt;Traditional evolution models often analyze the effect of single amino acid mutations independently. “VaxSeer adopts a large protein language model to learn the relationship between dominance and the combinatorial effects of mutations,” explains Wenxian Shi, a PhD student in MIT’s Department of Electrical Engineering and Computer Science, researcher at CSAIL, and lead author of a new paper on the work. “Unlike existing protein language models that assume a static distribution of viral variants, we model dynamic dominance shifts, making it better suited for rapidly evolving viruses like influenza.”&lt;/p&gt;&lt;p dir="ltr"&gt;An &lt;a href="https://www.nature.com/articles/s41591-025-03917-y"&gt;open-access report on the study&lt;/a&gt; was published today in &lt;em&gt;Nature Medicine.&lt;/em&gt;&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;strong&gt;The future of flu&lt;/strong&gt;&lt;/p&gt;&lt;p dir="ltr"&gt;VaxSeer has two core prediction engines: one that estimates how likely each viral strain is to spread (dominance), and another that estimates how effectively a vaccine will neutralize that strain (antigenicity). Together, they produce a predicted coverage score: a forward-looking measure of how well a given vaccine is likely to perform against future viruses.&lt;/p&gt;&lt;p dir="ltr"&gt;The scale of the score could be from an infinite negative to 0. The closer the score to 0, the better the antigenic match of vaccine strains to the circulating viruses. (You can imagine it as the negative of some kind of “distance.”)&lt;/p&gt;&lt;p dir="ltr"&gt;In a 10-year retrospective study, the researchers evaluated VaxSeer’s recommendations against those made by the World Health Organization (WHO) for two major flu subtypes: A/H3N2 and A/H1N1. For A/H3N2, VaxSeer’s choices outperformed the WHO’s in nine out of 10 seasons, based on retrospective empirical coverage scores (a surrogate metric of the vaccine effectiveness, calculated from the observed dominance from past seasons and experimental HI test results). The team used this to evaluate vaccine selections, as the effectiveness is only available for vaccines actually given to the population.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;For A/H1N1, it outperformed or matched the WHO in six out of 10 seasons. In one notable case, for the 2016 flu season, VaxSeer identified a strain that wasn’t chosen by the WHO until the following year. The model’s predictions also showed strong correlation with real-world vaccine effectiveness estimates, as reported by the CDC, Canada’s Sentinel Practitioner Surveillance Network, and Europe’s I-MOVE program. VaxSeer’s predicted coverage scores aligned closely with public health data on flu-related illnesses and medical visits prevented by vaccination.&lt;/p&gt;&lt;p dir="ltr"&gt;So how exactly does VaxSeer make sense of all these data? Intuitively, the model first estimates how rapidly a viral strain spreads over time using a protein language model, and then determines its dominance by accounting for competition among different strains.&lt;/p&gt;&lt;p dir="ltr"&gt;Once the model has calculated its insights, they’re plugged into a mathematical framework based on something called ordinary differential equations to simulate viral spread over time. For antigenicity, the system estimates how well a given vaccine strain will perform in a common lab test called the hemagglutination inhibition assay. This measures how effectively antibodies can&amp;nbsp;inhibit the virus from binding to human red blood cells, which is a widely used proxy for antigenic match/antigenicity.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;strong&gt;Outpacing evolution&lt;/strong&gt;&lt;/p&gt;&lt;p dir="ltr"&gt;“By modeling how viruses evolve and how vaccines interact with them, AI tools like VaxSeer could help health officials make better, faster decisions — and stay one step ahead in the race between infection and immunity,” says Shi.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;VaxSeer currently focuses only on the flu virus’s HA (hemagglutinin) protein,the major antigen of influenza. Future versions could incorporate other proteins like NA (neuraminidase), and factors like immune history, manufacturing constraints, or dosage levels. Applying the system to other viruses would also require large, high-quality datasets that track both viral evolution and immune responses — data that aren’t always publicly available. The team, however is currently working on the methods that can predict viral evolution in low-data regimes building on relations between viral families&lt;/p&gt;&lt;p dir="ltr"&gt;“Given the speed of viral evolution, current therapeutic development often lags behind.&amp;nbsp;VaxSeer is our attempt to catch up,” says Regina Barzilay, the School of Engineering Distinguished Professor for AI and Health at MIT, AI lead of Jameel Clinic, and CSAIL principal investigator.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;“This paper is impressive, but what excites me perhaps even more is the team’s ongoing work on predicting viral evolution in low-data settings,” says Assistant Professor Jon Stokes of the Department of Biochemistry and Biomedical Sciences at McMaster University in Hamilton, Ontario. “The implications go far beyond influenza. Imagine being able to anticipate how antibiotic-resistant bacteria or drug-resistant cancers might evolve, both of which can adapt rapidly. This kind of predictive modeling opens up a powerful new way of thinking about how diseases change, giving us the opportunity to stay one step ahead and design clinical interventions before escape becomes a major problem.”&lt;/p&gt;&lt;p dir="ltr"&gt;Shi and Barzilay wrote the paper with MIT CSAIL postdoc Jeremy Wohlwend ’16, MEng ’17, PhD ’25 and recent CSAIL affiliate Menghua Wu ’19, MEng ’20, PhD ’25. Their work was supported, in part, by the U.S. Defense Threat Reduction Agency and MIT Jameel Clinic.&lt;/p&gt;</content:encoded>
      <media:content url="https://news.mit.edu/sites/default/files/styles/news_article__cover_image__original/public/images/202508/mit-vaxseer-barzilay-shi-00_1.jpg?itok=kphvEHKT" medium="image" type="image/jpeg" width="390" height="260">
              <media:description type="plain">The VaxSeer system developed at MIT can predict dominant flu strains and identify the most protective vaccine candidates. The tool uses deep learning models trained on decades of viral sequences and lab test results to simulate how the flu virus might evolve and how the vaccines will respond. Pictured: Senior author Regina Barzilay (left) and first author Wenxian Shi.</media:description>
              <media:credit>Image: Alex Gagne</media:credit>
      </media:content>
        <category domain="https://news.mit.edu/topic/research">Research</category>
      <category domain="https://news.mit.edu/topic/viruses">Viruses</category>
      <category domain="https://news.mit.edu/topic/algorithms">Algorithms</category>
      <category domain="https://news.mit.edu/topic/data">Data</category>
      <category domain="https://news.mit.edu/topic/artificial-intelligence2">Artificial intelligence</category>
      <category domain="https://news.mit.edu/topic/machine-learning">Machine learning</category>
      <category domain="https://news.mit.edu/topic/influenza">Influenza</category>
      <category domain="https://news.mit.edu/topic/vaccines">Vaccines</category>
      <category domain="https://news.mit.edu/topic/medicine">Medicine</category>
      <category domain="https://news.mit.edu/topic/covid-19">Covid-19</category>
      <category domain="https://news.mit.edu/topic/health-care">Health care</category>
      <category domain="https://news.mit.edu/topic/disease">Disease</category>
      <category domain="https://news.mit.edu/topic/health">Health sciences and technology</category>
      <category domain="https://news.mit.edu/topic/drug-development">Drug development</category>
      <category domain="https://news.mit.edu/topic/computers">Computer science and technology</category>
      <category domain="https://news.mit.edu/topic/jameel-clinic">Jameel Clinic</category>
      <category domain="https://news.mit.edu/topic/electrical-engineering-computer-science-eecs">Electrical engineering and computer science (EECS)</category>
      <category domain="https://news.mit.edu/topic/computer-science-and-artificial-intelligence-laboratory-csail">Computer Science and Artificial Intelligence Laboratory (CSAIL)</category>
      <category domain="https://news.mit.edu/topic/school-engineering">School of Engineering</category>
      <category domain="https://news.mit.edu/topic/mit-schwarzman-college-computing">MIT Schwarzman College of Computing</category>
    </item>
<item>
  <title>Researchers glimpse the inner workings of protein language models</title>
  <link>https://news.mit.edu/2025/researchers-glimpse-inner-workings-protein-language-models-0818</link>
  <description>A new approach can reveal the features AI models use to predict proteins that might make good drug or vaccine targets.</description>
  <pubDate>Mon, 18 Aug 2025 15:00:00 -0400</pubDate>
    <guid isPermaLink="true">https://news.mit.edu/2025/researchers-glimpse-inner-workings-protein-language-models-0818</guid>
        <dc:creator>Anne Trafton | MIT News</dc:creator>
  <content:encoded>&lt;p&gt;Within the past few years, models that can predict the structure or function of proteins have been widely used for a variety of biological applications, such as identifying drug targets and designing new therapeutic antibodies.&lt;/p&gt;&lt;p&gt;These models, which are based on large language models (LLMs), can make very accurate predictions of a protein’s suitability for a given application. However, there’s no way to determine how these models make their predictions or which protein features play the most important role in those decisions.&lt;/p&gt;&lt;p&gt;In a new study, MIT researchers have used a novel technique to open up that “black box” and allow them to determine what features a protein language model takes into account when making predictions. Understanding what is happening inside that black box&amp;nbsp;could help researchers to choose better models for a particular task, helping to streamline the process of identifying new drugs or vaccine targets.&lt;/p&gt;&lt;p&gt;“Our work has broad implications for enhanced explainability in downstream tasks that rely on these representations,” says Bonnie Berger, the Simons Professor of Mathematics, head of the Computation and Biology group in MIT’s Computer Science and Artificial Intelligence Laboratory, and the senior author of the study. “Additionally, identifying features that protein language models track has the potential to reveal novel biological insights from these representations.”&lt;/p&gt;&lt;p&gt;Onkar Gujral, an MIT graduate student, is the lead author of the open-access &lt;a href="https://www.pnas.org/doi/10.1073/pnas.2506316122" target="_blank"&gt;study&lt;/a&gt;, which appears this week in the &lt;em&gt;Proceedings of the National Academy of Sciences.&lt;/em&gt; Mihir Bafna, an MIT graduate student in electrical engineering and computer science, and Eric Alm, an MIT professor of biological engineering, are also authors of the paper.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Opening the black box&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;In 2018, Berger and former MIT graduate student Tristan Bepler PhD ’20 &lt;a href="https://openreview.net/revisions?id=SygLehCqtm" target="_blank"&gt;introduced&lt;/a&gt; the first protein language model. Their model, like subsequent protein models that accelerated the development of&amp;nbsp;AlphaFold, such as&amp;nbsp;ESM2 and OmegaFold, was based on LLMs. These models, which include ChatGPT, can analyze huge amounts of text and figure out which words are most likely to appear together.&lt;/p&gt;&lt;p&gt;Protein language models use a similar approach, but instead of analyzing words, they analyze amino acid sequences. Researchers have used these models to predict the structure and function of proteins, and for applications such as identifying proteins that might bind to particular drugs.&lt;/p&gt;&lt;p&gt;In a&amp;nbsp;&lt;a href="https://news.mit.edu/2021/model-viruses-escape-immune-0114" target="_blank"&gt;2021 study&lt;/a&gt;, Berger and colleagues used a protein language model to predict which sections of viral surface proteins are less likely to mutate in a way that enables viral escape. This allowed them to identify possible targets for vaccines against influenza, HIV, and SARS-CoV-2.&lt;/p&gt;&lt;p&gt;However, in all of these studies, it has been impossible to know how the models were making their predictions.&lt;/p&gt;&lt;p&gt;“We would get out some prediction at the end, but we had absolutely no idea what was happening in the individual components of this black box,” Berger says.&lt;/p&gt;&lt;p&gt;In the new study, the researchers wanted to dig into how protein language models make their predictions. Just like LLMs, protein language models encode information as representations that consist of a pattern of activation of different “nodes” within a neural network. These nodes are analogous to the networks of neurons that store memories and other information within the brain.&lt;/p&gt;&lt;p&gt;The inner workings of LLMs are not easy to interpret, but within the past couple of years, researchers have begun using a type of algorithm known as a sparse autoencoder to help shed some light on how those models make their predictions. The new study from Berger’s lab is the first to use this algorithm on protein language models.&lt;/p&gt;&lt;p&gt;Sparse autoencoders work by adjusting how a protein is represented within a neural network. Typically, a given protein will be represented by a pattern of activation of a constrained number of neurons, for example, 480. A sparse autoencoder will expand that representation into a much larger number of nodes, say 20,000.&lt;/p&gt;&lt;p&gt;When information about a protein is encoded by only 480 neurons, each node lights up for multiple features, making it very difficult to know what features each node is encoding. However, when the neural network is expanded to 20,000 nodes, this extra space along with a sparsity constraint gives the information room to “spread out.” Now, a feature of the protein that was previously encoded by multiple nodes can occupy a single node.&lt;/p&gt;&lt;p&gt;“In a sparse representation, the neurons lighting up are doing so in a more meaningful manner,” Gujral says. “Before the sparse representations are created, the networks pack information so tightly together that it's hard to interpret the neurons.”&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Interpretable models&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Once the researchers obtained sparse representations of many proteins, they used an AI assistant called Claude (related to the popular Anthropic chatbot of the same name), to analyze the representations. In this case, they asked Claude to compare the sparse representations with the known features of each protein, such as molecular function, protein family, or location within a cell.&lt;/p&gt;&lt;p&gt;By analyzing thousands of representations, Claude can determine which nodes correspond to specific protein features, then describe them in plain English. For example, the algorithm might say, “This neuron appears to be detecting proteins involved in transmembrane transport of ions or amino acids, particularly those located in the plasma membrane.”&lt;/p&gt;&lt;p&gt;This process makes the nodes far more “interpretable,” meaning the researchers can tell what each node is encoding. They found that the features most likely to be encoded by these nodes were protein family and certain functions, including several different metabolic and biosynthetic processes.&lt;/p&gt;&lt;p&gt;“When you train a sparse autoencoder, you aren’t training it to be interpretable, but it turns out that by incentivizing the representation to be really sparse, that ends up resulting in interpretability,” Gujral says.&lt;/p&gt;&lt;p&gt;Understanding what features a particular protein model is encoding could help researchers choose the right model for a particular task, or tweak the type of input they give the model, to generate the best results. Additionally, analyzing the features that a model encodes could one day help biologists to learn more about the proteins that they are studying.&lt;/p&gt;&lt;p&gt;“At some point when the models get a lot more powerful, you could learn more biology than you already know, from opening up the models,” Gujral says.&lt;/p&gt;&lt;p&gt;The research was funded by the National Institutes of Health.&amp;nbsp;&lt;/p&gt;</content:encoded>
      <media:content url="https://news.mit.edu/sites/default/files/styles/news_article__cover_image__original/public/images/202508/MIT-model-interpret-01-press.jpg?itok=nA-s3rBF" medium="image" type="image/jpeg" width="390" height="260">
              <media:description type="plain">Understanding what is happening inside the “black box” of large protein models could help researchers to choose better models for a particular task, helping to streamline the process of identifying new drugs or vaccine targets.</media:description>
              <media:credit>Image: MIT News; iStock</media:credit>
      </media:content>
        <category domain="https://news.mit.edu/topic/artificial-intelligence2">Artificial intelligence</category>
      <category domain="https://news.mit.edu/topic/research">Research</category>
      <category domain="https://news.mit.edu/topic/proteins">Proteins</category>
      <category domain="https://news.mit.edu/topic/drug-discovery">Drug discovery</category>
      <category domain="https://news.mit.edu/topic/mathematics">Mathematics</category>
      <category domain="https://news.mit.edu/topic/biological-engineering">Biological engineering</category>
      <category domain="https://news.mit.edu/topic/electrical-engineering-computer-science-eecs">Electrical engineering and computer science (EECS)</category>
      <category domain="https://news.mit.edu/topic/computer-science-and-artificial-intelligence-laboratory-csail">Computer Science and Artificial Intelligence Laboratory (CSAIL)</category>
      <category domain="https://news.mit.edu/topic/school-engineering">School of Engineering</category>
      <category domain="https://news.mit.edu/topic/school-science">School of Science</category>
      <category domain="https://news.mit.edu/topic/mit-schwarzman-college-computing">MIT Schwarzman College of Computing</category>
      <category domain="https://news.mit.edu/topic/nih">National Institutes of Health (NIH)</category>
    </item>
<item>
  <title>A shape-changing antenna for more versatile sensing and communication</title>
  <link>https://news.mit.edu/2025/shape-changing-antenna-more-versatile-sensing-and-communication-0818</link>
  <description>You can adjust the frequency range of this durable, inexpensive antenna by squeezing or stretching its structure.</description>
  <pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
    <guid isPermaLink="true">https://news.mit.edu/2025/shape-changing-antenna-more-versatile-sensing-and-communication-0818</guid>
        <dc:creator>Adam Zewe | MIT News</dc:creator>
  <content:encoded>&lt;p&gt;MIT researchers have developed a reconfigurable antenna that dynamically adjusts its frequency range by changing its physical shape, making it more versatile for communications and sensing than static antennas.&lt;/p&gt;&lt;p&gt;A user can stretch, bend, or compress the antenna to make reversible changes to its radiation properties, enabling a device to operate in a wider frequency range without the need for complex, moving parts. With an adjustable frequency range, a reconfigurable antenna could adapt to changing environmental conditions and reduce the need for multiple antennas.&lt;/p&gt;&lt;p&gt;The word “antenna” may draw to mind metal rods like the “bunny ears” on top of old television sets, but the MIT team instead worked with metamaterials — engineered materials whose mechanical properties, such as stiffness and strength, depend on the geometric arrangement of the material’s components.&lt;/p&gt;&lt;p&gt;The result is a simplified design for a reconfigurable antenna that could be used for applications like energy transfer in wearable devices, motion tracking and sensing for augmented reality, or wireless communication across a wide range of network protocols.&lt;/p&gt;&lt;p&gt;In addition, the researchers developed an editing tool so users can generate customized metamaterial antennas, which can be fabricated using a laser cutter.&lt;/p&gt;&lt;p&gt;“Usually, when we think of antennas, we think of static antennas — they are fabricated to have specific properties and that is it. However, by using auxetic metamaterials, which can deform into three different geometric states, we can seamlessly change the properties of the antenna by changing its geometry, without fabricating a new structure. In addition, we can use changes in the antenna’s radio frequency properties, due to changes in the metamaterial geometry, as a new method of sensing for interaction design,” says lead author Marwa AlAlawi, a mechanical engineering graduate student at MIT.&lt;/p&gt;&lt;p&gt;Her co-authors include Regina Zheng and Katherine Yan, both MIT undergraduate students; Ticha Sethapakdi, an MIT graduate student in electrical engineering and computer science; Soo Yeon Ahn of the Gwangju Institute of Science and Technology in Korea; and co-senior authors Junyi Zhu, assistant professor at the University of Michigan; and Stefanie Mueller, the TIBCO Career Development Associate Professor in MIT’s departments of Electrical Engineering and Computer Science and Mechanical Engineering and leader of the Human-Computer Interaction Group at the Computer Science and Artificial Intelligence Lab. The &lt;a href="https://hcie.csail.mit.edu/research/Meta_antenna/Meta-antenna.html" target="_blank"&gt;research&lt;/a&gt; will be presented at the ACM Symposium on User Interface Software and Technology.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Making sense of antennas&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;While traditional antennas radiate and receive radio signals, in this work, the researchers looked at how the devices can act as sensors. The team’s goal was to develop a mechanical element that can also be used as an antenna for sensing.&lt;/p&gt;&lt;p&gt;To do this, they leveraged the antenna’s “resonance frequency,” which is the frequency at which the antenna is most efficient.&lt;/p&gt;&lt;p&gt;An antenna’s resonance frequency will shift due to changes in its shape. (Think about extending the left “bunny ear” to reduce TV static.) Researchers can capture these shifts for sensing. For instance, a reconfigurable antenna could be used in this way to detect the expansion of a person’s chest, to monitor their respiration.&lt;/p&gt;&lt;p&gt;To design a versatile reconfigurable antenna, the researchers used metamaterials. These engineered materials, which can be programmed to adopt different shapes, are composed of a periodic arrangement of unit cells that can be rotated, compressed, stretched, or bent.&lt;/p&gt;&lt;p&gt;By deforming the metamaterial structure, one can shift the antenna’s resonance frequency.&lt;/p&gt;&lt;p&gt;“In order to trigger changes in resonance frequency, we either need to change the antenna’s effective length or introduce slits and holes into it. Metamaterials allow us to get those different states from only one structure,” AlAlawi says.&lt;/p&gt;&lt;p&gt;The device, dubbed the meta-antenna, is composed of a dielectric layer of material sandwiched between two conductive layers.&lt;/p&gt;&lt;p&gt;To fabricate a meta-antenna, the researchers cut the dielectric laser out of a rubber sheet with a laser cutter. Then they added a patch on top of the dielectric layer using conductive spray paint, creating a resonating “patch antenna.”&lt;/p&gt;&lt;p&gt;But they found that even the most flexible conductive material couldn’t withstand the amount of deformation the antenna would experience.&lt;/p&gt;&lt;p&gt;“We did a lot of trial and error to determine that, if we coat the structure with flexible acrylic paint, it protects the hinges so they don’t break prematurely,” AlAlawi&amp;nbsp;explains.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;A means for makers&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;With the fabrication problem solved, the researchers built a tool that enables users to design and produce metamaterial antennas for specific applications.&lt;/p&gt;&lt;p&gt;The user can define the size of the antenna patch, choose a thickness for the dielectric layer, and set the length to width ratio of the metamaterial unit cells. Then the system automatically simulates the antenna’s resonance frequency range.&lt;/p&gt;&lt;p&gt;“The beauty of metamaterials is that, because it is an interconnected system of linkages, the geometric structure allows us to reduce the complexity of a mechanical system,” AlAlawi&amp;nbsp;says.&lt;/p&gt;&lt;p&gt;Using the design tool, the researchers incorporated meta-antennas into several smart devices, including a curtain that dynamically adjusts household lighting and headphones that seamlessly transition between noise-cancelling and transparent modes.&lt;/p&gt;&lt;p&gt;For the smart headphone, for instance, when the meta-antenna expands and bends, it shifts the resonance frequency by 2.6 percent, which switches the headphone mode. The team’s experiments also showed that meta-antenna structures are durable enough to withstand more than 10,000 compressions.&lt;/p&gt;&lt;p&gt;Because the antenna patch can be patterned onto any surface, it could be used with more complex structures. For instance, the antenna could be incorporated into smart textiles that perform noninvasive biomedical sensing or temperature monitoring.&lt;/p&gt;&lt;p&gt;In the future, the researchers want to design three-dimensional meta-antennas for a wider range of applications. They also want to add more functions to the design tool, improve the durability and flexibility of the metamaterial structure, experiment with different symmetric metamaterial patterns, and streamline some manual fabrication steps.&lt;/p&gt;&lt;p&gt;This research was funded, in part, by the Bahrain Crown Prince International Scholarship and the Gwangju Institute of Science and Technology.&lt;/p&gt;</content:encoded>
      <media:content url="https://news.mit.edu/sites/default/files/styles/news_article__cover_image__original/public/images/202508/MIT-MetaAntenna-01-press.jpg?itok=M02IGbJw" medium="image" type="image/jpeg" width="390" height="260">
              <media:description type="plain">A meta-antenna (shiny latticed material) could be incorporated into a curtain that dynamically adjusts household lighting. Here, a prototype is seen retracted (top left), expanded (bottom), and next to the latching mechanism (top right). </media:description>
              <media:credit>Credit: Courtesy of the researchers</media:credit>
      </media:content>
        <category domain="https://news.mit.edu/topic/research">Research</category>
      <category domain="https://news.mit.edu/topic/sensors">Sensors</category>
      <category domain="https://news.mit.edu/topic/electronics">Electronics</category>
      <category domain="https://news.mit.edu/topic/maker-movement">Maker movement</category>
      <category domain="https://news.mit.edu/topic/wireless">Wireless</category>
      <category domain="https://news.mit.edu/topic/mobile-devices">Mobile devices</category>
      <category domain="https://news.mit.edu/topic/human-computer-interaction">Human-computer interaction</category>
      <category domain="https://news.mit.edu/topic/computers">Computer science and technology</category>
      <category domain="https://news.mit.edu/topic/mechanical-engineering">Mechanical engineering</category>
      <category domain="https://news.mit.edu/topic/computer-science-and-artificial-intelligence-laboratory-csail">Computer Science and Artificial Intelligence Laboratory (CSAIL)</category>
      <category domain="https://news.mit.edu/topic/electrical-engineering-computer-science-eecs">Electrical engineering and computer science (EECS)</category>
      <category domain="https://news.mit.edu/topic/school-engineering">School of Engineering</category>
      <category domain="https://news.mit.edu/topic/mit-schwarzman-college-computing">MIT Schwarzman College of Computing</category>
    </item>
<item>
  <title>Building a lifeline for family caregivers across the US</title>
  <link>https://news.mit.edu/2025/ianacare-builds-lifeline-for-family-caregivers-across-us-0811</link>
  <description>Ianacare, co-founded by Steven Lee ’97, MEng ’98, equips caregivers with the resources, networks, and tools they need to support loved ones.</description>
  <pubDate>Mon, 11 Aug 2025 12:00:00 -0400</pubDate>
    <guid isPermaLink="true">https://news.mit.edu/2025/ianacare-builds-lifeline-for-family-caregivers-across-us-0811</guid>
        <dc:creator>Zach Winn | MIT News</dc:creator>
  <content:encoded>&lt;p&gt;There are 63 million people caring for family members with an illness or disability in the U.S. That translates to one in four adults devoting their time to helping loved ones with things like transportation, meals, prescriptions, and medical appointments.&lt;/p&gt;&lt;p&gt;Caregiving exacts a huge toll on the people responsible, and ianacare is seeking to lessen the burden. The company, founded by Steven Lee ’97, MEng ’98 and Jessica Kim, has built a platform that helps caregivers navigate available tools and local resources, build a network of friends and family to assist with everyday tasks, and coordinate meals, rides, and care shifts.&lt;/p&gt;&lt;p&gt;The name ianacare is short for “I am not alone care.” The company’s mission is to equip and empower the millions of people who perform a difficult and underappreciated role in our society.&lt;/p&gt;&lt;p&gt;“Family caregivers are the invisible backbone of the health care system,” Lee says. “Without them, the health care system would literally collapse, but they are still largely unrecognized. Ianacare acts as the front door for family caregivers. These caregivers are often thrust into this role untrained and unguided. But the moment they start, they have to become experts. Ianacare fills that gap.”&lt;/p&gt;&lt;p&gt;The company has partnered with employers and health care providers to serve more than 50,000 caregivers to date. And thanks to a partnerships with organizations like Elevance Health, the American Association of Retired Persons (AARP), and Medicare providers, its coordination and support tools are available to family caregivers across the country.&lt;/p&gt;&lt;p&gt;“Ultimately we want to make the biggest impact possible,” Lee says. “From a business standpoint, the 50,000 caregivers we’ve served is a huge number. But from the overall universe of caregivers that could use our help, it’s relatively small. We’re on a mission to help all 63 million caregivers.”&lt;/p&gt;&lt;p&gt;&lt;strong&gt;From ad tech to ianacare&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;As an electrical engineering and computer science student at MIT in the 1990s, Lee conducted research on early speech-recognition technology as part of the Spoken Language Systems group in MIT’s Computer Science and Artificial Intelligence Laboratory (CSAIL).&lt;/p&gt;&lt;p&gt;Following graduation, Lee started a company with Waikit Lau ’97 that optimized video advertising placement within streams. The company has gone through several mergers and acquisitions, but is now part of the public company Magnite, which places the ads on platforms like Netflix, Hulu, and Disney+.&lt;/p&gt;&lt;p&gt;Lee left the company in 2016 and began advising startups through programs including MIT’s Venture Mentoring Service as he looked to work on something he would find more meaningful.&lt;/p&gt;&lt;p&gt;“Over the years, the MIT network has been invaluable for connecting with customers, recruiting top talent, and engaging investors,” Lee says. “So much innovation flows out of MIT, and I’ve loved giving back, especially working alongside [VMS Venture Mentor] Paul Bosco ’95 and the rest of the VMS team. It’s deeply rewarding to share the best practices I’ve learned with the next generation of innovators.”&lt;/p&gt;&lt;p&gt;In 2017, Lee met Kim, who was caregiving for her mother with pancreatic cancer. Hearing about her experience brought him back to his own family’s challenges caring for his grandfather with Parkinson’s disease when Lee was a child.&lt;/p&gt;&lt;p&gt;“We realized the gaps that existed in caregiving support three decades ago still exist,” Lee says. “Nothing has changed.”&lt;/p&gt;&lt;p&gt;Officially launched in 2018,&amp;nbsp;ianacare may seem far-removed from speech recognition or ad technologies, but Lee sees the work as an extension of his previous experiences.&lt;/p&gt;&lt;p&gt;“In my mind, AI got its start in speech recognition, and the intelligence we use to surface recommendations and create care plans for family caregivers uses a lot of the same statistical modeling techniques I used in speech recognition and ad placement,” Lee says. “It all goes back to the foundation I got at MIT.”&lt;/p&gt;&lt;p&gt;The founders first launched a free solution that allowed caregivers to connect with friends and family members to coordinate caregiving tasks.&lt;/p&gt;&lt;p&gt;“In our app, you can coordinate with anyone who’s interested in helping,” Lee says. “When you share a struggle with a friend or co-worker, they always say, ‘How can I help?’ But caregivers rarely go back to them and actually ask. In our platform, you can add those people to your informal care team and ask the team for help with something instead of having to text someone directly, which you’re less likely to do.”&lt;/p&gt;&lt;p&gt;Next, the founders built an enterprise solution so businesses could help employee caregivers, adding features like resource directories and ways to find and select various caregiving tools.&lt;/p&gt;&lt;p&gt;“An immense amount of local resources are available, but nobody knows about them,” Lee says. “For instance, every county in the country has an Area Agency on Aging, but these agencies aren’t marketing experts, and caregivers don’t know where to get guidance.”&lt;/p&gt;&lt;p&gt;Last year, ianacare began working with AARP&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;and health care providers participating in the nationwide GUIDE model (for “Guiding an Improved Dementia Experience”) to improve the quality of life for dementia patients and their caregivers. Through the voluntary program, participants can use ianacare’s platform to coordinate care, access educational resources, and access free respite care up to $2,500 each year.&lt;/p&gt;&lt;p&gt;Lee says the CMS partnership gives ianacare a pathway to reach millions of people caring for dementia patients across the country.&lt;/p&gt;&lt;p&gt;“This is already a crisis, and it will get worse because we have an aging population and a capacity-constraint in our health care system,” Lee says. “The population above 65 is set to double between 2000 and 2040. We aren’t going to have three times the hospitals or three times the doctors or nurse practitioners. So, we can either make clinicians more efficient or move more health care into the home. That’s why we have empower family caregivers.”&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Aging with dignity&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Lee recalls one family who used ianacare after their son was born with a severe disease. The child only lived eight months, but for those eight months, the parents had meals delivered to them in the hospital by friends and family.&lt;/p&gt;&lt;p&gt;“It was not something they had to worry about the entire time their son was alive,” Lee says. “It’s been rewarding to help these people in so much need.”&lt;/p&gt;&lt;p&gt;Other ianacare users say the platform has helped them keep their parents out of the hospital and lessen their depression and anxiety around caregiving.&lt;/p&gt;&lt;p&gt;“Nobody wants to die in a hospital, so we’ve worked hard to honor the wishes of loved ones who want to age in the home,” Lee says. “We have a lot of examples of folks who, if our support was not there, their loved one would have had to enter a nursing home or institution. Ianacare is there to ensure the home is safe and that the caregiver can manage the care burden. It’s a win-win for everybody because it’s also less costly for the health care system.”&lt;/p&gt;</content:encoded>
      <media:content url="https://news.mit.edu/sites/default/files/styles/news_article__cover_image__original/public/images/202508/MIT-IanaCare-01-press.jpg?itok=28WugL_7" medium="image" type="image/jpeg" width="390" height="260">
              <media:description type="plain">The alumnus-founded Ianacare equips caregivers with the resources, network, and tools they need to thrive.</media:description>
              <media:credit>Image: Ianacare; MIT News</media:credit>
      </media:content>
        <category domain="https://news.mit.edu/topic/health-care">Health care</category>
      <category domain="https://news.mit.edu/topic/alumni">Alumni/ae</category>
      <category domain="https://news.mit.edu/topic/startups">Startups</category>
      <category domain="https://news.mit.edu/topic/innovation">Innovation and Entrepreneurship (I&amp;E)</category>
      <category domain="https://news.mit.edu/topic/aging">Aging</category>
      <category domain="https://news.mit.edu/topic/school-engineering">School of Engineering</category>
      <category domain="https://news.mit.edu/topic/computer-science-and-artificial-intelligence-laboratory-csail">Computer Science and Artificial Intelligence Laboratory (CSAIL)</category>
      <category domain="https://news.mit.edu/topic/mit-schwarzman-college-computing">MIT Schwarzman College of Computing</category>
    </item>
<item>
  <title>MIT tool visualizes and edits “physically impossible” objects</title>
  <link>https://news.mit.edu/2025/mit-meschers-tool-visualizes-edits-physically-impossible-objects-0804</link>
  <description>By visualizing Escher-like optical illusions in 2.5 dimensions, the “Meschers” tool could help scientists understand physics-defying shapes and spark new designs.</description>
  <pubDate>Mon, 04 Aug 2025 16:40:00 -0400</pubDate>
    <guid isPermaLink="true">https://news.mit.edu/2025/mit-meschers-tool-visualizes-edits-physically-impossible-objects-0804</guid>
        <dc:creator>Alex Shipps | MIT CSAIL</dc:creator>
  <content:encoded>&lt;p dir="ltr" id="docs-internal-guid-00f766d6-7fff-403a-378d-5aaf2f5b25af"&gt;M.C. Escher’s artwork is a gateway into a world of depth-defying optical illusions, featuring “impossible objects” that break the laws of physics with convoluted geometries. What you perceive his illustrations to be depends on your point of view — for example, a person seemingly walking upstairs may be heading down the steps if you tilt your head&amp;nbsp;&lt;a href="https://www.nga.gov/collection/art-object-page.54256.html"&gt;sideways&lt;/a&gt;.&amp;nbsp;&lt;br&gt;&lt;br&gt;Computer graphics scientists and designers can recreate these illusions in 3D, but only by bending or cutting a real shape and positioning it at a particular angle. This workaround has downsides, though: Changing the smoothness or lighting of the structure will expose that it isn’t actually an optical illusion, which also means you can’t accurately solve geometry problems on it.&lt;br&gt;&lt;br&gt;Researchers at MIT’s Computer Science and Artificial Intelligence Laboratory (CSAIL) have developed a unique approach to represent “impossible” objects in a more versatile way. Their “&lt;a href="https://anadodik.github.io/publication/meschers/"&gt;Meschers&lt;/a&gt;” tool converts images and 3D models into 2.5-dimensional structures, creating Escher-like depictions of things like windows, buildings, and even donuts. The approach helps users relight, smooth out, and study unique geometries while preserving their optical illusion.&lt;br&gt;&lt;br&gt;This tool could assist geometry researchers with calculating the distance between two points on a curved impossible surface (“geodesics”) and simulating how heat dissipates over it (“heat diffusion”). It could also help artists and computer graphics scientists create physics-breaking designs in multiple dimensions.&lt;br&gt;&lt;br&gt;Lead author and MIT PhD student Ana Dodik aims to design computer graphics tools that aren’t limited to replicating reality, enabling artists to express their intent independently of whether a shape can be realized in the physical world. “Using Meschers, we’ve unlocked a new class of shapes for artists to work with on the computer,” she says. “They could also help perception scientists understand the point at which an object truly becomes impossible.”&lt;/p&gt;&lt;p dir="ltr"&gt;Dodik and her colleagues will present their&amp;nbsp;&lt;a href="https://anadodik.github.io/publication/meschers/Meschers.pdf"&gt;paper&lt;/a&gt; at the SIGGRAPH conference in August.&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;strong&gt;Making impossible objects possible&lt;/strong&gt;&lt;/p&gt;&lt;p dir="ltr"&gt;Impossible objects can’t be fully replicated in 3D. Their constituent parts often look plausible, but these parts don’t glue together properly when assembled in 3D. But what can be computationally imitated, as the CSAIL researchers found out, is the process of how we perceive these shapes.&lt;br&gt;&lt;br&gt;Take the&amp;nbsp;&lt;a href="https://www.illusionsindex.org/i/impossible-triangle"&gt;Penrose Triangle&lt;/a&gt;, for instance. The object as a whole is physically impossible because the depths don’t “add up,” but we can recognize real-world 3D shapes (like its three L-shaped corners) within it. These smaller regions can be realized in 3D — a property called “local consistency” — but when we try to assemble them together, they don’t form a globally consistent shape.&lt;br&gt;&lt;br&gt;The Meschers approach models’ locally consistent regions without forcing them to be globally consistent, piecing together an Escher-esque structure. Behind the scenes, Meschers represents impossible objects as if we know their x and y coordinates in the image, as well as differences in z coordinates (depth) between neighboring pixels; the tool uses these differences in depth to reason about impossible objects indirectly.&lt;br&gt;&lt;br&gt;&lt;strong&gt;The many uses of Meschers&lt;/strong&gt;&lt;br&gt;&lt;br&gt;In addition to rendering impossible objects, Meschers can subdivide their structures into smaller shapes for more precise geometry calculations and smoothing operations. This process enabled the researchers to reduce visual imperfections of impossible shapes, such as a red heart outline they thinned out.&lt;br&gt;&lt;br&gt;The researchers also tested their tool on an “impossibagel,” where a bagel is shaded in a physically impossible way. Meschers helped Dodik and her colleagues simulate heat diffusion and calculate geodesic distances between different points of the model.&lt;br&gt;&lt;br&gt;“Imagine you’re an ant traversing this bagel, and you want to know how long it’ll take you to get across, for example,” says Dodik. “In the same way, our tool could help mathematicians analyze the underlying geometry of impossible shapes up close, much like how we study real-world ones.”&lt;/p&gt;&lt;p dir="ltr"&gt;Much like a magician, the tool can create optical illusions out of otherwise practical objects, making it easier for computer graphics artists to create impossible objects. It can also use “inverse rendering” tools to convert drawings and images of impossible objects into high-dimensional designs.&amp;nbsp;&lt;br&gt;&lt;br&gt;“Meschers demonstrates how computer graphics tools don’t have to be constrained by the rules of physical reality,” says senior author Justin Solomon, associate professor of electrical engineering and computer science and leader of the CSAIL Geometric Data Processing Group. “Incredibly, artists using Meschers can reason about shapes that we will never find in the real world.”&lt;/p&gt;&lt;p dir="ltr"&gt;Meschers can also aid computer graphics artists with tweaking the shading of their creations, while still preserving an optical illusion. This versatility would allow creatives to change the lighting of their art to depict a wider variety of scenes (like a sunrise or sunset) — as Meschers demonstrated by relighting a model of a dog on a skateboard.&lt;/p&gt;&lt;p dir="ltr"&gt;Despite its versatility, Meschers is just the start for Dodik and her colleagues. The team is considering designing an interface to make the tool easier to use while building more elaborate scenes. They’re also working with perception scientists to see how the computer graphics tool can be used more broadly.&lt;/p&gt;&lt;p&gt;Dodik and Solomon wrote the paper with CSAIL affiliates Isabella Yu ’24, SM ’25; PhD student Kartik Chandra SM ’23; MIT professors Jonathan Ragan-Kelley and Joshua Tenenbaum; and MIT Assistant Professor Vincent Sitzmann.&amp;nbsp;&lt;br&gt;&lt;br&gt;Their work was supported, in part, by the MIT Presidential Fellowship, the Mathworks Fellowship, the Hertz Foundation, the U.S. National Science Foundation, the Schmidt Sciences AI2050 fellowship, MIT Quest for Intelligence, the U.S. Army Research Office, U.S. Air Force Office of Scientific Research, SystemsThatLearn@CSAIL initiative, Google, the MIT–IBM Watson AI Laboratory, from the Toyota–CSAIL Joint Research Center, Adobe Systems, the Singapore Defence Science and Technology Agency, and the U.S. Intelligence Advanced Research Projects Activity.&lt;/p&gt;</content:encoded>
      <media:content url="https://news.mit.edu/sites/default/files/styles/news_article__cover_image__original/public/images/202507/mit-csail-Meschers.jpg?itok=mwr0SOLG" medium="image" type="image/jpeg" width="390" height="260">
              <media:description type="plain">“Meschers” can create multi-dimensional versions of objects that break the laws of physics with convoluted geometries, such as buildings you might see in an M.C. Escher illustration (left) and objects that are shaded in impossible ways (center and right).</media:description>
              <media:credit>Image: Alex Shipps/MIT CSAIL, using assets from Pixabay and the researchers</media:credit>
      </media:content>
        <category domain="https://news.mit.edu/topic/research">Research</category>
      <category domain="https://news.mit.edu/topic/mathematics">Mathematics</category>
      <category domain="https://news.mit.edu/topic/artificial-intelligence2">Artificial intelligence</category>
      <category domain="https://news.mit.edu/topic/arts">Arts</category>
      <category domain="https://news.mit.edu/topic/computers">Computer science and technology</category>
      <category domain="https://news.mit.edu/topic/algorithms">Algorithms</category>
      <category domain="https://news.mit.edu/topic/computer-graphics">Computer graphics</category>
      <category domain="https://news.mit.edu/topic/computer-vision">Computer vision</category>
      <category domain="https://news.mit.edu/topic/nsf">National Science Foundation (NSF)</category>
      <category domain="https://news.mit.edu/topic/mit-ibm-watson-ai-lab">MIT-IBM Watson AI Lab</category>
      <category domain="https://news.mit.edu/topic/quest-intelligence">MIT Siegel Family Quest for Intelligence</category>
      <category domain="https://news.mit.edu/topic/electrical-engineering-computer-science-eecs">Electrical engineering and computer science (EECS)</category>
      <category domain="https://news.mit.edu/topic/computer-science-and-artificial-intelligence-laboratory-csail">Computer Science and Artificial Intelligence Laboratory (CSAIL)</category>
      <category domain="https://news.mit.edu/topic/school-science">School of Science</category>
      <category domain="https://news.mit.edu/topic/school-engineering">School of Engineering</category>
      <category domain="https://news.mit.edu/topic/mit-schwarzman-college-computing">MIT Schwarzman College of Computing</category>
    </item>
<item>
  <title>New algorithms enable efficient machine learning with symmetric data</title>
  <link>https://news.mit.edu/2025/new-algorithms-enable-efficient-machine-learning-with-symmetric-data-0730</link>
  <description>This new approach could lead to enhanced AI models for drug and materials discovery. </description>
  <pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
    <guid isPermaLink="true">https://news.mit.edu/2025/new-algorithms-enable-efficient-machine-learning-with-symmetric-data-0730</guid>
        <dc:creator>Adam Zewe | MIT News</dc:creator>
  <content:encoded>&lt;p&gt;If you rotate an image of a molecular structure, a human can tell the rotated image is still the same molecule, but a machine-learning model might think it is a new data point. In computer science parlance, the molecule is “symmetric,” meaning the fundamental structure of that molecule remains the same if it undergoes certain transformations, like rotation.&lt;/p&gt;&lt;p&gt;If a drug discovery model doesn’t understand symmetry, it could make inaccurate predictions about molecular properties. But despite some empirical successes, it’s been unclear whether there is a computationally efficient method to train a good model that is guaranteed to respect symmetry.&lt;br&gt;&lt;br&gt;A new study by MIT researchers answers this question, and shows the first method for machine learning with symmetry that is provably efficient in terms of both the amount of computation and data needed.&lt;/p&gt;&lt;p&gt;These results clarify a foundational question, and they could aid researchers in the development of more powerful machine-learning models that are designed to handle symmetry. Such models would be useful in a variety of applications, from discovering new materials to identifying astronomical anomalies to unraveling complex climate patterns.&lt;/p&gt;&lt;p&gt;“These symmetries are important because they are some sort of information that nature is telling us about the data, and we should take it into account in our machine-learning models. We’ve now shown that it is possible to do machine-learning with symmetric data in an efficient way,” says Behrooz Tahmasebi, an MIT graduate student and co-lead author of this study.&lt;/p&gt;&lt;p&gt;He is joined on the &lt;a href="https://arxiv.org/pdf/2502.19758" target="_blank"&gt;paper&lt;/a&gt; by co-lead author and MIT graduate student Ashkan Soleymani; Stefanie Jegelka, an associate professor of electrical engineering and computer science (EECS) and a member of the Institute for Data, Systems, and Society (IDSS) and the Computer Science and Artificial Intelligence Laboratory (CSAIL); and senior author Patrick Jaillet, the Dugald C. Jackson Professor of Electrical Engineering and Computer Science and a principal investigator in the Laboratory for Information and Decision Systems (LIDS). The research was recently presented at the International Conference on Machine Learning.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Studying symmetry&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Symmetric data appear in many domains, especially the natural sciences and physics. A model that recognizes symmetries is able to identify an object, like a car, no matter where that object is placed in an image, for example.&lt;/p&gt;&lt;p&gt;Unless a machine-learning model is designed to handle symmetry, it could be less accurate and prone to failure when faced with new symmetric data in real-world situations. On the flip side, models that take advantage of symmetry could be faster and require fewer data for training.&lt;/p&gt;&lt;p&gt;But training a model to process symmetric data is no easy task.&lt;/p&gt;&lt;p&gt;One common approach is called data augmentation, where researchers transform each symmetric data point into multiple data points to help the model generalize better to new data. For instance, one could rotate a molecular structure many times to produce new training data, but if researchers want the model to be guaranteed to respect symmetry, this can be computationally prohibitive.&lt;/p&gt;&lt;p&gt;An alternative approach is to encode symmetry into the model’s architecture. A well-known example of this is a graph neural network (GNN), which inherently handles symmetric data because of how it is designed.&lt;/p&gt;&lt;p&gt;“Graph neural networks are fast and efficient, and they take care of symmetry quite well, but nobody really knows what these models are learning or why they work. Understanding GNNs is a main motivation of our work, so we started with a theoretical evaluation of what happens when data are symmetric,” Tahmasebi says.&lt;/p&gt;&lt;p&gt;They explored the statistical-computational tradeoff in machine learning with symmetric data. This tradeoff means methods that require fewer data can be more computationally expensive, so researchers need to find the right balance.&lt;/p&gt;&lt;p&gt;Building on this theoretical evaluation, the researchers designed an efficient algorithm for machine learning with symmetric data.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Mathematical combinations&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;To do this, they borrowed ideas from algebra to shrink and simplify the problem. Then, they reformulated the problem using ideas from geometry that effectively capture symmetry.&lt;/p&gt;&lt;p&gt;Finally, they combined the algebra and the geometry into an optimization problem that can be solved efficiently, resulting in their new algorithm.&lt;/p&gt;&lt;p&gt;“Most of the theory and applications were focusing on either algebra or geometry. Here we just combined them,” Tahmasebi says.&lt;/p&gt;&lt;p&gt;The algorithm requires fewer data samples for training than classical approaches, which would improve a model’s accuracy and ability to adapt to new applications.&lt;/p&gt;&lt;p&gt;By proving that scientists can develop efficient algorithms for machine learning with symmetry, and demonstrating how it can be done, these results could lead to the development of new neural network architectures that could be more accurate and less resource-intensive than current models.&lt;/p&gt;&lt;p&gt;Scientists could also use this analysis as a starting point to examine the inner workings of GNNs, and how their operations differ from the algorithm the MIT researchers developed.&lt;/p&gt;&lt;p&gt;“Once we know that better, we can design more interpretable, more robust, and more efficient neural network architectures,” adds Soleymani.&lt;/p&gt;&lt;p&gt;This research is funded, in part, by the National Research Foundation of Singapore, DSO National Laboratories of Singapore, the U.S. Office of Naval Research, the U.S. National Science Foundation, and an Alexander von Humboldt Professorship.&lt;/p&gt;</content:encoded>
      <media:content url="https://news.mit.edu/sites/default/files/styles/news_article__cover_image__original/public/images/202507/MIT_Learning-Symmetric-01.jpg?itok=jTPWN5G8" medium="image" type="image/jpeg" width="390" height="260">
              <media:description type="plain">A new study by MIT researchers shows the first method for machine learning with symmetry that is provably efficient in terms of both the amount of computation and data needed.</media:description>
              <media:credit>Credit: iStock, MIT News</media:credit>
      </media:content>
        <category domain="https://news.mit.edu/topic/research">Research</category>
      <category domain="https://news.mit.edu/topic/computers">Computer science and technology</category>
      <category domain="https://news.mit.edu/topic/artificial-intelligence2">Artificial intelligence</category>
      <category domain="https://news.mit.edu/topic/algorithms">Algorithms</category>
      <category domain="https://news.mit.edu/topic/machine-learning">Machine learning</category>
      <category domain="https://news.mit.edu/topic/data">Data</category>
      <category domain="https://news.mit.edu/topic/idss">IDSS</category>
      <category domain="https://news.mit.edu/topic/lids">Laboratory for Information and Decision Systems (LIDS)</category>
      <category domain="https://news.mit.edu/topic/computer-science-and-artificial-intelligence-laboratory-csail">Computer Science and Artificial Intelligence Laboratory (CSAIL)</category>
      <category domain="https://news.mit.edu/topic/electrical-engineering-computer-science-eecs">Electrical engineering and computer science (EECS)</category>
      <category domain="https://news.mit.edu/topic/school-engineering">School of Engineering</category>
      <category domain="https://news.mit.edu/topic/mit-schwarzman-college-computing">MIT Schwarzman College of Computing</category>
      <category domain="https://news.mit.edu/topic/nsf">National Science Foundation (NSF)</category>
    </item>
<item>
  <title>Robot, know thyself: New vision-based system teaches machines to understand their bodies</title>
  <link>https://news.mit.edu/2025/vision-based-system-teaches-machines-understand-their-bodies-0724</link>
  <description>Neural Jacobian Fields, developed by MIT CSAIL researchers, can learn to control any robot from a single camera, without any other sensors.</description>
  <pubDate>Thu, 24 Jul 2025 15:30:00 -0400</pubDate>
    <guid isPermaLink="true">https://news.mit.edu/2025/vision-based-system-teaches-machines-understand-their-bodies-0724</guid>
        <dc:creator>Rachel Gordon | MIT CSAIL</dc:creator>
  <content:encoded>&lt;p dir="ltr" id="docs-internal-guid-ebea3a61-7fff-ad11-8889-4ee6f72a24bc"&gt;In an office at MIT’s Computer Science and Artificial Intelligence Laboratory (CSAIL), a soft robotic hand carefully curls its fingers to grasp a small object. The intriguing part isn’t the mechanical design or embedded sensors — in fact, the hand contains none. Instead, the entire system relies on a single camera that watches the robot’s movements and uses that visual data to control it.&lt;/p&gt;&lt;p dir="ltr"&gt;This capability comes from a new system CSAIL scientists developed, offering a different perspective on robotic control. Rather than using hand-designed models or complex sensor arrays, it allows robots to learn how their bodies respond to control commands, solely through vision. The approach, called Neural Jacobian Fields (NJF), gives robots a kind of bodily self-awareness. An &lt;a href="https://www.nature.com/articles/s41586-025-09170-0"&gt;open-access paper about the work&lt;/a&gt; was published in&amp;nbsp;&lt;em&gt;Nature&lt;/em&gt; on June 25.&lt;/p&gt;&lt;p dir="ltr"&gt;“This work points to a shift from programming robots to teaching robots,” says Sizhe Lester Li, MIT PhD student in electrical engineering and computer science, CSAIL affiliate, and lead researcher on the work. “Today, many robotics tasks require extensive engineering and coding. In the future, we envision showing a robot what to do, and letting it learn how to achieve the goal autonomously.”&lt;/p&gt;&lt;p dir="ltr"&gt;The motivation stems from a simple but powerful reframing: The main barrier to affordable, flexible robotics isn't hardware — it’s control of capability, which could be achieved in multiple ways. Traditional robots are built to be rigid and sensor-rich, making it easier to construct a digital twin, a precise mathematical replica used for control. But when a robot is soft, deformable, or irregularly shaped, those assumptions fall apart. Rather than forcing robots to match our models, NJF flips the script — giving robots the ability to learn their own internal model from observation.&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;strong&gt;Look and learn&lt;/strong&gt;&lt;/p&gt;&lt;p dir="ltr"&gt;This decoupling of modeling and hardware design could significantly expand the design space for robotics. In soft and bio-inspired robots, designers often embed sensors or reinforce parts of the structure just to make modeling feasible. NJF lifts that constraint. The system doesn’t need onboard sensors or design tweaks to make control possible. Designers are freer to explore unconventional, unconstrained morphologies without worrying about whether they’ll be able to model or control them later.&lt;/p&gt;&lt;p dir="ltr"&gt;“Think about how you learn to control your fingers: you wiggle, you observe, you adapt,” says Li. “That’s what our system does. It experiments with random actions and figures out which controls move which parts of the robot.”&lt;/p&gt;&lt;p dir="ltr"&gt;The system has proven robust across a range of robot types. The team tested NJF on a pneumatic soft robotic hand capable of pinching and grasping, a rigid Allegro hand, a 3D-printed robotic arm, and even a rotating platform with no embedded sensors. In every case, the system learned both the robot’s shape and how it responded to control signals, just from vision and random motion.&lt;/p&gt;&lt;p dir="ltr"&gt;The researchers see potential far beyond the lab. Robots equipped with NJF could one day perform agricultural tasks with centimeter-level localization accuracy, operate on construction sites without elaborate sensor arrays, or navigate dynamic environments where traditional methods break down.&lt;/p&gt;&lt;p dir="ltr"&gt;At the core of NJF is a neural network that captures two intertwined aspects of a robot’s embodiment: its three-dimensional geometry and its sensitivity to control inputs. The system builds on neural radiance fields (NeRF), a technique that reconstructs 3D scenes from images by mapping spatial coordinates to color and density values. NJF extends this approach by learning not only the robot’s shape, but also a Jacobian field, a function that predicts how any point on the robot’s body moves in response to motor commands.&lt;/p&gt;&lt;p dir="ltr"&gt;To train the model, the robot performs random motions while multiple cameras record the outcomes. No human supervision or prior knowledge of the robot’s structure is required — the system simply infers the relationship between control signals and motion by watching.&lt;/p&gt;&lt;p dir="ltr"&gt;Once training is complete, the robot only needs a single monocular camera for real-time closed-loop control, running at about 12 Hertz. This allows it to continuously observe itself, plan, and act responsively. That speed makes NJF more viable than many physics-based simulators for soft robots, which are often too computationally intensive for real-time use.&lt;/p&gt;&lt;p dir="ltr"&gt;In early simulations, even simple 2D fingers and sliders were able to learn this mapping using just a few examples. By modeling how specific points deform or shift in response to action, NJF builds a dense map of controllability. That internal model allows it to generalize motion across the robot’s body, even when the data are noisy or incomplete.&lt;/p&gt;&lt;p dir="ltr"&gt;“What’s really interesting is that the system figures out on its own which motors control which parts of the robot,” says Li. “This isn’t programmed — it emerges naturally through learning, much like a person discovering the buttons on a new device.”&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;strong&gt;The future is soft&lt;/strong&gt;&lt;/p&gt;&lt;p dir="ltr"&gt;For decades, robotics has favored rigid, easily modeled machines — like the industrial arms found in factories — because their properties simplify control. But the field has been moving toward soft, bio-inspired robots that can adapt to the real world more fluidly. The trade-off? These robots are harder to model.&lt;/p&gt;&lt;p dir="ltr"&gt;“Robotics today often feels out of reach because of costly sensors and complex programming. Our goal with Neural Jacobian Fields is to lower the barrier, making robotics affordable, adaptable, and accessible to more people. Vision is a resilient, reliable sensor,” says senior author and MIT Assistant Professor Vincent Sitzmann, who leads the Scene Representation group. “It opens the door to robots that can operate in messy, unstructured environments, from farms to construction sites, without expensive infrastructure.”&lt;/p&gt;&lt;p dir="ltr"&gt;“Vision alone can provide the cues needed for localization and control — eliminating the need for GPS, external tracking systems, or complex onboard sensors. This opens the door to robust, adaptive behavior in unstructured environments, from drones navigating indoors or underground without maps to mobile manipulators working in cluttered homes or warehouses, and even legged robots traversing uneven terrain,” says co-author Daniela Rus, MIT professor of electrical engineering and computer science and director of CSAIL. “By learning from visual feedback, these systems develop internal models of their own motion and dynamics, enabling flexible, self-supervised operation where traditional localization methods would fail.”&lt;/p&gt;&lt;p dir="ltr"&gt;While training NJF currently requires multiple cameras and must be redone for each robot, the researchers are already imagining a more accessible version. In the future, hobbyists could record a robot’s random movements with their phone, much like you’d take a video of a rental car before driving off, and use that footage to create a control model, with no prior knowledge or special equipment required.&lt;/p&gt;&lt;p dir="ltr"&gt;The system doesn’t yet generalize across different robots, and it lacks force or tactile sensing, limiting its effectiveness on contact-rich tasks. But the team is exploring new ways to address these limitations: improving generalization, handling occlusions, and extending the model’s ability to reason over longer spatial and temporal horizons.&lt;/p&gt;&lt;p dir="ltr"&gt;“Just as humans develop an intuitive understanding of how their bodies move and respond to commands, NJF gives robots that kind of embodied self-awareness through vision alone,” says Li. “This understanding is a foundation for flexible manipulation and control in real-world environments. Our work, essentially, reflects a broader trend in robotics: moving away from manually programming detailed models toward teaching robots through observation and interaction.”&lt;/p&gt;&lt;p dir="ltr"&gt;This paper brought together the computer vision and self-supervised learning work from the Sitzmann lab and the expertise in soft robots from the Rus lab. Li, Sitzmann, and Rus co-authored the paper with CSAIL affiliates Annan Zhang SM ’22, a PhD student in electrical engineering and computer science (EECS); Boyuan Chen, a PhD student in EECS; Hanna Matusik, an undergraduate researcher in mechanical engineering; and Chao Liu, a postdoc in the Senseable City Lab at MIT.&amp;nbsp;&lt;br&gt;&lt;br&gt;The research was supported by the Solomon Buchsbaum Research Fund through MIT’s Research Support Committee, an MIT Presidential Fellowship, the National Science Foundation, and the Gwangju Institute of Science and Technology.&lt;/p&gt;</content:encoded>
      <media:content url="https://news.mit.edu/sites/default/files/styles/news_article__cover_image__original/public/images/202507/njf-mit-csail-00_0.png?itok=rLY1yOfT" medium="image" type="image/jpeg" width="390" height="260">
              <media:description type="plain">A 3D-printed robotic arm holds a pencil as it trains using random movements and a single camera — part of a new control system called Neural Jacobian Fields (NJF). Rather than relying on sensors or hand-coded models, NJF allows robots to learn how their bodies move in response to motor commands purely from visual observation, offering a pathway to more flexible, affordable, and self-aware robots.</media:description>
              <media:credit>Image courtesy of the researchers.</media:credit>
      </media:content>
        <category domain="https://news.mit.edu/topic/research">Research</category>
      <category domain="https://news.mit.edu/topic/robotics">Robotics</category>
      <category domain="https://news.mit.edu/topic/mechanical-engineering">Mechanical engineering</category>
      <category domain="https://news.mit.edu/topic/computer-vision">Computer vision</category>
      <category domain="https://news.mit.edu/topic/artificial-intelligence2">Artificial intelligence</category>
      <category domain="https://news.mit.edu/topic/machine-learning">Machine learning</category>
      <category domain="https://news.mit.edu/topic/computers">Computer science and technology</category>
      <category domain="https://news.mit.edu/topic/electrical-engineering-computer-science-eecs">Electrical engineering and computer science (EECS)</category>
      <category domain="https://news.mit.edu/topic/computer-science-and-artificial-intelligence-laboratory-csail">Computer Science and Artificial Intelligence Laboratory (CSAIL)</category>
      <category domain="https://news.mit.edu/topic/school-engineering">School of Engineering</category>
      <category domain="https://news.mit.edu/topic/mit-schwarzman-college-computing">MIT Schwarzman College of Computing</category>
    </item>
<item>
  <title>A new way to edit or generate images</title>
  <link>https://news.mit.edu/2025/new-way-edit-or-generate-images-0721</link>
  <description>MIT researchers found that special kinds of neural networks, called encoders or “tokenizers,” can do much more than previously realized.</description>
  <pubDate>Mon, 21 Jul 2025 15:00:00 -0400</pubDate>
    <guid isPermaLink="true">https://news.mit.edu/2025/new-way-edit-or-generate-images-0721</guid>
        <dc:creator>Steve Nadis | MIT CSAIL | Laboratory for Information and Decision Systems</dc:creator>
  <content:encoded>&lt;p&gt;AI image generation — which relies on neural networks to create new images from a variety of inputs, including text prompts — is projected to become a billion-dollar industry by the end of this decade. Even with today’s technology, if you wanted to make a fanciful picture of, say, a friend planting a flag on Mars or heedlessly flying into a black hole, it could take less than a second. However, before they can perform tasks like that, image generators are commonly trained on massive datasets containing millions of images that are often paired with associated text. Training these generative models can be an arduous chore that takes weeks or months, consuming vast computational resources in the process.&lt;/p&gt;&lt;p&gt;But what if it were possible to generate images through AI methods without using a generator at all? That real possibility, along with other intriguing ideas, was described in a &lt;a href="https://arxiv.org/pdf/2506.08257"&gt;research paper&lt;/a&gt; presented at the International Conference on Machine Learning (ICML 2025), which was held in Vancouver, British Columbia, earlier this summer. The paper, describing novel techniques for manipulating and generating images, was written by Lukas Lao Beyer, a graduate student researcher in MIT’s Laboratory for Information and Decision Systems (LIDS); Tianhong Li, a postdoc at MIT’s Computer Science and Artificial Intelligence Laboratory (CSAIL); Xinlei Chen of Facebook AI Research; Sertac Karaman, an MIT professor of aeronautics and astronautics and the director of LIDS; and Kaiming He, an MIT associate professor of electrical engineering and computer science.&lt;/p&gt;&lt;p&gt;This group effort had its origins in a class project for a graduate seminar on deep generative models that Lao Beyer took last fall. In conversations during the semester, it became apparent to both Lao Beyer and He, who taught the seminar, that this research had real potential, which went far beyond the confines of a typical homework assignment. Other collaborators were soon brought into the endeavor.&lt;/p&gt;&lt;p&gt;The starting point for Lao Beyer’s inquiry was a June 2024 paper, written by researchers from the Technical University of Munich and the Chinese company ByteDance, which introduced a new way of representing visual information called a one-dimensional tokenizer. With this device, which is also a kind of neural network, a 256x256-pixel image can be translated into a sequence of just 32 numbers, called tokens. “I wanted to understand how such a high level of compression could be achieved, and what the tokens themselves actually represented,” says Lao Beyer.&lt;/p&gt;&lt;p&gt;The previous generation of tokenizers would typically break up the same image into an array of 16x16 tokens — with each token encapsulating information, in highly condensed form, that corresponds to a specific portion of the original image. The new 1D tokenizers can encode an image more efficiently, using far fewer tokens overall, and these tokens are able to capture information about the entire image, not just a single quadrant. Each of these tokens, moreover, is a 12-digit number consisting of 1s and 0s, allowing for 2&lt;sup&gt;12&lt;/sup&gt; (or about 4,000) possibilities altogether. “It’s like a vocabulary of 4,000 words that makes up an abstract, hidden language spoken by the computer,” He explains. “It’s not like a human language, but we can still try to find out what it means.”&lt;/p&gt;&lt;p&gt;That’s exactly what Lao Beyer had initially set out to explore — work that provided the seed for the ICML 2025 paper. The approach he took was pretty straightforward. If you want to find out what a particular token does, Lao Beyer says, “you can just take it out, swap in some random value, and see if there is a recognizable change in the output.” Replacing one token, he found, changes the image quality, turning a low-resolution image into a high-resolution image or vice versa. Another token affected the blurriness in the background, while another still influenced the brightness. He also found a token that’s related to the “pose,” meaning that, in the image of a robin, for instance, the bird’s head might shift from right to left.&lt;/p&gt;&lt;p&gt;“This was a never-before-seen result, as no one had observed visually identifiable changes from manipulating tokens,” Lao Beyer says. The finding raised the possibility of a new approach to editing images. And the MIT group has shown, in fact, how this process can be streamlined and automated, so that tokens don’t have to be modified by hand, one at a time.&lt;/p&gt;&lt;p&gt;He and his colleagues achieved an even more consequential result involving image generation. A system capable of generating images normally requires a tokenizer, which compresses and encodes visual data, along with a generator that can combine and arrange these compact representations in order to create novel images. The MIT researchers found a way to create images without using a generator at all. Their new approach makes use of a 1D tokenizer and a so-called detokenizer (also known as a decoder), which can reconstruct an image from a string of tokens. However, with guidance provided by an off-the-shelf neural network called CLIP —&amp;nbsp;which cannot generate images on its own, but can measure how well a given image matches a certain text prompt&amp;nbsp;— the team was able to convert an image of a red panda, for example, into a tiger. In addition, they could create images of a tiger, or any other desired form, starting completely from scratch — from a situation in which all the tokens are initially assigned random values (and then iteratively tweaked so that the reconstructed image increasingly matches the desired text prompt).&lt;/p&gt;&lt;p&gt;The group demonstrated that with this same setup — relying on a tokenizer and detokenizer, but no generator — they could also do “inpainting,” which means filling in parts of images that had somehow been blotted out. Avoiding the use of a generator for certain tasks could lead to a significant reduction in computational costs because generators, as mentioned, normally require extensive training.&lt;/p&gt;&lt;p&gt;What might seem odd about this team’s contributions, He explains, “is that we didn’t invent anything new. We didn’t invent a 1D tokenizer, and we didn’t invent the CLIP model, either. But we did discover that new capabilities can arise when you put all these pieces together.”&lt;/p&gt;&lt;p&gt;“This work redefines the role of tokenizers,” comments&amp;nbsp;Saining Xie, a computer scientist at New York University. “It shows that&amp;nbsp;image tokenizers — tools usually used just to compress images — can actually do a lot more. The fact that a simple (but highly compressed) 1D tokenizer can handle tasks like inpainting or text-guided editing, without needing to train a full-blown generative model, is pretty surprising.”&lt;/p&gt;&lt;p&gt;Zhuang Liu of Princeton University agrees, saying that the work of the MIT group&amp;nbsp;“shows that we can generate and manipulate the images in a way that is much easier than we previously thought. Basically, it demonstrates that image generation can be a byproduct of a very effective image compressor, potentially reducing the cost of generating images several-fold.”&lt;/p&gt;&lt;p&gt;There could be many applications outside the field of computer vision, Karaman suggests. “For instance,&amp;nbsp;we could consider tokenizing the actions of robots or self-driving cars in the same way, which may rapidly broaden the impact of this work.”&lt;/p&gt;&lt;p&gt;Lao Beyer is thinking along similar lines,&amp;nbsp;noting that the&amp;nbsp;extreme amount of compression afforded by 1D tokenizers allows you to do “some amazing things,” which could be applied to other fields. For example, in the area of self-driving cars, which is one of his research interests, the tokens could represent, instead of images, the different routes that a vehicle might take.&lt;/p&gt;&lt;p&gt;Xie is also intrigued by the applications that may come from these innovative ideas. “There are some really cool use cases this could unlock,” he says.&amp;nbsp;&lt;/p&gt;</content:encoded>
      <media:content url="https://news.mit.edu/sites/default/files/styles/news_article__cover_image__original/public/images/202507/mit-token-opt3.jpg?itok=A759koh8" medium="image" type="image/jpeg" width="390" height="260">
              <media:description type="plain">A system capable of generating images normally requires a tokenizer, which compresses and encodes visual data, along with a generator that can combine and arrange these compact representations in order to create novel images. MIT researchers discovered a new method to create, convert, and “inpaint” images without using a generator at all. This image shows how an input image can be gradually modified by optimizing tokens. </media:description>
              <media:credit>Image courtesy of the authors. </media:credit>
      </media:content>
        <category domain="https://news.mit.edu/topic/school-engineering">School of Engineering</category>
      <category domain="https://news.mit.edu/topic/mit-schwarzman-college-computing">MIT Schwarzman College of Computing</category>
      <category domain="https://news.mit.edu/topic/aeronautics">Aeronautical and astronautical engineering</category>
      <category domain="https://news.mit.edu/topic/electrical-engineering-computer-science-eecs">Electrical engineering and computer science (EECS)</category>
      <category domain="https://news.mit.edu/topic/computer-science-and-artificial-intelligence-laboratory-csail">Computer Science and Artificial Intelligence Laboratory (CSAIL)</category>
      <category domain="https://news.mit.edu/topic/lids">Laboratory for Information and Decision Systems (LIDS)</category>
      <category domain="https://news.mit.edu/topic/computers">Computer science and technology</category>
      <category domain="https://news.mit.edu/topic/artificial-intelligence2">Artificial intelligence</category>
      <category domain="https://news.mit.edu/topic/machine-learning">Machine learning</category>
      <category domain="https://news.mit.edu/topic/computer-vision">Computer vision</category>
      <category domain="https://news.mit.edu/topic/research">Research</category>
    </item>
<item>
  <title>The unique, mathematical shortcuts language models use to predict dynamic scenarios</title>
  <link>https://news.mit.edu/2025/unique-mathematical-shortcuts-language-models-use-to-predict-dynamic-scenarios-0721</link>
  <description>Language models follow changing situations using clever arithmetic, instead of sequential tracking. By controlling when these approaches are used, engineers could improve the systems’ capabilities.</description>
  <pubDate>Mon, 21 Jul 2025 08:00:00 -0400</pubDate>
    <guid isPermaLink="true">https://news.mit.edu/2025/unique-mathematical-shortcuts-language-models-use-to-predict-dynamic-scenarios-0721</guid>
        <dc:creator>Alex Shipps | MIT CSAIL</dc:creator>
  <content:encoded>&lt;p dir="ltr" id="docs-internal-guid-5c520fa2-7fff-f3a1-cecc-40dddcfcf883"&gt;Let’s say you’re reading a story, or playing a game of chess. You may not have noticed, but each step of the way, your mind kept track of how the situation (or “state of the world”) was changing. You can imagine this as a sort of sequence of events list, which we use to update our prediction of what will happen next.&lt;br&gt;&lt;br&gt;Language models like ChatGPT also track changes inside their own “mind” when finishing off a block of code or anticipating what you’ll write next. They typically make educated guesses using transformers — internal architectures that help the models understand sequential data — but the systems are sometimes incorrect because of flawed thinking patterns. Identifying and tweaking these underlying mechanisms helps language models become more reliable prognosticators, especially with more dynamic tasks like forecasting weather and financial markets.&lt;br&gt;&lt;br&gt;But do these AI systems process developing situations like we do? A new&amp;nbsp;&lt;a href="https://arxiv.org/html/2503.02854v1"&gt;paper&lt;/a&gt; from researchers in MIT’s Computer Science and Artificial Intelligence Laboratory (CSAIL) and Department of Electrical Engineering and Computer Science shows that the models instead use clever mathematical shortcuts between each progressive step in a sequence, eventually making reasonable predictions. The team made this observation by going under the hood of language models, evaluating how closely they could keep track of objects that change position rapidly. Their findings show that engineers can control when language models use particular workarounds as a way to improve the systems’ predictive capabilities.&lt;br&gt;&lt;br&gt;&lt;strong&gt;Shell games&lt;/strong&gt;&lt;/p&gt;&lt;p dir="ltr" id="docs-internal-guid-5c520fa2-7fff-f3a1-cecc-40dddcfcf883"&gt;The researchers analyzed the inner workings of these models using a clever experiment reminiscent of a classic concentration game. Ever had to guess the final location of an object after it’s placed under a cup and shuffled with identical containers? The team used a similar test, where the model guessed the final arrangement of particular digits (also called a permutation). The models were given a starting sequence, such as “42135,” and instructions about when and where to move each digit, like moving the “4” to the third position and onward, without knowing the final result.&lt;br&gt;&lt;br&gt;In these experiments, transformer-based models gradually learned to predict the correct final arrangements. Instead of shuffling the digits based on the instructions they were given, though, the systems aggregated information between successive states (or individual steps within the sequence) and calculated the final permutation.&lt;/p&gt;&lt;p dir="ltr"&gt;One go-to pattern the team observed, called the “Associative Algorithm,” essentially organizes nearby steps into groups and then calculates a final guess. You can think of this process as being structured like a tree, where the initial numerical arrangement is the “root.” As you move up the tree, adjacent steps are grouped into different branches and multiplied together. At the top of the tree is the final combination of numbers, computed by multiplying each resulting sequence on the branches together.&lt;br&gt;&lt;br&gt;The other way language models guessed the final permutation was through a crafty mechanism called the “Parity-Associative Algorithm,” which essentially whittles down options before grouping them. It determines whether the final arrangement is the result of an even or odd number of rearrangements of individual digits. Then, the mechanism groups adjacent sequences from different steps before multiplying them, just like the Associative Algorithm.&lt;br&gt;&lt;br&gt;“These behaviors tell us that transformers perform simulation by associative scan. Instead of following state changes step-by-step, the models organize them into hierarchies,” says MIT PhD student and CSAIL affiliate Belinda Li SM ’23, a lead author on the paper. “How do we encourage transformers to learn better state tracking? Instead of imposing that these systems form inferences about data in a human-like, sequential way, perhaps we should cater to the approaches they naturally use when tracking state changes.”&lt;br&gt;&lt;br&gt;“One avenue of research has been to expand test-time computing along the depth dimension, rather than the token dimension — by increasing the number of transformer layers rather than the number of chain-of-thought tokens during test-time reasoning,” adds Li. “Our work suggests that this approach would allow transformers to build deeper reasoning trees.”&lt;br&gt;&lt;br&gt;&lt;strong&gt;Through the looking glass&lt;/strong&gt;&lt;br&gt;&lt;br&gt;Li and her co-authors observed how the Associative and Parity-Associative algorithms worked using tools that allowed them to peer inside the “mind” of language models.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;They first used a method called “probing,” which shows what information flows through an AI system. Imagine you could look into a model’s brain to see its thoughts at a specific moment — in a similar way, the technique maps out the system’s mid-experiment predictions about the final arrangement of digits.&lt;/p&gt;&lt;p dir="ltr"&gt;A tool called “activation patching” was then used to show where the language model processes changes to a situation. It involves meddling with some of the system’s “ideas,” injecting incorrect information into certain parts of the network while keeping other parts constant, and seeing how the system will adjust its predictions.&lt;/p&gt;&lt;p dir="ltr"&gt;These tools revealed when the algorithms would make errors and when the systems “figured out” how to correctly guess the final permutations. They observed that the Associative Algorithm learned faster than the Parity-Associative Algorithm, while also performing better on longer sequences. Li attributes the latter’s difficulties with more elaborate instructions to an over-reliance on heuristics (or rules that allow us to compute a reasonable solution fast) to predict permutations.&lt;/p&gt;&lt;p dir="ltr"&gt;“We’ve found that when language models use a heuristic early on in training, they’ll start to build these tricks into their mechanisms,” says Li. “However, those models tend to generalize worse than ones that don’t rely on heuristics. We found that certain pre-training objectives can deter or encourage these patterns, so in the future, we may look to design techniques that discourage models from picking up bad habits.”&lt;/p&gt;&lt;p dir="ltr"&gt;The researchers note that their experiments were done on small-scale language models fine-tuned on synthetic data, but found the model size had little effect on the results. This suggests that fine-tuning larger language models, like GPT 4.1, would likely yield similar results. The team plans to examine their hypotheses more closely by testing language models of different sizes that haven’t been fine-tuned, evaluating their performance on dynamic real-world tasks such as tracking code and following how stories evolve.&lt;br&gt;&lt;br&gt;Harvard University postdoc Keyon Vafa, who was not involved in the paper, says that the researchers’ findings could create opportunities to advance language models. “Many uses of large language models rely on tracking state: anything from providing recipes to writing code to keeping track of details in a conversation,” he says. “This paper makes significant progress in understanding how language models perform these tasks. This progress provides us with interesting insights into what language models are doing and offers promising new strategies for improving them.”&lt;/p&gt;&lt;p&gt;Li wrote the paper with MIT undergraduate student Zifan “Carl” Guo and senior author Jacob Andreas, who is an MIT associate professor of electrical engineering and computer science and CSAIL principal investigator. Their research was supported, in part, by Open Philanthropy, the MIT Quest for Intelligence, the National Science Foundation, the Clare Boothe Luce Program for Women in STEM, and a Sloan Research Fellowship.&lt;br&gt;&lt;br&gt;The researchers presented their research at the International Conference on Machine Learning (ICML) this week.&lt;/p&gt;</content:encoded>
      <media:content url="https://news.mit.edu/sites/default/files/styles/news_article__cover_image__original/public/images/202507/mit-csail-llms.jpg?itok=h_Eov8uW" medium="image" type="image/jpeg" width="390" height="260">
              <media:description type="plain">Researchers from MIT CSAIL and Department of Electrical Engineering and Computer Science evaluated how closely language models could keep track of objects that change position rapidly. They found that they could steer the models toward or away from particular approaches, improving the system’s predictive capabilities.</media:description>
              <media:credit>Image designed by Alex Shipps, using assets from Shutterstock and Pixabay.</media:credit>
      </media:content>
        <category domain="https://news.mit.edu/topic/research">Research</category>
      <category domain="https://news.mit.edu/topic/algorithms">Algorithms</category>
      <category domain="https://news.mit.edu/topic/data">Data</category>
      <category domain="https://news.mit.edu/topic/artificial-intelligence2">Artificial intelligence</category>
      <category domain="https://news.mit.edu/topic/machine-learning">Machine learning</category>
      <category domain="https://news.mit.edu/topic/quest-intelligence">MIT Siegel Family Quest for Intelligence</category>
      <category domain="https://news.mit.edu/topic/computers">Computer science and technology</category>
      <category domain="https://news.mit.edu/topic/nsf">National Science Foundation (NSF)</category>
      <category domain="https://news.mit.edu/topic/electrical-engineering-computer-science-eecs">Electrical engineering and computer science (EECS)</category>
      <category domain="https://news.mit.edu/topic/computer-science-and-artificial-intelligence-laboratory-csail">Computer Science and Artificial Intelligence Laboratory (CSAIL)</category>
      <category domain="https://news.mit.edu/topic/school-engineering">School of Engineering</category>
      <category domain="https://news.mit.edu/topic/mit-schwarzman-college-computing">MIT Schwarzman College of Computing</category>
    </item>
<item>
  <title>New tool gives anyone the ability to train a robot</title>
  <link>https://news.mit.edu/2025/new-tool-gives-anyone-ability-to-train-robot-0717</link>
  <description>MIT engineers designed a versatile interface that allows users to teach robots new skills in intuitive ways.</description>
  <pubDate>Thu, 17 Jul 2025 00:00:00 -0400</pubDate>
    <guid isPermaLink="true">https://news.mit.edu/2025/new-tool-gives-anyone-ability-to-train-robot-0717</guid>
        <dc:creator>Jennifer Chu | MIT News</dc:creator>
  <content:encoded>&lt;p&gt;Teaching a robot new skills used to require coding expertise. But a new generation of robots could potentially learn from just about anyone.&lt;/p&gt;&lt;p&gt;Engineers are designing robotic helpers that can “learn from demonstration.” This more natural training strategy enables a person to lead a robot through a task, typically in one of three ways: via remote control, such as operating a joystick to remotely maneuver a robot; by physically moving the robot through the motions; or by performing the task themselves while the robot watches and mimics.&lt;/p&gt;&lt;p&gt;Learning-by-doing robots usually train in just one of these three demonstration approaches. But MIT engineers have now developed a three-in-one training interface that allows a robot to learn a task through any of the three training methods. The interface is in the form of a handheld, sensor-equipped tool that can attach to many common collaborative robotic arms. A person can use the attachment to teach a robot to carry out a task by remotely controlling the robot, physically manipulating it, or demonstrating the task themselves — whichever style they prefer or best suits the task at hand.&lt;/p&gt;&lt;p&gt;The MIT team tested the new tool, which they call a “versatile demonstration interface,” on a standard collaborative robotic arm. Volunteers with manufacturing expertise used the interface to perform two manual tasks that are commonly carried out on factory floors.&lt;/p&gt;&lt;p&gt;The researchers say the new interface offers increased training flexibility that could expand the type of users and “teachers” who interact with robots. It may also enable robots to learn a wider set of skills. For instance, a person could remotely train a robot to handle toxic substances, while further down the production line another person could physically move the robot through the motions of boxing up a product, and at the end of the line, someone else could use the attachment to draw a company logo as the robot watches and learns to do the same.&lt;/p&gt;&lt;p&gt;“We are trying to create highly intelligent and skilled teammates that can effectively work with humans to get complex work done,” says Mike Hagenow, a postdoc at MIT in the Department of Aeronautics and Astronautics. “We believe flexible demonstration tools can help far beyond the manufacturing floor, in other domains where we hope to see increased robot adoption, such as home or caregiving settings.”&lt;/p&gt;&lt;p&gt;Hagenow will present a &lt;a href="https://arxiv.org/abs/2410.19141v2" target="_blank"&gt;paper detailing the new interface&lt;/a&gt;, at the IEEE Intelligent Robots and Systems (IROS) conference in October. The paper’s MIT co-authors are Dimosthenis Kontogiorgos, a postdoc at the MIT Computer Science and Artificial Intelligence Lab (CSAIL); Yanwei Wang PhD ’25, who recently earned a doctorate in electrical engineering and computer science; and Julie Shah, MIT professor and head of the Department of Aeronautics and Astronautics.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Training together&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Shah’s group at MIT designs robots that can work alongside humans in the workplace, in hospitals, and at home. A main focus of her research is developing systems that enable people to teach robots new tasks or skills “on the job,” as it were. Such systems would, for instance, help a factory floor worker quickly and naturally adjust a robot’s maneuvers to improve its task in the moment, rather than pausing to reprogram the robot’s software from scratch — a skill that a worker may not necessarily have.&lt;/p&gt;&lt;p&gt;The team’s new work builds on an emerging strategy in robot learning called “learning from demonstration,” or LfD, in which robots are designed to be trained in more natural, intuitive ways. In looking through the LfD literature, Hagenow and Shah found LfD training methods developed so far fall generally into the three main categories of teleoperation, kinesthetic training, and natural teaching.&lt;/p&gt;&lt;p&gt;One training method may work better than the other two for a particular person or task. Shah and Hagenow wondered whether they could design a tool that combines all three methods to enable a robot to learn more tasks from more people.&lt;/p&gt;&lt;p&gt;“If we could bring together these three different ways someone might want to interact with a robot, it may bring benefits for different tasks and different people,” Hagenow says.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Tasks at hand&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;With that goal in mind, the team engineered a new versatile demonstration interface (VDI). The interface is a handheld attachment that can fit onto the arm of a typical collaborative robotic arm. The attachment is equipped with a camera and markers that track the tool’s position and movements over time, along with force sensors to measure the amount of pressure applied during a given task.&lt;/p&gt;&lt;p&gt;When the interface is attached to a robot, the entire robot can be controlled remotely, and the interface’s camera records the robot’s movements, which the robot can use as training data to learn the task on its own. Similarly, a person can physically move the robot through a task, with the interface attached. The VDI can also be detached and physically held by a person to perform the desired task. The camera records the VDI’s motions, which the robot can also use to mimic the task when the VBI is reattached.&lt;/p&gt;&lt;p&gt;To test the attachment’s usability, the team brought the interface, along with a collaborative robotic arm, to a local innovation center where manufacturing experts learn about and test technology that can improve factory-floor processes. The researchers set up an experiment where they asked volunteers at the center to use the robot and all three of the interface’s training methods to complete two common manufacturing tasks: press-fitting and molding. In press-fitting, the user trained the robot to press and fit pegs into holes, similar to many fastening tasks. For molding, a volunteer trained the robot to push and roll a rubbery, dough-like substance evenly around the surface of a center rod, similar to some thermomolding tasks.&lt;/p&gt;&lt;p&gt;For each of the two tasks, the volunteers were asked to use each of the three training methods, first teleoperating the robot using a joystick, then kinesthetically manipulating the robot, and finally, detaching the robot’s attachment and using it to “naturally” perform the task as the robot recorded the attachment’s force and movements.&lt;/p&gt;&lt;p&gt;The researchers found the volunteers generally preferred the natural method over teleoperation and kinesthetic training. The users, who were all experts in manufacturing, did offer scenarios in which each method might have advantages over the others. Teleoperation, for instance, may be preferable in training a robot to handle hazardous or toxic substances. Kinesthetic training could help workers adjust the positioning of a robot that is tasked with moving heavy packages. And natural teaching could be beneficial in demonstrating tasks that involve delicate and precise maneuvers.&lt;/p&gt;&lt;p&gt;“We imagine using our demonstration interface in flexible manufacturing environments where one robot might assist across a range of tasks that benefit from specific types of demonstrations,” says Hagenow, who plans to refine the attachment’s design based on user feedback and will use the new design to test robot learning. “We view this study as demonstrating how greater flexibility in collaborative robots can be achieved through interfaces that expand the ways that end-users interact with robots during teaching.”&lt;/p&gt;&lt;p&gt;This work was supported, in part, by the MIT Postdoctoral Fellowship Program for Engineering Excellence and the Wallenberg Foundation Postdoctoral Research Fellowship.&lt;/p&gt;</content:encoded>
      <media:content url="https://news.mit.edu/sites/default/files/styles/news_article__cover_image__original/public/images/202507/MIT-Versatile-Training-01-PRESS.jpg?itok=zF3YXPzi" medium="image" type="image/jpeg" width="390" height="260">
              <media:description type="plain">A new handheld interface developed by MIT engineers enables a person to teach a robot new skills, using any of three training approaches: natural teaching (top left), kinesthetic training (middle), and teleoperation.</media:description>
              <media:credit>Credit: Courtesy of the researchers</media:credit>
      </media:content>
        <category domain="https://news.mit.edu/topic/research">Research</category>
      <category domain="https://news.mit.edu/topic/robotics">Robotics</category>
      <category domain="https://news.mit.edu/topic/automation">automation</category>
      <category domain="https://news.mit.edu/topic/computer-vision">Computer vision</category>
      <category domain="https://news.mit.edu/topic/machine-learning">Machine learning</category>
      <category domain="https://news.mit.edu/topic/manufacturing">Manufacturing</category>
      <category domain="https://news.mit.edu/topic/aeronautics">Aeronautical and astronautical engineering</category>
      <category domain="https://news.mit.edu/topic/electrical-engineering-computer-science-eecs">Electrical engineering and computer science (EECS)</category>
      <category domain="https://news.mit.edu/topic/computer-science-and-artificial-intelligence-laboratory-csail">Computer Science and Artificial Intelligence Laboratory (CSAIL)</category>
      <category domain="https://news.mit.edu/topic/school-engineering">School of Engineering</category>
      <category domain="https://news.mit.edu/topic/mit-schwarzman-college-computing">MIT Schwarzman College of Computing</category>
    </item>
<item>
  <title>Can AI really code? Study maps the roadblocks to autonomous software engineering</title>
  <link>https://news.mit.edu/2025/can-ai-really-code-study-maps-roadblocks-to-autonomous-software-engineering-0716</link>
  <description>A team of researchers has mapped the challenges of AI in software development, and outlined a research agenda to move the field forward.</description>
  <pubDate>Wed, 16 Jul 2025 16:55:00 -0400</pubDate>
    <guid isPermaLink="true">https://news.mit.edu/2025/can-ai-really-code-study-maps-roadblocks-to-autonomous-software-engineering-0716</guid>
        <dc:creator>Rachel Gordon | MIT CSAIL</dc:creator>
  <content:encoded>&lt;p dir="ltr" id="docs-internal-guid-3c70286d-7fff-a737-d9dc-ea0a5bb6f3af"&gt;Imagine a future where artificial intelligence quietly shoulders the drudgery of software development: refactoring tangled code, migrating legacy systems, and hunting down race conditions, so that human engineers can devote themselves to architecture, design, and the genuinely novel problems still beyond a machine’s reach. Recent advances appear to have nudged that future tantalizingly close, but a new paper by researchers at MIT’s Computer Science and Artificial Intelligence Laboratory (CSAIL) and several collaborating institutions argues that this potential future reality demands a hard look at present-day challenges.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;Titled “&lt;a href="https://arxiv.org/pdf/2503.22625" target="_blank"&gt;Challenges and Paths Towards AI for Software Engineering&lt;/a&gt;,” the work maps the many software-engineering tasks beyond code generation, identifies current bottlenecks, and highlights research directions to overcome them, aiming to let humans focus on high-level design while routine work is automated.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;“Everyone is talking about how we don’t need programmers anymore, and there’s all this automation now available,” says Armando Solar‑Lezama, MIT professor of electrical engineering and computer science, CSAIL principal investigator, and senior author of the study. “On the one hand, the field has made tremendous progress. We have tools that are way more powerful than any we’ve seen before. But there’s also a long way to go toward really getting the full promise of automation that we would expect.”&lt;/p&gt;&lt;p dir="ltr"&gt;Solar-Lezama argues that popular narratives often shrink software engineering to “the undergrad programming part: someone hands you a spec for a little function and you implement it, or solving LeetCode-style programming interviews.” Real practice is far broader. It includes everyday refactors that polish design, plus sweeping migrations that move millions of lines from COBOL to Java and reshape entire businesses. It requires nonstop testing and analysis — fuzzing, property-based testing, and other methods — to catch concurrency bugs, or patch zero-day flaws. And it involves the maintenance grind: documenting decade-old code, summarizing change histories for new teammates, and reviewing pull requests for style, performance, and security.&lt;/p&gt;&lt;p dir="ltr"&gt;Industry-scale code optimization — think re-tuning GPU kernels or the relentless, multi-layered refinements behind Chrome’s V8 engine — remains stubbornly hard to evaluate. Today’s headline metrics were designed for short, self-contained problems, and while multiple-choice tests still dominate natural-language research, they were never the norm in AI-for-code. The field’s de facto yardstick, SWE-Bench, simply asks a model to patch a GitHub issue: useful, but still akin to the “undergrad programming exercise” paradigm. It touches only a few hundred lines of code, risks data leakage from public repositories, and ignores other real-world contexts — AI-assisted refactors, human–AI pair programming, or performance-critical rewrites that span millions of lines. Until benchmarks expand to capture those higher-stakes scenarios, measuring progress — and thus accelerating it — will remain an open challenge.&lt;/p&gt;&lt;p dir="ltr"&gt;If measurement is one obstacle, human‑machine communication is another. First author Alex  Gu, an MIT graduate student in electrical engineering and computer science, sees today’s interaction as “a thin line of communication.” When he asks a system to generate code, he often receives a large, unstructured file and even a set of unit tests, yet those tests tend to be superficial. This gap extends to the AI’s ability to effectively use the wider suite of software engineering tools, from debuggers to static analyzers, that humans rely on for precise control and deeper understanding. “I don’t really have much control over what the model writes,” he says. “Without a channel for the AI to expose its own confidence — ‘this part’s correct … this part, maybe double‑check’ — developers risk blindly trusting hallucinated logic that compiles, but collapses in production. Another critical aspect is having the AI know when to defer to the user for clarification.”&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;Scale compounds these difficulties. Current AI models struggle profoundly with large code bases, often spanning millions of lines. Foundation models learn from public GitHub, but “every company’s code base is kind of different and unique,” Gu says, making proprietary coding conventions and specification requirements fundamentally out of distribution. The result is code that looks plausible yet calls non‑existent functions, violates internal style rules, or fails continuous‑integration pipelines. This often leads to AI-generated code that “hallucinates,” meaning it creates content that looks plausible but doesn’t align with the specific internal conventions, helper functions, or architectural patterns of a given company.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;Models will also often retrieve incorrectly, because it retrieves code with a similar name (syntax) rather than functionality and logic, which is what a model might need to know how to write the function. “Standard retrieval techniques are very easily fooled by pieces of code that are doing the same thing but look different,” says Solar‑Lezama.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;The authors mention that since there is no silver bullet to these issues, they’re calling instead for community‑scale efforts: richer, having data that captures the process of developers writing code (for example, which code developers keep versus throw away, how code gets refactored over time, etc.), shared evaluation suites that measure progress on refactor quality, bug‑fix longevity, and migration correctness; and transparent tooling that lets models expose uncertainty and invite human steering rather than passive acceptance. Gu frames the agenda as a “call to action” for larger open‑source collaborations that no single lab could muster alone. Solar‑Lezama imagines incremental advances—“research results taking bites out of each one of these challenges separately”—that feed back into commercial tools and gradually move AI from autocomplete sidekick toward genuine engineering partner.&lt;/p&gt;&lt;p dir="ltr"&gt;“Why does any of this matter? Software already underpins finance, transportation, health care, and the minutiae of daily life, and the human effort required to build and maintain it safely is becoming a bottleneck. An AI that can shoulder the grunt work — and do so without introducing hidden failures — would free developers to focus on creativity, strategy, and ethics” says Gu. “But that future depends on acknowledging that code completion is the easy part; the hard part is everything else. Our goal isn’t to replace programmers. It’s to amplify them. When AI can tackle the tedious and the terrifying, human engineers can finally spend their time on what only humans can do.”&lt;/p&gt;&lt;p dir="ltr"&gt;“With so many new works emerging in AI for coding, and the community often chasing the latest trends, it can be hard to step back and reflect on which problems are most important to tackle,” says Baptiste Rozière, an AI scientist at Mistral AI, who wasn’t involved in the paper. “I enjoyed reading this paper because it offers a clear overview of the key tasks and challenges in AI for software engineering. It also outlines promising directions for future research in the field.”&lt;/p&gt;&lt;p dir="ltr"&gt;Gu and Solar-Lezama wrote the paper with University of California at Berkeley Professor Koushik Sen and PhD students Naman Jain and Manish Shetty, Cornell University Assistant Professor Kevin Ellis and PhD student Wen-Ding Li, Stanford University Assistant Professor Diyi Yang and PhD student Yijia Shao, and incoming Johns Hopkins University assistant professor Ziyang Li. Their work was supported, in part, by the National Science Foundation (NSF), SKY Lab industrial sponsors and affiliates, Intel Corp. through an NSF grant, and the Office of Naval Research.&lt;br&gt;&lt;br&gt;The researchers are presenting their work at the International Conference on Machine Learning (ICML).&amp;nbsp;&lt;/p&gt;</content:encoded>
      <media:content url="https://news.mit.edu/sites/default/files/styles/news_article__cover_image__original/public/images/202507/mit-csail-%20AI-coding.jpg?itok=xzvXTUmh" medium="image" type="image/jpeg" width="390" height="260">
              <media:description type="plain">A new paper by MIT CSAIL researchers maps the many software-engineering tasks beyond code generation, identifies bottlenecks, and highlights research directions to overcome them. The goal: to let humans focus on high-level design, while routine work is automated.</media:description>
              <media:credit>Image: Alex Shipps/MIT CSAIL, using assets from Shutterstock and Pixabay</media:credit>
      </media:content>
        <category domain="https://news.mit.edu/topic/research">Research</category>
      <category domain="https://news.mit.edu/topic/artificial-intelligence2">Artificial intelligence</category>
      <category domain="https://news.mit.edu/topic/machine-learning">Machine learning</category>
      <category domain="https://news.mit.edu/topic/algorithms">Algorithms</category>
      <category domain="https://news.mit.edu/topic/software">Software</category>
      <category domain="https://news.mit.edu/topic/programming">Programming</category>
      <category domain="https://news.mit.edu/topic/computers">Computer science and technology</category>
      <category domain="https://news.mit.edu/topic/programming-languages">Programming languages</category>
      <category domain="https://news.mit.edu/topic/human-computer-interaction">Human-computer interaction</category>
      <category domain="https://news.mit.edu/topic/nsf">National Science Foundation (NSF)</category>
      <category domain="https://news.mit.edu/topic/computer-science-and-artificial-intelligence-laboratory-csail">Computer Science and Artificial Intelligence Laboratory (CSAIL)</category>
      <category domain="https://news.mit.edu/topic/electrical-engineering-computer-science-eecs">Electrical engineering and computer science (EECS)</category>
      <category domain="https://news.mit.edu/topic/mit-schwarzman-college-computing">MIT Schwarzman College of Computing</category>
      <category domain="https://news.mit.edu/topic/school-engineering">School of Engineering</category>
    </item>
<item>
  <title>Five MIT faculty elected to the National Academy of Sciences for 2025</title>
  <link>https://news.mit.edu/2025/faculty-elected-national-academy-sciences-0714</link>
  <description>Rodney Brooks, Parag Pathak, Scott Sheffield, Benjamin Weiss, Yukiko Yamashita, and 13 MIT alumni are recognized by their peers for their outstanding contributions to research.</description>
  <pubDate>Mon, 14 Jul 2025 14:45:00 -0400</pubDate>
    <guid isPermaLink="true">https://news.mit.edu/2025/faculty-elected-national-academy-sciences-0714</guid>
        <dc:creator>Sarah Costello | School of Science</dc:creator>
  <content:encoded>&lt;p&gt;The National Academy of Sciences (NAS) has elected 120 members and 30 international members, including five MIT faculty members and 13 MIT alumni. Professors Rodney Brooks, Parag Pathak, Scott Sheffield, Benjamin Weiss, and Yukiko Yamashita were elected in recognition of their “distinguished and continuing achievements in original research.” Membership to the National Academy of Sciences is one of the highest honors a scientist can receive in their career.&lt;/p&gt;&lt;p&gt;Elected MIT alumni include: David Altshuler ’86, Rafael Camerini-Otero ’66, Kathleen Collins PhD ’92, George Daley PhD ’89, Scott Doney PhD ’91, John Doyle PhD ’91, Jonathan Ellman ’84, Shanhui Fan PhD ’97, Julia Greer ’97, Greg Lemke ’78, Stanley Perlman PhD ’72, David Reichman PhD ’97, and Risa Wechsler ’96.&amp;nbsp;&lt;/p&gt;&lt;p&gt;Those elected this year bring the total number of active members to 2,662, with 556 international members. The NAS is a private, nonprofit institution that was established under a congressional charter signed by President Abraham Lincoln in 1863. It recognizes achievement in science by election to membership, and — with the National Academy of Engineering and the National Academy of Medicine — provides science, engineering, and health policy advice to the federal government and other organizations.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Rodney Brooks&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="https://www.csail.mit.edu/person/rodney-brooks"&gt;Rodney A. Brooks&lt;/a&gt; is the Panasonic Professor of Robotics Emeritus at MIT and the chief technical officer and co-founder of Robust AI. Previously, he was founder, chair, and CTO of Rethink Robotics and founder and CTO of iRobot Corp. He is also the former director of the MIT Artificial Intelligence Laboratory and the MIT Computer Science and Artificial Intelligence Laboratory. Brooks received degrees in pure mathematics from the Flinders University of South Australia and a PhD in computer science from Stanford University in 1981. He held research positions at Carnegie Mellon University and MIT, and a faculty position at Stanford before joining the faculty of MIT in 1984.&lt;/p&gt;&lt;p&gt;Brooks’ research is concerned with both the engineering of intelligent robots to operate in unstructured environments, and with understanding human intelligence through building humanoid robots. He has published papers and books in model-based computer vision, path planning, uncertainty analysis, robot assembly, active vision, autonomous robots, micro-robots, micro-actuators, planetary exploration, representation, artificial life, humanoid robots, and compiler design.&lt;/p&gt;&lt;p&gt;Brooks is a member of the National Academy of Engineering, a founding fellow of the Association for the Advancement of Artificial Intelligence, a fellow of the American Academy of Arts and Sciences, the American Association for the Advancement of Science, the Association for Computing Machinery, a foreign fellow of The Australian Academy of Technological Sciences and Engineering, and a corresponding member of the Australian Academy of Science. He won the Computers and Thought Award at the 1991 International Joint Conference on Artificial Intelligence, and the IEEE Founders Medal in 2023.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Parag Pathak&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="https://economics.mit.edu/people/faculty/parag-pathak"&gt;Parag Pathak&lt;/a&gt; is the Class of 1922 Professor of Economics and a founder and director of MIT’s&amp;nbsp;&lt;a href="https://blueprintlabs.mit.edu/"&gt;Blueprint Labs&lt;/a&gt;. He joined the MIT faculty in 2008 after completing his PhD in business economics and his master’s and bachelor’s degrees in applied mathematics, all at Harvard University.&lt;/p&gt;&lt;p&gt;Pathak is best known for his work on market design and education. His research has informed student placement and school choice mechanisms across the United States, including in Boston, New York City, Chicago, and Washington, and his recent work applies ideas from market design to the rationing of vital medical resources. Pathak has also authored leading studies on school quality, charter schools, and affirmative action. In urban economics, he has measured the effects of foreclosures on house prices and how the housing market reacted to the end of rent control in Cambridge, Massachusetts.&lt;/p&gt;&lt;p&gt;Pathak’s research on market design was recognized with the 2018 John Bates Clark Medal, given by the American Economic Association to the economist under 40 whose work is judged to have made the most significant contribution to the field.&amp;nbsp;He is a fellow of the American Academy of Arts and Sciences, the Econometric Society, and the Society for the Advancement of Economic Theory. Pathak is also the founding co-director of the market design working group at the&amp;nbsp;&lt;a href="https://www.nber.org/"&gt;National Bureau of Economic Research&lt;/a&gt;, and a co-founder of&amp;nbsp;&lt;a href="https://avela.org/"&gt;Avela Education&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Scott Sheffield&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="https://math.mit.edu/directory/profile.html?pid=243"&gt;Scott Sheffield&lt;/a&gt;, Leighton Family Professor of Mathematics, joined the MIT faculty in 2008 after a faculty appointment at the Courant Institute at New York University. He received a PhD in mathematics from Stanford University in 2003 under the supervision of Amir Dembo, and completed BA and MA degrees in mathematics from Harvard University in 1998.&lt;/p&gt;&lt;p&gt;Sheffield is a probability theorist, working on geometrical questions that arise in such areas as statistical physics, game theory, and metric spaces, as well as long-standing problems in percolation theory and the theory of random surfaces.&lt;/p&gt;&lt;p&gt;In 2017, Sheffield received the Clay Research Award with Jason Miller, “in recognition of their groundbreaking and conceptually novel work on the geometry of Gaussian free field and its application to the solution of open problems in the theory of two-dimensional random structures.” In 2023, he received the Leonard Eisenbud Prize with Jason Miller “for works on random two-dimensional geometries, and in particular on Liouville Quantum Gravity.” Later in 2023, Sheffield received the Frontiers of Science Award with Jason Miller for the paper “Liouville quantum gravity and the Brownian map I: the QLE(8/3,0) metric.” Sheffield is a fellow of the American Academy of Arts and Science.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Benjamin Weiss&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="https://eaps.mit.edu/people/faculty/benjamin-weiss/"&gt;Benjamin Weiss&lt;/a&gt; is the Robert R. Schrock Professor of Earth and Planetary Sciences. He studied physics at Amherst College as an undergraduate and went on to study planetary science and geology at Caltech, where he earned a master’s degree in 2001 and PhD in 2003. Weiss’ doctoral dissertation on Martian meteorite ALH 84001 revealed records of the ancient Martian climate and magnetic field, and provided evidence some meteorites could transfer materials from Mars to Earth without heat-sterilization. Weiss became a member of the Department of Earth, Atmospheric and Planetary Sciences faculty in 2004 and is currently chair of the Program in Planetary Science.&lt;/p&gt;&lt;p&gt;A specialist in magnetometry, Weiss seeks to understand the formation and evolution of the Earth, terrestrial planets, and small solar system bodies through laboratory analysis, spacecraft observations, and fieldwork. He is known for key insights into the history of our solar system, including discoveries about the early nebular magnetic field, the moon’s long-lived core dynamo, and asteroids that generated core dynamos in the past. In addition to leadership roles on current, active NASA missions — as deputy principal investigator for Psyche, and co-investigator for Mars Perseverance and Europa Clipper — Weiss has also been part of science teams for the SpaceIL Beresheet, JAXA Hayabusa 2, and ESA Rosetta spacecraft.&lt;/p&gt;&lt;p&gt;As principal investigator of the MIT Planetary Magnetism Laboratory, Weiss works to develop high-sensitivity, high-resolution techniques in magnetic microscopy to image the magnetic fields embedded in rock samples collected from meteorites, the lunar surface, and sites around the Earth. Studying these magnetic signatures can help answer questions about the conditions of the early solar system, past climates on Earth and Mars, and factors that promote habitability.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Yukiko Yamashita&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="https://biology.mit.edu/profile/yukiko-yamashita/"&gt;Yukiko Yamashita&lt;/a&gt; is a professor of biology at MIT, a core member of the Whitehead Institute for Biomedical Research, and an investigator at the Howard Hughes Medical Institute (HHMI). Yamashita earned her BS in biology in 1994 and her PhD in biophysics in 1999 from Kyoto University. From 2001 to 2006, she did postdoctoral research at Stanford University. She was appointed to the University of Michigan faculty in 2007 and was named an HHMI Investigator in 2014. She became a member of the Whitehead Institute and a professor of biology at MIT in 2020.&lt;/p&gt;&lt;p&gt;Yukiko Yamashita studies two fundamental aspects of multicellular organisms: how cell fates are diversified via asymmetric cell division, and how genetic information is transmitted through generations via the germline.&lt;/p&gt;&lt;p&gt;Two remarkable feats of multicellular organisms are generation of many distinct cell types via asymmetric cell division and transmission of the germline genome to the next generation, essentially in eternity. Studying these processes using the &lt;em&gt;Drosophila&lt;/em&gt; male germline as a model system has led us to venture into new areas of study, such as functions of satellite DNA, “genomic junk,” and how they might be involved in speciation.&lt;/p&gt;&lt;p&gt;Yamashita is a member of the American Academy of Arts and Sciences, a fellow of the American Society for Cell Biology, and the winner of the Tsuneko and Reiji Okazaki Award in 2016. She was named a MacArthur Fellow in 2011.&lt;/p&gt;</content:encoded>
      <media:content url="https://news.mit.edu/sites/default/files/styles/news_article__cover_image__original/public/images/202507/mit-faculty-National-Academy-of-Sciences-2025.jpg?itok=FFQaUQN3" medium="image" type="image/jpeg" width="390" height="260">
              <media:description type="plain">Clockwise from top left: Rodney Brooks, Parag Pathak, Scott Sheffield, Yukiko Yamashita, and Benjamin Weiss</media:description>
              <media:credit>Photos courtesy of the faculty.</media:credit>
      </media:content>
        <category domain="https://news.mit.edu/topic/awards">Awards, honors and fellowships</category>
      <category domain="https://news.mit.edu/topic/faculty">Faculty</category>
      <category domain="https://news.mit.edu/topic/biology">Biology</category>
      <category domain="https://news.mit.edu/topic/eaps">EAPS</category>
      <category domain="https://news.mit.edu/topic/economics">Economics</category>
      <category domain="https://news.mit.edu/topic/whitehead-institute">Whitehead Institute</category>
      <category domain="https://news.mit.edu/topic/electrical-engineering-computer-science-eecs">Electrical engineering and computer science (EECS)</category>
      <category domain="https://news.mit.edu/topic/computer-science-and-artificial-intelligence-laboratory-csail">Computer Science and Artificial Intelligence Laboratory (CSAIL)</category>
      <category domain="https://news.mit.edu/topic/school-engineering">School of Engineering</category>
      <category domain="https://news.mit.edu/topic/school-science">School of Science</category>
      <category domain="https://news.mit.edu/topic/mit-schwarzman-college-computing">MIT Schwarzman College of Computing</category>
      <category domain="https://news.mit.edu/topic/school-humanities-arts-and-social-sciences">School of Humanities Arts and Social Sciences</category>
      <category domain="https://news.mit.edu/topic/national-academy-sciences-nas">National Academy of Sciences (NAS)</category>
    </item>
<item>
  <title>Simulation-based pipeline tailors training data for dexterous robots</title>
  <link>https://news.mit.edu/2025/simulation-based-pipeline-tailors-training-data-dexterous-robots-0711</link>
  <description>The PhysicsGen system, developed by MIT researchers, helps robots handle items in homes and factories by tailoring training data to a particular machine.</description>
  <pubDate>Fri, 11 Jul 2025 15:20:00 -0400</pubDate>
    <guid isPermaLink="true">https://news.mit.edu/2025/simulation-based-pipeline-tailors-training-data-dexterous-robots-0711</guid>
        <dc:creator>Alex Shipps | MIT CSAIL</dc:creator>
  <content:encoded>&lt;p dir="ltr" id="docs-internal-guid-f6145a87-7fff-9bf5-3320-00f62c855acb"&gt;When ChatGPT or Gemini give what seems to be an expert response to your burning questions, you may not realize how much information it relies on to give that reply. Like other popular generative artificial intelligence (AI) models, these chatbots rely on backbone systems called foundation models that train on billions, or even trillions, of data points.&lt;/p&gt;&lt;p dir="ltr"&gt;In a similar vein, engineers are hoping to build foundation models that train a range of robots on new skills like picking up, moving, and putting down objects in places like homes and factories. The problem is that it’s difficult to collect and transfer instructional data across robotic systems. You could teach your system by teleoperating the hardware step-by-step using technology like virtual reality (VR), but that can be time-consuming. Training on videos from the internet is less instructive, since the clips don’t provide a step-by-step, specialized task walk-through for particular robots.&lt;br&gt;&lt;br&gt;A simulation-driven approach called “PhysicsGen” from MIT’s Computer Science and Artificial Intelligence Laboratory (CSAIL) and the Robotics and AI Institute customizes robot training data to help robots find the most efficient movements for a task. The system can multiply a few dozen VR demonstrations into nearly 3,000 simulations per machine. These high-quality instructions are then mapped to the precise configurations of mechanical companions like robotic arms and hands.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;PhysicsGen creates data that generalize to specific robots and condition via a three-step process. First, a VR headset tracks how humans manipulate objects like blocks using their hands. These interactions are mapped in a 3D physics simulator at the same time, visualizing the key points of our hands as small spheres that mirror our gestures. For example, if you flipped a toy over, you’d see 3D shapes representing different parts of your hands rotating a virtual version of that object.&lt;br&gt;&lt;br&gt;The pipeline then remaps these points to a 3D model of the setup of a specific machine (like a robotic arm), moving them to the precise “joints” where a system twists and turns. Finally, PhysicsGen uses trajectory optimization — essentially simulating the most efficient motions to complete a task — so the robot knows the best ways to do things like repositioning a box.&lt;/p&gt;&lt;p dir="ltr"&gt;Each simulation is a detailed training data point that walks a robot through potential ways to handle objects. When implemented into a policy (or the action plan that the robot follows), the machine has a variety of ways to approach a task, and can try out different motions if one doesn’t work.&lt;/p&gt;&lt;p dir="ltr"&gt;“We’re creating robot-specific data without needing humans to re-record specialized demonstrations for each machine,” says Lujie Yang, an MIT PhD student in electrical engineering and computer science and CSAIL affiliate who is the lead author of a new&amp;nbsp;&lt;a href="https://arxiv.org/abs/2502.20382"&gt;paper&lt;/a&gt; introducing the project. “We’re scaling up the data in an autonomous and efficient way, making task instructions useful to a wider range of machines.”&lt;br&gt;&lt;br&gt;Generating so many instructional trajectories for robots could eventually help engineers build a massive dataset to guide machines like robotic arms and dexterous hands. For example, the pipeline might help two robotic arms collaborate on picking up warehouse items and placing them in the right boxes for deliveries. The system may also guide two robots to work together in a household on tasks like putting away cups.&lt;br&gt;&lt;br&gt;PhysicsGen’s potential also extends to converting data designed for older robots or different environments into useful instructions for new machines. “Despite being collected for a specific type of robot, we can revive these prior datasets to make them more generally useful,” adds Yang.&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;strong&gt;Addition by multiplication&lt;/strong&gt;&lt;br&gt;&lt;br&gt;PhysicsGen turned just 24 human demonstrations into thousands of simulated ones, helping both digital and real-world robots reorient objects.&lt;br&gt;&lt;br&gt;Yang and her colleagues first tested their pipeline in a virtual experiment where a floating robotic hand needed to rotate a block into a target position. The digital robot executed the task at a rate of 81 percent accuracy by training on PhysicGen’s massive dataset, a 60 percent improvement from a baseline that only learned from human demonstrations.&lt;br&gt;&lt;br&gt;The researchers also found that PhysicsGen could improve how virtual robotic arms collaborate to manipulate objects. Their system created extra training data that helped two pairs of robots successfully accomplish tasks as much as 30 percent more often than a purely human-taught baseline.&lt;/p&gt;&lt;p dir="ltr"&gt;In an experiment with a pair of real-world robotic arms, the researchers observed similar improvements as the machines teamed up to flip a large box into its designated position. When the robots deviated from the intended trajectory or mishandled the object, they were able to recover mid-task by referencing alternative trajectories from their library of instructional data.&lt;br&gt;&lt;br&gt;Senior author Russ Tedrake, who is the Toyota Professor of Electrical Engineering and Computer Science, Aeronautics and Astronautics, and Mechanical Engineering at MIT, adds that this imitation-guided data generation technique combines the strengths of human demonstration with the power of robot motion planning algorithms.&lt;br&gt;&lt;br&gt;“Even a single demonstration from a human can make the motion planning problem much easier,” says Tedrake, who is also a senior vice president of large behavior models at the Toyota Research Institute and CSAIL principal investigator. “In the future, perhaps the foundation models will be able to provide this information, and this type of data generation technique will provide a type of post-training recipe for that model.”&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;strong&gt;The future of PhysicsGen&lt;/strong&gt;&lt;/p&gt;&lt;p dir="ltr"&gt;Soon, PhysicsGen may be extended to a new frontier: diversifying the tasks a machine can execute.&lt;/p&gt;&lt;p dir="ltr"&gt;“We’d like to use PhysicsGen to teach a robot to pour water when it’s only been trained to put away dishes, for example,” says Yang. “Our pipeline doesn’t just generate dynamically feasible motions for familiar tasks; it also has the potential of creating a diverse library of physical interactions that we believe can serve as building blocks for accomplishing entirely new tasks a human hasn’t demonstrated.”&lt;/p&gt;&lt;p dir="ltr"&gt;Creating lots of widely applicable training data may eventually help build a foundation model for robots, though MIT researchers caution that this is a somewhat distant goal. The CSAIL-led team is investigating how PhysicsGen can harness vast, unstructured resources&amp;nbsp;— like internet videos — as seeds for simulation. The goal: transform everyday visual content into rich, robot-ready data that could teach machines to perform tasks no one explicitly showed them.&lt;/p&gt;&lt;p dir="ltr"&gt;Yang and her colleagues also aim to make PhysicsGen even more useful for robots with diverse shapes and configurations in the future. To make that happen, they plan to leverage datasets with demonstrations of real robots, capturing how robotic joints move instead of human ones.&lt;/p&gt;&lt;p dir="ltr"&gt;The researchers also plan to incorporate reinforcement learning, where an AI system learns by trial and error, to make PhysicsGen expand its dataset beyond human-provided examples. They may augment their pipeline with advanced perception techniques to help a robot perceive and interpret their environment visually, allowing the machine to analyze and adapt to the complexities of the physical world.&lt;/p&gt;&lt;p dir="ltr"&gt;For now, PhysicsGen shows how AI can help us teach different robots to manipulate objects within the same category, particularly rigid ones. The pipeline may soon help robots find the best ways to handle soft items (like fruits) and deformable ones (like clay), but those interactions aren’t easy to simulate yet.&lt;/p&gt;&lt;p&gt;Yang and Tedrake wrote the paper with two CSAIL colleagues: co-lead author and MIT PhD student Hyung Ju “Terry” Suh SM ’22 and MIT PhD student Bernhard Paus Græsdal. Robotics and AI Institute researchers Tong Zhao ’22, MEng ’23, Tarik Kelestemur, Jiuguang Wang, and Tao Pang PhD ’23 are also authors. Their work was supported by the Robotics and AI Institute and Amazon.&lt;br&gt;&lt;br&gt;The researchers recently presented their work at the Robotics: Science and Systems conference.&lt;/p&gt;</content:encoded>
      <media:content url="https://news.mit.edu/sites/default/files/styles/news_article__cover_image__original/public/images/202507/MIT-PhysicsGen.jpg?itok=TCRqyXpM" medium="image" type="image/jpeg" width="390" height="260">
              <media:description type="plain">PhysicsGen can multiply a few dozen virtual reality demonstrations into nearly 3,000 simulations per machine for mechanical companions like robotic arms and hands.</media:description>
              <media:credit>Image designed by Alex Shipps/MIT CSAIL, using photos from the researchers.</media:credit>
      </media:content>
        <category domain="https://news.mit.edu/topic/research">Research</category>
      <category domain="https://news.mit.edu/topic/robotics">Robotics</category>
      <category domain="https://news.mit.edu/topic/machine-learning">Machine learning</category>
      <category domain="https://news.mit.edu/topic/augmented-and-virtual-reality">Augmented and virtual reality</category>
      <category domain="https://news.mit.edu/topic/data">Data</category>
      <category domain="https://news.mit.edu/topic/3-d">3-D</category>
      <category domain="https://news.mit.edu/topic/computers">Computer science and technology</category>
      <category domain="https://news.mit.edu/topic/electrical-engineering-computer-science-eecs">Electrical engineering and computer science (EECS)</category>
      <category domain="https://news.mit.edu/topic/computer-science-and-artificial-intelligence-laboratory-csail">Computer Science and Artificial Intelligence Laboratory (CSAIL)</category>
      <category domain="https://news.mit.edu/topic/mit-schwarzman-college-computing">MIT Schwarzman College of Computing</category>
      <category domain="https://news.mit.edu/topic/school-engineering">School of Engineering</category>
    </item>
<item>
  <title>AI shapes autonomous underwater “gliders”</title>
  <link>https://news.mit.edu/2025/ai-shapes-autonomous-underwater-gliders-0709</link>
  <description>An AI pipeline developed by CSAIL researchers enables unique hydrodynamic designs for bodyboard-sized vehicles that glide underwater and could help scientists gather marine data.</description>
  <pubDate>Wed, 09 Jul 2025 16:35:00 -0400</pubDate>
    <guid isPermaLink="true">https://news.mit.edu/2025/ai-shapes-autonomous-underwater-gliders-0709</guid>
        <dc:creator>Alex Shipps | MIT CSAIL</dc:creator>
  <content:encoded>&lt;p dir="ltr" id="docs-internal-guid-cf34fa1c-7fff-d3f4-d4e3-35ce42aa6255"&gt;Marine scientists have long marveled at how animals like fish and seals swim so efficiently despite having different shapes. Their bodies are optimized for efficient, hydrodynamic aquatic navigation so they can exert minimal energy when traveling long distances.&lt;br&gt;&lt;br&gt;Autonomous vehicles can drift through the ocean in a similar way, collecting data about vast underwater environments. However, the shapes of these gliding machines are less diverse than what we find in marine life — go-to designs often resemble tubes or torpedoes, since they’re fairly hydrodynamic as well. Plus, testing new builds requires lots of real-world trial-and-error.&lt;br&gt;&lt;br&gt;Researchers from MIT’s Computer Science and Artificial Intelligence Laboratory (CSAIL) and the University of Wisconsin at Madison propose that AI could help us explore uncharted glider designs more conveniently. Their method uses machine learning to test different 3D designs in a physics simulator, then molds them into more hydrodynamic shapes. The resulting model can be fabricated via a 3D printer using significantly less energy than hand-made ones.&lt;br&gt;&lt;br&gt;The MIT scientists say that this design pipeline could create new, more efficient machines that help oceanographers measure water temperature and salt levels, gather more detailed insights about currents, and monitor the impacts of climate change. The team demonstrated this potential by producing two gliders roughly the size of a boogie board: a two-winged machine resembling an airplane, and a unique, four-winged object resembling a flat fish with four fins.&lt;/p&gt;&lt;p dir="ltr"&gt;Peter Yichen Chen, MIT CSAIL postdoc and co-lead researcher on the project, notes that these designs are just a few of the novel shapes his team’s approach can generate. “We’ve developed a semi-automated process that can help us test unconventional designs that would be very taxing for humans to design,” he says. “This level of shape diversity hasn’t been explored previously, so most of these designs haven’t been tested in the real world.”&lt;/p&gt;&lt;p dir="ltr"&gt;But how did AI come up with these ideas in the first place? First, the researchers found 3D models of over 20 conventional sea exploration shapes, such as submarines, whales, manta rays, and sharks. Then, they enclosed these models in “deformation cages” that map out different articulation points that the researchers pulled around to create new shapes.&lt;/p&gt;&lt;p dir="ltr"&gt;The CSAIL-led team built a dataset of conventional and deformed shapes before simulating how they would perform at different “angles-of-attack” — the direction a vessel will tilt as it glides through the water. For example, a swimmer may want to dive at a -30 degree angle to retrieve an item from a pool.&lt;/p&gt;&lt;p dir="ltr"&gt;These diverse shapes and angles of attack were then used as inputs for a neural network that essentially anticipates how efficiently a glider shape will perform at particular angles and optimizes it as needed.&lt;br&gt;&lt;br&gt;&lt;strong&gt;Giving gliding robots a lift&lt;/strong&gt;&lt;/p&gt;&lt;p dir="ltr"&gt;The team’s neural network simulates how a particular glider would react to underwater physics, aiming to capture how it moves forward and the force that drags against it. The goal: find the best lift-to-drag ratio, representing how much the glider is being held up compared to how much it’s being held back. The higher the ratio, the more efficiently the vehicle travels; the lower it is, the more the glider will slow down during its voyage.&lt;/p&gt;&lt;p dir="ltr"&gt;Lift-to-drag ratios are key for flying planes: At takeoff, you want to maximize lift to ensure it can glide well against wind currents, and when landing, you need sufficient force to drag it to a full stop.&lt;/p&gt;&lt;p dir="ltr"&gt;Niklas Hagemann, an MIT graduate student in architecture and CSAIL affiliate, notes that this ratio is just as useful if you want a similar gliding motion in the ocean.&lt;br&gt;&lt;br&gt;“Our pipeline modifies glider shapes to find the best lift-to-drag ratio, optimizing its performance underwater,” says Hagemann, who is also a co-lead author on a&amp;nbsp;&lt;a href="https://arxiv.org/abs/2505.00222"&gt;paper&lt;/a&gt; that was presented at the International Conference on Robotics and Automation in June. “You can then export the top-performing designs so they can be 3D-printed.”&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;strong&gt;Going for a quick glide&lt;/strong&gt;&lt;br&gt;&lt;br&gt;While their AI pipeline seemed realistic, the researchers needed to ensure its predictions about glider performance were accurate by experimenting in more lifelike environments.&lt;/p&gt;&lt;p dir="ltr"&gt;They first fabricated their two-wing design as a scaled-down vehicle resembling a paper airplane. This glider was taken to MIT’s Wright Brothers Wind Tunnel, an indoor space with fans that simulate wind flow. Placed at different angles, the glider’s predicted lift-to-drag ratio was only about 5 percent higher on average than the ones recorded in the wind experiments — a small difference between simulation and reality.&lt;br&gt;&lt;br&gt;A digital evaluation involving a visual, more complex physics simulator also supported the notion that the AI pipeline made fairly accurate predictions about how the gliders would move. It visualized how these machines would descend in 3D.&lt;br&gt;&lt;br&gt;To truly evaluate these gliders in the real world, though, the team needed to see how their devices would fare underwater. They printed two designs that performed the best at specific points-of-attack for this test: a jet-like device at 9 degrees and the four-wing vehicle at 30 degrees.&lt;/p&gt;&lt;p dir="ltr"&gt;Both shapes were fabricated in a 3D printer as hollow shells with small holes that flood when fully submerged. This lightweight design makes the vehicle easier to handle outside of the water and requires less material to be fabricated. The researchers placed a tube-like device inside these shell coverings, which housed a range of hardware, including a pump to change the glider’s buoyancy, a mass shifter (a device that controls the machine’s angle-of-attack), and electronic components.&lt;br&gt;&lt;br&gt;Each design outperformed a handmade torpedo-shaped glider by moving more efficiently across a pool. With higher lift-to-drag ratios than their counterpart, both AI-driven machines exerted less energy, similar to the effortless ways marine animals navigate the oceans.&lt;br&gt;&lt;br&gt;As much as the project is an encouraging step forward for glider design, the researchers are looking to narrow the gap between simulation and real-world performance. They are also hoping to develop machines that can react to sudden changes in currents, making the gliders more adaptable to seas and oceans.&lt;br&gt;&lt;br&gt;Chen adds that the team is looking to explore new types of shapes, particularly thinner glider designs. They intend to make their framework faster, perhaps bolstering it with new features that enable more customization, maneuverability, or even the creation of miniature vehicles.&lt;br&gt;&lt;br&gt;Chen and Hagemann co-led research on this project with OpenAI researcher Pingchuan Ma SM ’23, PhD ’25. They authored the paper with Wei Wang, a University of Wisconsin at Madison assistant professor and recent CSAIL postdoc; John Romanishin ’12, SM ’18, PhD ’23; and two MIT professors and CSAIL members: lab director Daniela Rus and senior author Wojciech Matusik. Their work was supported, in part, by a Defense Advanced Research Projects Agency (DARPA) grant and the MIT-GIST Program.&lt;/p&gt;</content:encoded>
      <media:content url="https://news.mit.edu/sites/default/files/styles/news_article__cover_image__original/public/images/202506/mit-auv-Gliders.png?itok=HN0AnVPX" medium="image" type="image/jpeg" width="390" height="260">
              <media:description type="plain">MIT researchers used a new machine-learning method to produce two real-world underwater gliders: a two-winged machine resembling an airplane (lower right), and a unique, four-winged object (lower left).</media:description>
              <media:credit>Image courtesy of the researchers.</media:credit>
      </media:content>
        <category domain="https://news.mit.edu/topic/research">Research</category>
      <category domain="https://news.mit.edu/topic/artificial-intelligence2">Artificial intelligence</category>
      <category domain="https://news.mit.edu/topic/machine-learning">Machine learning</category>
      <category domain="https://news.mit.edu/topic/robotics">Robotics</category>
      <category domain="https://news.mit.edu/topic/computers">Computer science and technology</category>
      <category domain="https://news.mit.edu/topic/design">Design</category>
      <category domain="https://news.mit.edu/topic/autonomous-underwater-vehicles-auv">Autonomous Underwater Vehicles (AUV)</category>
      <category domain="https://news.mit.edu/topic/invention">Invention</category>
      <category domain="https://news.mit.edu/topic/3-d-printing">3-D printing</category>
      <category domain="https://news.mit.edu/topic/marine-biology">Marine biology</category>
      <category domain="https://news.mit.edu/topic/mit-schwarzman-college-computing">MIT Schwarzman College of Computing</category>
      <category domain="https://news.mit.edu/topic/electrical-engineering-computer-science-eecs">Electrical engineering and computer science (EECS)</category>
      <category domain="https://news.mit.edu/topic/computer-science-and-artificial-intelligence-laboratory-csail">Computer Science and Artificial Intelligence Laboratory (CSAIL)</category>
      <category domain="https://news.mit.edu/topic/school-engineering">School of Engineering</category>
      <category domain="https://news.mit.edu/topic/darpa">Defense Advanced Research Projects Agency (DARPA)</category>
    </item>
<item>
  <title>Study could lead to LLMs that are better at complex reasoning</title>
  <link>https://news.mit.edu/2025/study-could-lead-llms-better-complex-reasoning-0708</link>
  <description>Researchers developed a way to make large language models more adaptable to challenging tasks like strategic planning or process optimization.</description>
  <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
    <guid isPermaLink="true">https://news.mit.edu/2025/study-could-lead-llms-better-complex-reasoning-0708</guid>
        <dc:creator>Adam Zewe | MIT News</dc:creator>
  <content:encoded>&lt;p&gt;For all their impressive capabilities, large language models (LLMs) often fall short when given challenging new tasks that require complex reasoning skills.&lt;/p&gt;&lt;p&gt;While an accounting firm’s LLM might excel at summarizing financial reports, that same model could fail unexpectedly if tasked with predicting market trends or identifying fraudulent transactions.&lt;/p&gt;&lt;p&gt;To make LLMs more adaptable, MIT researchers investigated how a certain training technique can be strategically deployed to boost a model’s performance on unfamiliar, difficult problems.&lt;/p&gt;&lt;p&gt;They show that test-time training, a method that involves temporarily updating some of a model’s inner workings during deployment, can lead to a sixfold improvement in accuracy. The researchers developed a framework for implementing a test-time training strategy that uses examples of the new task to maximize these gains.&lt;/p&gt;&lt;p&gt;Their work could improve a model’s flexibility, enabling an off-the-shelf LLM to adapt to complex tasks that require planning or abstraction. This could lead to LLMs that would be more accurate in many applications that require logical deduction, from medical diagnostics to supply chain management.&lt;/p&gt;&lt;p&gt;“Genuine learning — what we did here with test-time training — is something these models can’t do on their own after they are shipped. They can’t gain new skills or get better at a task. But we have shown that if you push the model a little bit to do actual learning, you see that huge improvements in performance can happen,” says Ekin Akyürek PhD ’25, lead author of the study.&lt;/p&gt;&lt;p&gt;Akyürek is joined on the &lt;a href="https://arxiv.org/pdf/2411.07279" target="_blank"&gt;paper&lt;/a&gt; by graduate students Mehul Damani, Linlu Qiu, Han Guo, and Jyothish Pari; undergraduate Adam Zweiger; and senior authors Yoon Kim, an assistant professor of Electrical Engineering and Computer Science (EECS) and a member of the Computer Science and Artificial Intelligence Laboratory (CSAIL); and Jacob Andreas, an associate professor in EECS and a member of CSAIL. The research will be presented at the International Conference on Machine Learning.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Tackling hard domains&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;LLM users often try to improve the performance of their model on a new task using a technique called in-context learning. They feed the model a few examples of the new task as text prompts which guide the model’s outputs.&lt;/p&gt;&lt;p&gt;But in-context learning doesn’t always work for problems that require logic and reasoning.&lt;/p&gt;&lt;p&gt;The MIT researchers investigated how test-time training can be used in conjunction with in-context learning to boost performance on these challenging tasks. Test-time training involves updating some model parameters — the internal variables it uses to make predictions — using a small amount of new data specific to the task at hand.&lt;/p&gt;&lt;p&gt;The researchers explored how test-time training interacts with in-context learning. They studied design choices that maximize the performance improvements one can coax out of a general-purpose LLM.&lt;/p&gt;&lt;p&gt;“We find that test-time training is a much stronger form of learning.&amp;nbsp;While simply providing examples can modestly boost accuracy, actually updating the model with those examples can lead to significantly better performance, particularly&amp;nbsp;in challenging domains,” Damani says.&lt;/p&gt;&lt;p&gt;In-context learning requires a small set of task examples, including problems and their solutions. The researchers use these examples to create a task-specific dataset needed for test-time training.&lt;/p&gt;&lt;p&gt;To expand the size of this dataset, they create new inputs by slightly changing the problems and solutions in the examples, such as by horizontally flipping some input data. They find that training the model on the outputs of this new dataset leads to the best performance.&lt;/p&gt;&lt;p&gt;In addition, the researchers only update a small number of model parameters using a technique called low-rank adaption, which improves the efficiency of the test-time training process.&lt;/p&gt;&lt;p&gt;“This is important because our method needs to be efficient if it is going to be deployed in the real world. We find that you can get huge improvements in accuracy with a very small amount of parameter training,” Akyürek says.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Developing new skills&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Streamlining the process is key, since test-time training is employed on a per-instance basis, meaning a user would need to do this for each individual task. The updates to the model are only temporary, and the model reverts to its original form after making a prediction.&lt;/p&gt;&lt;p&gt;A model that usually takes less than a minute to answer a query might take five or 10 minutes to provide an answer with test-time training, Akyürek adds.&lt;/p&gt;&lt;p&gt;“We wouldn’t want to do this for all user queries, but it is useful if you have a very hard task that you want to the model to solve well. There also might be tasks that are too challenging for an LLM to solve without this method,” he says.&lt;/p&gt;&lt;p&gt;The researchers tested their approach on two benchmark datasets of extremely complex problems, such as IQ puzzles. It boosted accuracy as much as sixfold over techniques that use only in-context learning.&lt;/p&gt;&lt;p&gt;Tasks that involved structured patterns or those which used completely unfamiliar types of data showed the largest performance improvements.&lt;/p&gt;&lt;p&gt;“For simpler tasks, in-context learning might be OK. But updating the parameters themselves might develop a new skill in the model,” Damani says.&lt;/p&gt;&lt;p&gt;In the future, the researchers want to use these insights toward the development of models that continually learn.&lt;/p&gt;&lt;p&gt;The long-term goal is an LLM that, given a query, can automatically determine if it needs to use test-time training to update parameters or if it can solve the task using in-context learning, and then implement the best test-time training strategy without the need for human intervention.&lt;/p&gt;&lt;p&gt;This work is supported, in part, by the MIT-IBM Watson AI Lab and the National Science Foundation.&lt;/p&gt;</content:encoded>
      <media:content url="https://news.mit.edu/sites/default/files/styles/news_article__cover_image__original/public/images/202507/MIT-fewshot-01-press.jpg?itok=WPSxbpOJ" medium="image" type="image/jpeg" width="390" height="260">
              <media:description type="plain">MIT researchers have shown how strategically applying a method known as test-time training with task-specific examples can boost the accuracy of an LLM more than sixfold.</media:description>
              <media:credit>Credit: Jose-Luis Olivares, MIT; iStock</media:credit>
      </media:content>
        <category domain="https://news.mit.edu/topic/research">Research</category>
      <category domain="https://news.mit.edu/topic/computers">Computer science and technology</category>
      <category domain="https://news.mit.edu/topic/artificial-intelligence2">Artificial intelligence</category>
      <category domain="https://news.mit.edu/topic/machine-learning">Machine learning</category>
      <category domain="https://news.mit.edu/topic/algorithms">Algorithms</category>
      <category domain="https://news.mit.edu/topic/data">Data</category>
      <category domain="https://news.mit.edu/topic/computer-science-and-artificial-intelligence-laboratory-csail">Computer Science and Artificial Intelligence Laboratory (CSAIL)</category>
      <category domain="https://news.mit.edu/topic/electrical-engineering-computer-science-eecs">Electrical engineering and computer science (EECS)</category>
      <category domain="https://news.mit.edu/topic/school-engineering">School of Engineering</category>
      <category domain="https://news.mit.edu/topic/mit-schwarzman-college-computing">MIT Schwarzman College of Computing</category>
      <category domain="https://news.mit.edu/topic/mit-ibm-watson-ai-lab">MIT-IBM Watson AI Lab</category>
      <category domain="https://news.mit.edu/topic/nsf">National Science Foundation (NSF)</category>
    </item>
<item>
  <title>MIT and Mass General Hospital researchers find disparities in organ acceptance</title>
  <link>https://news.mit.edu/2025/mit-mgh-researchers-find-disparities-organ-acceptance-0703</link>
  <description>In an analysis of over 160,000 transplant candidates, researchers found that race is linked to how likely an organ offer is to be accepted on behalf of a patient.</description>
  <pubDate>Thu, 03 Jul 2025 10:00:00 -0400</pubDate>
    <guid isPermaLink="true">https://news.mit.edu/2025/mit-mgh-researchers-find-disparities-organ-acceptance-0703</guid>
        <dc:creator>Alex Ouyang | Abdul Latif Jameel Clinic for Machine Learning in Health</dc:creator>
  <content:encoded>&lt;p dir="ltr"&gt;In 1954, the world’s first successful organ transplant took place at Brigham and Women’s Hospital, in the form of a kidney donated from one twin to the other. At the time, a group of doctors and scientists had correctly theorized that the recipient’s antibodies were unlikely to reject an organ from an identical twin.&amp;nbsp;One &lt;a href="https://www.nobelprize.org/prizes/medicine/1960/medawar/facts/"&gt;Nobel Prize&lt;/a&gt; and a few decades later, advancements in immune-suppressing drugs increased the viability of and demand for organ transplants. Today, over 1 million organ transplants have been performed in the United States,&amp;nbsp;&lt;a href="https://unos.org/transplant/history/"&gt;more than any other country in the world&lt;/a&gt;.&lt;/p&gt;&lt;p dir="ltr"&gt;The impressive scale of this achievement was made possible due to advances in organ matching systems: The first computer-based organ matching system was released in 1977. Despite continued innovation in computing, medicine, and matching technology over the years,&amp;nbsp;&lt;a href="https://www.organdonor.gov/learn/organ-donation-statistics"&gt;over 100,000 people&lt;/a&gt; in the U.S. are currently on the national transplant waiting list and 13 people die each day waiting for an organ transplant.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;Most computational research in organ allocation is focused on the initial stages, when waitlisted patients are being prioritized for organ transplants. In a new &lt;a href="https://dl.acm.org/doi/10.1145/3715275.3732097" target="_blank"&gt;paper&lt;/a&gt; presented at &lt;a href="https://facctconference.org/"&gt;ACM Conference on Fairness, Accountability, and Transparency&lt;/a&gt; (FAccT) in Athens, Greece, researchers from MIT and Massachusetts General Hospital focused on the final, less-studied stage: organ offer acceptance, when an offer is made and the physician at the transplant center decides on behalf of the patient whether to accept or reject the offered organ.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;“I don’t think we were terribly surprised, but we were obviously disappointed,” co-first author and MIT PhD student Hammaad Adam says. Using computational models to analyze transplantation data from over 160,000 transplant candidates in the Scientific Registry of Transplant Recipients (SRTR) between 2010 and 2020, the researchers found that physicians were overall less likely to accept liver and lung offers on behalf of Black candidates, resulting in additional barriers for Black patients in the organ offer acceptance process.&amp;nbsp;&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;For livers, Black patients had 7 percent lower odds of offer acceptance than white patients. When it came to lungs, the disparity became even larger, with 20 percent lower odds of having an offer acceptance than white patients with similar characteristics.&lt;/p&gt;&lt;p dir="ltr"&gt;The data don’t necessarily point to clinician bias as the main influence. “The bigger takeaway is that even if there are factors that justify clinical decision-making, there could be clinical conditions that we didn’t control for, that are more common for Black patients,” Adam explains. If the wait-list fails to account for certain patterns in decision-making, they could create obstacles in the process even if the process itself is “unbiased.”&lt;/p&gt;&lt;p dir="ltr"&gt;The researchers also point out that high variability in offer acceptance and risk tolerances among transplant centers is a potential factor complicating the decision-making process. Their FAccT paper references a 2020 paper published in&amp;nbsp;&lt;a href="https://jamanetwork.com/journals/jamacardiology/fullarticle/2764757"&gt;&lt;em&gt;JAMA Cardiology&lt;/em&gt;&lt;/a&gt;, which concluded that wait-list candidates listed at transplant centers with lower offer acceptance rates have a higher likelihood of mortality.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;Another key finding was that an offer was more likely to be accepted if the donor and candidate were of the same race. The paper describes this trend as “concerning,” given the historical inequities in organ procurement that have limited donation from racial and ethnic minority groups.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;Previous work from Adam and his collaborators has aimed to address this gap. Last year, they compiled and released&amp;nbsp;&lt;a href="https://physionet.org/content/orchid/2.0.0/"&gt;Organ Retrieval and Collection of Health Information for Donation&lt;/a&gt; (ORCHID), the first multi-center dataset describing the performance of organ procurement organizations (OPOs). ORCHID contains 10 years’ worth of OPO data, and is intended to facilitate research that addresses bias in organ procurement.&lt;/p&gt;&lt;p dir="ltr"&gt;“Being able to do good work in this field takes time,” says Adam, who notes that the entirety of the organ offer acceptance project took years to complete. To his knowledge, only&amp;nbsp;&lt;a href="https://jamanetwork.com/journals/jama/fullarticle/2816670"&gt;one paper&lt;/a&gt; to date studies the association between offer acceptance and race.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;While the bureaucratic and highly interdisciplinary nature of clinical AI projects can dissuade computer science graduate students from pursuing them, Adam committed to the project for the duration of his PhD in the lab of associate professor of electrical engineering Marzyeh Ghassemi, an affiliate of the&amp;nbsp;&lt;a href="https://jclinic.mit.edu/"&gt;MIT Jameel Clinic&lt;/a&gt; and the&amp;nbsp;&lt;a href="https://imes.mit.edu/"&gt;Institute of Medical Engineering and Sciences&lt;/a&gt;.&lt;/p&gt;&lt;p dir="ltr"&gt;To graduate students interested in pursuing clinical AI research projects, Adam recommends that they “free [themselves] from the cycle of publishing every four months.”&lt;/p&gt;&lt;p dir="ltr"&gt;“I found it freeing, to be honest — it’s OK if these collaborations take a while,” he says. “It’s hard to avoid that. I made the conscious choice a few years ago and I was happy doing that work.”&lt;/p&gt;&lt;p&gt;This work was supported with funding from the&amp;nbsp;&lt;a href="https://jclinic.mit.edu/"&gt;MIT Jameel Clinic&lt;/a&gt;. It was also supported, in part, by Takeda Development Center Americas Inc. (successor in interest to Millennium Pharmaceuticals Inc.), an NIH Ruth L. Kirschstein National Research Service Award, a CIFAR AI Chair at the Vector Institute, and by the National Institutes of Health.&lt;/p&gt;</content:encoded>
      <media:content url="https://news.mit.edu/sites/default/files/styles/news_article__cover_image__original/public/images/202506/FAccT-Organ-Allocation.jpg?itok=5Ws7ainC" medium="image" type="image/jpeg" width="390" height="260">
              <media:description type="plain">The first successful organ transplant was less than 75 years ago. Despite significant progress since then, many patients still fall through the gaps of what remains a complicated procedure.</media:description>
              <media:credit>Image: Alex Ouyang/MIT Jameel Clinic</media:credit>
      </media:content>
        <category domain="https://news.mit.edu/topic/research">Research</category>
      <category domain="https://news.mit.edu/topic/health-care">Health care</category>
      <category domain="https://news.mit.edu/topic/computers">Computer science and technology</category>
      <category domain="https://news.mit.edu/topic/health">Health sciences and technology</category>
      <category domain="https://news.mit.edu/topic/technology-society">Technology and society</category>
      <category domain="https://news.mit.edu/topic/race-and-gender">Race and gender</category>
      <category domain="https://news.mit.edu/topic/jameel-clinic">Jameel Clinic</category>
      <category domain="https://news.mit.edu/topic/idss">IDSS</category>
      <category domain="https://news.mit.edu/topic/institute-medical-engineering-and-science-imes-0">Institute for Medical Engineering and Science (IMES)</category>
      <category domain="https://news.mit.edu/topic/computer-science-and-artificial-intelligence-laboratory-csail">Computer Science and Artificial Intelligence Laboratory (CSAIL)</category>
      <category domain="https://news.mit.edu/topic/electrical-engineering-computer-science-eecs">Electrical engineering and computer science (EECS)</category>
      <category domain="https://news.mit.edu/topic/school-engineering">School of Engineering</category>
      <category domain="https://news.mit.edu/topic/mit-schwarzman-college-computing">MIT Schwarzman College of Computing</category>
      <category domain="https://news.mit.edu/topic/nih">National Institutes of Health (NIH)</category>
    </item>
<item>
  <title>Using generative AI to help robots jump higher and land safely</title>
  <link>https://news.mit.edu/2025/using-generative-ai-help-robots-jump-higher-land-safely-0627</link>
  <description>MIT CSAIL researchers combined GenAI and a physics simulation engine to refine robot designs. The result: a machine that out-jumped a robot designed by humans.</description>
  <pubDate>Fri, 27 Jun 2025 13:00:00 -0400</pubDate>
    <guid isPermaLink="true">https://news.mit.edu/2025/using-generative-ai-help-robots-jump-higher-land-safely-0627</guid>
        <dc:creator>Alex Shipps | MIT CSAIL</dc:creator>
  <content:encoded>&lt;p dir="ltr" id="docs-internal-guid-6e565cc0-7fff-cbe0-c9eb-bc35383dc425"&gt;Diffusion models like OpenAI’s DALL-E are becoming increasingly useful in helping brainstorm new designs. Humans can prompt these systems to generate an image, create a video, or refine a blueprint, and come back with ideas they hadn’t considered before.&lt;br&gt;&lt;br&gt;But did you know that generative artificial intelligence (GenAI) models are also making headway in creating working robots?&amp;nbsp;&lt;a href="https://www.youtube.com/watch?v=LSzasdvD3Ss"&gt;Recent&lt;/a&gt; diffusion-based approaches have generated structures and the systems that control them from scratch. With or without a user’s input, these models can make new designs and then evaluate them in simulation before they’re fabricated.&lt;br&gt;&lt;br&gt;A new approach from MIT’s Computer Science and Artificial Intelligence Laboratory (CSAIL) applies this generative know-how toward improving humans’ robotic designs. Users can draft a 3D model of a robot and specify which parts they’d like to see a diffusion model modify, providing its dimensions beforehand. GenAI then brainstorms the optimal shape for these areas and tests its ideas in simulation. When the system finds the right design, you can save and then fabricate a working, real-world robot with a 3D printer, without requiring additional tweaks.&lt;br&gt;&lt;br&gt;The researchers used this approach to create a robot that leaps up an average of roughly 2 feet, or 41 percent higher than a similar machine they created on their own. The machines are nearly identical in appearance: They’re both made of a type of plastic called polylactic acid, and while they initially appear flat, they spring up into a diamond shape when a motor pulls on the cord attached to them. So what exactly did AI do differently?&lt;br&gt;&lt;br&gt;A closer look reveals that the AI-generated linkages are curved, and resemble thick drumsticks (the musical instrument drummers use), whereas the standard robot’s connecting parts are straight and rectangular.&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;strong&gt;Better and better blobs&lt;/strong&gt;&lt;/p&gt;&lt;p dir="ltr"&gt;The researchers began to refine their jumping robot by sampling 500 potential designs using an initial embedding vector — a numerical representation that captures high-level features to guide the designs generated by the AI model. From these, they selected the top 12 options based on performance in simulation and used them to optimize the embedding vector.&lt;/p&gt;&lt;p dir="ltr"&gt;This process was repeated five times, progressively guiding the AI model to generate better designs. The resulting design resembled a blob, so the researchers prompted their system to scale the draft to fit their 3D model. They then fabricated the shape, finding that it indeed improved the robot’s jumping abilities.&lt;/p&gt;&lt;p dir="ltr"&gt;The advantage of using diffusion models for this task, according to co-lead author and CSAIL postdoc Byungchul Kim, is that they can find unconventional solutions to refine robots.&lt;/p&gt;&lt;p dir="ltr"&gt;“We wanted to make our machine jump higher, so we figured we could just make the links connecting its parts as thin as possible to make them light,” says Kim. “However, such a thin structure can easily break if we just use 3D printed material. Our diffusion model came up with a better idea by suggesting a unique shape that allowed the robot to store more energy before it jumped, without making the links too thin. This creativity helped us learn about the machine’s underlying physics.”&lt;/p&gt;&lt;p dir="ltr"&gt;The team then tasked their system with drafting an optimized foot to ensure it landed safely. They repeated the optimization process, eventually choosing the best-performing design to attach to the bottom of their machine. Kim and his colleagues found that their AI-designed machine fell far less often than its baseline, to the tune of an 84 percent improvement.&lt;/p&gt;&lt;p dir="ltr"&gt;The diffusion model’s ability to upgrade a robot’s jumping and landing skills suggests it could be useful in enhancing how other machines are designed. For example, a company working on manufacturing or household robots could use a similar approach to improve their prototypes, saving engineers time normally reserved for iterating on those changes.&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;strong&gt;The balance behind the bounce&lt;/strong&gt;&lt;/p&gt;&lt;p dir="ltr"&gt;To create a robot that could jump high and land stably, the researchers recognized that they needed to strike a balance between both goals. They represented both jumping height and landing success rate as numerical data, and then trained their system to find a sweet spot between both embedding vectors that could help build an optimal 3D structure.&lt;/p&gt;&lt;p dir="ltr"&gt;The researchers note that while this AI-assisted robot outperformed its human-designed counterpart, it could soon reach even greater new heights. This iteration involved using materials that were compatible with a 3D printer, but future versions would jump even higher with lighter materials.&lt;br&gt;&lt;br&gt;Co-lead author and MIT PhD student and CSAIL affiliate Tsun-Hsuan “Johnson” Wang says the project is a jumping-off point for new robotics designs that generative AI could help with.&lt;/p&gt;&lt;p dir="ltr"&gt;“We want to branch out to more flexible goals,” says Wang. “Imagine using natural language to guide a diffusion model to draft a robot that can pick up a mug, or operate an electric drill.”&lt;br&gt;&lt;br&gt;Kim says that a diffusion model could also help to generate articulation and ideate on how parts connect, potentially improving how high the robot would jump. The team is also exploring the possibility of adding more motors to control which direction the machine jumps and perhaps improve its landing stability.&lt;/p&gt;&lt;p&gt;The researchers’ work was supported, in part, by the National Science Foundation’s Emerging Frontiers in Research and Innovation program, the Singapore-MIT Alliance for Research and Technology’s Mens, Manus and Machina program, and the Gwangju Institute of Science and Technology (GIST)-CSAIL Collaboration. They presented their work at the 2025 International Conference on Robotics and Automation.&lt;/p&gt;</content:encoded>
      <media:content url="https://news.mit.edu/sites/default/files/styles/news_article__cover_image__original/public/images/202506/MIT-Jumping_Robot%20%285%29.png?itok=uV6zSNBB" medium="image" type="image/jpeg" width="390" height="260">
              <media:description type="plain">Byungchul Kim (left) and Tsun-Hsuan "Johnson" Wang applied generative AI to improve robots designed by humans.</media:description>
              <media:credit>Image courtesy of MIT CSAIL</media:credit>
      </media:content>
        <category domain="https://news.mit.edu/topic/school-engineering">School of Engineering</category>
      <category domain="https://news.mit.edu/topic/school-science">School of Science</category>
      <category domain="https://news.mit.edu/topic/mit-schwarzman-college-computing">MIT Schwarzman College of Computing</category>
      <category domain="https://news.mit.edu/topic/electrical-engineering-computer-science-eecs">Electrical engineering and computer science (EECS)</category>
      <category domain="https://news.mit.edu/topic/computer-science-and-artificial-intelligence-laboratory-csail">Computer Science and Artificial Intelligence Laboratory (CSAIL)</category>
      <category domain="https://news.mit.edu/topic/computers">Computer science and technology</category>
      <category domain="https://news.mit.edu/topic/artificial-intelligence2">Artificial intelligence</category>
      <category domain="https://news.mit.edu/topic/machine-learning">Machine learning</category>
      <category domain="https://news.mit.edu/topic/robotics">Robotics</category>
      <category domain="https://news.mit.edu/topic/design">Design</category>
      <category domain="https://news.mit.edu/topic/3-d-printing">3-D printing</category>
      <category domain="https://news.mit.edu/topic/research">Research</category>
    </item>
<item>
  <title>Researchers present bold ideas for AI at MIT Generative AI Impact Consortium kickoff event</title>
  <link>https://news.mit.edu/2025/researchers-present-bold-ideas-ai-mit-generative-ai-impact-consortium-event-0620</link>
  <description>Presentations targeted high-impact intersections of AI and other areas, such as health care, business, and education.</description>
  <pubDate>Fri, 20 Jun 2025 16:45:00 -0400</pubDate>
    <guid isPermaLink="true">https://news.mit.edu/2025/researchers-present-bold-ideas-ai-mit-generative-ai-impact-consortium-event-0620</guid>
        <dc:creator>Amanda Diehl | MIT Schwarzman College of Computing</dc:creator>
  <content:encoded>&lt;p&gt;&lt;a href="https://news.mit.edu/2025/introducing-mit-generative-ai-impact-consortium-0203"&gt;Launched in February of this year&lt;/a&gt;, the &lt;a href="http://genai.mit.edu/"&gt;MIT Generative AI Impact Consortium &lt;/a&gt;(MGAIC), a presidential initiative led by MIT’s Office of Innovation and Strategy and administered by the MIT Stephen A. Schwarzman College of Computing, issued a call for proposals, inviting researchers from across MIT to submit ideas for innovative projects studying high-impact uses of generative AI models.&lt;/p&gt;&lt;p&gt;The call received 180 submissions from nearly 250 faculty members, spanning all of MIT’s five schools and the college. The overwhelming response across the Institute exemplifies the growing interest in AI and follows in the wake of &lt;a href="https://news.mit.edu/2023/mit-generative-ai-week-fosters-dialogue-across-disciplines-1211"&gt;MIT’s Generative AI Week&lt;/a&gt; and &lt;a href="https://mitpress.mit.edu/preprints-of-generative-ai-impact-papers-publish-through-mit-presss-mit-open-publishing-services-mitops/"&gt;call for impact papers&lt;/a&gt;. &lt;a href="https://computing-dev.mit.edu/mit-generative-ai-impact-consortium-seed-grants/"&gt;Fifty-five proposals were selected&lt;/a&gt; for MGAIC’s inaugural seed grants, with several more selected to be funded by the consortium’s founding company members.&lt;/p&gt;&lt;p&gt;Over 30 funding recipients presented their proposals to the greater MIT community at a kickoff event on May 13. Anantha P. Chandrakasan, chief innovation and strategy officer and dean of the School of Engineering who is head of the consortium, welcomed the attendees and thanked the consortium’s founding industry members.&lt;/p&gt;&lt;p&gt;“The amazing response to our call for proposals is an incredible testament to the energy and creativity that MGAIC has sparked at MIT. We are especially grateful to our founding members, whose support and vision helped bring this endeavor to life,” adds Chandrakasan. “One of the things that has been most remarkable about MGAIC is that this is a truly cross-Institute initiative. Deans from all five schools and the college collaborated in shaping and implementing it.”&lt;/p&gt;&lt;p&gt;Vivek F. Farias, the Patrick J. McGovern (1959) Professor at the MIT Sloan School of Management and co-faculty director of the consortium with Tim Kraska, associate professor of electrical engineering and computer science in the MIT Computer Science and Artificial Intelligence Laboratory (CSAIL), emceed the afternoon of five-minute lightning presentations.&lt;/p&gt;&lt;p&gt;Presentation highlights include:&lt;/p&gt;&lt;p&gt;“AI-Driven Tutors and Open Datasets for Early Literacy Education,” presented by Ola Ozernov-Palchik, a research scientist at the McGovern Institute for Brain Research, proposed a refinement for AI-tutors for pK-7 students to potentially decrease literacy disparities.&lt;/p&gt;&lt;p&gt;“Developing jam_bots: Real-Time Collaborative Agents for Live Human-AI Musical Improvisation,” presented by Anna Huang, assistant professor of music and assistant professor of electrical engineering and computer science, and Joe Paradiso, the Alexander W. Dreyfoos (1954) Professor in Media Arts and Sciences at the MIT Media Lab, aims to enhance human-AI musical collaboration in real-time for live concert improvisation.&lt;/p&gt;&lt;p&gt;“GENIUS: GENerative Intelligence for Urban Sustainability,” presented by Norhan Bayomi, a postdoc at the MIT Environmental Solutions Initiative and a research assistant in the Urban Metabolism Group, which aims to address the critical gap of a standardized approach in evaluating and benchmarking cities’ climate policies.&lt;/p&gt;&lt;p&gt;Georgia Perakis, the John C Head III Dean (Interim) of the MIT Sloan School of Management and professor of operations management, operations research, and statistics, who serves as co-chair of the GenAI Dean’s oversight group with Dan Huttenlocher, dean of the MIT Schwarzman College of Computing, ended the event with closing remarks that emphasized “the readiness and eagerness of our community to lead in this space.”&lt;/p&gt;&lt;p&gt;“This is only the beginning,” she continued. “We are at the front edge of a historic moment — one where MIT has the opportunity, and the responsibility, to shape the future of generative AI with purpose, with excellence, and with care.”&lt;/p&gt;</content:encoded>
      <media:content url="https://news.mit.edu/sites/default/files/styles/news_article__cover_image__original/public/images/202506/mit-Anantha.jpg?itok=6ENWNAbm" medium="image" type="image/jpeg" width="390" height="260">
              <media:description type="plain">Anantha P. Chandrakasan, chief innovation and strategy officer and dean of the School of Engineering who is head of the MIT Generative AI Impact Consortium (MGAIC), kicks off an afternoon of presentations.</media:description>
              <media:credit>Photo: Jiin Kang</media:credit>
      </media:content>
        <category domain="https://news.mit.edu/topic/special-events">Special events and guest speakers</category>
      <category domain="https://news.mit.edu/topic/computers">Computer science and technology</category>
      <category domain="https://news.mit.edu/topic/artificial-intelligence2">Artificial intelligence</category>
      <category domain="https://news.mit.edu/topic/research">Research</category>
      <category domain="https://news.mit.edu/topic/technology-society">Technology and society</category>
      <category domain="https://news.mit.edu/topic/faculty">Faculty</category>
      <category domain="https://news.mit.edu/topic/music-and-theater-arts">Music and theater arts</category>
      <category domain="https://news.mit.edu/topic/human-computer-interaction">Human-computer interaction</category>
      <category domain="https://news.mit.edu/topic/robotics">Robotics</category>
      <category domain="https://news.mit.edu/topic/industry">Industry</category>
      <category domain="https://news.mit.edu/topic/collaboration">Collaboration</category>
      <category domain="https://news.mit.edu/topic/funding">Funding</category>
      <category domain="https://news.mit.edu/topic/electrical-engineering-computer-science-eecs">Electrical engineering and computer science (EECS)</category>
      <category domain="https://news.mit.edu/topic/computer-science-and-artificial-intelligence-laboratory-csail">Computer Science and Artificial Intelligence Laboratory (CSAIL)</category>
      <category domain="https://news.mit.edu/topic/mit-schwarzman-college-computing">MIT Schwarzman College of Computing</category>
      <category domain="https://news.mit.edu/topic/mcgovern-institute-0">McGovern Institute</category>
      <category domain="https://news.mit.edu/topic/school-science">School of Science</category>
      <category domain="https://news.mit.edu/topic/mit-sloan-school-management">MIT Sloan School of Management</category>
      <category domain="https://news.mit.edu/topic/media-lab-0">Media Lab</category>
      <category domain="https://news.mit.edu/topic/school-architecture-and-planning">School of Architecture and Planning</category>
      <category domain="https://news.mit.edu/topic/school-engineering">School of Engineering</category>
      <category domain="https://news.mit.edu/topic/school-humanities-arts-and-social-sciences">School of Humanities Arts and Social Sciences</category>
    </item>
<item>
  <title>Unpacking the bias of large language models</title>
  <link>https://news.mit.edu/2025/unpacking-large-language-model-bias-0617</link>
  <description>In a new study, researchers discover the root cause of a type of bias in LLMs, paving the way for more accurate and reliable AI systems.</description>
  <pubDate>Tue, 17 Jun 2025 16:00:00 -0400</pubDate>
    <guid isPermaLink="true">https://news.mit.edu/2025/unpacking-large-language-model-bias-0617</guid>
        <dc:creator>Adam Zewe | MIT News</dc:creator>
  <content:encoded>&lt;p&gt;Research has shown that large language models (LLMs) tend to overemphasize information at the beginning and end of a document or conversation, while neglecting the middle.&lt;/p&gt;&lt;p&gt;This “position bias” means that, if a lawyer is using an LLM-powered virtual assistant to retrieve a certain phrase in a 30-page affidavit, the LLM is more likely to find the right text if it is on the initial or final pages.&lt;/p&gt;&lt;p&gt;MIT researchers have discovered the mechanism behind this phenomenon.&lt;/p&gt;&lt;p&gt;They created a theoretical framework to study how information flows through the machine-learning architecture that forms the backbone of LLMs. They found that certain design choices which control how the model processes input data can cause position bias.&lt;/p&gt;&lt;p&gt;Their experiments revealed that model architectures, particularly those affecting how information is spread across input words within the model, can give rise to or intensify position bias, and that training data also contribute to the problem.&lt;/p&gt;&lt;p&gt;In addition to pinpointing the origins of position bias, their framework can be used to diagnose and correct it in future model designs.&lt;/p&gt;&lt;p&gt;This could lead to more reliable chatbots that stay on topic during long conversations, medical AI systems that reason more fairly when handling a trove of patient data, and code assistants that pay closer attention to all parts of a program.&lt;/p&gt;&lt;p&gt;“These models are black boxes, so as an LLM user, you probably don’t know that position bias can cause your model to be inconsistent. You just feed it your documents in whatever order you want and expect it to work. But by understanding the underlying mechanism of these black-box models better, we can improve them by addressing these limitations,” says Xinyi Wu, a graduate student in the MIT Institute for Data, Systems, and Society (IDSS) and the Laboratory for Information and Decision Systems (LIDS), and first author of a &lt;a href="https://arxiv.org/pdf/2502.01951" target="_blank"&gt;paper&lt;/a&gt; on this research.&lt;/p&gt;&lt;p&gt;Her co-authors include Yifei Wang, an MIT postdoc; and senior authors Stefanie Jegelka, an associate professor of electrical engineering and computer science (EECS) and a member of IDSS and the Computer Science and Artificial Intelligence Laboratory (CSAIL); and Ali Jadbabaie, professor and head of the Department of Civil and Environmental Engineering, a core faculty member of IDSS, and a principal investigator in LIDS. The research will be presented at the International Conference on Machine Learning.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Analyzing attention&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;LLMs like Claude, Llama, and GPT-4 are powered by a type of neural network architecture known as a transformer. Transformers are designed to process sequential data, encoding a sentence into chunks called tokens and then learning the relationships between tokens to predict what words comes next.&lt;/p&gt;&lt;p&gt;These models have gotten very good at this because of the attention mechanism, which uses interconnected layers of data processing nodes to make sense of context by allowing tokens to selectively focus on, or attend to, related tokens.&lt;/p&gt;&lt;p&gt;But if every token can attend to every other token in a 30-page document, that quickly becomes computationally intractable. So, when engineers build transformer models, they often employ attention masking techniques which limit the words a token can attend to.&lt;/p&gt;&lt;p&gt;For instance, a causal mask only allows words to attend to those that came before it.&lt;/p&gt;&lt;p&gt;Engineers also use positional encodings to help the model understand the location of each word in a sentence, improving performance.&lt;/p&gt;&lt;p&gt;The MIT researchers built a graph-based theoretical framework to explore how these modeling choices, attention masks and positional encodings, could affect position bias.&lt;/p&gt;&lt;p&gt;“Everything is coupled and tangled within the attention mechanism, so it is very hard to study. Graphs are a flexible language to describe the dependent relationship among words within the attention mechanism and trace them across multiple layers,” Wu says.&lt;/p&gt;&lt;p&gt;Their theoretical analysis suggested that causal masking gives the model an inherent bias toward the beginning of an input, even when that bias doesn’t exist in the data.&lt;/p&gt;&lt;p&gt;If the earlier words are relatively unimportant for a sentence’s meaning, causal masking can cause the transformer to pay more attention to its beginning anyway.&lt;/p&gt;&lt;p&gt;“While it is often true that earlier words and later words in a sentence are more important, if an LLM is used on a task that is not natural language generation, like ranking or information retrieval, these biases can be extremely harmful,” Wu says.&lt;/p&gt;&lt;p&gt;As a model grows, with additional layers of attention mechanism, this bias is amplified because earlier parts of the input are used more frequently in the model’s reasoning process.&lt;/p&gt;&lt;p&gt;They also found that using positional encodings to link words more strongly to nearby words can mitigate position bias. The technique refocuses the model’s attention in the right place, but its effect can be diluted in models with more attention layers.&lt;/p&gt;&lt;p&gt;And these design choices are only one cause of position bias — some can come from training data the model uses to learn how to prioritize words in a sequence.&lt;/p&gt;&lt;p&gt;“If you know your data are biased in a certain way, then you should also finetune your model on top of adjusting your modeling choices,” Wu says.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Lost in the middle&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;After they’d established a theoretical framework, the researchers performed experiments in which they systematically varied the position of the correct answer in text sequences for an information retrieval task.&lt;/p&gt;&lt;p&gt;The experiments showed a “lost-in-the-middle” phenomenon, where retrieval accuracy followed a U-shaped pattern. Models performed best if the right answer was located at the beginning of the sequence. Performance declined the closer it got to the middle before rebounding a bit if the correct answer was near the end.&lt;/p&gt;&lt;p&gt;Ultimately, their work suggests that using a different masking technique, removing extra layers from the attention mechanism, or strategically employing positional encodings could reduce position bias and improve a model’s accuracy.&lt;/p&gt;&lt;p&gt;“By doing a combination of theory and experiments, we were able to look at the consequences of model design choices that weren’t clear at the time. If you want to use a model in high-stakes applications, you must know when it will work, when it won’t, and why,” Jadbabaie says.&lt;/p&gt;&lt;p&gt;In the future, the researchers want to further explore the effects of positional encodings and study how position bias could be strategically exploited in certain applications.&lt;/p&gt;&lt;p&gt;“These researchers offer a rare theoretical lens into the attention mechanism at the heart of the transformer model. They provide a compelling analysis that clarifies longstanding quirks in transformer behavior, showing that attention mechanisms, especially with causal masks, inherently bias models toward the beginning of sequences. The paper achieves the best of both worlds — mathematical clarity paired with insights that reach into the guts of real-world systems,” says Amin Saberi, professor and director of the Stanford University Center for Computational Market Design, who was not involved with this work.&lt;/p&gt;&lt;p&gt;This research is supported, in part, by the U.S. Office of Naval Research, the National Science Foundation, and an Alexander von Humboldt Professorship.&lt;/p&gt;</content:encoded>
      <media:content url="https://news.mit.edu/sites/default/files/styles/news_article__cover_image__original/public/images/202506/MIT-transform-bias-01-press.jpg?itok=qeAFK3E3" medium="image" type="image/jpeg" width="390" height="260">
              <media:description type="plain">MIT researchers discovered the underlying cause of position bias, a phenomenon that causes large language models to overemphasize the beginning or end of a document or conversation, while neglecting the middle.</media:description>
              <media:credit>Credit: MIT News; iStock</media:credit>
      </media:content>
        <category domain="https://news.mit.edu/topic/research">Research</category>
      <category domain="https://news.mit.edu/topic/computers">Computer science and technology</category>
      <category domain="https://news.mit.edu/topic/artificial-intelligence2">Artificial intelligence</category>
      <category domain="https://news.mit.edu/topic/algorithms">Algorithms</category>
      <category domain="https://news.mit.edu/topic/machine-learning">Machine learning</category>
      <category domain="https://news.mit.edu/topic/data">Data</category>
      <category domain="https://news.mit.edu/topic/human-computer-interaction">Human-computer interaction</category>
      <category domain="https://news.mit.edu/topic/idss">IDSS</category>
      <category domain="https://news.mit.edu/topic/lids">Laboratory for Information and Decision Systems (LIDS)</category>
      <category domain="https://news.mit.edu/topic/computer-science-and-artificial-intelligence-laboratory-csail">Computer Science and Artificial Intelligence Laboratory (CSAIL)</category>
      <category domain="https://news.mit.edu/topic/electrical-engineering-computer-science-eecs">Electrical engineering and computer science (EECS)</category>
      <category domain="https://news.mit.edu/topic/civil-engineering">Civil and environmental engineering</category>
      <category domain="https://news.mit.edu/topic/school-engineering">School of Engineering</category>
      <category domain="https://news.mit.edu/topic/mit-schwarzman-college-computing">MIT Schwarzman College of Computing</category>
      <category domain="https://news.mit.edu/topic/nsf">National Science Foundation (NSF)</category>
    </item>

  </channel>
</rss>
