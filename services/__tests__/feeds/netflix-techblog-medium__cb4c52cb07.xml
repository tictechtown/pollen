<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:cc="http://cyber.law.harvard.edu/rss/creativeCommonsRssModule.html">
    <channel>
        <title><![CDATA[Netflix TechBlog - Medium]]></title>
        <description><![CDATA[Learn about Netflix’s world class engineering efforts, company culture, product developments and more. - Medium]]></description>
        <link>https://netflixtechblog.com?source=rss----2615bd06b42e---4</link>
        <image>
            <url>https://cdn-images-1.medium.com/proxy/1*TGH72Nnw24QL3iV9IOm4VA.png</url>
            <title>Netflix TechBlog - Medium</title>
            <link>https://netflixtechblog.com?source=rss----2615bd06b42e---4</link>
        </image>
        <generator>Medium</generator>
        <lastBuildDate>Fri, 19 Dec 2025 22:21:13 GMT</lastBuildDate>
        <atom:link href="https://netflixtechblog.com/feed" rel="self" type="application/rss+xml"/>
        <webMaster><![CDATA[yourfriends@medium.com]]></webMaster>
        <atom:link href="http://medium.superfeedr.com" rel="hub"/>
        <item>
            <title><![CDATA[How Temporal Powers Reliable Cloud Operations at Netflix]]></title>
            <link>https://netflixtechblog.com/how-temporal-powers-reliable-cloud-operations-at-netflix-73c69ccb5953?source=rss----2615bd06b42e---4</link>
            <guid isPermaLink="false">https://medium.com/p/73c69ccb5953</guid>
            <dc:creator><![CDATA[Netflix Technology Blog]]></dc:creator>
            <pubDate>Mon, 15 Dec 2025 23:51:59 GMT</pubDate>
            <atom:updated>2025-12-16T00:01:15.478Z</atom:updated>
            <content:encoded><![CDATA[<p>By <a href="https://www.linkedin.com/in/jacobmeyers35/">Jacob Meyers</a> and <a href="https://www.linkedin.com/in/robzienert/">Rob Zienert</a></p><p><a href="https://temporal.io/">Temporal</a> is a <a href="https://docs.temporal.io/evaluate/understanding-temporal#durable-execution">Durable Execution</a> platform which allows you to write code “as if failures don’t exist”. It’s become increasingly critical to Netflix since its initial adoption in 2021, with users ranging from the operators of our <a href="https://about.netflix.com/en/news/how-netflix-works-with-isps-around-the-globe-to-deliver-a-great-viewing-experience">Open Connect</a> global CDN to our <a href="https://medium.com/netflix-techblog/behind-the-streams-live-at-netflix-part-1-d23f917c2f40">Live</a> reliability teams now depending on Temporal to operate their business-critical services. In this post, I’ll give a high-level overview of what Temporal offers users, the problems we were experiencing operating Spinnaker that motivated its initial adoption at Netflix, and how Temporal helped us reduce the number of transient deployment failures at Netflix from <strong>4% to 0.0001%</strong>.</p><h3>A Crash Course on (some of) Spinnaker</h3><p><a href="https://netflixtechblog.com/global-continuous-delivery-with-spinnaker-2a6896c23ba7">Spinnaker</a> is a multi-cloud continuous delivery platform that powers the vast majority of Netflix’s software deployments. It’s composed of several (mostly nautical themed) microservices. Let’s double-click on two in particular to understand the problems we were facing that led us to adopting Temporal.</p><p>In case you’re completely new to Spinnaker, Spinnaker’s fundamental tool for deployments is the <em>Pipeline</em>. A Pipeline is composed of a sequence of steps called <em>Stages</em>, which themselves can be decomposed into one or more <em>Tasks</em>, or other Stages. An example deployment pipeline for a production service may consist of these stages: Find Image -&gt; Run Smoke Tests -&gt; Run Canary -&gt; Deploy to us-east-2 -&gt; Wait -&gt; Deploy to us-east-1.</p><figure><img alt="An example Spinnaker Pipeline" src="https://cdn-images-1.medium.com/max/1024/1*7sGhc8LhyqQlW9Uiq76TWQ.png" /><figcaption>An example Spinnaker Pipeline for a Netflix service</figcaption></figure><p>Pipeline configuration is extremely flexible. You can have Stages run completely serially, one after another, or you can have a mix of concurrent and serial Stages. Stages can also be executed conditionally based on the result of previous stages. This brings us to our first Spinnaker service: <em>Orca</em>. Orca is the <a href="https://raw.githubusercontent.com/spinnaker/orca/refs/heads/master/logo.jpg">orca-stration</a> engine of Spinnaker. It’s responsible for managing the execution of the Stages and Tasks that a Pipeline unrolls into and coordinating with other Spinnaker services to actually execute them.</p><p>One of those collaborating services is called <em>Clouddriver</em>. In the example Pipeline above, some of the Stages will require interfacing with cloud infrastructure. For example, the canary deployment involves creating ephemeral hosts to run an experiment, and a full deployment of a new version of the service may involve spinning up new servers and then tearing down the old ones. We call these sorts of operations that mutate cloud infrastructure <em>Cloud Operations</em>. Clouddriver’s job is to decompose and execute Cloud Operations sent to it by Orca as part of a deployment. Cloud Operations sent from Orca to Clouddriver are relatively high level (for example: createServerGroup), so Clouddriver understands how to translate these into lower-level cloud provider API calls.</p><p>Pain points in the interaction between Orca and Clouddriver and the implementation details of Cloud Operation execution in Clouddriver are what led us to look for new solutions and ultimately migrate to Temporal, so we’ll next look at the anatomy of a Cloud Operation. Cloud Operations in the OSS version of Spinnaker still work as described below, so motivated readers can follow along in <a href="https://github.com/spinnaker/clouddriver">source code</a>, however our migration to Temporal is entirely closed-source following a fork from OSS in 2020 to allow Netflix to make larger pivots to the product such as this one.</p><h4><strong>The Original Cloud Operation Flow</strong></h4><p>A Cloud Operation’s execution goes something like this:</p><ol><li>Orca, in orchestrating a Pipeline execution, decides a particular Cloud Operation needs to be performed. It sends a POST request to Clouddriver’s /ops endpoint with an untyped bag-of-fields.</li><li>Clouddriver attempts to resolve the operation Orca sent into a set of AtomicOperation s— internal operations that only Clouddriver understands.</li><li>If the payload was valid and Clouddriver successfully resolved the operation, it will immediately return a Task ID to Orca.</li><li>Orca will immediately begin polling Clouddriver’s GET /task/&lt;id&gt; endpoint to keep track of the status of the Cloud Operation.</li><li>Asynchronously, Clouddriver begins executing AtomicOperations using <em>its own</em> internal orchestration engine. Ultimately, the AtomicOperations resolve into cloud provider API calls. As the Cloud Operation progresses, Clouddriver updates an internal state store to surface progress to Orca.</li><li>Eventually, if all went well, Clouddriver will mark the Cloud Operation complete, which eventually surfaces to Orca in its polling. Orca considers the Cloud Operation finished, and the deployment can progress.</li></ol><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*Y57y00EsM2YGRph9IRNmLQ.png" /><figcaption>A sequence diagram of a Cloud Operation execution</figcaption></figure><p>This works well enough on the happy path, but veer off the happy path and dragons begin to emerge:</p><ol><li>Clouddriver has its own internal orchestration system independent of Orca to allow Orca to query the progress of Cloud Operation. This is largely undifferentiated lifting relative to Clouddriver’s goal of actuating cloud infrastructure changes, and ultimately adds complexity and surface area for bugs to the application. Additionally, Orca is tightly coupled to Clouddriver’s orchestration system — it must understand how to poll Clouddriver, interpret the status, and handle errors returned by Clouddriver.</li><li>Distributed systems are messy — networks and external services are unreliable. While executing a Cloud Operation, Clouddriver could experience transient network issues, or the cloud provider it’s attempting to call into may be having an outage, or any number of issues in between. Despite all of this, Clouddriver must be as reliable as reasonably possible as a core platform service. To deal with this shape of issue, Clouddriver internally evolved complex retry logic, further adding cognitive complexity to the system.</li><li>Remember how a Cloud Operation gets decomposed by Clouddriver into AtomicOperations? Sometimes, if there’s a failure in the middle of a Cloud Operation, we need to be able to roll back what was done in AtomicOperations prior to the failure. This led to a homegrown Saga framework being implemented inside Clouddriver. While this did result in a big step forward in reliability of Cloud Operations facing transient failures because the Saga framework <em>also</em> allowed replaying partially-failed Cloud Operations, it added yet more undifferentiated lifting inside the service.</li><li>The task state kept by Clouddriver was <em>instance-local</em>. In other words, if the Clouddriver instance carrying out a Cloud Operation crashed, that Cloud Operation state was lost, and Orca would eventually time out polling for the task status. The Saga implementation mentioned above mitigated this for certain operations, but was not widely adopted across all cloud providers supported by Spinnaker.</li></ol><p>We introduced a <em>lot</em> of incidental complexity into Clouddriver in an effort to keep Cloud Operation execution reliable, and despite all this deployments still failed around 4% of the time due to transient Cloud Operation failures.</p><p>Now, I can already hear you saying: “So what? Can’t people re-try their deployments if they fail?” While true, some pipelines take <em>days</em> to complete for complex deployments, and a failed Cloud Operation mid-way through requires re-running the <em>whole</em> thing. This was detrimental to engineering productivity at Netflix in a non-trivial way. Rather than continue trying to build a faster horse, we began to look elsewhere for our reliable orchestration requirements, which is where Temporal comes in.</p><h3>Temporal: Basic Concepts</h3><p>Temporal is an open source product that offers a durable execution platform for your applications. Durable execution means that the platform will ensure your programs run to completion despite adverse conditions. With Temporal, you organize your business logic into <em>Workflows</em>, which are a deterministic series of steps. The steps inside of Workflows are called <em>Activities</em>, which is where you encapsulate all your non-deterministic logic that needs to happen in the course of executing your Workflows. As your Workflows execute in processes called <em>Workers</em>, the Temporal server durably stores their execution state so that in the event of failures your Workflows can be retried or even migrated to a different Worker. This makes Workflows incredibly resilient to the sorts of transient failures Clouddriver was susceptible to. Here’s a simple example Workflow in Java that runs an Activity to send an email once every 30 days:</p><pre>@WorkflowInterface<br>public interface SleepForDaysWorkflow {<br>    @WorkflowMethod<br>    void run();<br>}<br><br>public class SleepForDaysWorkflowImpl implements SleepForDaysWorkflow {<br><br>    private final SendEmailActivities emailActivities =<br>            Workflow.newActivityStub(<br>                    SendEmailActivities.class,<br>                    ActivityOptions.newBuilder()<br>                            .setStartToCloseTimeout(Duration.ofSeconds(10))<br>                            .build());<br><br>    @Override<br>    public void run() {<br>        while (true) {<br>            // Activities already carry retries/timeouts via options.<br>            emailActivities.sendEmail();<br><br>            // Pause the workflow for 30 days before sending the next email.<br>            Workflow.sleep(Duration.ofDays(30));<br>        }<br>    }<br>}<br><br>@ActivityInterface<br>public interface SendEmailActivities {<br>    void sendEmail();<br>}</pre><p>There’s some interesting things to note about this Workflow:</p><ol><li>Workflows and Activities are just code, so you can test them using the same techniques and processes as the rest of your codebase.</li><li>Activities are automatically retried by Temporal with configurable exponential backoff.</li><li>Temporal manages all the execution state of the Workflow, including timers (like the one used by Workflow.sleep). If the Worker executing this workflow were to have its power cable unplugged, Temporal would ensure another Worker continues to execute it (even during the 30 day sleep).</li><li>Workflow sleeps are not compute-intensive, and they don’t tie up the process.</li></ol><p>You might already begin to see how Temporal solves a lot of the problems we had with Clouddriver. Ultimately, we decided to pull the trigger on migrating Cloud Operation execution to Temporal.</p><h3>Cloud Operations with Temporal</h3><p>Today, we execute Cloud Operations as Temporal workflows. Here’s what that looks like.</p><ol><li>Orca, using a Temporal client, sends a request to Temporal to execute an UntypedCloudOperationRunner Workflow. The contract of the Workflow looks something like this:</li></ol><pre>@WorkflowInterface<br>interface UntypedCloudOperationRunner {<br>  /**<br>   * Runs a cloud operation given an untyped payload.<br>   *<br>   * WorkflowResult is a thin wrapper around OutputType providing a standard contract for<br>   * clients to determine if the CloudOperation was successful and fetching any errors.<br>   */<br>  @WorkflowMethod<br>  fun &lt;OutputType : CloudOperationOutput&gt; run(stageContext: Map&lt;String, Any?&gt;, operationType: String): WorkflowResult&lt;OutputType&gt;<br>}</pre><p>2. The Clouddriver Temporal worker is constantly polling Temporal for work. A worker will eventually see a task for an UntypedCloudOperationRunner Workflow and start executing it.</p><p>3. Similar to before with resolution into AtomicOperations, Clouddriver does some pre-processing of the bag-of-fields in stageContext and resolves it to a strongly typed implementation of the CloudOperation Workflow interface based on the operationType input and the stageContext:</p><pre>interface CloudOperation&lt;I : CloudOperationInput, O : CloudOperationOutput&gt; {<br>  @WorkflowMethod<br>  fun operate(input: I, credentials: AccountCredentials&lt;out Any&gt;): O<br>}</pre><p>4. Clouddriver starts a <a href="https://docs.temporal.io/child-workflows">Child Workflow</a> execution of the CloudOperation implementation it resolved. The child workflow will execute Activities which handle the actual cloud provider API calls to mutate infrastructure.</p><p>5. Orca uses its Temporal Client to await completion of the UntypedCloudOperationRunner Workflow. Once it’s complete, Temporal notifies the client and sends the result and Orca can continue progressing the deployment.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*leM3bH8iyb65_cmtl3vm4A.png" /><figcaption>Sequence diagram of a Cloud Operation execution with Temporal</figcaption></figure><h3>Results and Lessons Learned from the Migration</h3><p>A shiny new architecture is great, but equally important is the non-glamorous work of refactoring legacy systems to fit the new architecture. How did we integrate Temporal into critical dependencies of all Netflix engineers transparently?</p><p>The answer, of course, is a combination of abstraction and dynamic configuration. We built a CloudOperationRunner interface in Orca to encapsulate whether the Cloud Operation was being executed via the legacy path or Temporal. At runtime, <a href="https://netflixtechblog.com/announcing-archaius-dynamic-properties-in-the-cloud-bc8c51faf675">Fast Properties</a> (Netflix’s dynamic configuration system) determined which path a stage that needed to execute a Cloud Operation would take. We could set these properties quite granularly — by Stage type, cloud provider account, Spinnaker application, Cloud Operation type (createServerGroup), and cloud provider (either AWS or <a href="https://netflix.github.io/titus/">Titus</a> in our case). The Spinnaker services themselves were the first to be deployed using Temporal, and within two quarters, all applications at Netflix were onboarded.</p><h4>Impact</h4><p>What did we have to show for it all? With Temporal as the orchestration engine for Cloud Operations, the percentage of deployments that failed due to transient Cloud Operation failures dropped from 4% to 0.0001%. For those keeping track at home, that’s a four and a half order of magnitude reduction. Virtually eliminating this failure mode for deployments was a huge win for developer productivity, especially for teams with long and complex deployment pipelines.</p><p>Beyond the improvement in deployment success metrics, we saw a number of other benefits:</p><ol><li>Orca no longer needs to directly communicate with Clouddriver to start Cloud Operations or poll their status with Temporal as the intermediary. The services are less coupled, which is a win for maintainability.</li><li>Speaking of maintainability, with Temporal doing the heavy lifting of orchestration and retries inside of Clouddriver, we got to remove a lot of the homegrown logic we’d built up over the years for the same purpose.</li><li>Since Temporal manages execution state, Clouddriver instances became stateless and Cloud Operation execution can bounce between instances with impunity. We can treat Clouddriver instances more like cattle and enable things like <a href="https://netflixtechblog.com/the-netflix-simian-army-16e57fbab116">Chaos Monkey</a> for the service which we were previously prevented from doing.</li><li>Migrating Cloud Operation steps into Activities was a forcing function to re-write the logic to be idempotent. Since Temporal retries activities by default, it’s generally recommended they be idempotent. This alone fixed a number of issues that existed previously when operations were retried in Clouddriver.</li><li>We set the retry timeout for Activities in Clouddriver to be two hours by default. This gives us a long leash to fix-forward or rollback Clouddriver if we introduce a regression before customer deployments fail — to them, it might just look like a deployment is taking longer than usual.</li><li>Cloud Operations are much easier to introspect than before. Temporal ships with a great UI to help visualize Workflow and Activity executions, which is a huge boon for debugging live Workflows executing in production. The Temporal SDKs and server also emit a lot of useful metrics.</li></ol><figure><img alt="A Cloud Operation Workflow as seen from the Temporal UI. This operation executes 3 Activities: DescribeAutoScalingGroup, GetHookConfigurations, and ResizeServerGroup" src="https://cdn-images-1.medium.com/max/1024/1*zmCyjwzTXji921mulJjmTw.png" /><figcaption>Execution of a resizeServerGroup Cloud Operation as seen from the Temporal UI. This operation executes 3 Activities: DescribeAutoScalingGroup, GetHookConfigurations, and ResizeServerGroup</figcaption></figure><h4>Lessons Learned</h4><p>With the benefit of hindsight, there are also some lessons we can share from this migration:</p><p>1. <strong>Avoid unnecessary Child Workflows</strong>: Structuring Cloud Operations as an UntypedCloudOperationRunner Workflow that starts Child Workflows to actually execute the Cloud Operation’s logic was unnecessary and the indirection made troubleshooting more difficult. There are <a href="https://community.temporal.io/t/purpose-of-child-workflows/652">situations</a> where Child Workflows are appropriate, but in this case we were using them as a tool for code organization, which is generally unnecessary. We could’ve achieved the same effect with class composition in the top-level parent Workflow.</p><p>2. <strong>Use single argument objects</strong>: At first, we structured Workflow and Activity functions with variable arguments, much as you’d write normal functions. This can be problematic for Temporal because of Temporal’s <a href="https://community.temporal.io/t/workflow-determinism/4027">determinism constraints</a>. Adding or removing an argument from a function signature is <strong>not</strong> a backward-compatible change, and doing so can break long-running workflows — and it’s not immediately obvious in code review your change is problematic. The preferred pattern is to use a single serializable class to host all your arguments for Workflows and Activities — these can be more freely changed without breaking determinism.</p><p>3. <strong>Separate business failures from workflow failures</strong>: We like the pattern of the WorkflowResult type that UntypedCloudOperationRunner returns in the interface above. It allows us to communicate business process failures without failing the Workflow itself and have more overall nuance in error handling. This is a pattern we’ve carried over to other Workflows we’ve implemented since.</p><h3>Temporal at Netflix Today</h3><p>Temporal adoption has skyrocketed at Netflix since its initial introduction for Spinnaker. Today, we have hundreds of use cases, and we’ve seen adoption double in the last year with no signs of slowing down.</p><p>One major difference between initial adoption and today is that Netflix migrated from an on-prem Temporal deployment to using <a href="https://temporal.io/cloud">Temporal Cloud</a>, which is Temporal’s SaaS offering of the Temporal server. This has let us scale Temporal adoption while running a lean team. We’ve also built up a robust internal platform around Temporal Cloud to integrate with Netflix’s internal ecosystem and make onboarding for our developers as easy as possible. Stay tuned for a future post digging into more specifics of our Netflix Temporal platform.</p><h3>Acknowledgement</h3><p>We all stand on the shoulders of giants in software. I want to call out that I’m retelling the work of my two stunning colleagues <a href="https://www.linkedin.com/in/chris-smalley/">Chris Smalley</a> and <a href="https://www.linkedin.com/in/robzienert/">Rob Zienert</a> in this post, who were the two aforementioned engineers who introduced Temporal and carried out the migration.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=73c69ccb5953" width="1" height="1" alt=""><hr><p><a href="https://netflixtechblog.com/how-temporal-powers-reliable-cloud-operations-at-netflix-73c69ccb5953">How Temporal Powers Reliable Cloud Operations at Netflix</a> was originally published in <a href="https://netflixtechblog.com">Netflix TechBlog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Netflix Live Origin]]></title>
            <link>https://netflixtechblog.com/netflix-live-origin-41f1b0ad5371?source=rss----2615bd06b42e---4</link>
            <guid isPermaLink="false">https://medium.com/p/41f1b0ad5371</guid>
            <category><![CDATA[live-origin]]></category>
            <category><![CDATA[live-streaming]]></category>
            <category><![CDATA[content-delivery-network]]></category>
            <category><![CDATA[cloud-storage]]></category>
            <dc:creator><![CDATA[Netflix Technology Blog]]></dc:creator>
            <pubDate>Mon, 15 Dec 2025 17:38:16 GMT</pubDate>
            <atom:updated>2025-12-15T17:38:14.921Z</atom:updated>
            <content:encoded><![CDATA[<p><a href="https://www.linkedin.com/in/xiaomei-liu-b475711/">Xiaomei Liu</a>, <a href="https://www.linkedin.com/in/joseph-lynch-9976a431/">Joseph Lynch</a>, <a href="https://www.linkedin.com/in/chrisnewton2/">Chris Newton</a></p><h3>Introduction</h3><p><a href="https://netflixtechblog.com/building-a-reliable-cloud-live-streaming-pipeline-for-netflix-8627c608c967">Behind the Streams: Building a Reliable Cloud Live Streaming Pipeline for Netflix</a> introduced the architecture of the streaming pipeline. This blog post looks at the custom Origin Server we built for Live — the Netflix Live Origin. It sits at the demarcation point between the cloud live streaming pipelines on its upstream side and the distribution system, Open Connect, Netflix’s in-house Content Delivery Network (CDN), on its downstream side, and acts as a broker managing what content makes it out to Open Connect and ultimately to the client devices.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*44sJszKXEHZvSHnEQgYiiw.png" /><figcaption><strong>Live Streaming Distribution and Origin Architecture</strong></figcaption></figure><p>Netflix Live Origin is a multi-tenant microservice operating on EC2 instances within the AWS cloud. We lean on standard HTTP protocol features to communicate with the Live Origin. The Packager pushes segments to it using PUT requests, which place a file into storage at the particular location named in the URL. The storage location corresponds to the URL that is used when the Open Connect side issues the corresponding GET request.</p><p>Live Origin architecture is influenced by key technical decisions of the live streaming architecture. First, resilience is achieved through redundant regional live streaming pipelines, with failover orchestrated at the server-side to reduce client complexity. The implementation of <a href="https://netflixtechblog.com/building-a-reliable-cloud-live-streaming-pipeline-for-netflix-8627c608c967">epoch locking at the cloud encoder</a> enables the origin to select a segment from either encoding pipeline. Second, Netflix adopted a manifest design with <a href="https://netflixtechblog.com/behind-the-streams-live-at-netflix-part-1-d23f917c2f40">segment templates and constant segment duration</a> to avoid frequent manifest refresh. The constant duration templates enable Origin to predict the segment publishing schedule.</p><h3>Multi-pipeline and multi-region aware origin</h3><p>Live streams inevitably contain defects due to the non-deterministic nature of live contribution feeds and strict real-time segment publishing timelines. Common defects include:</p><ul><li><strong>Short segments:</strong> Missing video frames and audio samples.</li><li><strong>Missing segments:</strong> Entire segments are absent.</li><li><strong>Segment timing discontinuity:</strong> Issues with the Track Fragment Decode Time.</li></ul><p>Communicating segment discontinuity from the server to the client via a segment template-based manifest is impractical, and these defective segments can disrupt client streaming.</p><p>The redundant cloud streaming pipelines operate independently, encompassing distinct cloud regions, contribution feeds, encoder, and packager deployments. This independence substantially mitigates the probability of simultaneous defective segments across the dual pipelines. Owing to its strategic placement within the distribution path, the live origin naturally emerges as a component capable of intelligent candidate selection.</p><p>The Netflix Live Origin features multi-pipeline and multi-region awareness. When a segment is requested, the live origin checks candidates from each pipeline in a deterministic order, selecting the first valid one. Segment defects are detected via lightweight media inspection at the packager. This defect information is provided as metadata when the segment is published to the live origin. In the rare case of concurrent defects at the dual pipeline, the segment defects can be communicated downstream for intelligent client-side error concealment.</p><h3>Open Connect streaming optimization</h3><p>When the Live project started, Open Connect had become highly optimised for VOD content delivery — <a href="https://freenginx.org/en/">nginx</a> had been chosen many years ago as the Web Server since it is highly capable in this role, and a number of enhancements had been added to it and to the underlying operating system (BSD). Unlike traditional CDNs, Open Connect is more of a distributed origin server — VOD assets are pre-positioned onto carefully selected server machines (OCAs, or Open Connect Appliances) rather than being filled on demand.</p><p>Alongside the VOD delivery, an on-demand fill system has been used for non-VOD assets — this includes artwork and the downloadable portions of the clients, etc. These are also served out of the same <a href="https://freenginx.org/en/">nginx</a> workers, albeit under a distinct server block, using a distinct set of hostnames.</p><p>Live didn’t fit neatly into this ‘small object delivery’ model, so we extended the proxy-caching functionality of <a href="https://freenginx.org/en/">nginx</a> to address Live-specific needs. We will touch on some of these here related to optimized interactions with the Origin Server. Look for a future blog post that will go into more details on the Open Connect side.</p><p>The segment templates provided to clients are also provided to the OCAs as part of the Live Event Configuration data. Using the Availability Start Time and Initial Segment number, the OCA is able to determine the legitimate range of segments for each event at any point in time — requests for objects outside this range can be rejected, preventing unnecessary requests going up through the fill hierarchy to the origin. If a request makes it through to the origin, and the segment isn’t available yet, the origin server will return a 404 Status Code (indicating File Not Found) with the expiration policy of that error so that it can be cached within Open Connect until just before that segment is expected to be published.</p><p>If the Live Origin knows when segments are being pushed to it, and knows what the live edge is — when a request is received for the immediately next object, rather than handing back another 404 error (which would go all the way back through Open Connect to the client), the Live Origin can ‘hold open’ the request, and service it once the segment has been published to it. By doing this, the degree of chatter within the network handling requests that arrive early has been significantly reduced. As part of this, millisecond grain caching was added to <a href="https://freenginx.org/en/">nginx</a> to enhance the standard HTTP Cache Control, which only works at second granularity, a long time when segments are generated every 2 seconds.</p><h4>Streaming metadata enhancement</h4><p>The HTTP standard allows for the addition of request and response headers that can be used to provide additional information as files move between clients and servers. The HTTP headers provide notifications of events within the stream in a highly scalable way that is independently conveyed to client devices, regardless of their playback position within the stream.</p><p>These notifications are provided to the origin by the live streaming pipeline and are inserted by the origin in the form of headers, appearing on the segments generated at that point in time (and persist to future segments — they are cumulative). Whenever a segment is received at an OCA, this notification information is extracted from the response headers and used to update an in-memory data structure, keyed by event ID; and whenever a segment is served from the OCA, the latest such notification data is attached to the response. This means that, given any flow of segments into an OCA, it will always have the most recent notification data, even if all clients requesting it are behind the live edge. In fact, the notification information can be conveyed on any response, not just those supplying new segments.</p><h4>Cache invalidation and origin mask</h4><p>An invalidation system has been available since the early days of the project. It can be used to “flush” all content associated with an event by altering the key used when looking up objects in cache — this is done by incorporating a version number into the cache key that can then be bumped on demand. This is used during pre-event testing so that the network can be returned to a pristine state for the test with minimal fuss.</p><p>Each segment published by the Live Origin conveys the encoding pipeline it was generated by, as well as the region it was requested from. Any issues that are found after segments make their way into the network can be remedied by an enhanced invalidation system that takes such variants into account. It is possible to invalidate (that is, cause to be considered expired) segments in a range of segment numbers, but only if they were sourced from encoder A, or from Encoder A, but only if retrieved from region X.</p><p>In combination with Open Connect’s enhanced cache invalidation, the Netflix Live Origin allows <em>selective encoding pipeline masking</em> to exclude a range of segments from a particular pipeline when serving segments to Open Connect. The enhanced cache invalidation and origin masking enable live streaming operations to hide known problematic segments (e.g., segments causing client playback errors) from streaming clients once the bad segments are detected, protecting millions of streaming clients during the DVR playback window.</p><h3>Origin storage architecture</h3><p>Our original storage architecture for the Live Origin was simple: just use <a href="https://aws.amazon.com/s3/">AWS S3</a> like we do for SVOD. This served us well initially for our low-traffic events, but as we scaled up we discovered that Live streaming has unique latency and workload requirements that differ significantly from on-demand where we have significant time ahead-of-time to pre-position content. While S3 met its stated uptime guarantees, our strict 2-second retry budget inherent to Live events (where every write is critical) led us to explore optimizations specifically tailored for real-time delivery at scale. AWS S3 is an amazing object store, but our Live streaming requirements were closer to those of a global low-latency highly-available database. So, we went back to the drawing board and started from the requirements. The Origin required:</p><ol><li>[HA Writes] Extremely high <em>write</em> availability, ideally as close to full write availability within a single AWS region, with low second replication delay to other regions. Any failed write operation within 500ms is considered a bug that must be triaged and prevented from re-occurring.</li><li>[Throughput] High write throughput, with hundreds of MiB replicating across regions</li><li>[Large Partitions] Efficiently support O(MiB) writes that accumulate to O(10k) keys per partition with O(GiB) total size per event.</li><li>[Strong Consistency] Within the same region, we needed read-your-write semantics to hit our &lt;1s read delay requirements (must be able to read published segments)</li><li>[Origin Storm] During worst-case load involving Open Connect edge cases, we may need to handle O(<strong>GiB</strong>) of read throughput <em>without affecting writes</em>.</li></ol><p>Fortunately, Netflix had previously invested in building a <a href="https://netflixtechblog.com/introducing-netflixs-key-value-data-abstraction-layer-1ea8a0a11b30">KeyValue Storage Abstraction</a> that cleverly leveraged <a href="https://youtu.be/sQ-_jFgOBng?t=1061">Apache Cassandra</a> to provide chunked storage of MiB or even GiB values. This abstraction was initially built to support cloud saves of Game state. The Live use case would push the boundaries of this solution, however, in terms of availability for writes (#1), cumulative partition size (#3), and read throughput during Origin Storm (#5).</p><h4>High Availability for Writes of Large Payloads</h4><p>The <a href="https://youtu.be/paTtLhZFsGE?t=1077">KeyValue Payload Chunking and Compression Algorithm</a> breaks O(MiB) work down so each part can be idempotently retried and hedged to maintain strict latency service level objectives, as well as spreading the data across the full cluster. When we combine this algorithm with Apache Cassandra’s local-quorum consistency model, which allows write availability even with an entire Availability Zone outage, plus a write-optimized <a href="https://en.wikipedia.org/wiki/Log-structured_merge-tree">Log-Structured Merge Tree</a> (LSM) storage engine, we could meet the first four requirements. After iterating on the performance and availability of this solution, we were not only able to achieve the write availability required, but did so with a P99 <em>tail</em> latency that was similar to the status quo’s P50 <em>average </em>latency while also handling cross-region replication behind the scenes for the Origin. This new solution was significantly more expensive (as expected, databases backed by SSD cost more), but minimizing cost was <em>not</em> a key objective and low latency with high availability was:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*bUPc4gC-mSDcybayhBJQ8g.png" /><figcaption><strong>Storage System Write Performance</strong></figcaption></figure><h4>High Availability Reads at Gbps Throughputs</h4><p>Now that we solved the write reliability problem, we had to handle the Origin Storm failure case, where potentially dozens of Open Connect top-tier caches could be requesting multiple O(MiB) video segments at once. Our back-of-the-envelope calculations showed worst-case read throughput in the O(100Gbps) range, which would normally be extremely expensive for a strongly-consistent storage engine like Apache Cassandra. With careful tuning of chunk access, we were able to respond to reads at network line rate (100Gbps) from Apache Cassandra, but we observed unacceptable performance and availability degradation on concurrent writes. To resolve this issue, we introduced write-through caching of chunks using our distributed caching system <a href="https://github.com/Netflix/EVCache">EVCache</a>, which is based on Memcached. This allows almost all reads to be served from a highly scalable cache, allowing us to easily hit 200Gbps and beyond without affecting the write path, achieving read-write separation.</p><h4>Final Storage Architecture</h4><p>In the final storage architecture, the Live Origin writes and reads to KeyValue, which manages a write-through cache to EVCache (memcached) and implements a safe chunking protocol that spreads large values and partitions them out across the storage cluster (Apache Cassandra). This allows almost all read load to be handled from cache, with only misses hitting the storage. This combination of cache and highly available storage has met the demanding needs of our Live Origin for over a year now.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*yA9H-BFemM_-99FXBlVOMg.png" /><figcaption><strong>Storage System High Level Architecture</strong></figcaption></figure><p>Delivering this consistent low latency for large writes with cross-region replication and consistent write-through caching to a distributed cache required solving numerous hard problems with novel techniques, which we plan to share in detail during a future post.</p><h3>Scalability and scalable architecture</h3><p>Netflix’s live streaming platform must handle a high volume of diverse stream renditions for each live event. This complexity stems from supporting various video encoding formats (each with multiple encoder ladders), numerous audio options (across languages, formats, and bitrates), and different content versions (e.g., with or without advertisements). The combination of these elements, alongside concurrent event support, leads to a significant number of unique stream renditions per live event. This, in turn, necessitates a high Requests Per Second (RPS) capacity from the multi-tenant live origin service to ensure publishing-side scalability.</p><p>In addition, Netflix’s global reach presents distinct challenges to the live origin on the retrieval side. During the Tyson vs. Paul fight event in 2024, a historic peak of 65 million concurrent streams was observed. Consequently, a scalable architecture for live origin is essential for the success of large-scale live streaming.</p><h4>Scaling architecture</h4><p>We chose to build a highly scalable origin instead of relying on the traditional origin shields approach for better end-to-end cache consistency control and simpler system architecture. The live origin in this architecture directly connects with top-tier Open Connect nodes, which are geographically distributed across several sites. To minimize the load on the origin, only designated nodes per stream rendition at each site are permitted to directly fill from the origin.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*jW7eBCQtjlna0VKaWrKz_A.png" /><figcaption><strong>Netflix Live Origin Scalability Architecture</strong></figcaption></figure><p>While the origin service can autoscale horizontally using EC2 instances, there are other system resources that are not autoscalable, such as storage platform capacity and AWS to Open Connect backbone bandwidth capacity. Since in live streaming, not all requests to the live origin are of the same importance, the origin is designed to prioritize more critical requests over less critical requests when system resources are limited. The table below outlines the request categories, their identification, and protection methods.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*dYKFJkq22KI8sDBW_Njmog.png" /></figure><h4>Publishing isolation</h4><p>Publishing traffic, unlike potentially surging CDN retrieval traffic, is predictable, making path isolation a highly effective solution. As shown in the scalability architecture diagram, the origin utilizes separate EC2 publishing and CDN stacks to protect the latency and failure-sensitive origin writes. In addition, the storage abstraction layer features distinct clusters for key-value (KV) read and KV write operations. Finally, the storage layer itself separates read (EVCache) and write (Cassandra) paths. This comprehensive path isolation facilitates independent cloud scaling of publishing and retrieval, and also prevents CDN-facing traffic surges from impacting the performance and reliability of origin publishing.</p><h4>Priority rate limiting</h4><p>Given Netflix’s scale, managing incoming requests during a traffic storm is challenging, especially considering non-autoscalable system resources. The Netflix Live Origin implemented priority-based rate limiting when the underlying system is under stress. This approach ensures that requests with greater user impact are prioritized to succeed, while requests with lower user impact are allowed to fail during times of stress in order to protect the streaming infrastructure and are permitted to retry later to succeed.</p><p>Leveraging Netflix’s microservice platform priority rate limiting feature, the origin prioritizes live edge traffic over DVR traffic during periods of high load on the storage platform. The live edge vs. DVR traffic detection is based on the predictable segment template. The template is further cached in memory on the origin node to enable priority rate limiting without access to the datastore, which is valuable especially during periods of high datastore stress.</p><p>To mitigate traffic surges, TTL cache control is used alongside priority rate limiting. When the low-priority traffic is impacted, the origin instructs Open Connect to slow down and cache identical requests for 5 seconds by setting a max-age = 5s and returns an HTTP 503 error code. This strategy effectively dampens traffic surges by preventing repeated requests to the origin within that 5-second window.</p><p>The following diagrams illustrate origin priority rate limiting with simulated traffic. The nliveorigin_mp41 traffic is the low-priority traffic and is mixed with other high-priority traffic. In the first row: the 1st diagram shows the request RPS, the 2nd diagram shows the percentage of request failure. In the second row, the 1st diagram shows datastore resource utilization, and the 2nd diagram shows the origin retrieval P99 latency. The results clearly show that only the low-priority traffic (nliveorigin_mp41) is impacted at datastore high utilization, and the origin request latency is under control.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*-_YP6H3sEDaw1lS8prH4mQ.png" /><figcaption><strong>Origin Priority Rate Limiting</strong></figcaption></figure><h4>404 storm and cache optimization</h4><p>Publishing isolation and priority rate limiting successfully protect the live origin from DVR traffic storms. However, the traffic storm generated by requests for non-existent segments presents further challenges and opportunities for optimization.</p><p>The live origin structures metadata hierarchically as event &gt; stream rendition &gt; segment, and the segment publishing template is maintained at the stream rendition level. This hierarchical organization allows the origin to preemptively reject requests with an HTTP 404(not found)/410(Gone) error, leveraging highly cacheable event and stream rendition level metadata, avoiding unnecessary queries to the segment level metadata:</p><ul><li>If the event is unknown, reject the request with 404</li><li>If the event is known, but the segment request timing does not match the expected publishing timing, reject the request with 404 and cache control TTL matching the expected publishing time</li><li>If the event is known, the requested segment is never generated or misses the retry deadline, reject the request with a 410 error, preventing the client from repeatedly requesting</li></ul><p>At the storage layer, metadata is stored separately from media data in the control plane datastore. Unlike the media datastore, the control plane datastore does not use a distributed cache to avoid cache inconsistency. Event and rendition level metadata benefits from a high cache hit ratio when in-memory caching is utilized at the live origin instance. During traffic storms involving non-existent segments, the cache hit ratio for control plane access easily exceeds 90%.</p><p>The use of in-memory caching for metadata effectively handles 404 storms at the live origin without causing datastore stress. This metadata caching complements the storage system’s distributed media cache, providing a complete solution for traffic surge protection.</p><h3>Summary</h3><p>The Netflix Live Origin, built upon an optimized storage platform, is specifically designed for live streaming. It incorporates advanced media and segment publishing scheduling awareness and leverages enhanced intelligence to improve streaming quality, optimize scalability, and improve Open Connect live streaming operations.</p><h3>Acknowledgement</h3><p>Many teams and stunning colleagues contributed to the Netflix live origin. Special thanks to <a href="https://www.linkedin.com/in/flavioribeiro/?originalSubdomain=br">Flavio Ribeiro</a> for advocacy and sponsorship of the live origin project; to <a href="https://www.linkedin.com/in/rummadis/">Raj Ummadisetty</a>, <a href="https://www.linkedin.com/in/prudhviraj9/">Prudhviraj Karumanchi</a> for the storage platform; to <a href="https://www.linkedin.com/in/rosanna-lee-197920/">Rosanna Lee</a>, <a href="https://www.linkedin.com/in/hunterford/">Hunter Ford</a>, and <a href="https://www.linkedin.com/in/thiagopnts/">Thiago Pontes</a> for storage lifecycle management; to <a href="https://www.linkedin.com/in/ameya-vasani-8904304/">Ameya Vasani</a> for e2e test framework; <a href="https://www.linkedin.com/in/thomas-symborski-b4216728/">Thomas Symborski</a> for orchestrator integration; to <a href="https://www.linkedin.com/in/jschek/">James Schek</a> for Open Connect integration; to <a href="https://www.linkedin.com/in/kzwang/">Kevin Wang</a> for platform priority rate limit; to <a href="https://www.linkedin.com/in/di-li-09663968/">Di Li</a>, <a href="mailto:nhubbard@netflix.com">Nathan Hubbard</a> for origin scalability testing.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=41f1b0ad5371" width="1" height="1" alt=""><hr><p><a href="https://netflixtechblog.com/netflix-live-origin-41f1b0ad5371">Netflix Live Origin</a> was originally published in <a href="https://netflixtechblog.com">Netflix TechBlog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[AV1 — Now Powering 30% of Netflix Streaming]]></title>
            <link>https://netflixtechblog.com/av1-now-powering-30-of-netflix-streaming-02f592242d80?source=rss----2615bd06b42e---4</link>
            <guid isPermaLink="false">https://medium.com/p/02f592242d80</guid>
            <category><![CDATA[av1]]></category>
            <category><![CDATA[video-encoding]]></category>
            <category><![CDATA[streaming]]></category>
            <category><![CDATA[aomedia]]></category>
            <category><![CDATA[netflix]]></category>
            <dc:creator><![CDATA[Netflix Technology Blog]]></dc:creator>
            <pubDate>Thu, 04 Dec 2025 20:09:30 GMT</pubDate>
            <atom:updated>2025-12-04T20:09:25.801Z</atom:updated>
            <content:encoded><![CDATA[<h3><strong>AV1 — Now Powering 30% of Netflix Streaming</strong></h3><p><a href="https://www.linkedin.com/in/liwei-guo/">Liwei Guo</a>, <a href="https://www.linkedin.com/in/henryzhili/">Zhi Li</a>, <a href="https://www.linkedin.com/in/sheldon-radford/">Sheldon Radford</a>, <a href="https://www.linkedin.com/in/jeffrwatts/">Jeff Watts</a></p><p>Streaming video has become an integral part of our daily lives. At Netflix, our top priority is delivering the best possible entertainment experience to our members, regardless of their devices or network conditions. One of the key technologies enabling this is <a href="https://aomedia.org/specifications/av1/">AV1</a>, a modern, open video codec that is rapidly transforming both how we stream content and how users experience it. Today, AV1 powers approximately 30% of all Netflix viewing, marking a major milestone in our efforts to bring more efficient and higher-quality streaming to our members.</p><p>In this post, we’ll revisit Netflix’s AV1 journey to date, highlight emerging use cases, and share adoption trends across the device ecosystem. Having witnessed AV1’s significant impact，and with <a href="https://aomedia.org/press%20releases/AOMedia-Announces-Year-End-Launch-of-Next-Generation-Video-Codec-AV2-on-10th-Anniversary/">AV2 on the horizon</a>, we’re more excited than ever about how open codecs will continue to revolutionize streaming for everyone.</p><h3>AV1: A Modern, Open Codec</h3><p>Since entering the streaming business in 2007, Netflix has primarily relied on H.264/AVC as its streaming format. However, we quickly recognized that a modern, open codec would benefit not only Netflix, but the entire multimedia industry. In 2015, together with a group of like-minded industry leaders, Netflix co-founded the <a href="https://aomedia.org/">Alliance for Open Media (AOMedia)</a> to develop and promote next generation, open source media technologies. The AV1 codec became the first major project of this collaboration, with ambitious goals: to deliver significant improvements in compression efficiency over state-of-the-art codecs, and to introduce rich features that enable new use cases. After three years of collaborative development, AV1 was officially released in 2018.</p><h3>Netflix’s AV1 Journey: From Android to TVs and Beyond</h3><h4><strong>Piloting on Android Mobile</strong></h4><p>When we first set out to bring AV1 streaming to Netflix members, Android was the ideal starting point. Android’s flexibility allowed us to quickly integrate a software AV1 decoder using the efficient <a href="https://code.videolan.org/videolan/dav1d">dav1d</a> library, which was already optimized for ARM chipsets in mobile devices.</p><p>AV1’s superior compression efficiency was especially valuable for mobile users, many of whom are mindful of their data usage and network conditions. By adopting AV1, we were able to deliver noticeably better video quality at lower bitrates. For members relying on cellular data, this meant crisper images with fewer compression artifacts, even when bandwidth was limited. <a href="https://netflixtechblog.com/netflix-now-streaming-av1-on-android-d5264a515202">Launching AV1 support on Android</a> in 2020 marked a significant step forward for Netflix on mobile, making high-quality streaming more accessible and enjoyable for members everywhere.</p><h4><strong>Front-and-Center for Netflix VOD Streaming</strong></h4><p>The success of our AV1 launch on Android proved its value for Netflix streaming, motivating us to expand support to smart TVs and other large-screen devices, where most of our members watch their favorite shows.</p><p>Smart TVs depend on hardware decoders for efficient high-quality playback. We worked closely with device manufacturers and SoC vendors to certify these devices, ensuring they are both conformant and performant. This collaborative effort enabled our AV1 streaming to TV devices in <a href="https://netflixtechblog.com/bringing-av1-streaming-to-netflix-members-tvs-b7fc88e42320">late 2021</a>. Shortly thereafter, we expanded AV1 streaming to web browsers (in 2022) and continued to broaden device support. In 2023, this included Apple devices with the introduction of AV1 hardware support in the new M3 and A17 Pro chips.</p><p>As more devices began shipping with AV1 hardware support, a rapidly growing share of our members could enjoy the benefits of this advanced codec. Combined with our investment in adding AV1 streams across the entire catalog, AV1 viewing share has been consistently increasing in recent years. Today, AV1 accounts for approximately 30% of all Netflix streaming, making it our second most-used codec — and it’s on track to become number one very soon. The payoff has been substantial.</p><ul><li><strong>Elevating Streaming Experience Across the Board</strong>: Large-screen TVs and other devices demand higher bitrates to deliver stunning 4K, high frame rate (HFR) experiences. AV1’s superior compression efficiency has allowed us to provide these experiences using less data, making high-quality streaming more accessible and reliable. On average, AV1 streaming sessions achieve VMAF scores¹ that are 4.3 points higher than AVC and 0.9 points higher than HEVC sessions. At the same time, AV1 sessions use one-third less bandwidth than both AVC and HEVC, resulting in 45% fewer buffering interruptions. Moreover, Netflix’s diverse content catalog benefits universally from AV1, with improvements across all content types.</li><li><strong>Driving Network Efficiency Worldwide</strong>: Netflix streams are delivered through our own content delivery network (<a href="https://openconnect.netflix.com/en/?utm_referrer=https://www.google.com/">Open Connect</a>), in partnership with local ISPs around the globe. With more than 300 million members, Netflix streaming constitutes a non-trivial portion of global internet traffic. Because AV1 is a more efficient codec, its streams are smaller in size (while providing even better visual quality). By shifting a substantial share of our streaming to AV1, we reduce overall internet bandwidth consumption, and lessen system and network load for both Netflix and our partners.</li></ul><h4>Unlocking Advanced Experiences</h4><p>In addition to its superior compression efficiency, AV1 was designed to support a rich set of features. Once we established a robust framework for the continuous expansion of AV1 streaming, we quickly shifted our focus towards exploring AV1’s unique features to unlock even more advanced and immersive experiences for our members.</p><p><strong>High-Dynamic-Range(HDR)<br></strong>HDR brings enhanced detail, vivid colors, and greater clarity to images. As a premium streaming service, Netflix has been a pioneer in adopting HDR, offering HDR streaming since 2016. In March 2025, we launched <a href="https://netflixtechblog.com/hdr10-now-streaming-on-netflix-c9ab1f4bd72b">AV1 HDR streaming</a>. We chose HDR10+ as the HDR format for its use of dynamic metadata, which enabled us to adapt the tone mapping per device in a scene-dependent manner.</p><p>As anticipated, the combination of AV1 and HDR10+ allows us to deliver images with greater detail, more vibrant colors, and an overall heightened sense of immersion for our members. At the moment, 85% of our HDR catalog (from the perspective of view-hours) has AV1-HDR10+ coverage, and this number is expected to reach 100% in the next couple of months.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/759/1*Ubhj9prgqb0zuTHt6oOx0g.png" /><figcaption><strong><em>Photographs of devices displaying the same (cropped) frame with HDR10 metadata (left) and HDR10+ metadata (right). Notice the preservation of the flashlight detail in the HDR10+ capture, and the over-exposure of the region under the flashlight in the HDR10 one.</em></strong></figcaption></figure><p><strong>Cinematic Film Grain<br></strong>Film grain is a hallmark of the cinematic experience, widely used in the movie industry to enhance a film’s depth, texture, and realism. However, because film grain is inherently random, faithfully representing it in digital video requires a significant amount of data. This presents a unique challenge for streaming: restricting the bitrate can result in grain that appears unnatural or distorted, while increasing the bitrate to accurately preserve cinematic grain almost inevitably leads to elevated rebuffering. The AV1 specification incorporates a unique solution called Film Grain Synthesis (FGS). Instead of encoding grain as part of every frame, the grain is stripped out before encoding and then resynthesized at the decoder using parameters sent in the bitstream, delivering a realistic cinematic film grain experience without the usual data costs.</p><p>This approach represents a significant shift from traditional compression and streaming techniques. Our team invested substantial effort in fine-tuning the media processing pipeline, ensuring FGS delivers robust performance at scale. In July 2025, we successfully <a href="https://netflixtechblog.com/av1-scale-film-grain-synthesis-the-awakening-ee09cfdff40b">productized AV1 FGS</a>, and the results were astonishing: AV1 with FGS could deliver videos with cinematic film grain at a bitrate well within the capabilities of typical household internet connections. For non-FGS AV1 encodings, even at much higher bitrate, they may not be able to achieve comparable quality.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1016/1*9fB5xuoFpbpN8ZQIzTHDtg.png" /><figcaption><strong><em>The same (cropped) frame from source (left), regular AV1 stream encoded at 8274kbps (middle) and AV1 FGS stream encoded at 2804 kbps (right). The AV1 FGS stream reduces the bitrate by 66% while delivering clearly better quality.</em></strong></figcaption></figure><h4><strong>Beyond VOD Streaming</strong></h4><p>So far, our AV1 journey has been mainly on VOD, but we see significant opportunities for AV1 beyond traditional VOD streaming. On a mission to entertain the world, Netflix has constantly explored and established other ways to bring joy to our members, and we believe AV1 could contribute to the success of these new products.</p><p><strong>Live Streaming<br></strong>Debuting in 2023, live streaming has experienced <a href="https://help.netflix.com/en/node/129840">rapid growth</a> at Netflix, becoming a key part of our streaming offerings in just two short years. We are actively evaluating the use of AV1 in live streaming, as we believe it could help further scale Netflix’s live programming:</p><ul><li><strong>Hyper-scale concurrent viewership: </strong>Live streaming at Netflix means delivering content to <a href="https://www.netflix.com/tudum/articles/jake-paul-vs-mike-tyson-live-release-date-news">tens of millions</a> of viewers simultaneously. AV1’s superior compression efficiency could significantly reduce the required bandwidth, enabling us to deliver high-quality live experiences to large audiences without compromising video quality.</li><li><strong>Customizable graphics overlay</strong>: for live sport events such as football, tennis and boxing, graphics overlays have become an integral part of the member experience — from embedding game statistics to delivering sponsorships. AV1 offers an opportunity to make the graphics highly customizable: layered coding is supported in AV1’s main profile, allowing encoding the main content in the base layer, and graphics in the enhancement layer, and easily swapping out one version of the enhancement layer with another. We envision that the use of AV1’s layered coding can greatly simplify the live streaming workflow and reduce delivery costs.</li></ul><p><strong>Cloud Gaming<br></strong>Cloud gaming is a new Netflix offering that is currently in the <a href="https://help.netflix.com/en/node/132197">beta phase</a> and is available to members in select countries. The game engines run on cloud servers, while the rendered graphics are streamed directly to members’ devices. By removing barriers and transforming every Netflix-enabled device into a game console, Cloud gaming aims to deliver a seamless, “play anywhere” experience for our members. For a glimpse of this in action, <a href="https://www.linkedin.com/feed/update/urn:li:activity:7382077927875825664/">watch as Co-CEO Greg Peters and CTO Elizabeth Stone play a round of Boggle Party — powered entirely by Netflix’s cloud gaming platform</a>!</p><p>Unlike traditional video streaming, cloud gaming requires that every player action is reflected instantly on the screen to ensure a responsive and immersive experience. This makes delivering high-quality video frames with extremely low latency, despite fluctuating network conditions, one of the biggest challenges in cloud gaming.</p><p>Our team is actively working on productizing AV1 for cloud gaming. Given AV1’s high compression efficiency, we can reduce frame sizes, helping video frames get through even when network conditions become challenging. This positions AV1 as a promising technology for enabling a high-quality, low-latency gaming experience across a wide range of devices.</p><h3>A Device Ecosystem United for AV1</h3><p>Netflix is a streaming company, and we have worked diligently to create highly efficient and standards-conformant AV1 streams for our catalog. However, an equally, if not more, important factor in AV1’s success is the widespread support from device manufacturers. Throughout our AV1 journey, we have been impressed by the unprecedented pace at which the device ecosystem has embraced AV1.</p><p>Just six months after the AV1 specification was finalized, the open-source AV1 decoder library sponsored by AOM, dav1d, was released. Small, performant, and highly resource-efficient, dav1d bridged the gap for early adopters like Netflix while hardware solutions were still in development. Continuous improvements to its performance and compatibility have made dav1d the preferred choice for a wide range of platforms and practical applications. Today, it serves as <a href="https://aomedia.org/av1-adoption-showcase/google-story/">Android’s default software decoder</a>. Additionally, it plays a key role in web browsers — for Netflix, it powers approximately 40% of our browser playback. This broad adoption has significantly expanded access to high-quality AV1 streaming, even in the absence of dedicated hardware decoders.</p><p>Netflix maintains a close working relationship with device manufacturers and SoC vendors, and we have witnessed first-hand their enthusiasm for adopting AV1. To ensure optimal streaming performance, Netflix has a rigorous certification process to verify proper support for our streaming formats on devices. AV1 was added to this certification process in 2019, and since then, we have seen a steady increase in the number of devices with full AV1 decoding capabilities. Over the past five years (2021–2025), 88% of large-screen devices, including TVs, set-top boxes, and streaming sticks, submitted for Netflix certification have supported AV1, with the vast majority offering full 4K@60fps capability. Notably, since 2023, almost all devices we have received for certification are AV1-capable.</p><p>We have also been impressed by the robustness of AV1 implementations across these devices. As mentioned earlier, FGS is an innovative tool that departs from traditional codec architectures and was not included in our initial full-scale AV1 streaming rollout. When we launched FGS this July, we worked closely with our partners to ensure broad device compatibility. We are pleased with the successful progress made, and AV1 with FGS is now supported across a significant and growing number of in-field devices.</p><h3>Looking Ahead: AV1 Today, AV2 Tomorrow</h3><p>As we reflect on our AV1 journey, it’s clear that the codec has already transformed the streaming experience for hundreds of millions of Netflix members worldwide. Thanks to industry-wide collaboration and rapid device adoption, AV1 is delivering higher quality, greater efficiency, and new cinematic features to more screens than ever before.</p><p>Looking ahead, we are excited about the forthcoming release of AV2, announced by the Alliance for Open Media for the end of 2025. <a href="https://www.youtube.com/watch?v=RUMwMe_2Dqo">AV2 is poised to set a new benchmark for compression efficiency and streaming capabilities, building on the solid foundation laid by AV1</a>. At Netflix, we remain committed to adopting the best open technologies to delight our members around the globe. While AV2 represents the future of streaming, AV1 is very much the present — serving as the backbone of our platform and powering exceptional entertainment experiences across a vast and ever-expanding ecosystem of devices.</p><h3>Acknowledgement</h3><p>The success of AV1 at Netflix is the result of the dedication, expertise, and collaboration of many teams across the company — including Encoding, Clients, Device Certification, Partner Engineering, Data Science &amp; Engineering, Infra, Platform, etc.</p><p>We would also like to thank <a href="https://www.linkedin.com/in/artemdanylenko/">Artem Danylenko</a>, <a href="https://www.linkedin.com/in/aditya-mavlankar-7139791/">Aditya Mavlankar</a>, <a href="https://www.linkedin.com/in/anne-aaron/">Anne Aaron</a>, <a href="https://www.linkedin.com/in/cyril-concolato-567a522/">Cyril Concolato</a>, <a href="https://www.linkedin.com/in/allanzp/">Allan Zhou</a> and <a href="https://www.linkedin.com/in/anush-moorthy-b8451142/">Anush Moorthy</a> for their valuable comments and feedback on earlier drafts of this post.</p><h3>Footnotes</h3><ol><li>These numbers represent a snapshot of data from November 13, 2025. Actual values may vary slightly from day to day and across different regions, depending on the mix of content, devices, and internet connectivity.</li></ol><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*xuLP8glDDcj-DBYO8djmNA.png" /></figure><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=02f592242d80" width="1" height="1" alt=""><hr><p><a href="https://netflixtechblog.com/av1-now-powering-30-of-netflix-streaming-02f592242d80">AV1 — Now Powering 30% of Netflix Streaming</a> was originally published in <a href="https://netflixtechblog.com">Netflix TechBlog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Supercharging the ML and AI Development Experience at Netflix]]></title>
            <link>https://netflixtechblog.com/supercharging-the-ml-and-ai-development-experience-at-netflix-b2d5b95c63eb?source=rss----2615bd06b42e---4</link>
            <guid isPermaLink="false">https://medium.com/p/b2d5b95c63eb</guid>
            <category><![CDATA[metaflow]]></category>
            <category><![CDATA[developer-tools]]></category>
            <category><![CDATA[mlops]]></category>
            <category><![CDATA[machine-learning]]></category>
            <category><![CDATA[artificial-intelligence]]></category>
            <dc:creator><![CDATA[Netflix Technology Blog]]></dc:creator>
            <pubDate>Tue, 04 Nov 2025 20:33:44 GMT</pubDate>
            <atom:updated>2025-11-04T20:33:42.852Z</atom:updated>
            <content:encoded><![CDATA[<h3>Supercharging the ML and AI Development Experience at Netflix with Metaflow</h3><p><a href="https://www.linkedin.com/in/shashanksrikanth/"><em>Shashank Srikanth</em></a>, <a href="https://www.linkedin.com/in/romain-cledat-4a211a5/"><em>Romain Cledat</em></a></p><p><a href="https://docs.metaflow.org">Metaflow</a> — a framework we started and <a href="https://netflixtechblog.com/open-sourcing-metaflow-a-human-centric-framework-for-data-science-fa72e04a5d9">open-sourced</a> in 2019 — now powers <a href="https://netflixtechblog.com/supporting-diverse-ml-systems-at-netflix-2d2e6b6d205d">a wide range of ML and AI systems across Netflix</a> and at <a href="https://github.com/Netflix/metaflow/blob/master/ADOPTERS.md">many other companies</a>. It is well loved by users for helping them take their ML/AI workflows from <a href="https://docs.metaflow.org/introduction/what-is-metaflow#how-does-metaflow-support-prototyping-and-production-use-cases">prototype to production</a>, allowing them to focus on building cutting-edge systems that bring joy and entertainment to audiences worldwide.</p><p>Metaflow allows users to:</p><ol><li><strong>Iterate and ship quickly </strong>by minimizing friction</li><li><strong>Operate systems reliably</strong> in production with minimal overhead, at Netflix scale.</li></ol><p>Metaflow works with many battle-hardened tooling to address the second point — among them <a href="https://netflixtechblog.com/100x-faster-how-we-supercharged-netflix-maestros-workflow-engine-028e9637f041">Maestro</a>, our newly open-sourced workflow orchestrator that powers nearly every ML and AI system at Netflix and serves as a backbone for Metaflow itself.</p><p>In this post, we focus on the first point and introduce a new Metaflow functionality, <strong>Spin</strong>, that helps users <strong>accelerate their iterative development process</strong>. By the end, you’ll have a solid understanding of Spin’s capabilities and learn how to try it out yourself with <strong>Metaflow 2.19</strong>.</p><h3>Iterative development in ML and AI workflows</h3><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*0I8DAvXCQEN1RpTTC0ZyaQ.jpeg" /><figcaption>Developing a Metaflow flow with cards in VSCode</figcaption></figure><p>To understand our approach to improving the ML and AI development experience, it helps to consider how these workflows differ from traditional software engineering.</p><p>ML and AI development revolves not just around code but also around data and models, which are large, mutable, and computationally expensive to process. Iteration cycles can involve long-running data transformations, model training, and stochastic processes that yield slightly different results from run to run. These characteristics make fast, stateful iteration a critical part of productive development.</p><p>This is where notebooks — such as Jupyter, <a href="https://observablehq.com/documentation/notebooks/">Observable</a>, or <a href="https://marimo.io/">Marimo</a> — shine. Their ability to preserve state in memory allows developers to load a dataset once and iteratively explore, transform, and visualize it without reloading or recomputing from scratch. This persistent, interactive environment turns what would otherwise be a slow, rigid loop into a fluid, exploratory workflow — perfectly suited to the needs of ML and AI practitioners.</p><p>Because ML and AI development is computationally intensive, stochastic, and data- and model-centric, tools that optimize iteration speed must treat state management as a first-class design concern. Any system aiming to improve the development experience in this domain must therefore enable quick, incremental experimentation without losing continuity between iterations.</p><h3>New: rapid, iterative development with spin</h3><p>At first glance, Metaflow code looks like a workflow — similar to <a href="https://airflow.apache.org/">Airflow</a> — but there’s another way to look at it: each Metaflow @step serves as <a href="https://docs.metaflow.org/metaflow/basics#what-should-be-a-step">a checkpoint boundary</a>. At the end of every step, Metaflow automatically persists all instance variables as <em>artifacts</em>, allowing the execution to <a href="https://docs.metaflow.org/metaflow/debugging#how-to-use-the-resume-command">resume</a> seamlessly from that point onward. The below animation shows this behavior in action:</p><figure><img alt="An animated GIF showing how resume can be used in Metaflow. The GIF shows how using `flow.py resume join` makes Metaflow clone previously executed steps and resumes the computation from the `join` step and continues executing till the end of the flow." src="https://cdn-images-1.medium.com/max/1024/1*AEDpnt-YULYV4mcyrwLk7g.gif" /><figcaption>Using resume in Metaflow</figcaption></figure><p>In a sense, we can consider a @step similar to a notebook cell: it is the smallest unit of execution that updates state upon completion. It does have a few differences that address the issues with notebook cells:</p><ul><li><strong>The execution order is explicit and deterministic: </strong>no surprises due to out-of-order cell execution;</li><li><strong>The state is not hidden: </strong>state is explicitly stored as self. variables as shared state, which can be <a href="https://docs.metaflow.org/metaflow/client">discovered and inspected</a>;</li><li><strong>The state is versioned and persisted</strong> making results more reproducible.</li></ul><p>While <strong>Metaflow</strong>’s resume feature can approximate the incremental and iterative development approach of notebooks, it restarts execution from the selected step onward, introducing more latency between iterations. In contrast, a <strong>notebook</strong> allows near-instant feedback by letting users tweak and rerun individual cells while seamlessly reusing data from earlier cells held in memory.</p><p>The new spin command in Metaflow 2.19 addresses this gap. Similar to executing a single notebook cell, it quickly executes a single Metaflow @step — with all the state carried over from the parent step. As a result, users can develop and debug Metaflow steps as easily as a cell in a notebook.</p><p>The effect becomes clear when considering the three complementary execution modes — run, resume, and spin — side by side, mapping them to the corresponding notebook behavior:</p><figure><img alt="Diagram showing the various modes of execution in Metaflow: Run, Resume and Spin" src="https://cdn-images-1.medium.com/max/1024/1*DgRIxOu-7keiFoHia9JMrg.png" /><figcaption>Run, Resume and Spin “modes”</figcaption></figure><p>Another major difference isn’t just what gets executed, but what gets recorded. Both run and resume create a full, versioned run with complete metadata and artifacts, while spin skips tracking altogether. It’s built for fast, throw-away iterations during development.</p><p>The one-minute clip below illustrates a typical iterative development workflow that alternates between run and spin. In this example, we are building a flow that reads a dataset from a Parquet file and trains a separate model for each product category, focusing on computer-related categories.</p><iframe src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2F3RNMM-lthm0%3Ffeature%3Doembed&amp;display_name=YouTube&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3D3RNMM-lthm0&amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2F3RNMM-lthm0%2Fhqdefault.jpg&amp;type=text%2Fhtml&amp;schema=youtube" width="854" height="480" frameborder="0" scrolling="no"><a href="https://medium.com/media/36b60cb7c79bf5c77ddd1f60cc97ae8b/href">https://medium.com/media/36b60cb7c79bf5c77ddd1f60cc97ae8b/href</a></iframe><p>As shown in the video, we start by creating a flow from scratch and running a minimal version of it to persist test artifacts — in this case, a Parquet dataset. From there, we can use spin to iterate on one step at a time, incrementally building out the flow, for example, by adding the parallel training steps demonstrated in the clip.</p><p>Once the flow has been iterated on locally, it can be seamlessly deployed to production orchestrators like Maestro or <a href="https://docs.metaflow.org/production/scheduling-metaflow-flows/scheduling-with-argo-workflows">Argo</a>, and <a href="https://docs.metaflow.org/scaling/remote-tasks/requesting-resources">scaled up</a> on compute platforms such as AWS Batch, Titus, Kubernetes and more. Thus, the experience is as smooth as developing in a notebook, but the outcome is a production-ready, scalable workflow, implemented as an idiomatic Python project!</p><h3>Spin up smooth development in VSCode/Cursor</h3><p>Instead of typing run and spin manually in the terminal, we can bind them to keyboard shortcuts. For example, <a href="https://github.com/outerbounds/metaflow-dev-vscode">the simple metaflow-dev VS Code extension</a> (works with Cursor as well) maps Ctrl+Opt+R to run and Ctrl+Opt+S to spin. Just hack away, hit Ctrl+Opt+S, and the extension will save your file and spin the step you are currently editing.</p><p>One area where spin truly shines is in creating mini-dashboards and reports with <a href="https://docs.metaflow.org/metaflow/visualizing-results">Metaflow Cards</a>. Visualization is another strong point of notebooks but the combination of spin and cards makes Metaflow a very compelling alternative for developing real-time and post-execution visualizations. Developing cards is inherently iterative and visual (much like building web pages) where you want to tweak code and see the results instantly. This workflow is readily available with the combination of VSCode/Cursor, which includes a built-in web-view, <a href="https://docs.metaflow.org/metaflow/visualizing-results/effortless-task-inspection-with-default-cards#using-local-card-viewer">the local card viewer</a>, and spin.</p><p>To see the trio of tools — along with the VS Code extension — in action, in this short clip we add observability to the train step that we built in the earlier example:</p><iframe src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FhoRO5eePjqo%3Ffeature%3Doembed&amp;display_name=YouTube&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DhoRO5eePjqo&amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2FhoRO5eePjqo%2Fhqdefault.jpg&amp;type=text%2Fhtml&amp;schema=youtube" width="854" height="480" frameborder="0" scrolling="no"><a href="https://medium.com/media/e2596ec88a7e65b4bad1b6ba7a2a167c/href">https://medium.com/media/e2596ec88a7e65b4bad1b6ba7a2a167c/href</a></iframe><p>A major benefit of Metaflow Cards is that we don’t need to deploy any extra services, data streams, and databases for observability. Just develop visual outputs as above, deploy the flow, and wehave a complete system in production with reporting and visualizations included.</p><h3>Spin to the next level: injecting inputs, inspecting outputs</h3><p>Spin does more than just run code — it also lets us take full control of a spun @step’s inputs and outputs, enabling a range of advanced patterns.</p><p>In contrast to notebooks, we can spin any arbitrary @step in a flow using state from any past run, making it easy to test functions with different inputs. For example, if we have multiple models produced by separate runs, we could spin an inference step, supplying a different model run each time.</p><p>We can also override artifact values or inject arbitrary Python objects — similar to a notebook cell — for spin. Simply specify a Python module with an ARTIFACTS dictionary:</p><pre>ARTIFACTS = {<br>  &quot;model&quot;: &quot;kmeans&quot;,<br>  &quot;k&quot;: 15<br>}</pre><p>and point spin at the module:</p><pre>spin train --artifacts-module artifacts.py</pre><p>By default spin doesn’t persist artifacts, but we can easily change this by adding --persist. Even in this case, artifacts are not persisted in the usual Metaflow datastore but to a directory-specific location which you can easily clean up after testing. We can access the results with <a href="https://docs.metaflow.org/metaflow/client">the Client API</a> as usual — just specify the directory you want to inspect with inspect_spin:</p><pre>from metaflow import inspect_spin<br><br>inspect_spin(&quot;.&quot;)<br>Flow(&quot;TrainingFlow&quot;).latest_run[&quot;train&quot;].task[&quot;model&quot;].data</pre><p>Being able to inspect and modify a step’s inputs and outputs on the fly unlocks a powerful use case:<strong> unit testing individual steps</strong>. We can use spin programmatically through <a href="https://docs.metaflow.org/metaflow/managing-flows/runner">the Runner API</a> and assert the results:</p><pre>from metaflow import Runner<br><br>with Runner(&quot;flow.py&quot;).spin(&quot;train&quot;, persist=True) as spin:<br>  assert spin.task[&quot;model&quot;].data == &quot;kmeans&quot;</pre><h3>Making AI agents spin</h3><p>In addition to speeding up development for humans, spin turns out to be surprisingly handy for coding agents too. There are two major advantages to teaching AI how to spin:</p><ol><li><strong>It accelerates the development loop</strong>. Agents don’t naturally understand what’s slow, or why speed matters, so they need to be nudged to favor faster tools over slower ones.</li><li><strong>It helps surface errors faster </strong>and contextualizes them to a specific piece of code, increasing the chance that the agent is able to fix errors by itself.</li></ol><p>Metaflow users are already <a href="https://claude.com/product/claude-code">using</a> Claude Code; spin makes this even easier. In the example below, we added the following section in a CLAUDE.md file:</p><pre>## Developing Metaflow code<br>Follow this incremental development workflow that ensures quick iterations<br>and correct results. You must create a flow incrementally, step by step<br>following this process:<br>1. Create a flow skeleton with empty `@step`s.<br>2. Add a data loading step.<br>3. `run` the flow.<br>4. Populate the next step and use `spin` to test it with the correct inputs.<br>5. `run` the flow to record outputs from the new step.<br>5. Iterate on (4–5) until all steps have been implemented and work correctly.<br>6. `run` the whole flow to ensure final correctness.<br><br>To test a flow, run the flow as follows<br>```<br>python flow.py - environment=pypi run<br>```<br><br>Do this once before running `spin`.<br>As you are building the flow, you `spin` to test steps quickly.<br>For instance<br>```<br>python flow.py - environment=pypi spin train<br>```</pre><p>Just based on these quick instructions, the agent is able to use spin effectively. Take a look at the following inspirational example that one-shots Claude to create a flow, along the lines of our earlier examples, which trains a classifier to predict product categories:</p><iframe src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FkcCkLXernR0%3Ffeature%3Doembed&amp;display_name=YouTube&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DkcCkLXernR0&amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2FkcCkLXernR0%2Fhqdefault.jpg&amp;type=text%2Fhtml&amp;schema=youtube" width="854" height="480" frameborder="0" scrolling="no"><a href="https://medium.com/media/706e83950e73a05ca41b4fb463702690/href">https://medium.com/media/706e83950e73a05ca41b4fb463702690/href</a></iframe><p>In the video, we can see Claude using spin around the 45-second mark to test a preprocess step. The step initially fails due to a classic data science pitfall: during testing, Claude samples only a small subset of data, causing some classes to be underrepresented. The first spin surfaces the issue, which Claude then fixes by switching to stratified sampling — and finally does another spin to confirm the fix, before proceeding to complete the task.</p><h3>The inner loop of end-to-end ML/AI</h3><p>To circle back to where we started, our motivation for adding spin — and for creating Metaflow in the first place — is to accelerate development cycles so we can deliver more joy to our subscribers, faster. Ultimately, we believe there’s no single magic feature that makes this possible. It takes all parts of an ML/AI platform working together coherently — spin included.</p><p>From this perspective, it’s useful to place spin in the context of other Metaflow features. It’s designed for the innermost loop of model and business-logic development, with the added benefit of supporting unit testing during deployment, as shown in the overall blueprint of the Metaflow toolchain below.</p><figure><img alt="Metaflow tool-chain." src="https://cdn-images-1.medium.com/max/1024/1*9cd6SHFrW7A4iWMZHYkVkw.png" /><figcaption>Metaflow tool-chain</figcaption></figure><p>In this diagram, the solid blue boxes represent different Metaflow commands, while the blue text denotes decorators and other features. In particular, note the <em>Shared Functionality</em> box — another key focus area for us over the past year — which includes <a href="https://netflixtechblog.com/introducing-configurable-metaflow-d2fb8e9ba1c6">configuration management</a> and <a href="https://docs.metaflow.org/metaflow/composing-flows/introduction">custom decorators</a>. These capabilities let domain-specific teams and platform providers tailor Metaflow to their own use cases. Following our ethos of composability, all of these features integrate seamlessly with spin as well.</p><p>Another key design philosophy of Metaflow is to let projects start small and simple, adding complexity only when it becomes necessary. So don’t be overwhelmed by the diagram above. To get started, install Metaflow easily with</p><pre>pip install metaflow</pre><p>and take your first baby @steps for a spin! Check out the <a href="https://docs.metaflow.org/metaflow/authoring-flows/introduction">docs</a> and for questions, support, and feedback, join the friendly <a href="http://chat.metaflow.org">Metaflow Community Slack</a>.</p><h3>Acknowledgments</h3><p>We would like to thank our partners at <a href="https://outerbounds.com">Outerbounds</a>, and particularly <a href="https://www.linkedin.com/in/villetuulos/">Ville Tuulos</a>, <a href="https://www.linkedin.com/in/savingoyal/">Savin Goyal</a>, and <a href="https://www.linkedin.com/in/madhur-tandon/">Madhur Tandon</a>, for their collaboration on this feature, from initial ideation to review, testing and documentation. We would also like to acknowledge the rest of the Model Development and Management team (<a href="https://www.linkedin.com/in/maria-alder/">Maria Alder</a>, <a href="https://www.linkedin.com/in/david-j-berg/">David J. Berg</a>, <a href="https://www.linkedin.com/in/shaojingli/">Shaojing Li</a>, <a href="https://www.linkedin.com/in/rui-lin-483a83111/">Rui Lin</a>, <a href="https://www.linkedin.com/in/nissanpow/">Nissan Pow</a>, <a href="https://www.linkedin.com/in/chaoying-wang/">Chaoying Wang</a>, <a href="https://www.linkedin.com/in/reginalw/">Regina Wang</a>, <a href="https://www.linkedin.com/in/shuishiyang/">Seth Yang</a>, <a href="https://www.linkedin.com/in/zitingyu/">Darin Yu</a>) for their input and comments.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=b2d5b95c63eb" width="1" height="1" alt=""><hr><p><a href="https://netflixtechblog.com/supercharging-the-ml-and-ai-development-experience-at-netflix-b2d5b95c63eb">Supercharging the ML and AI Development Experience at Netflix</a> was originally published in <a href="https://netflixtechblog.com">Netflix TechBlog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Post-Training Generative Recommenders with Advantage-Weighted Supervised Finetuning]]></title>
            <link>https://netflixtechblog.com/post-training-generative-recommenders-with-advantage-weighted-supervised-finetuning-61a538d717a9?source=rss----2615bd06b42e---4</link>
            <guid isPermaLink="false">https://medium.com/p/61a538d717a9</guid>
            <category><![CDATA[reinforcement-learning]]></category>
            <category><![CDATA[recommendations]]></category>
            <category><![CDATA[ai]]></category>
            <category><![CDATA[large-language-models]]></category>
            <dc:creator><![CDATA[Netflix Technology Blog]]></dc:creator>
            <pubDate>Sat, 25 Oct 2025 22:01:00 GMT</pubDate>
            <atom:updated>2025-10-25T22:00:58.801Z</atom:updated>
            <content:encoded><![CDATA[<p>Author: <a href="https://keertanavc.github.io/">Keertana Chidambaram</a>, <a href="https://www.linkedin.com/in/qiuling-xu-a445b815a">Qiuling Xu</a>, <a href="https://www.linkedin.com/in/markhsiao/">Ko-Jen Hsiao</a>, <a href="https://www.linkedin.com/in/moumitab/">Moumita Bhattacharya</a></p><p>(*The work was done when Keertana interned at Netflix.)</p><h3>Introduction</h3><p>This blog focuses on post-training generative recommender systems. Generative recommenders (GRs) represent a new paradigm in the field of recommendation systems (e.g. <a href="https://github.com/meta-recsys/generative-recommenders">HSTU</a>, <a href="https://arxiv.org/abs/2502.18965">OneRec</a>). These models draw inspiration from recent advancements in transformer architectures used for language and vision tasks. They approach the recommendation problem, including both ranking and retrieval, as a sequential transduction task. This perspective enables generative training, where the model learns by imitating the next event in a sequence of user activities, thereby effectively modeling user behavior over time.</p><p>However, a key challenge with simply replicating observed user patterns is that it may not always lead to the best possible recommendations. User interactions are influenced by a variety of factors — such as trends, or external suggestions — and the system’s view of these interactions is inherently limited. For example, if a user tries a popular show but later indicates it wasn’t a good fit, a model that only imitates this behavior might continue to recommend similar content, missing the chance to enhance the user’s experience.</p><p>This highlights the importance of incorporating user preferences and feedback, rather than solely relying on observed behavior, to improve recommendation quality. In the context of recommendation systems, we benefit from a wealth of user feedback, which includes explicit signals such as ratings and reviews, as well as implicit signals like watch time, click-through rates, and overall engagement. This abundance of feedback serves as a valuable resource for improving model performance.</p><p>Given the recent success of reinforcement learning techniques in post-training large language models, such as DPO and GRPO, this study investigates whether similar methods can be applied to generative recommenders. Ultimately, our goal is to identify both the opportunities and challenges in using these techniques to enhance the quality and relevance of recommendations.</p><p>Unlike language models, post-training generative recommenders presents unique challenges. One of the most significant is the difficulty of obtaining counterfactual feedback in recommendation scenarios. The recommendation feedback is generated on-policy — that is, it reflects users’ real-time interactions with the system as they naturally use it. Since a typical user sequence can span weeks or even years of activity, it is impractical to ask users to review or provide feedback on hypothetical, counterfactual experiences. As a result, the absence of counterfactual data makes it challenging to apply post-training methods such as PPO or DPO, which require feedback from counterfactual user sequences.</p><p>Furthermore, post-training methods typically rely on a reward model — either implicit or explicit — to guide optimization. The quality of reward models heavily influences the effectiveness of post-training. In the context of recommendation systems, however, reward signals tend to be much noisier. For instance, if we use watch time as an implicit reward, it may not always accurately reflect user satisfaction: a viewer might stop watching a favorite show simply due to time constraints, while finishing a lengthy show doesn’t necessarily indicate genuine enjoyment.</p><p>To address these post-training challenges, we introduce a novel algorithm called Advantage-Weighted Supervised Fine-tuning (A-SFT). Our analysis first demonstrates that reward models in recommendation systems often exhibit higher uncertainty due to the issues discussed above. Rather than relying solely on these uncertain reward models, A-SFT combines supervised fine-tuning with the advantage function to more effectively guide post-training optimization. This approach proves especially effective when the reward model has high variance but still provides valuable directional signals. We benchmark A-SFT against four other representative methods, and our results show that A-SFT achieves better alignment between the pre-trained generative recommendation model and the reward model.</p><p>In Figure 1, we conceptualize the pros and cons of different post-training paradigms. For example, Online Reinforcement Learning is most useful when the reward model has a good generalization ability, and behavior cloning is suitable when no reward models are available. Using these algorithms under fitting use cases is the key to a successful post-training. For example, over-exploitation of noisy reward models will hurt task performance, as guidance from the reward models can be simply noise. Conversely, not leveraging a good reward model leaves out potential improvements. We find A-SFT fits the sweet point between offline reinforcement learning and behavior cloning, where it benefits from the directional signals in those noisy estimations and is less dependent on the reward accuracy.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*N9QNNLspEJJCxQjtNl9HZw.png" /></figure><p>Figure 1: The landscape of RL algorithms based on the reward models’ accuracy</p><h3>Challenges in Post-training for Recommendation</h3><p>Reinforcement Learning from Human Feedback (RLHF) is the most popular framework for post-training large language models. In this framework, human annotators evaluate and rank different outputs generated by a model. This feedback is then used to train a reward model that predicts how well a model output aligns with human preferences. This reward model then serves as a proxy for human judgment during reinforcement learning, guiding the model to generate outputs that are more likely to be preferred by humans.</p><p>While traditional RLHF methods like PPO or DPO are effective for aligning LLMs, there are several challenges in applying them directly to large-scale recommendation systems:</p><ol><li>Lack of Counter-factual Observations</li></ol><p>As in typical RLHF settings, collecting real-time feedback from a diverse user base across a wide range of items is both costly and impractical. The data in recommendation are generated by the real-time user interests. Any third-party annotators or even the user themselves lack the practical means to evaluate an alternative reality. For example, it is impractical to ask the Netflix users to evaluate hundreds of unseen movies. Consequently, we lack a live environment in which to perform reinforcement learning.</p><p>2. Noisy Reward Models</p><p>In addition to the limited counter-factual data, the recommendation task itself has a higher randomness by its nature. The recommendation data has less structure than language data. Users choose to watch some shows not because there is a grammar rule that nouns need to follow by the verbs. In fact, the users’ choices usually exhibit a level of permutation invariance, where swapping the order of events in the user history still makes a valid activity sequence. This randomness in the behaviors makes learning a good reward model extremely difficult. Often the reward models we learnt still have a large margin of errors.</p><p>Here is an ablation study we did on the reward model performance with O(Millions) users and O(Billions) of tokens. The reward model uses an open-sourced HSTU architecture in the convenience of reproducing this study. We adopt the standard RLHF approach of training a reward model using offline, human-collected feedback. We start by creating a proxy reward, scored on a scale from 1 to 5 in the convenience of understanding. This reward model is co-trained as a shallow reward head on top of the generative recommender. It predicts the reward for the most recently selected title based on a user’s interaction history. To evaluate its effectiveness, we compare the model’s performance against two simple baselines: (1) predicting the next reward as the average reward the user has given in their past interactions, and (2) predicting it as the average reward that all users have assigned to that particular title in previous interactions.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*tmYdEXqVjSkne__-k6gSig.png" /></figure><p>Table 1: Reward model performance metrics</p><p>We observe that the model’s predictions do not significantly outperform the simple baselines. This result is intuitive, as a user’s historical interactions typically cover only a small subset of titles, making it difficult to accurately predict their responses to the vast number of unexplored titles in the catalogue. We expect this to be a potential issue for any large recommendation systems where the ratio between explored and unexplored titles is very small.</p><p>3. Lack of Logged Policy</p><p>In recommendation systems, the policy that generated the logged data is typically unknown and cannot be directly estimated. Offline reinforcement learning methods often rely on Inverse Propensity Scoring (IPS) to debias such data by reweighting interactions according to the logging policy’s action probabilities. However, estimating the logging policy accurately is challenging and prone to error, which can introduce additional biases, and IPS itself is known to suffer from high variance. Consequently, offline RL approaches that depend on IPS are ill-suited for our setting.</p><h3>Advantage Weighted Supervised Fine Tuning</h3><p>Given the three challenges outlined above, we propose a new algorithm Advantage-Weighted SFT (A-SFT). It leverages a combination of supervised fine-tuning and advantage reweighting from reinforcement learning. The key observation is as follows. Despite the reward estimation for each individual event having a high uncertainty, we find the estimations of rewards contain directional signals between high-reward and low-reward events. These signals could help better align the model during post-training.</p><p>A central factor in this study is the generalization ability of the reward model. Better generalization enables more accurate predictions of user preferences for unseen titles, thereby making exploration more effective. For reward models with moderate to high generalization power, both online RL methods such as PPO and offline RL methods such as CQL can perform effectively. However, in our setting, reward model generalization is worse than the language counterparts’, which makes these algorithms less appropriate. In addition, the use of techniques like inverse propensity scoring (IPS) introduces a heightened risk of high-variance estimates, prompting us to exclude algorithms such as off-policy REINFORCE.</p><p>Our proposed method A-SFT does not rely on IPS. With no need of prior knowledge of the logging policy, it can be generally applied to cases where observation of the environments are limited or biased. This is particularly useful to the recommendation setting due to the user feedback loop and distribution shifts with time. Without knowing the logging policy, A-SFT still provides means to control the policy deviation between the current policy and logging policy by tuning the parameter. This design provides essential means to control the learnt bias from uncertain reward models. We show that A-SFT outperforms baseline behavior cloning by directly optimizing observed rewards.</p><p>The advantage-weighted SFT algorithm is as follows:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*eP-_FyLRs6vrGnwyIp_j_A.png" /></figure><p>For the results presented in this blog post, we treat the recommendation problem as a contextual bandit, i.e. given a history of user interactions as the context, can we recommend a high reward next title recommendation for the user?</p><h3>Benchmarks</h3><p>We compared representative algorithms including PPO, IPO, DPO, CQL and SFT as the baselines:</p><ol><li><strong>Reward weighted Behavior Cloning</strong>: This benchmark algorithm modifies supervised fine-tuning (SFT) by weighting the loss with the raw rewards of the chosen item instead of weighing the loss with advantage as in the proposed algorithm.</li><li><strong>Rejection Sampling Direct Preference Optimization / Identity Preference Optimization (RS DPO/IPO)</strong>: this is a variant of DPO/IPO where, for each user history x, ​we generate contrasting response pairs by training an ensemble of reward models to estimate confidence intervals for the reward of multiple potential responses y. If the lower bound of the reward confidence interval for one response​ is less than the upper bound for another response, then this pair is used to train DPO/IPO.</li><li><strong>Conservative Q-Learning (CQL)</strong>: This is a standard offline algorithm that learns a conservative Q function, penalizing overestimation of Q-values, particularly in regions of the state-action space with little or no reward data.</li><li><strong>Proximal Policy Optimization (PPO)</strong>: This is a standard RLHF (Reinforcement Learning from Human Feedback) algorithm that uses reward models as an online environment. PPO learns an advantage function and optimizes the policy to maximize expected reward while maintaining proximity to the initial policy.</li></ol><p>We sampled a separate test set of O(Millions) users. This test set is collected on a future date after the training.</p><h3>Offline Evaluation Results</h3><p>We evaluate our algorithm on a dataset of high-reward user trajectories. For sake of simplicity, we consider a trajectory to have a high reward if the accumulated reward is higher than the median of the population. We present the following metrics for the held out test dataset:</p><ol><li><strong>NDCG@k</strong>: This measures the ranking quality of the recommended items up to position k. It accounts for the position of relevant items in the recommendation list, assigning higher scores when relevant items appear higher in the ranking. The gain is discounted logarithmically at lower ranks, and the result is normalized by the ideal ranking (i.e., the best possible ordering of items).</li><li><strong>HR@k</strong>: This measures the proportion of test cases in which the ground-truth chosen item y appears in the top k recommendations. It is a binary metric per test case (hit or miss) and is averaged over all test cases.</li><li><strong>MRR</strong>: MRR evaluates the ranking quality by measuring the reciprocal of the rank at which the chosen item appears in the recommendation list. The metric is averaged across all test cases.</li><li><strong>Reward Model as A Judge</strong>: We use the reward model to evaluate the policy for future user events. We propose to use an ensemble of reward models for the evaluation to increase confidence. The result is based on the discounted reward generated for a few steps. The standard deviation is less than 4%.</li></ol><p>We measure the percentage improvement in each metric compared to the baseline, Reward Weighted Behavior Cloning(BC). We notice that advantage weighted SFT shows the largest improvement in metrics, outweighing BC as well as reward model dependent algorithms like CQL, PPO, DPO and IPO.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*Z8wcCOETlobx8T_oVuDRiA.png" /></figure><p>Our experiments show that advantage weighted SFT is a simple but promising approach for post-training generative recommenders as it deals with the issue of poor reward model generalizations and lack of IPS. More specifically, we find PPO, IPO and DPO achieve a good reward score, but also causes the overfitting from the reward model. Conservative Q-Learning achieves more robust improvements but does not fully capture the potential signals in the reward modeling. A-SFT achieved both better recommendation metrics and reward scores.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=61a538d717a9" width="1" height="1" alt=""><hr><p><a href="https://netflixtechblog.com/post-training-generative-recommenders-with-advantage-weighted-supervised-finetuning-61a538d717a9">Post-Training Generative Recommenders with Advantage-Weighted Supervised Finetuning</a> was originally published in <a href="https://netflixtechblog.com">Netflix TechBlog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Behind the Streams: Real-Time Recommendations for Live Events Part 3]]></title>
            <link>https://netflixtechblog.com/behind-the-streams-real-time-recommendations-for-live-events-e027cb313f8f?source=rss----2615bd06b42e---4</link>
            <guid isPermaLink="false">https://medium.com/p/e027cb313f8f</guid>
            <category><![CDATA[live-streaming]]></category>
            <category><![CDATA[netflix]]></category>
            <category><![CDATA[architecture]]></category>
            <category><![CDATA[distributed-systems]]></category>
            <dc:creator><![CDATA[Netflix Technology Blog]]></dc:creator>
            <pubDate>Tue, 21 Oct 2025 00:53:29 GMT</pubDate>
            <atom:updated>2025-10-21T01:01:11.153Z</atom:updated>
            <content:encoded><![CDATA[<p>By: <a href="https://www.linkedin.com/in/krisrange/">Kris Range</a>, <a href="https://www.linkedin.com/in/gulatiankush/">Ankush Gulati</a>, <a href="https://www.linkedin.com/in/jimpisaacs/">Jim Isaacs</a>, <a href="https://www.linkedin.com/in/jennifer-s-0019a516/">Jennifer Shin</a>, <a href="https://www.linkedin.com/in/jeremy-kelly-526a30180/">Jeremy Kelly</a>, <a href="https://www.linkedin.com/in/jason-t-26850b26/">Jason Tu</a></p><p><em>This is part 3 in a series called “Behind the Streams”. Check out </em><a href="https://netflixtechblog.com/behind-the-streams-live-at-netflix-part-1-d23f917c2f40"><em>part 1</em></a><em> and </em><a href="https://netflixtechblog.com/building-a-reliable-cloud-live-streaming-pipeline-for-netflix-8627c608c967"><em>part 2</em></a><em> to learn more.</em></p><p>Picture this: It’s seconds before the biggest fight night in Netflix history. Sixty-five million fans are waiting, devices in hand, hearts pounding. The countdown hits zero. What does it take to get everyone to the action on time, every time? At Netflix, we’re used to on-demand viewing where everyone chooses their own moment. But with live events, millions are eager to join in at once. Our job: make sure our members never miss a beat.</p><p>When Live events break streaming records <a href="https://about.netflix.com/en/news/60-million-households-tuned-in-live-for-jake-paul-vs-mike-tyson">¹</a> <a href="https://about.netflix.com/en/news/netflix-nfl-christmas-gameday-reaches-65-million-us-viewers">²</a> <a href="https://about.netflix.com/en/news/over-41-million-global-viewers-on-netflix-watch-terence-crawford-defeat">³</a>, our infrastructure faces the ultimate stress test. Here’s how we engineered a discovery experience for a global audience excited to see a knockout.</p><h3>Why are Live Events Different?</h3><p>Unlike Video on Demand (VOD), members want to catch live events as they happen. There’s something uniquely exciting about being part of the moment. That means we only have a brief window to recommend a Live event at just the right time. Too early, excitement fades; too late, the moment is missed. Every second counts.</p><p>To capture that excitement, we enhanced our recommendation delivery systems to serve real-time suggestions, providing members richer and more compelling signals to hit play in the moment when it matters most. The challenge? Sending dynamic, timely updates concurrently to over a hundred million devices worldwide without creating a <a href="https://en.wikipedia.org/wiki/Thundering_herd_problem">thundering herd effect</a> that would overwhelm our cloud services. Simply scaling up linearly isn’t efficient and reliable. For popular events, it could also divert resources from other critical services. We needed a smarter and more scalable solution than just adding more resources.</p><h3>Orchestrating the moment: Real-time Recommendations</h3><p>With millions of devices online and live event schedules that can shift in real time, the challenge was to keep everyone perfectly in sync. We set out to solve this by building a system that doesn’t just react, but adapts by dynamically updating recommendations as the event unfolds. We identified the need to balance three constraints:</p><ul><li><strong>Time</strong>: the duration required to coordinate an update.</li><li><strong>Request throughput</strong><em>: </em>the capacity of our cloud services to handle requests.</li><li><strong>Compute cardinality</strong>: the variety of requests necessary to serve a unique update.</li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*-z6A8FriBAbJW5BcwMrZTA.png" /><figcaption>Visualizing constraints for real-time updates</figcaption></figure><p>We solved this constraint optimization problem by splitting the real-time recommendations into two phases: <strong>prefetching</strong> and <strong>real-time broadcasting</strong>. First, we prefetch the necessary data ahead of time, distributing the load over a longer period to avoid traffic spikes. When the Live event starts or ends, we broadcast a low cardinality message to all connected devices, prompting them to use the prefetched data locally. The timing of the broadcast also adapts when event times shift to preserve accuracy with the production of the Live event. By combining these two phases, we’re able to keep our members’ devices in sync and solve the thundering herd problem. To maximize device reach, especially for those with unstable networks, we use “at least once” broadcasts to ensure every device gets the latest updates and can catch up on any previously missed broadcasts as soon as they’re back online.</p><p>The first phase optimizes <strong>request throughput </strong>and <strong>compute cardinality</strong> by prefetching materialized recommendations, displayed title metadata, and artwork for a Live event. As members naturally browse their devices before the event, this data is prepopulated and stored locally in device cache, awaiting the notification trigger to serve the recommendations instantaneously. By distributing these requests naturally over time ahead of the event, we can eliminate any related traffic spikes and avoid the need for large-scale, real-time system scaling.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*QmB99SosLqs-JEo0gp1wwg.png" /><figcaption>A phased approach, smoothing traffic requests over time with a real-time low-cardinality broadcast</figcaption></figure><p>The second phase optimizes<strong> request throughput </strong>and<strong> time </strong>to update<strong> </strong>devices by broadcasting a low-cardinality, real-time message to all connected devices at critical moments in a Live event’s lifecycle. Each broadcast payload includes a <strong>state key</strong> and a <strong>timestamp</strong>. The state key indicates the current stage of the Live event, allowing devices to use their pre-fetched data to update cached responses locally without additional server requests. The timestamp ensures that if a device misses a broadcast due to network issues, it can catch up by replaying missed updates upon reconnecting. This mechanism guarantees devices receive updates at least once, significantly increasing delivery reliability even on unstable networks.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*h6CIrfYnpR24NS5hgvxEGA.png" /><figcaption>A phased approach optimizes each constraint to ensure we can deliver for the big moment!</figcaption></figure><blockquote>Moment in Numbers: During peak load, we have successfully delivered updates at multiple stages of our events to over 100 million devices in under a minute.</blockquote><h3>Under the Hood: How It Works</h3><p>With the big picture in mind, let’s examine how these pieces interact in practice.</p><p>In the diagram below, the Message Producer microservice centralizes all of the business logic. It continuously monitors live events for setup and timing changes. When it detects an update, it schedules broadcasts to be sent at precisely the right moment. The Message Producer also standardizes communication by providing a concise GraphQL schema for both device queries and broadcast payloads.</p><p>Rather than sending broadcasts directly to devices via WebSocket, the Message Producer hands them off to the Message Router. The Message Router is part of a robust two-tier pub/sub architecture built on proven technologies like <a href="https://netflixtechblog.com/pushy-to-the-limit-evolving-netflixs-websocket-proxy-for-the-future-b468bc0ff658">Pushy</a> (our WebSocket proxy), Apache Kafka, and <a href="https://netflixtechblog.com/introducing-netflixs-key-value-data-abstraction-layer-1ea8a0a11b30">Netflix’s KV key-value store</a>. The Message Router tracks subscriptions at the Pushy node granularity, while Pushy nodes map the subscriptions to individual connections, creating a low-latency fanout that minimizes compute and bandwidth requirements.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*Kc1l_Wnc4i08xFA8JnRLCA.png" /></figure><p>Devices interface with our GraphQL <a href="https://netflix.github.io/dgs/">Domain Graph Service (DGS)</a>. These schemas offer multiple query interfaces for prefetching, allowing devices to tailor their requests to the specific experience being presented. Each response adheres to a consistent API that resolves to a map of stage keys, enabling fast lookups and keeping business logic off the device. Our broadcast schema specifies WebSocket connection parameters, the current event stage, and the timestamp of the last broadcast message. When a device receives a broadcast, it injects the payload directly into its cache, triggering an immediate update and re-render of the interface.</p><h3>Balancing the Moment: Throughput Management</h3><p>In addition to building the new technology to support real-time recommendations, we also evaluated our existing systems for potential traffic hotspots. Using high-watermark traffic projections for live events, we generated synthetic traffic to simulate game-day scenarios and observed how our online services handled these bursts. Through this process, several common patterns emerged:</p><p><strong>Breaking the Cache Synchrony</strong></p><p>Our game-day simulations revealed that while our approach mitigated the immediate thundering herd risks driven by member traffic during the events, live events introduced unexpected mini thundering herds in our systems hours before and after the actual events. The surge of members joining just in time for these events led to concentrated cache expirations and recomputations, which created traffic spikes well outside the event window that we did not anticipate. This was not a problem for VOD content because the member traffic patterns are a lot smoother. We found that fixed TTLs caused cache expirations and refresh-traffic spikes to happen all at once. To address this, we added jitter to server and client cache expirations to spread out refreshes and smooth out traffic spikes.</p><p><strong>Adaptive Traffic Prioritization</strong></p><p>While our services already leverage traffic prioritization and partitioning based on factors such as request type and device type, live events introduced a distinct challenge. These events generated brief traffic bursts that were intensely spiky and placed significant strain on our systems. Through simulations, we recognized the need for an additional event-driven layer of traffic management.</p><p>To tackle this, we improved our traffic sharding strategies by using event-based signals. This enabled us to route live event traffic to dedicated clusters with more aggressive scaling policies. We also added a dynamic traffic prioritization ruleset that activates whenever we see high requests per second (RPS) to ensure our systems can handle the surge smoothly. During these peaks, we aggressively deprioritize non-critical server-driven updates so that our systems can devote resources to the most time-sensitive computations. This approach ensures smooth performance and reliability when demand is at its highest.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/729/1*PYhFbFxK5PbOEAnYtTfD4Q.jpeg" /><figcaption>Snapshot of non-critical traffic volume decline (in %) for a member-facing service during a live event — achieved via aggressive de-prioritization</figcaption></figure><h3>Looking Ahead</h3><p>When we set out to build a seamlessly scalable scheduled viewing experience, our goal was to create a dynamic and richer member experience for live content. Popular live events like the Crawford v. Canelo fight and the NFL Christmas games truly put our systems to the test. Along the way, we also uncovered valuable learnings that continue to shape our work. Our attempts to deprioritize traffic to other non-critical services caused unexpected call patterns and spikes in traffic elsewhere. Similarly, in hindsight, we also learned that the high traffic volume from popular events caused excessive non-essential logging and was putting unnecessary pressure on our ingestion pipelines.</p><p>None of this work would have been possible without our stunning colleagues at Netflix who collaborated across multiple functions to architect, build, and test these approaches, ensuring members can easily access events at the right moment: UI Engineering, Cloud Gateway, Data Science &amp; Engineering, Search and Discovery, Evidence Engineering, Member Experience Foundations, Content Promotion and Distribution, Operations and Reliability, Device Playback, Experience and Design and Product Management.</p><p>As Netflix’s content offering expands to include new formats like live titles, free-to-air linear content, and games, we’re excited to build on what we’ve accomplished and look ahead to even more possibilities. Our roadmap includes extending the capabilities we developed for scheduled live viewing to these emerging formats. We’re also focused on enhancing our engineering tooling for greater visibility into operations, message delivery, and error handling to help us continue to deliver the best possible experience for our members.</p><h3>Join Us for What’s Next</h3><p>We’re just scratching the surface of what’s possible as we bring new live experiences to members around the world. If you are looking to solve interesting technical challenges in a <a href="https://jobs.netflix.com/culture">unique culture</a>, then <a href="https://jobs.netflix.com/">apply</a> for a role that captures your curiosity.</p><p><em>Look out for future blog posts in our “Behind the Streams” series, where we’ll explore the systems that ensure viewers can watch live streams once they manage to find and play them.</em></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=e027cb313f8f" width="1" height="1" alt=""><hr><p><a href="https://netflixtechblog.com/behind-the-streams-real-time-recommendations-for-live-events-e027cb313f8f">Behind the Streams: Real-Time Recommendations for Live Events Part 3</a> was originally published in <a href="https://netflixtechblog.com">Netflix TechBlog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[How and Why Netflix Built a Real-Time Distributed Graph: Part 1 — Ingesting and Processing Data…]]></title>
            <link>https://netflixtechblog.com/how-and-why-netflix-built-a-real-time-distributed-graph-part-1-ingesting-and-processing-data-80113e124acc?source=rss----2615bd06b42e---4</link>
            <guid isPermaLink="false">https://medium.com/p/80113e124acc</guid>
            <category><![CDATA[data-engineering]]></category>
            <category><![CDATA[big-data]]></category>
            <category><![CDATA[software-architecture]]></category>
            <dc:creator><![CDATA[Netflix Technology Blog]]></dc:creator>
            <pubDate>Fri, 17 Oct 2025 18:42:37 GMT</pubDate>
            <atom:updated>2025-11-14T20:30:51.607Z</atom:updated>
            <content:encoded><![CDATA[<h3>How and Why Netflix Built a Real-Time Distributed Graph: Part 1 — Ingesting and Processing Data Streams at Internet Scale</h3><p>Authors: <a href="https://www.linkedin.com/in/ataruc/">Adrian Taruc</a> and <a href="https://www.linkedin.com/in/jamesdalydalton/">James Dalton</a></p><p><em>This is the first entry of a multi-part blog series describing how we built a Real-Time Distributed Graph (RDG). In Part 1, we will discuss the motivation for creating the RDG and the architecture of the data processing pipeline that populates it.</em></p><h3>Introduction</h3><p>The Netflix product experience historically consisted of a single core offering: streaming video on demand. Our members logged into the app, browsed, and watched titles such as Stranger Things, Squid Game, and Bridgerton. Although this is still the core of our product, our business has changed significantly over the last few years. For example, we introduced ad-supported plans, live programming events (e.g., <a href="https://www.netflix.com/tudum/articles/jake-paul-vs-mike-tyson-live-release-date-news">Jake Paul vs. Mike Tyson</a> and <a href="https://www.netflix.com/tudum/articles/nfl-games-on-netflix">NFL Christmas Day Games</a>), and <a href="https://about.netflix.com/en/news/let-the-games-begin-a-new-way-to-experience-entertainment-on-mobile">mobile games</a> as part of a Netflix subscription. This evolution of our business has created a new class of problems where we have to analyze member interactions with the app across different business verticals. Let’s walk through a simple example scenario:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*TPFlIvYqGC3L2x1A-KqkyQ.png" /></figure><ol><li>Imagine a Netflix member logging into the app on their smartphone and beginning to watch an episode of Stranger Things.</li><li>Eventually, they decide to watch on a bigger screen, so they log into the app on a smart TV in their home and continue watching the same episode.</li><li>Finally, after completing the episode, they log into the app on their tablet and play the game “Stranger Things: 1984”.</li></ol><p>We want to know that these three activities belong to the same member, despite occurring at different times and across various devices. In a traditional data warehouse, these events would land in at least two different tables and may be processed at different cadences. But in a graph system, they become connected almost instantly. Ultimately, analyzing member interactions in the app across domains empowers Netflix to create more personalized and engaging experiences.</p><p>In the early days of our business expansion, discovering these relationships and contextual insights was extremely difficult. Netflix is famous for adopting a microservices architecture — hundreds of microservices developed and maintained by hundreds of individual teams. Some notable benefits of microservices are:</p><ol><li><strong>Service Decomposition</strong>: The overall platform is separated into smaller services, each responsible for a specific business capability. This modularity allows for independent service development, deployment, and scaling.</li><li><strong>Data Isolation</strong>: Each service manages its own data, reducing interdependencies. This allows teams to choose the most suitable data schemas and storage technologies for their services.</li></ol><p><strong>However, these benefits also led to drawbacks for our data science and engineering partners.</strong> In practice, the separation of business concerns and service development ultimately resulted in a separation of data. Manually stitching data together from our data warehouse and siloed databases was an onerous task for our partners. Our data engineering team recognized we needed a solution to process and store our enormous swath of interconnected data while enabling fast querying to discover insights. Although we could have structured the data in various ways, we ultimately settled on a graph representation. We believe a graph offers key advantages, specifically:</p><ul><li><strong>Relationship-Centric Queries:</strong> Graphs enable fast “hops” across multiple nodes and edges without expensive joins or manual denormalization that would be required in table-based data models.</li><li><strong>Flexibility as Relationships Grow:</strong> As new connections and entities emerge, graphs can quickly adapt without significant schema changes or re-architecture.</li><li><strong>Pattern and Anomaly Detection:</strong> Our stakeholders’ use cases often require identifying hidden relationships, cycles, or groupings in the data — capabilities much more naturally expressed and efficiently executed using graph traversals than siloed point lookups.</li></ul><p>This is why we set out to build a Real-Time Distributed Graph, or “RDG” for short.</p><h3>Ingestion and Processing</h3><p>Three main layers in the system power the RDG:</p><ol><li><strong>Ingestion and Processing</strong> — receive events from disparate upstream data sources and use them to generate graph nodes and edges.</li><li><strong>Storage</strong> — write nodes and edges to persistent data stores.</li><li><strong>Serving</strong> — expose ways for internal clients to query graph nodes and edges.</li></ol><p><strong>The rest of this post will focus on the first layer, while subsequent posts in this blog series will cover the other layers.</strong> The diagram below depicts a high-level overview of the ingestion and processing pipeline:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*Jy0eVxvB-AzFNijNfRb9ZA.png" /></figure><p>Building and updating the RDG in real-time requires continuously processing vast volumes of incoming data. Batch processing systems and traditional data warehouses cannot offer the low latency needed to maintain an up-to-date graph that supports real-time applications. We opted for a stream processing architecture, enabling us to update the graph’s data as events happen, thus minimizing delay and ensuring the system reflects the latest member actions with the Netflix app.</p><h3>Kafka as the Ingestion Backbone</h3><p>Member actions in the Netflix app are published to our API Gateway, which then writes them as records to <a href="https://kafka.apache.org/">Apache Kafka</a> topics. Kafka is the mechanism through which internal data applications can consume these events. It provides durable, replayable streams that downstream processors, such as <a href="https://flink.apache.org/">Apache Flink</a> jobs, can consume in real-time.</p><p>Our team’s applications consume several different Kafka topics, each generating up to roughly <strong>1 million messages per second</strong>. Topic records are encoded in the <a href="https://avro.apache.org/">Apache Avro</a> format, and Avro schemas are persisted in an internal centralized schema registry. In order to strike a balance between maintaining data availability and managing the financial expenses of storage infrastructure, we tailor retention policies for each topic according to its throughput and record size. We also persist topic records to <a href="https://iceberg.apache.org/">Apache Iceberg</a> data warehouse tables, which allows us to backfill data in scenarios where older data is no longer available in the Kafka topics.</p><h3>Processing Data with Apache Flink</h3><p>The event records in the Kafka streams are ingested by Flink jobs. We chose Flink because of its strong capabilities around near-real-time event processing. There is also robust internal platform support for Flink within Netflix, which allows jobs to integrate with Kafka and various storage backends seamlessly. At a high level, the anatomy of an RDG Flink job looks like this:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*0G-yrGzB_ZbgaRlPbextTQ.png" /></figure><p>For the sake of simplicity, the diagram above depicts a basic flow in which a member logs into their Netflix account and begins watching an episode of Stranger Things. Reading the diagram from left to right:</p><ul><li>The actions of logging into the app and watching the Stranger Things episode are ultimately written as events to Kafka topics.</li><li>The Flink job consumes event records from the upstream Kafka topics.</li><li>Next, we have a series of Flink processor functions that:</li></ul><ol><li>Apply filtering and projections to remove noise based on the individual fields that are present — or in some cases, not present — in the events.</li><li>Enrich events with additional metadata, which are stored and accessed by the processor functions via side inputs.</li><li>Transform events into graph primitives — nodes representing entities (e.g., member accounts and show/movie titles), and edges representing relationships or interactions between them. In this example, the diagram only shows a few nodes and an edge to keep things simple. However, in reality, we create and update up to a few dozen different nodes and edges, depending on the member actions that occurred within the Netflix app.</li><li>Buffer, detect, and deduplicate overlapping updates that occur to the same nodes and edges within a small, configurable time window. This step reduces the data throughput we publish downstream. It is implemented using stateful process functions and timers.</li><li>Publish nodes and edges records to <a href="https://netflixtechblog.com/data-mesh-a-data-movement-and-processing-platform-netflix-1288bcab2873">Data Mesh</a>, an abstraction layer that connects data applications and storage systems. We write a total (nodes + edges) of <strong>more than 5 million records per second</strong> to Data Mesh, which handles persisting the records to various data stores that other internal services can query.</li></ol><h3>From One Job to Many: Scaling Flink the Hard Way</h3><p>Initially, we tried having just one Flink job that consumed all the Kafka source topics. However, this quickly became a big operational headache since different topics can have different data volumes and throughputs at different times during the day. Consequently, tuning the monolithic Flink job became extremely difficult — we struggled to find CPU, memory, job parallelism, and checkpointing interval configurations that ensured job stability.</p><p>Instead, we pivoted to having a 1:1 mapping from the Kafka source topic to the consuming Flink job. Although this led to additional operational overhead due to more jobs to develop and deploy, each job has been much simpler to maintain, analyze, and tune.</p><p>Similarly, each node and edge type is written to a separate Kafka topic. This means we have significantly more Kafka topics to manage. However, we decided the tradeoff of having bespoke tuning and scaling per topic was worth it. We also designed the graph data model to be as generic and flexible as possible, so adding new types of nodes and edges would be an infrequent operation.</p><h3>Acknowledgements</h3><p>We would be remiss if we didn’t give a special shout-out to our stunning colleagues who work on the internal Netflix data platform. Building the RDG was a multi-year effort that required us to design novel solutions, and the investments and foundations from our platform teams were critical to its successful creation. You make the lives of Netflix data engineers much easier, and the RDG would not exist without your diligent collaboration!</p><p>—</p><p>Thanks for reading the first season of the RDG blog series. Check out <a href="https://netflixtechblog.medium.com/how-and-why-netflix-built-a-real-time-distributed-graph-part-2-building-a-scalable-storage-layer-ff4a8dbd3d1f">Part 2</a>, where we go over the storage layer containing the graph’s various nodes and edges.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=80113e124acc" width="1" height="1" alt=""><hr><p><a href="https://netflixtechblog.com/how-and-why-netflix-built-a-real-time-distributed-graph-part-1-ingesting-and-processing-data-80113e124acc">How and Why Netflix Built a Real-Time Distributed Graph: Part 1 — Ingesting and Processing Data…</a> was originally published in <a href="https://netflixtechblog.com">Netflix TechBlog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[100X Faster: How We Supercharged Netflix Maestro’s Workflow Engine]]></title>
            <link>https://netflixtechblog.com/100x-faster-how-we-supercharged-netflix-maestros-workflow-engine-028e9637f041?source=rss----2615bd06b42e---4</link>
            <guid isPermaLink="false">https://medium.com/p/028e9637f041</guid>
            <category><![CDATA[distributed-systems]]></category>
            <category><![CDATA[data]]></category>
            <category><![CDATA[orchestration]]></category>
            <category><![CDATA[machine-learning]]></category>
            <category><![CDATA[workflow]]></category>
            <dc:creator><![CDATA[Netflix Technology Blog]]></dc:creator>
            <pubDate>Mon, 29 Sep 2025 16:10:40 GMT</pubDate>
            <atom:updated>2025-09-29T16:09:08.008Z</atom:updated>
            <content:encoded><![CDATA[<p>By <a href="https://www.linkedin.com/in/jheua/">Jun He</a>, <a href="https://www.linkedin.com/in/yingyi-zhang-a0a164111/">Yingyi Zhang</a>, <a href="https://www.linkedin.com/in/spearsem/">Ely Spears</a></p><h3>TL;DR</h3><p>We recently upgraded the Maestro engine to go beyond scalability and improved its performance by <strong>100X</strong>! The overall overhead is reduced from seconds to milliseconds. We have updated the Maestro open source project with this improvement! Please visit the <a href="https://github.com/Netflix/maestro">Maestro GitHub repository</a> to get started. If you find it useful, please <a href="https://github.com/Netflix/maestro">give us a star</a>.</p><h3>Introduction</h3><p>In our previous <a href="https://netflixtechblog.com/maestro-netflixs-workflow-orchestrator-ee13a06f9c78">blog post</a>, we introduced Maestro as a horizontally scalable workflow orchestrator designed to manage large-scale Data/ML workflows at Netflix. Over the past two and a half years, Maestro has achieved its design goal and successfully supported massive workflows with hundreds of thousands of jobs, managing millions of executions daily. As the adoption of Maestro increases at Netflix, new use cases have emerged, driven by Netflix’s evolving business needs, such as Live, Ads, and Games. To meet these needs, some of the workflows are now scheduled on a sub-hourly basis. Additionally, Maestro is increasingly being used for low-latency use cases, such as ad hoc queries, beyond traditional daily or hourly scheduled ETL data pipeline use cases.</p><p>While Maestro excels in orchestrating various heterogeneous workflows and managing user end-to-end development experiences, users have experienced noticeable speedbumps (i.e. ten seconds overhead) from the Maestro engine during workflow executions and development, affecting overall efficiency and productivity. Although being fully scalable to support Netflix-scale use cases, the processing overhead from Maestro internal engine state transitions and lifecycle activities have become a bottleneck, particularly during development cycles. Users have expressed the need for a high performance workflow engine to support iterative development use cases.</p><p>To visualize our end users’ needs for the workflow orchestrator, we create a 5-layer structure graph shown below. Before the change, Maestro reached level 4 but faced challenges to satisfy the user’s needs in level 5. With the new engine design, Maestro is able to power the users to work with their highest capacity and spark joy for end users during their development over the Maestro.</p><figure><img alt="Figure 1. A 5-layer structure showing needs for the workflow orchestrator" src="https://cdn-images-1.medium.com/max/562/1*q871tK1C7Y8VSAXVOmu4Ig.png" /><figcaption>Figure 1. A 5-layer structure showing needs for the workflow orchestrator.</figcaption></figure><p>In this blog post, we will share our new engine details, explain our design trade-off decisions, and share learnings from this redesign work.</p><h3>Architectural Evolution of Maestro</h3><h4>Before the change</h4><p>To understand the improvements, we will first revisit the original architecture of Maestro to understand why the overhead is high. The system was divided into three main layers, as illustrated in the diagram below. In the sections that follow we will explain each layer and the role it played in our performance optimization.</p><figure><img alt="Figure 2. The architecture diagram before the evolution." src="https://cdn-images-1.medium.com/max/1024/1*198QCvklU8o6aUrDdaBGRA.png" /><figcaption>Figure 2. The architecture diagram before the evolution.</figcaption></figure><p><strong>Maestro API and Step Runtime Layer</strong></p><p>This layer offers seamless integrations with other Netflix services (e.g., compute engines like Spark and Trino). Using Maestro, thousands of practitioners build production workflows using a paved path to access platform services . They can focus primarily on their business logic while relying on Maestro to manage the lifecycle of jobs and workflows plus the integration with data platform services and required integrations such as for authentication, monitoring and alerting. This layer functioned efficiently without introducing significant overhead.</p><p><strong>Maestro Engine Layer</strong></p><p>The Maestro engine serves several crucial functions:</p><ul><li>Managing the lifecycle of workflows, their steps and maintaining their state machines</li><li>Supporting all user actions (e.g., start, restart, stop, pause) on workflow and step entities</li><li>Translating complex Maestro workflow graphs into parallel flows, where each flow is an array of sequentially chained flow tasks, translating every step into a flow task, and then executing transformed flows using the internal flow engine</li><li>Acting as a middle layer to maintain isolation between the Maestro step runtime layer and the underlying flow engine layer</li><li>Implementing required data access patterns and writing Maestro data into the database</li></ul><p>In terms of speed, this layer had acceptable overhead but faced edge cases (e.g. a step might be concurrently executed by two workers at the same time, causing race conditions) due to lacking a strong guarantee from the internal flow engine and the external distributed job queue.</p><p><strong>Maestro Internal Flow Engine Layer</strong></p><p>The Maestro internal flow engine performed <strong>2</strong> primary functions:</p><ul><li>Calling task’s execution functions at a given interval.</li><li>Starting the next tasks in an array of sequential task flows (not a graph), if applicable.</li></ul><p>This foundational layer was based on Netflix OSS Conductor 2.x (<a href="https://github.com/Netflix/conductor/releases/tag/v3.0.0">deprecated since Apr 2021</a>), which requires a dedicated set of separate database tables and distributed job queues.</p><p>The existing implementation of this layer introduces an impactful overhead (e.g. a few seconds to tens of seconds overall delays). The lack of strong guarantees (e.g. exactly once publishing) from this layer leads to race conditions which cause stuck jobs or lost executions.</p><h4>Options to consider</h4><p>We have evaluated three options to address those existing issues:</p><ul><li>Option 1: Implement an internal flow engine optimized for Maestro specific use cases</li><li>Option 2: Upgrade Conductor library to 4.0, which addresses the overheads and offers other improvements and enhancements compared with Conductor 2.X.</li><li>Option 3: Use Temporal as the internal flow engine</li></ul><p>One aspect that influenced our assessment of option two is that Conductor 2 provided a final callback capability in the state machine that was contributed specifically for Maestro’s use case to ensure database synchronization between the Conductor and Maestro engine states. It would require porting this functionality to Conductor 4 though it had been dropped given no other Conductor use cases besides Maestro relied on this. By rewriting the flow engine it would allow removal of several complex internal databases and database synchronization requirements which was attractive for simplifying operational reliability. Given Maestro did not need the full set of state engine features offered by Conductor, this motivated us to consider a flow engine rewrite as a higher priority.</p><p>The decision for Temporal was more straightforward. Temporal is optimized towards facilitating inter-process orchestration and would involve calling an external service to interact with the Temporal flow engine. Given Maestro is operating greater than a million tasks per day, many of which are long running, we felt it was an unnecessary source of risk to couple the DAG engine execution with an external service call. If our requirements went beyond lightweight state transition management we might reconsider because Temporal is a very robust control plane orchestration system, but for our needs it introduced complexity and potential reliability weak spots when there was no direct need for the advanced feature set that it offered.</p><p>After considering Option 2 and Option 3, we developed more conviction that Maestro’s architecture could be greatly simplified by not using a full DAG evaluation engine and having to maintain the state machine for two systems (Maestro and Conductor/Temporal). Therefore, we have decided to go with Option 1.</p><h4>After the change</h4><p>To address these issues, we completely rewrote the Maestro internal flow engine layer to satisfy Maestro’s specific needs and optimize its performance. This new flow engine is lightweight with minimal dependencies, focusing on excelling in the two primary functions mentioned <a href="#4bd7">above</a>. We also replaced existing distributed job queues with internal ones to provide a strong guarantee.</p><p>The new engine is <strong>highly performant, efficient, scalable, and fault-tolerant</strong>. It is the foundation for all upper components of Maestro and provides the following guarantees to avoid race conditions:</p><ul><li>A single step should only be executed by a single worker at any given time</li><li>Step state should never be rolled back</li><li>Steps should always eventually run to a terminal state</li><li>The internal flow state should be eventually consistent with the Maestro workflow state</li><li>External API and user actions should not cause race conditions on the workflow execution</li></ul><p>Here is the new architecture diagram after the change, which is much simpler with less dependencies:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*ZGutO7STwBr81NQ2qAWjYQ.png" /><figcaption>Figure 3. The architecture diagram after the evolution.</figcaption></figure><h3>New Flow Engine Optimization</h3><p>The new flow engine significantly boosts speed by maintaining state in memory. It ensures consistency by using Maestro engine’s database as the source of truth for workflow and step states. During bootstrapping, the flow engine rebuilds its in-memory state from the database, improving performance and simplifying the overall architecture. This is in contrast to the previous design in which multiple databases had to be reconciled against one another (Conductor’s tables and Maestro’s tables) or else suffer race conditions and rare orphaned job status.</p><p>The flow engine operates on in-memory flow states, resembling a <a href="https://docs.aws.amazon.com/whitepapers/latest/database-caching-strategies-using-redis/caching-patterns.html#write-through">write through caching pattern</a>. Updates to workflow or step state in the database also update the in-memory flow state. If in-memory state is lost, the flow engine rebuilds it from the database, ensuring eventual consistency and resolving race conditions.</p><p>This design delivers lower latency and higher throughput, avoids inconsistencies from dual persistence, simplifies the architecture, and keeps the in‑memory view eventually consistent with the database.</p><h4>Maintaining Scalability While Gaining Speed</h4><p>With the new engine, we significantly boost performance by collocating flows and their tasks on the same node throughout their lifecycle. Therefore, states of a flow and its tasks will stay in a single node’s memory without persisting to the database. This stickiness and locality bring great performance benefits but inevitably impact scalability since tasks are no longer reassigned to a new worker of the whole cluster in each polling cycle.</p><p>To maintain horizontal scalability, we introduced a flow group concept to partition running flows into groups. In this way, each Maestro flow engine instance only needs to maintain ownership of groups rather than individual flows, reducing maintenance costs (e.g., heartbeat) and simplifying reconciliation by allowing each Maestro node to load flows for a group in batches. Each Maestro node claims ownership of a group of flows through a flow group actor and manages their entire lifecycle via child flow actors. If ownership is lost due to node failure or long JVM GC, another node can claim the group to resume flow executions by reconciling internal state from Maestro database. The following diagram illustrates the ownership maintenance.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*nvUSa5zxZOWi4G8EIh9bQA.png" /><figcaption>Figure 4. Ownership maintenance sequence diagram.</figcaption></figure><h4>Flow Partitioning</h4><p>To efficiently distribute traffic, Maestro assigns a consistent group ID to flows/workflows by a simple stable ID assignment method, as shown in the diagram’s Partitioning Function box. We chose this simpler partitioning strategy over advanced ones, e.g. consistent hashing, primarily due to execution and reconciliation costs and consistency challenges in a distributed system.</p><p>Since Maestro decomposes workflows into hierarchical internal flows (e.g., foreach), parent flows need to interact with child flows across different groups. To enable this, the maximal group number from the parent, denoted as N’ in the diagram, is passed down to all child flows. This allows child flows, such as subworkflows or foreach iterations, to recompute their own group IDs and also ensures that a parent flow can always determine the group ID of its child flows using only their workflow identifiers.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*PdP7OEWMFDpbNTZZNuKe7Q.png" /><figcaption>Figure 5. Flow group partitioning mechanism diagram.</figcaption></figure><p>After a flow’s group ID is determined, the flow operator routes the flow request to the appropriate node. Each node owns a specific range of group IDs. For example, in the diagram, Node 1 owns groups 0, 1, and 2, while Node 3 owns groups 6, 7, and 8. The groups then contain the individual flows (e.g., Flow A, Flow B).</p><p>In this design, the group size is configurable and nodes can also have different group size configurations. The following diagram shows a flow group partitioning example while the maximal group number is changed during the engine execution without impacting any existing workflows.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*cHk1MpAAcjfKC0GvRL57gA.png" /><figcaption>Figure 6. A flow group partitioning example.</figcaption></figure><p>In short, Maestro flow engine shares the group info across the parent and child workflows to provide a flexible and stable partitioning mechanism to distribute work across the cluster.</p><h4>Queue Optimization</h4><p>We replaced both external distributed job queues in the existing system with internal ones, preserving the same fault‑tolerance and recovery guarantees while reducing latency and boosting throughput.</p><p>For the internal flow engine, the queue is a simple in‑memory Java blocking queue. It requires no persistence and can be rebuilt from Maestro state during reconciliation.</p><p>For the Maestro engine, we implemented a database‑backed in‑memory queue that provides <strong>exactly‑once publishing and at‑least‑once delivery guarantees</strong>, addressing multiple edge cases that previously required manual state correction.</p><p>This design is similar to the<a href="https://docs.aws.amazon.com/prescriptive-guidance/latest/cloud-design-patterns/transactional-outbox.html"> transactional outbox pattern</a>. In the same transaction that updates Maestro tables, a row is inserted into the `maestro_queue` table. Upon transaction commit, the job is immediately pushed to a queue worker on the same node, eliminating polling latency. After successful processing, the worker deletes the row from the database. A periodic sweeper re-enqueues any rows whose timeout has expired, ensuring another worker picks them up if a worker stalls or a node fails.</p><p>This design handles failures cleanly. If the transaction fails, both data and message roll back atomically, no partial publishing. If a worker or node fails after commit, the timeout mechanism ensures the job is retried elsewhere. On restart, a node rebuilds its in‑memory queue from the queue table, providing at-least-once delivery guarantee.</p><p>To enhance scalability and avoid contention across event types, each event type is assigned a `queue_id`. Job messages are then partitioned by `queue_id`, optimizing performance and maintaining system efficiency under high load.</p><h3>From Stateless Worker Model to Stateful Actor Model</h3><p>Maestro previously used a shared-nothing stateless worker model with a polling mechanism. When a task started, its identifier was enqueued to a distributed task queue. A worker from the flow engine would pick the task identifier from the queue, load the complete states of the whole workflow (including the flow itself and every task), execute the task interface method once, write the updated task data back to the database, and put the task back in the queue with a polling delay. The worker would then forget this task and start polling the next one.</p><p>That architecture was simple and horizontally scalable (excluding database scalability considerations), but it had drawbacks. The process introduced considerable overhead due to polling intervals and state loading. The time spent in one polling cycle on distributed queues, loading complete states, and other DB queries was significant.</p><p>As Maestro engine decomposes complex workflow graphs into multiple flows, actions might involve multiple flows spanning multiple polling cycles, adding up to significant overhead (around ten seconds in the worst cases). Also, this design didn’t offer strong execution guarantees mainly because the distributed job queue could only provide at-least-once guarantees. Tasks might be dequeued and dispatched to multiple workers, workers might reset states in certain race conditions, or load stale states of other tasks and make incorrect decisions. For example, after a long garbage-collection pause or network hiccup, two workers can pick up the same task: one sets the task status as completed and then unblocks the downstream steps to move forward. However, the other worker, working off stale state, resets the task status back to running, leaving the whole workflow in a conflicting state.</p><p>In the new design, we developed a stateful actor model, keeping internal states in memory. All tasks of a workflow are collocated in the same Maestro node, providing the best performance as states are in the same JVM.</p><h4>Actor-Based Model</h4><p>The new flow engine fits well into an actor model. We also deliberately designed it to allow sharing certain local states (read-only) between parent, child, and sibling actors. This optimization gains performance benefits without losing thread safety due to Maestro’s use cases. We used Java 21’s virtual thread support to implement it with minimal dependencies.</p><p>The new actor-based flow engine is fully message/event-driven and can take actions immediately when events are received, eliminating polling interval delays. To maintain compatibility with the existing polling-based logic, we developed a wakeup mechanism. This model requires flow actors and their child task actors to be collocated in the same JVM for communication over the in-memory queue. Since the Maestro engine already decomposes large-scale workflow instances into many small flows, each flow has a limited number of tasks that fit well into memory.</p><p>Below is a high-level overview of the Maestro execution flow based on the actor model.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*acx3i0wXvowwOm0t0ufOHA.png" /><figcaption>Figure 7. The high level overview of the Maestro execution.</figcaption></figure><ul><li>When a workflow starts or during reconciliation, the flow engine inserts (if not existing) or loads the Maestro workflow and step instance from the database, transforming it into the internal flow and task state. This state remains in JVM memory until evicted (e.g., when the workflow instance reaches a terminal state).</li><li>A virtual thread is created for each entity (workflow instance or step attempt) as an actor to handle all updates or actions for this entity, ensuring thread safety and eliminating distributed locks and potential race conditions.</li><li>Each virtual thread actor contains an in-memory state, a thread-safe blocking queue, and a state machine to update states, ensuring thread safety and high efficiency.</li><li>Actors are organized hierarchically, with flow actors managing all their task actors. Flow actors and their task actors are kept in the same JVM for locality benefits, with the ability to relocate flow instances to other nodes if needed.</li><li>An event can wake up a virtual thread by pushing a message to the actor’s job queue, enabling Maestro to move toward an event-driven approach alongside the current polling-based approach.</li><li>A reconciliation process transforms the Maestro data model into the internal flow data.</li></ul><h4>Virtual Thread Based Implementation</h4><p>We chose Java virtual threads to implement various actors (e.g. group actors and flow actors), which simplified the actor model implementation. With a smaller amount of code, we developed a fully functional and highly performant event-driven distributed flow engine. Virtual threads fit very well in use cases like state machine transitions within actors. They are lightweight enough to be created in a large number without Out-Of-Memory risks.</p><p>However, virtual threads can potentially deadlock. They’re not suitable for executing user-provided logic or complex step runtime logic that might depend on external libraries or services outside our control. To address this, we separate flow engine execution from task execution logic by adding a separate worker thread pool (not virtual threads) to run actual step runtime business logic like launching containers or making external API calls. Flow/task actors can <a href="https://github.com/Netflix/maestro/blob/main/maestro-flow/src/main/java/com/netflix/maestro/flow/engine/ExecutionContext.java#L96-L100">wait indefinitely for the future of the thread poll executor to complete</a> but don’t perform actual execution, allowing us to benefit from virtual threads while avoiding deadlock issues.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*ulNZD0dkW4Tj50OaQ0e8aw.png" /><figcaption>Figure 8. Virtual thread and worker thread separation.</figcaption></figure><h4>Providing Strong Execution Guarantees</h4><p>To provide strong execution guarantees, we implemented a generation ID-based solution to ensure that a single flow or task is executed by only one actor at any time, with states that never roll back and eventually reach a terminal state.</p><p>When a node claims a new group or a group with an expired heartbeat, it updates the database table row and increments the group generation ID. During node bootstrap, the group actor updates all its owned flows’ generation IDs while rebuilding internal flow states. When creating a new flow, the group actor verifies that the database generation ID matches its in-memory generation ID, otherwise rejecting the creation and reporting a retryable error to the caller. Please check <a href="https://github.com/Netflix/maestro/blob/main/maestro-flow/src/main/java/com/netflix/maestro/flow/dao/MaestroFlowDao.java">the source code</a> for the implementation details.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*UexwsCGLLE-lrzV3HVzCEg.png" /><figcaption>Figure 9. An example sequence diagram showing how generation id provides a strong guarantee.</figcaption></figure><p>Additionally, the new flow engine supports both event-driven execution and polling-based periodic reconciliation. Event-driven support allows us to extend polling intervals for state reconciliation at a very low cost, while polling-based reconciliation relaxes event delivery requirements to at-most-once.</p><h3>Testing, Validation and Rollout</h3><p>Migrating hundreds of thousands of Netflix data processing jobs to a new workflow engine required meticulous planning and execution to avoid data corruption, unexpected traffic patterns, and edge cases that could hinder performance gains. We adopted a principled approach to ensure a smooth transition:</p><ol><li><strong>Realistic Testing:</strong> Our testing mirrored real-world use cases as closely as possible.</li><li><strong>Balanced Approach:</strong> We balanced the need for rapid delivery with comprehensive testing.</li><li><strong>Minimal User Disruption:</strong> The goal was for users to be unaware of the underlying changes.</li><li><strong>Clear Communication:</strong> For cases requiring user involvement, clear communication was provided.</li></ol><h4>Maestro Test Framework</h4><p>To achieve our testing goals, we developed an adaptable testing framework for Maestro. This framework addresses the limitations of static unit and integration tests by providing a more dynamic and comprehensive approach, mimicking organic production traffic. It complements existing tests to instill confidence when rolling out major changes, such as new DAG engines.</p><p>The framework is designed to sample real user workflows, disconnecting business logic from external side effects like data reads or writes. This allows us to run workflow graphs of various shapes and sizes, reflecting the diverse use cases across Netflix. While system integrations are handled through deployment pipeline integration tests, the ability to exercise a wide variety of workflow topologies (e.g., parallel executions, for-each jobs, conditional branching and parameter passing between jobs) was crucial for ensuring the new flow engine’s correctness and performance.</p><p>The prototype workflow for the test framework focuses on auto-testing parameters, involving two main steps:</p><p><strong>1. Caching Production Workflows:</strong></p><ul><li>Successful production instances are queried from a historical Maestro feed table over a specified period.</li><li>Run parameters, initiator, and instance IDs are extracted and organized into an instance data map.</li><li>YAML definitions and subworkflow IDs are pulled from S3 storage.</li><li>Both workflow definitions and instance data are cached on S3 for subsequent steps.</li></ul><p><strong>2. Pushing, Running, and Monitoring Workflows:</strong></p><ul><li>Cached workflow definitions and instance data are loaded.</li><li>Notebook-based jobs are replaced with custom notebooks, and certain job types (e.g., vanilla container runtime jobs, templated data movement jobs) and signal triggers are converted to a special no-op job type or skipped.</li><li>Abstract job types like Write-Audit-Publish are expressed as a single step template but are translated to multiple reified nodes of the DAG when executed. These are auto-translated into several custom notebook job types to replace the generated nodes.</li><li>Workflows and subworkflows are pushed, with only non-subworkflows being run using original production instance information.</li><li>1. In the parent workflow, each sub-workflow is replaced with a special no-op placeholder so that the overall topology is preserved but without executing any side-effects of child workflows and avoid cases using dynamic runtime parameter logic.</li><li>2. Each sub-workflow is then separately treated like a top-level parent workflow not initiated from its parent, to exercise the actual workflow steps of the sub-workflow.</li><li>The custom notebook internally compares all passed parameters for each job.</li><li>Workflow instances are monitored until termination (success or failure).</li><li>An email detailing failed workflow instances is generated.</li></ul><p>Future phases of the test framework aim to expand support for native steps, more templates, Titus and Metaflow workflows, and include more robust signal testing. Further integration with the ecosystem, including dedicated Genie clusters for no-op jobs and DGS for our internal workflow UI feature verification, is also being explored.</p><h4>Rollout Plan</h4><p>Our rollout strategy prioritized minimal user disruption. We determined that an entire workflow, from its root instance, must reside in either the old or new flow engine, preventing mixed operations that could lead to complex failure modes and manual data reconciliation.</p><p>To facilitate this, we established a parallel infrastructure for the new workflow engine and leveraged our orchestrator gateway API to hide any routing or redirection logic from users. This approach provided excellent isolation for managing the migration. Initially, specific workflows could explicitly opt in via a system flag, allowing us to observe their execution and gain confidence. By scaling up traffic to the parallel infrastructure in direct proportion to what was scaled down from the original infrastructure, the dual infrastructure cost increase was negligible.</p><p>Once confident, we transitioned to a percentage-based cutover. In the event of a sustained failure in the new engine, our team could roll back a workflow by removing it from the new engine’s database and restarting it in the original stack. However, one consequence of rollback was that failed workflows had to restart from the beginning, recomputing previously successful steps, to ensure all artifacts were generated from a consistent flow engine.</p><p>Leveraging Maestro’s 10-day workflow timeout, we migrated users without disruption. Existing executions would either complete or time out. Upon restarting (due to failure/timeout) or triggering a new instance (due to success), the workflow would be picked up by the new engine. This effectively allowed us to gradually “drain” traffic from the old engine to the new one with no user involvement.</p><p>While the plan generally proceeded as expected with limited edge cases, we did encounter a few challenges:</p><ul><li><strong>Stuck Workflows:</strong> Around 50 workflows with defunct or incorrect ownership information entered a stuck state. In some cases, a backlog of queued instances behind a stuck instance created a race condition in which a new instance would be started immediately when an old instance was terminated, perpetually keeping the workflow on the old engine. For these, we proactively contacted users to negotiate manual stop-and-restart times, forcing them onto the new engine.</li><li><strong>Configuration Discrepancies:</strong> A significant lesson learned was the importance of meticulous record-keeping and management of parallel infrastructure components. We discovered alerts, system flags, and feature flags configured for one stack but not the other. This led to a failure in a partner team’s system that dynamically rolled out a Python migration by analyzing workflow configurations. The absence of a required feature flag in the new engine stack caused the process to be silently skipped, resulting in incorrect Python version configurations for about 40 workflows. Although quickly remediated, this caused user inconvenience as affected workflows needed to be restarted and verified for no lingering data corruption issues. This issue also highlighted limitations in the testing framework since runtime configuration based on external API calls to the configuration service were not exercised in simulated workflow executions.</li></ul><p>Despite these challenges, the migration was a success. We migrated over 60,000 active workflows generating over a million data processing tasks daily with almost no user involvement. By observing the flow engine’s lifecycle management latency, we validated a reduction in step launch overhead from around 5 seconds to 50 milliseconds. Workflow start overhead (incurred once per each workflow execution) also improved from 200 milliseconds to 50 milliseconds. Aggregating this over a million daily step executions translates to saving approximately 57 days of flow engine overhead per day, leading to a snappier user experience, more timely workflow status for data practitioners and greater overall task throughput for the same infrastructure scale.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/826/1*63TFS2uVWjXxlZGpHhTpoA.jpeg" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/874/1*yxfbCcs1Y2h4dpxJ5hZkDA.jpeg" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/948/1*6Sq7ozdrxqeHI56SvejJkw.png" /></figure><p>We additionally realized significant benefits internally with reduced maintenance effort due to the new flow engine’s simplified set of database components. We were able to delete nearly 40TB of obsolete tables related to the previous stateless flow engine and saw a 90% reduction in internal database query traffic which had previously been a significant source of system alerts for the team.</p><h3>Conclusion</h3><p>The architectural evolution of Maestro represents a significant leap in performance, reducing overhead from seconds to milliseconds. This redesign with a stateful actor model not only enhances speed by 100X but also maintains scalability and reliability, ensuring Maestro continues to meet the diverse needs of Netflix’s data and ML workflows.</p><p>Key takeaways from this evolution include:</p><ul><li><strong>Performance matters:</strong> Even in a system designed for scale, the speed of individual operations significantly impacts user experience and productivity.</li><li><strong>Simplicity wins:</strong> Reducing dependencies and simplifying architecture not only improved performance but also enhanced reliability and maintainability.</li><li><strong>Strong guarantees are essential:</strong> Providing strong execution guarantees eliminates race conditions and edge cases that previously required manual intervention.</li><li><strong>Locality optimizations pay off:</strong> Collocating related flows and tasks in the same JVM dramatically reduces overhead from the Maestro engine.</li><li><strong>Modern language features help:</strong> Java 21’s virtual threads enabled an elegant actor-based implementation with minimal code complexity and dependencies.</li></ul><p>We’re excited to share these improvements with the open-source community and look forward to seeing how Maestro continues to evolve. The performance gains we’ve achieved open new possibilities for low-latency workflow orchestration use cases while continuing to support the massive scale that Netflix and other organizations require.</p><p>Visit the <a href="https://github.com/Netflix/maestro">Maestro GitHub repository</a> to explore these improvements. If you have any questions, thoughts, or comments about Maestro, please feel free to create a <a href="https://github.com/Netflix/maestro/issues">GitHub issue</a> in the Maestro repository. We are eager to hear from you. If you are passionate about solving large scale orchestration problems, please <a href="https://explore.jobs.netflix.net/careers?query=Data%20Platform&amp;Teams=Engineering&amp;domain=netflix.com&amp;sort_by=relevance">join us</a>.</p><h3>Acknowledgements</h3><p>Special thanks to Big Data Orchestration team members for general contributions to Maestro and diligent review, discussion and incident response required to make this project successful: Davis Shepherd, Natallia Dzenisenka, Praneeth Yenugutala, Brittany Truong, Jonathan Indig, Deepak Ramalingam, Binbing Hou, Zhuoran Dong, Victor Dusa, and Gabriel Ikpaetuk — and and internal partners Yun Li and Romain Cledat.</p><p>Thank you to Anoop Panicker and Aravindan Ramkumar from our partner organization that leads Conductor development in Netflix. They helped us understand issues in Conductor 2.X that initially motivated the rearchitecture and helped provide context on later versions of Conductor that defined some of the core trade-offs for the decision to implement a custom DAG engine in Maestro.</p><p>We’d also like to thank our partners on the Data Security &amp; Infrastructure and Engineering Support teams who helped identify and rapidly fix the configuration discrepancy error encountered during production rollout: Amer Hesson, Ye Ji, Sungmin Lee, Brandon Quan, Anmol Khurana, and Manav Garekar.</p><p>A special thanks also goes out to partners from the Data Experience team including Jeff Bothe, Justin Wei, and Andrew Seier. The flow engine speed improvement was actually so dramatic that it broke some integrations with our internal workflow UI that reported state transition durations. Our partners helped us catch and fix UI regressions before they shipped to avoid impact to users.</p><p>We also thank Prashanth Ramdas, Anjali Norwood, Eva Tse, Charles Zhao, Sumukh Shivaprakash, Joey Lynch, Harikrishna Menon, Marcelo Mayworm, Charles Smith and other leaders for their constructive feedback and guidance on the Maestro project.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=028e9637f041" width="1" height="1" alt=""><hr><p><a href="https://netflixtechblog.com/100x-faster-how-we-supercharged-netflix-maestros-workflow-engine-028e9637f041">100X Faster: How We Supercharged Netflix Maestro’s Workflow Engine</a> was originally published in <a href="https://netflixtechblog.com">Netflix TechBlog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Building a Resilient Data Platform with Write-Ahead Log at Netflix]]></title>
            <link>https://netflixtechblog.com/building-a-resilient-data-platform-with-write-ahead-log-at-netflix-127b6712359a?source=rss----2615bd06b42e---4</link>
            <guid isPermaLink="false">https://medium.com/p/127b6712359a</guid>
            <category><![CDATA[message-queue]]></category>
            <category><![CDATA[software-architecture]]></category>
            <category><![CDATA[database]]></category>
            <category><![CDATA[distributed-systems]]></category>
            <category><![CDATA[reliability]]></category>
            <dc:creator><![CDATA[Netflix Technology Blog]]></dc:creator>
            <pubDate>Fri, 26 Sep 2025 18:57:07 GMT</pubDate>
            <atom:updated>2025-09-26T18:57:04.288Z</atom:updated>
            <content:encoded><![CDATA[<p>By <a href="https://www.linkedin.com/in/prudhviraj9">Prudhviraj Karumanchi</a>, <a href="https://www.linkedin.com/in/samuelfu/">Samuel Fu</a>, <a href="https://www.linkedin.com/in/sriram-rangarajan-35169715/">Sriram Rangarajan</a>, <a href="https://www.linkedin.com/in/vidhya-arvind-11908723">Vidhya Arvind</a>, <a href="https://www.linkedin.com/in/yunwang-io/">Yun Wang</a>, <a href="https://www.linkedin.com/in/john-l-693b7915a/">John Lu</a></p><h3>Introduction</h3><p>Netflix operates at a massive scale, serving hundreds of millions of users with diverse content and features. Behind the scenes, ensuring data consistency, reliability, and efficient operations across various services presents a continuous challenge. At the heart of many critical functions lies the concept of a Write-Ahead Log (WAL) abstraction. At Netflix scale, every challenge gets amplified. Some of the key challenges we encountered include:</p><ul><li>Accidental data loss and data corruption in databases</li><li>System entropy across different datastores (e.g., writing to Cassandra and Elasticsearch)</li><li>Handling updates to multiple partitions (e.g., building secondary indices on top of a NoSQL database)</li><li>Data replication (in-region and across regions)</li><li>Reliable retry mechanisms for<strong> </strong>real time data pipeline at scale</li><li>Bulk deletes to database causing OOM on the Key-Value nodes</li></ul><p>All the above challenges either resulted in production incidents or outages, consumed significant engineering resources, or led to bespoke solutions and technical debt. During one particular incident, a developer issued an ALTER TABLE command that led to data corruption. Fortunately, the data was fronted by a cache, so the ability to extend cache TTL quickly together with the app writing the mutations to Kafka allowed us to recover. Absent the resilience features on the application, there would have been permanent data loss. As the data platform team, we needed to provide resilience and guarantees to protect not just this application, but all the critical applications we have at Netflix.</p><p>Regarding the retry mechanisms for real time data pipelines, Netflix operates at a massive scale where failures (network errors, downstream service outages, etc.) are inevitable. We needed a reliable and scalable way to retry failed messages, without sacrificing throughput.</p><p>With these problems in mind, we decided to build a system that would solve all the aforementioned issues and continue to serve the future needs of Netflix in the online data platform space. Our Write-Ahead Log (WAL) is a distributed system that captures data changes, provides strong durability guarantees, and reliably delivers these changes to downstream consumers. This blog post dives into how Netflix is building a generic WAL solution to address common data challenges, enhance developer efficiency, and power high-leverage capabilities like secondary indices, enable cross-region replication for non-replicated storage engines, and support widely used patterns like delayed queues.</p><h3>API</h3><p>Our API is intentionally simple, exposing just the essential parameters. WAL has one main API endpoint, <em>WriteToLog</em>, abstracting away the internal implementation and ensuring that users can onboard easily.</p><pre>rpc WriteToLog (WriteToLogRequest) returns (WriteToLogResponse) {...}<br><br>/**<br>  * WAL request message<br>  * namespace: Identifier for a particular WAL<br>  * lifecycle: How much delay to set and original write time <br>  * payload: Payload of the message<br>  * target: Details of where to send the payload <br>  */<br>message WriteToLogRequest {<br>  string namespace = 1;<br>  Lifecycle lifecycle = 2;<br>  bytes payload = 3;<br>  Target target = 4;<br>}<br><br>/**<br>  * WAL response message<br>  * durable: Whether the request succeeded, failed, or unknown<br>  * message: Reason for failure<br>  */<br>message WriteToLogResponse {<br>  Trilean durable = 1;<br>  string message = 2;<br>}</pre><p>A <em>namespace</em> defines where and how data is stored, providing logical separation while abstracting the underlying storage systems. Each <em>namespace</em> can be configured to use different queues: Kafka, SQS, or combinations of multiple. <em>Namespace</em> also serves as a central configuration of settings, such as backoff multiplier or maximum number of retry attempts, and more. This flexibility allows our Data Platform to route different use cases to the most suitable storage system based on performance, durability, and consistency needs.</p><p>WAL can assume different <em>personas</em> depending on the namespace configuration.</p><h4><strong>Persona #1 (<em>Delayed Queues</em>)</strong></h4><p>In the example configuration below, the Product Data Systems (PDS) <em>namespace</em> uses SQS as the underlying message queue, enabling delayed messages. PDS uses Kafka extensively, and failures (network errors, downstream service outages, etc.) are inevitable. We needed a reliable and scalable way to retry failed messages, without sacrificing throughput. That’s when PDS started leveraging WAL for delayed messages.</p><pre>&quot;persistenceConfigurations&quot;: {<br>  &quot;persistenceConfiguration&quot;: [<br>  {<br>    &quot;physicalStorage&quot;: {<br>      &quot;type&quot;: &quot;SQS&quot;,<br>    },<br>    &quot;config&quot;: {<br>      &quot;wal-queue&quot;: [<br>        &quot;dgwwal-dq-pds&quot;<br>      ],<br>      &quot;wal-dlq-queue&quot;: [<br>        &quot;dgwwal-dlq-pds&quot;<br>      ],<br>      &quot;queue.poll-interval.secs&quot;: 10,<br>      &quot;queue.max-messages-per-poll&quot;: 100<br>    }<br>  }<br>  ]<br>}</pre><h4><strong>Persona #2 (<em>Generic Cross-Region Replication</em>)</strong></h4><p>Below is the namespace configuration for cross-region replication of <a href="https://netflixtechblog.com/caching-for-a-global-netflix-7bcc457012f1">EVCache</a> using WAL, which replicates messages from a source region to multiple destinations. It uses Kafka under the hood.</p><pre>&quot;persistence_configurations&quot;: {<br>  &quot;persistence_configuration&quot;: [<br>  {<br>    &quot;physical_storage&quot;: {<br>      &quot;type&quot;: &quot;KAFKA&quot;<br>    },<br>    &quot;config&quot;: {<br>      &quot;consumer_stack&quot;: &quot;consumer&quot;,<br>      &quot;context&quot;: &quot;This is for cross region replication for evcache_foobar&quot;,<br>      &quot;target&quot;: {<br>        &quot;euwest1&quot;: &quot;dgwwal.foobar.cluster.eu-west-1.netflix.net&quot;,<br>        &quot;type&quot;: &quot;evc-replication&quot;,<br>        &quot;useast1&quot;: &quot;dgwwal.foobar.cluster.us-east-1.netflix.net&quot;,<br>        &quot;useast2&quot;: &quot;dgwwal.foobar.cluster.us-east-2.netflix.net&quot;,<br>        &quot;uswest2&quot;: &quot;dgwwal.foobar.cluster.us-west-2.netflix.net&quot;<br>      },<br>      &quot;wal-kafka-dlq-topics&quot;: [],<br>      &quot;wal-kafka-topics&quot;: [<br>        &quot;evcache_foobar&quot;<br>      ],<br>      &quot;wal.kafka.bootstrap.servers.prefix&quot;: &quot;kafka-foobar&quot;<br>    }<br>  }<br>  ]<br>}</pre><h4><strong>Persona #3 (Handling <em>multi-partition mutations</em>)</strong></h4><p>Below is the namespace configuration for supporting <em>mutateItems</em> API in <a href="https://netflixtechblog.com/introducing-netflixs-key-value-data-abstraction-layer-1ea8a0a11b30">Key-Value</a>, where multiple write requests can go to different partitions and have to be eventually consistent. A key detail in the below configuration is the presence of Kafka and durable_storage. These data stores are required to facilitate two phase commit semantics, which we will discuss in detail below.</p><pre>&quot;persistence_configurations&quot;: {<br>  &quot;persistence_configuration&quot;: [<br>  {<br>    &quot;physical_storage&quot;: {<br>      &quot;type&quot;: &quot;KAFKA&quot;<br>    },<br>    &quot;config&quot;: {<br>      &quot;consumer_stack&quot;: &quot;consumer&quot;,<br>      &quot;contacts&quot;: &quot;unknown&quot;,<br>      &quot;context&quot;: &quot;WAL to support multi-id/namespace mutations for dgwkv.foobar&quot;,<br>      &quot;durable_storage&quot;: {<br>        &quot;namespace&quot;: &quot;foobar_wal_type&quot;,<br>        &quot;shard&quot;: &quot;walfoobar&quot;,<br>        &quot;type&quot;: &quot;kv&quot;<br>      },<br>      &quot;target&quot;: {},<br>      &quot;wal-kafka-dlq-topics&quot;: [<br>        &quot;foobar_kv_multi_id-dlq&quot;<br>      ],<br>      &quot;wal-kafka-topics&quot;: [<br>        &quot;foobar_kv_multi_id&quot;<br>      ],<br>      &quot;wal.kafka.bootstrap.servers.prefix&quot;: &quot;kaas_kafka-dgwwal_foobar7102&quot;<br>    }<br>  }<br>  ]<br>}</pre><p>An important note is that requests to WAL support at-least once semantics due to the underlying implementation.</p><h3>Under the Hood</h3><p>The core architecture consists of several key components working together.</p><p><strong>Message Producer and Message Consumer separation:</strong> The message producer receives incoming messages from client applications and adds them into the queue, while the message consumer processes messages from the queue and sends them to the targets. Because of this separation, other systems can bring their own pluggable producers or consumers, depending on their use cases. WAL’s control plane allows for a pluggable model, which, depending on the use-case, allows us to switch between different message queues.</p><p><strong>SQS and Kafka with a dead letter queue by default</strong>: Every WAL <em>namespace</em> has its own message queue and gets a dead letter queue (DLQ) by default, because there can be transient errors and hard errors. Application teams using <a href="https://netflixtechblog.com/introducing-netflixs-key-value-data-abstraction-layer-1ea8a0a11b30">Key-Value</a> abstraction simply need to toggle a flag to enable WAL and get all this functionality without needing to understand the underlying complexity.</p><ul><li><strong>Kafka-backed namespaces</strong>: handle standard message processing</li><li><strong>SQS-backed namespaces</strong>: support delayed queue semantics (we added custom logic to go beyond the standard defaults enforced in terms of delay, size limits, etc)</li><li><strong>Complex multi-partition scenarios:</strong> use queues and durable storage</li></ul><p><strong>Target Flexibility</strong>: The messages added to WAL are pushed to the target datastores. Targets can be Cassandra databases, Memcached caches, Kafka queues, or upstream applications. Users can specify the target via namespace configuration and in the API itself.</p><figure><img alt="Architecture of WAL" src="https://cdn-images-1.medium.com/max/1024/1*tfnrbP7oD_r9iEesLhACpA.png" /><figcaption>Architecture of WAL</figcaption></figure><h3>Deployment Model</h3><p>WAL is deployed using the <a href="https://netflixtechblog.medium.com/data-gateway-a-platform-for-growing-and-protecting-the-data-tier-f1ed8db8f5c6">Data Gateway infrastructure</a>. This means that WAL deployments automatically come with mTLS, connection management, authentication, runtime and deployment configurations out of the box.</p><p>Each data gateway abstraction (including WAL) is deployed as a <em>shard</em>. A <em>shard</em> is a physical concept describing a group of hardware instances. Each use case of WAL is usually deployed as a separate <em>shard</em>. For example, the Ads Events service will send requests to WAL <em>shard A</em>, while the Gaming Catalog service will send requests to WAL <em>shard </em>B, allowing for separation of concerns and avoiding noisy neighbour problems.</p><p>Each <em>shard</em> of WAL can have multiple <em>namespaces</em>. A <em>namespace</em> is a logical concept describing a configuration. Each request to WAL has to specify its <em>namespace</em> so that WAL can apply the correct configuration to the request. Each <em>namespace</em> has its own configuration of queues to ensure isolation per use case. If the underlying queue of a WAL <em>namespace</em> becomes the bottleneck of throughput, the operators can choose to add more queues on the fly by modifying the <em>namespace</em> configurations. The concept of <em>shards</em> and <em>namespaces</em> is shared across all Data Gateway Abstractions, including <a href="https://netflixtechblog.com/introducing-netflixs-key-value-data-abstraction-layer-1ea8a0a11b30">Key-Value</a>, <a href="https://netflixtechblog.com/netflixs-distributed-counter-abstraction-8d0c45eb66b2">Counter</a>, <a href="https://netflixtechblog.com/introducing-netflix-timeseries-data-abstraction-layer-31552f6326f8">Timeseries</a>, etc. The <em>namespace</em> configurations are stored in a globally replicated Relational SQL database to ensure availability and consistency.</p><figure><img alt="Deployment model of WAL" src="https://cdn-images-1.medium.com/max/1024/1*Gh2O_tTvxZxlbRmKn9Atag.png" /><figcaption>Deployment model of WAL</figcaption></figure><p>Based on certain CPU and network thresholds, the Producer group and the Consumer group of each <em>shard</em> will (separately) automatically scale up the number of instances to ensure the service has low latency, high throughput and high availability. WAL, along with other abstractions, also uses the <a href="https://netflixtechblog.medium.com/performance-under-load-3e6fa9a60581">Netflix adaptive load shedding libraries</a> and Envoy to automatically shed requests beyond a certain limit. WAL can be deployed to multiple regions, so each region will deploy its own group of instances.</p><h3>Solving different flavors of problems with no change to the core architecture</h3><p>The WAL addresses multiple data reliability challenges with no changes to the core architecture:</p><p><strong>Data Loss Prevention:</strong> In case of database downtime, WAL can continue to hold the incoming mutations. When the database becomes available again, replay mutations back to the database. The tradeoff is eventual consistency rather than immediate consistency, and no data loss.</p><p><strong>Generic Data Replication:</strong> For systems like EVCache (using Memcached) and RocksDB that do not support replication by default, WAL provides systematic replication (both in-region and across-region). The target can be another application, another WAL, or another queue — it’s completely pluggable through configuration.</p><p><strong>System Entropy and Multi-Partition Solutions: </strong>Whether dealing with writes across two databases (like Cassandra and Elasticsearch) or mutations across multiple partitions in one database, the solution is the same — write to WAL first, then let the WAL consumer handle the mutations. No more asynchronous repairs needed; WAL handles retries and backoff automatically.</p><p><strong>Data Corruption Recovery:</strong> In case of DB corruptions, restore to the last known good backup, then replay mutations from WAL omitting the offending write/mutation.</p><p>There are some major differences between using WAL and directly using Kafka/SQS. WAL is an abstraction on the underlying queues, so the underlying technology can be swapped out depending on use cases with no code changes. WAL emphasizes an easy yet effective API that saves users from complicated setups and configurations. We leverage the control plane to pivot technologies behind WAL when needed without app or client intervention.</p><h3>WAL usage at Netflix</h3><h4>Delay Queue</h4><p>The most common use case for WAL is as a Delay Queue. If an application is interested in sending a request at a certain time in the future, it can offload its requests to WAL, which guarantees that their requests will land after the specified delay.</p><p>Netflix’s Live Origin processes and delivers Netflix live stream video chunks, storing its video data in a Key-Value <em>abstraction</em> backed by Cassandra and EVCache. When Live Origin decides to delete certain video data after an event is completed, it issues delete requests to the Key-Value abstraction. However, the large amount of delete requests in a short burst interfere with the more important real-time read/write requests, causing performance issues in Cassandra and timeouts for the incoming live traffic. To get around this, Key-Value issues the delete requests to WAL first, with a random delay and jitter set for each delete request. WAL, after the delay, sends the delete requests back to Key-Value. Since the deletes are now a flatter curve of requests over time, Key-Value is then able to send the requests to the datastore with no issues.</p><figure><img alt="Requests being spread out over time through delayed requests" src="https://cdn-images-1.medium.com/max/1024/1*7JV6kc5QMyfviAJIjVXqew.png" /><figcaption>Requests being spread out over time through delayed requests</figcaption></figure><p>Additionally, WAL is used by many services that utilize Kafka to stream events, including Ads, Gaming, Product Data Systems, etc. Whenever Kafka requests fail for any reason, the client apps will send WAL a request to retry the kafka request with a delay. This abstracts away the backoff and retry layer of Kafka for many teams, increasing developer efficiency.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*xnAYzUCsEQ18qyCOnUmuPg.png" /><figcaption>Backoff and delayed retries for clients producing to Kafka</figcaption></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*sbEVgICN9qbEecEPYiSHWQ.png" /><figcaption>Backoff and delayed retries for clients consuming from Kafka</figcaption></figure><h4>Cross-Region Replication</h4><p>WAL is also used for global cross-region replication. The architecture of WAL is generic and allows any datastore/applications to onboard for cross-region replication. Currently, the largest use case is <a href="https://netflixtechblog.com/caching-for-a-global-netflix-7bcc457012f1">EVCache</a>, and we are working to onboard other storage engines.</p><p>EVCache is deployed by clusters of Memcached instances across multiple regions, where each cluster in each region shares the same data. Each region’s client apps will write, read, or delete data from the EVCache cluster of the same region. To ensure global consistency, the EVCache client of one region will replicate write and delete requests to all other regions. To implement this, the EVCache client that originated the request will send the request to a WAL corresponding to the EVCache cluster and region.</p><p>Since the EVCache client acts as the message producer group in this case, WAL only needs to deploy the message consumer groups. From there, the multiple message consumers are set up to each target region. They will read from the Kafka topic, and send the replicated write or delete requests to a Writer group in their target region. The Writer group will then go ahead and replicate the request to the EVCache server in the same region.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*twUIRRILpBEXb085wCdP7A.png" /><figcaption>EVCache Global Cross-Region Replication Implemented through WAL</figcaption></figure><p>The biggest benefits of this approach, compared to our legacy architecture, is being able to migrate from multi-tenant architecture to single tenant architecture for the most latency sensitive applications. For example, Live Origin will have its own dedicated Message Consumer and Writer groups, while a less latency sensitive service can be multi-tenant. This helps us reduce the blast radius of the issues and also prevents noisy neighbor issues.</p><h4>Multi-Table Mutations</h4><p>WAL is used by <a href="https://netflixtechblog.com/introducing-netflixs-key-value-data-abstraction-layer-1ea8a0a11b30">Key-Value</a> service to build the MutateItems API. WAL enables the API’s multi-table and multi-id mutations by implementing 2-phase commit semantics under the hood. For this discussion, we can assume that Key-Value service is backed by Cassandra, and each of its <em>namespaces</em> represents a certain table in a Cassandra DB.</p><p>When a Key-Value client issues a MutateItems request to Key-Value server, the request can contain multiple PutItems or DeleteItems requests. Each of those requests can go to different ids and <em>namespaces</em>, or Cassandra tables.</p><pre>message MutateItemsRequest {<br> repeated MutationRequest mutations = 1;<br> message MutationRequest {<br>  oneof mutation {<br>    PutItemsRequest put = 1;<br>    DeleteItemsRequest delete = 2;<br>  }<br> }<br>}</pre><p>The MutateItems request operates on an eventually consistent model. When the Key-Value server returns a success response, it guarantees that every operation within the MutateItemsRequest will eventually complete successfully. Individual put or delete operations may be partitioned into smaller chunks based on request size, meaning a single operation could spawn multiple chunk requests that must be processed in a specific sequence.</p><p>Two approaches exist to ensure Key-Value client requests achieve success. The synchronous approach involves client-side retries until all mutations complete. However, this method introduces significant challenges; datastores might not natively support transactions and provide no guarantees about the entire request succeeding. Additionally, when more than one replica set is involved in a request, latency occurs in unexpected ways, and the entire request chain must be retried. Also, partial failures in synchronous processing can leave the database in an inconsistent state if some mutations succeed while others fail, requiring complex rollback mechanisms or leaving data integrity compromised. The asynchronous approach was ultimately adopted to address these performance and consistency concerns.</p><p>Given Key-Value’s stateless architecture, the service cannot maintain the mutation success state or guarantee order internally. Instead, it leverages a Write-Ahead Log (WAL) to guarantee mutation completion. For each MutateItems request, Key-Value forwards individual put or delete operations to WAL as they arrive, with each operation tagged with a sequence number to preserve ordering. After transmitting all mutations, Key-Value sends a completion marker indicating the full request has been submitted.</p><p>The WAL producer receives these messages and persists the content, state, and ordering information to a durable storage. The message producer then forwards only the completion marker to the message queue. The message consumer retrieves these markers from the queue and reconstructs the complete mutation set by reading the stored state and content data, ordering operations according to their designated sequence. Failed mutations trigger re-queuing of the completion marker for subsequent retry attempts.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*voSQEpItvosZjPqeTpBseg.png" /><figcaption>Architecture of Multi-Table Mutations through WAL</figcaption></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*EVFnGkr57z5cSB5kNI91iA.png" /><figcaption>Sequence diagram for Multi-Table Mutations through WAL</figcaption></figure><h3>Closing Thoughts</h3><p>Building Netflix’s generic Write-Ahead Log system has taught us several key lessons that guided our design decisions:</p><p><strong>Pluggable Architecture is Core: </strong>The ability to support different targets, whether databases, caches, queues, or upstream applications, through configuration rather than code changes has been fundamental to WAL’s success across diverse use cases.</p><p><strong>Leverage Existing Building Blocks: </strong>We had control plane infrastructure, Key-Value abstractions, and other components already in place. Building on top of these existing abstractions allowed us to focus on the unique challenges WAL needed to solve.</p><p><strong>Separation of Concerns Enables Scale:</strong> By separating message processing from consumption and allowing independent scaling of each component, we can handle traffic surges and failures more gracefully.</p><p><strong>Systems Fail — Consider Tradeoffs Carefully: </strong>WAL itself has failure modes, including traffic surges, slow consumers, and non-transient errors. We use abstractions and operational strategies like data partitioning and backpressure signals to handle these, but the tradeoffs must be understood.</p><h3>Future work</h3><ul><li>We are planning to add secondary indices in Key-Value service leveraging WAL.</li><li>WAL can also be used by a service to guarantee sending requests to multiple datastores. For example, a database and a backup, or a database and a queue at the same time etc.</li></ul><h3>Acknowledgements</h3><p>Launching WAL was a collaborative effort involving multiple teams at Netflix, and we are grateful to everyone who contributed to making this idea a reality. We would like to thank the following teams for their roles in this launch.</p><ul><li>Caching team — Additional thanks to <a href="https://www.linkedin.com/in/shihhaoyeh/">Shih-Hao Yeh</a>, <a href="https://www.linkedin.com/in/akashdeepgoel/">Akashdeep Goel</a> for contributing to cross region replication for KV, EVCache etc. and owning this service.</li><li>Product Data System team — <a href="https://www.linkedin.com/in/carlos-jmh/">Carlos Matias Herrero</a>, <a href="https://www.linkedin.com/in/bbremen/">Brandon Bremen</a> for contributing to the delay queue design and being early adopters of WAL giving valuable feedback.</li><li>KeyValue and Composite abstractions team — <a href="https://www.linkedin.com/in/rummadis/">Raj Ummadisetty</a> for feedback on API design and mutateItems design discussions. <a href="https://www.linkedin.com/in/rajiv-shringi/">Rajiv Shringi</a><strong> </strong>for feedback on API design.</li><li>Kafka and Real Time Data Infrastructure teams — <a href="https://www.linkedin.com/in/nickmahilani/">Nick Mahilani</a> for feedback and inputs on integrating the WAL client into Kafka client. <a href="https://www.linkedin.com/in/sundaram-ananthanarayanan-97b8b545/">Sundaram Ananthanarayan</a> for design discussions around the possibility of leveraging Flink for some of the WAL use cases.</li><li><a href="https://jolynch.github.io/">Joseph Lynch</a> for providing strategic direction and organizational support for this project.</li></ul><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=127b6712359a" width="1" height="1" alt=""><hr><p><a href="https://netflixtechblog.com/building-a-resilient-data-platform-with-write-ahead-log-at-netflix-127b6712359a">Building a Resilient Data Platform with Write-Ahead Log at Netflix</a> was originally published in <a href="https://netflixtechblog.com">Netflix TechBlog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Scaling Muse: How Netflix Powers Data-Driven Creative Insights at Trillion-Row Scale]]></title>
            <link>https://netflixtechblog.com/scaling-muse-how-netflix-powers-data-driven-creative-insights-at-trillion-row-scale-aa9ad326fd77?source=rss----2615bd06b42e---4</link>
            <guid isPermaLink="false">https://medium.com/p/aa9ad326fd77</guid>
            <category><![CDATA[druid]]></category>
            <category><![CDATA[data-engineering]]></category>
            <category><![CDATA[big-data]]></category>
            <dc:creator><![CDATA[Netflix Technology Blog]]></dc:creator>
            <pubDate>Mon, 22 Sep 2025 21:24:20 GMT</pubDate>
            <atom:updated>2025-09-22T21:24:17.071Z</atom:updated>
            <content:encoded><![CDATA[<p>By <a href="https://www.linkedin.com/in/andrew-pierce-34443a7/">Andrew Pierce</a>, <a href="https://www.linkedin.com/in/chris-thrailkill-a268914/">Chris Thrailkill</a>, <a href="https://www.linkedin.com/in/victor-chiapaikeo-974a501b/">Victor Chiapaikeo</a></p><p>At Netflix, we prioritize getting timely data and insights into the hands of the people who can act on them. One of our key internal applications for this purpose is Muse. Muse’s ultimate goal is to help Netflix members discover content they’ll love by ensuring our promotional media is as effective and authentic as possible. It achieves this by equipping creative strategists and launch managers with data-driven insights showing which artwork or video clips resonate best with global or regional audiences and flagging outliers such as potentially misleading (clickbait-y) assets. These kinds of applications fall under Online Analytical Processing (OLAP), a category of systems designed for complex querying and data exploration. However, enabling Muse to support new, more advanced filtering and grouping capabilities while maintaining high performance and data accuracy has been a challenge. Previous posts have touched on <a href="https://netflixtechblog.com/artwork-personalization-c589f074ad76">artwork personalization</a> and our <a href="https://netflixtechblog.com/introducing-impressions-at-netflix-e2b67c88c9fb">impressions architecture</a>. <strong>In this post, we’ll discuss some steps we’ve taken to evolve the Muse data serving layer to enable new capabilities while maintaining high performance and data accuracy.</strong></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*22EDwImi4b5D3tZuY8KXjQ.png" /><figcaption>Muse application</figcaption></figure><h3>An Evolving Architecture</h3><p>Like many early analytics applications, Muse began as a simple dashboard powered by batch data pipelines (Spark¹) and a modest Druid² cluster. As the application evolved, so did user demands. Users wanted new features like outlier detection and notification delivery, media comparison and playback, and advanced filtering, all while requiring lower latency and supporting ever-growing datasets (in the order of trillions of rows a year). One of the most challenging requirements was enabling dynamic analysis of promotional media performance by “audience” affinities: internally defined, algorithmically inferred labels representing collections of viewers with similar tastes. Answering questions like “Does specific promotional media resonate more with Character Drama fans or Pop Culture enthusiasts?” required augmenting already voluminous impression and playback data. Supporting filtering and grouping by these many-to-many audience relationships led to a combinatorial explosion in data volume, pushing the limits of our original architecture.</p><p>To address these complexities and support the evolving needs of our users, we undertook a significant evolution of Muse’s architecture. Today’s Muse is a React app that queries a GraphQL layer served with a set of Spring Boot GRPC microservices. In the remainder of this post, we’ll focus on steps we took to scale the data microservice, its backing ETL, and our Druid cluster. <strong>Specifically, we’ve changed the data model to rely on HyperLogLog (HLL) sketches, used </strong><a href="https://hollow.how/"><strong>Hollow</strong></a><strong> for access to in-memory, precomputed aggregates, and taken a series of steps to tune Druid. To ensure the accuracy of these changes, we relied heavily on internal debugging tools to validate pre- and post-changes.</strong></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*-VJUGD9aJ3BNr-q0MgawIg.png" /><figcaption><em>Muse’s Current Architecture</em></figcaption></figure><h4>Moving to HyperLogLog (HLL) Sketches for Distinct Counts</h4><p>Some of the most important metrics we track are impressions, the number of times an asset is shown to a user within a time window, and qualified plays, which links a playback event with a minimum duration back to a specific impression. Calculating these metrics requires counting distinct users. However, performing distinct counts in distributed systems is resource-intensive and challenging. For instance, to determine how many unique profiles have ever seen a particular asset, we need to compare each new set of profile ids with those from all days before it, potentially spanning months or even years.</p><p>For performance, we can trade accuracy. The <a href="https://datasketches.apache.org/">Apache Datasketches library</a> allows us to get distinct count estimates that are within a 1–2% error. This is tunable with a precision parameter called logK (0.8% in our case with logK of 17). We build sketches in two places:</p><ol><li>During Druid ingest: we use the <a href="https://druid.apache.org/docs/latest/development/extensions-core/datasketches-hll/#aggregators">HLLSketchBuild aggregator</a> with Druid <a href="https://druid.apache.org/docs/latest/ingestion/rollup/">rollup set to true</a> to reduce our data in preparation for fast distinct counting</li><li>During our Spark ETL: we persist precomputed aggregates like all-time impressions per asset in the form of HLL sketches. Each day, we merge a new HLL sketch into the existing one using a combination of <a href="https://spark.apache.org/docs/3.5.1/api/java/org/apache/spark/sql/functions.html#hll_union(org.apache.spark.sql.Column,org.apache.spark.sql.Column)">hll_union</a> and <a href="https://spark.apache.org/docs/3.5.1/api/java/org/apache/spark/sql/functions.html#hll_union_agg(org.apache.spark.sql.Column)">hll_union_agg</a> (<a href="https://www.databricks.com/blog/apache-spark-3-apache-datasketches-new-sketch-based-approximate-distinct-counting">functions added by our very own Ryan Berti</a>)</li></ol><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*29CBkjnyIn_1e9V1R0m8hg.png" /><figcaption><em>We use Datasketches in our ETL and serving systems</em></figcaption></figure><p>HLL has been a huge performance boost for us both within the serving and ETL layer. Across our most common OLAP query patterns, we’ve seen latencies reduce by approx 50%. Nevertheless, running APPROX_COUNT_DISTINCT over large date ranges on the Druid cluster for very large titles exhausts limited threads, especially in high-concurrency situations. To further offload Druid query volume and preserve cluster threads, we’ve also relied extensively on the <a href="https://github.com/Netflix/hollow">Hollow library</a>.</p><h4>Hollow as a Read-Only Key Value Store for Precomputed Aggregates</h4><p>Our in-house Hollow³ infrastructure allows us to easily create Hollow feeds — essentially highly compressed and performant in-memory key/value stores — from Iceberg⁴ tables. In this setup, dedicated producer servers listen for changes to Iceberg tables, and when updates occur, they push the latest data to downstream consumers. On the consumer side, our Spring Boot applications listen to announcements from these producers and automatically refresh in-memory caches with the latest dataset.</p><p>This architecture has enabled us to migrate several data access patterns from Druid to Hollow, specifically ones with a limited number of parameter combinations per title. One of these was fetching distinct filter dimensions. For example, while most Netflix-branded titles are released globally, licensed titles often have rights restrictions that limit their availability to specific countries and time windows. As a result, a particular licensed title might only be available to members in Germany and Luxembourg.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1014/1*m4Tdj3YixQfPmnEaMGdspA.png" /><figcaption><em>Distinct countries queried from a Hollow feed for the assets for Manta Manta</em></figcaption></figure><p>In the past, retrieving these distinct country values per asset required issuing a SELECT DISTINCT query to our Druid cluster. With Hollow, we maintain a feed of distinct dimension values, allowing us to perform stream operations like the one below directly on a cached dataset.</p><pre>/**<br> * Returns the possible filter values for a dimension such as countries<br> */<br>public List&lt;Dimension&gt; getDimensions(long movieId, String dimensionId) {<br>    // Access in-memory Hollow feed with near instant query time<br>    Map&lt;String, List&lt;Dimension&gt;&gt; dimensions = dimensionsHollowConsumer.lookup(movieId);<br>    return dimensions.getOrDefault(dimensionId, List.of()).stream()<br>        .sorted(Comparator.comparing(Dimension::getName))<br>        .toList();<br>}</pre><p>Although it adds complexity to our service by requiring more intricate request routing and a higher memory footprint, pre-computed aggregates have given us greater stability and performance. In the case of fetching distinct dimensions, we’ve observed query times drop from hundreds of milliseconds to just tens of milliseconds. More importantly, this shift has offloaded high concurrency demands from our Druid cluster, resulting in more consistent query performance. In addition to this use case, cached pre-computed aggregates also power features such as retrieving recently launched titles, accessing all-time asset metrics, and serving various pieces of title metadata.</p><h4>Tuning Druid</h4><p>Even with the efficiencies gained from HLL sketches and Hollow feeds, ensuring that our Druid cluster operates performantly has been an ongoing challenge. Fortunately, at Netflix, we are in the company of multiple <a href="https://www.apache.org/foundation/governance/pmcs">Apache Druid PMC members</a> like <a href="https://www.linkedin.com/in/maytasm/">Maytas Monsereenusorn</a> and <a href="https://www.linkedin.com/in/jessetuglu/">Jesse Tuğlu</a> who have helped us wring out every ounce of performance. Some of the key optimizations we’ve implemented include:</p><ul><li><strong>Increasing broker count relative to historical nodes:</strong> We aim for a broker-to-historical ratio close to the <a href="https://druid.apache.org/docs/latest/operations/basic-cluster-tuning/#number-of-brokers">recommended 1:15</a>, which helps improve query throughput.</li><li><strong>Tuning segment sizes:</strong> By targeting the <a href="https://druid.apache.org/docs/latest/operations/segment-optimization/">300–700 MB “sweet spot”</a> for segment sizes, primarily using the tuningConfig.targetRowsPerSegment parameter during ingestion — we ensure that each segment a single historical thread scans is not overly large.</li><li><strong>Leveraging Druid lookups for data enrichment:</strong> Since joins can be prohibitively expensive in Druid, we use <a href="https://druid.apache.org/docs/latest/querying/lookups/">lookups</a> at query time for any key column enrichment.</li><li><strong>Optimizing search predicates:</strong> We ensure that all search predicates operate on physical columns rather than virtual ones, creating necessary columns during ingestion with <a href="https://druid.apache.org/docs/latest/ingestion/ingestion-spec/#transforms">transformSpec.transforms</a>.</li><li><strong>Filtering and slimming data sources at ingest:</strong> By applying filters within <a href="https://druid.apache.org/docs/latest/ingestion/ingestion-spec/#filter">transformSpec.filter</a> and removing all unused columns in <a href="https://druid.apache.org/docs/latest/ingestion/ingestion-spec/#dimensionsspec">dimensionsSpec.dimensions</a>, we keep our data sources lean and improve the possibility of <a href="https://druid.apache.org/docs/latest/ingestion/rollup">higher rollup yield</a>.</li><li><strong>Use of multi-value dimensions:</strong> Exploiting the Druid <a href="https://druid.apache.org/docs/latest/querying/multi-value-dimensions/">multi-value dimension</a> feature was key to overcoming the “many-to-many” combinatorial quandary when integrating audience filtering and grouping functionality mentioned in the “An Evolving Architecture” section above.</li></ul><p>Together, these optimizations, combined with previous ones, have decreased our p99 Druid latencies by roughly 50%.</p><h4>Validation &amp; Rollout</h4><p>Rolling out these changes to our metrics system required a thorough validation and release strategy. Our approach prioritized both data integrity and user trust, leveraging a blend of automation, targeted tooling, and incremental exposure to production traffic. At the core of our strategy was a parallel stack deployment: both the legacy and new metric stacks operated side-by-side within the Muse Data microservice. This setup allowed us to validate data quality, monitor real-world performance, and mitigate risk by enabling seamless fallback at any stage.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/831/1*gmcpkg9ehK1iX4Z6tKsetw.png" /></figure><p>We adopted a two-pronged validation process:</p><ul><li><strong>Automated Offline Validation: </strong>Using Jupyter Notebooks, we automated the sampling and comparison of key metrics across both the legacy and new stacks. Our sampling set included a representative mix: recently accessed titles, high-profile launches, and edge-case titles with unique handling requirements. This allowed us to catch subtle discrepancies in metrics early in the process. Iterative testing on this set guided fixes, such as tuning the HLL logK parameter and benchmarking end-to-end latency improvements.</li><li><strong>In-App Data Comparison Tooling: </strong>To facilitate rapid triage, we built a developer-facing comparison feature within our application that displays data from both the legacy and new metric stacks side by side. The tool automatically highlights any significant differences, making it easy to quickly spot and investigate discrepancies identified during offline validation or reported by users.</li></ul><p>We implemented several release best practices to mitigate risk and maintain stability:</p><ul><li><strong>Staggered Implementation by Application Segment: </strong>We developed and deployed the new metric stack in stages, focusing on specific application segments. This meant building out support for asset types like artwork and video separately and then further dividing by CEE phase (Explore, Exploit). By implementing changes segment by segment, we were able to isolate issues early, validate each piece independently, and reduce overall risk during the migration.</li><li><strong>Shadow Testing (“Dark Launch”):</strong> Prior to exposing the new stack to end users, we mirrored production traffic asynchronously to the new implementation. This allowed us to validate real-world latency and catch potential faults in a live environment, without impacting the actual user experience.</li><li><strong>Granular Feature Flagging: </strong>We implemented fine-grained feature flags to control exposure within each segment. This allowed us to target specific user groups or titles and instantly roll back or adjust the rollout scope if any issues were detected, ensuring rapid mitigation with minimal disruption.</li></ul><h3>Learnings and Next Steps</h3><p>Our journey with Muse tested the limits of several parts of the stack: the ETL layer, the Druid layer, and the data serving layer. While some choices, like leveraging Netflix’s in-house Hollow infrastructure, were influenced by available resources, simple principles like offloading query volume, pre-filtering of rows and columns before Druid rollup, and optimizing search predicates (along with a bit of HLL magic) went a long way in allowing us to support new capabilities while maintaining performance. Additionally, engineering best practices like producing side-by-side implementations and backwards-compatible changes enabled us to roll out revisions steadily while maintaining rigorous validation standards. Looking ahead, we’ll continue to build on this foundation by supporting a wider range of content types like Live and Games, incorporating synopsis data, deepening our understanding of how assets work together to influence member choosing, and incorporating new metrics to distinguish between “effective” and “authentic” promotional assets, in service of helping members find content that truly resonates with them.</p><p>¹ Apache Spark is an open-source analytics engine for processing large-scale data, enabling tasks like batch processing, machine learning, and stream processing.</p><p>² Apache Druid is a high-performance, real-time analytics database designed for quickly querying large volumes of data.</p><p>³ Hollow is a Java library for efficient in-memory storage and access to moderately sized, read-only datasets, making it ideal for high-performance data retrieval.</p><p>⁴ Apache Iceberg is an open-source table format designed for large-scale analytical datasets stored in data lakes. It provides a robust and reliable way to manage data in formats like Parquet or ORC within cloud object storage or distributed file systems.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=aa9ad326fd77" width="1" height="1" alt=""><hr><p><a href="https://netflixtechblog.com/scaling-muse-how-netflix-powers-data-driven-creative-insights-at-trillion-row-scale-aa9ad326fd77">Scaling Muse: How Netflix Powers Data-Driven Creative Insights at Trillion-Row Scale</a> was originally published in <a href="https://netflixtechblog.com">Netflix TechBlog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
    </channel>
</rss>