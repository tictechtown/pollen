<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:cc="http://cyber.law.harvard.edu/rss/creativeCommonsRssModule.html">
    <channel>
        <title><![CDATA[Lyft Engineering - Medium]]></title>
        <description><![CDATA[Stories from Lyft Engineering. - Medium]]></description>
        <link>https://eng.lyft.com?source=rss----25cd379abb8---4</link>
        <image>
            <url>https://cdn-images-1.medium.com/proxy/1*TGH72Nnw24QL3iV9IOm4VA.png</url>
            <title>Lyft Engineering - Medium</title>
            <link>https://eng.lyft.com?source=rss----25cd379abb8---4</link>
        </image>
        <generator>Medium</generator>
        <lastBuildDate>Fri, 19 Dec 2025 22:16:37 GMT</lastBuildDate>
        <atom:link href="https://eng.lyft.com/feed" rel="self" type="application/rss+xml"/>
        <webMaster><![CDATA[yourfriends@medium.com]]></webMaster>
        <atom:link href="http://medium.superfeedr.com" rel="hub"/>
        <item>
            <title><![CDATA[From Python3.8 to Python3.10: Our Journey Through a Memory Leak]]></title>
            <link>https://eng.lyft.com/from-python3-8-to-python3-10-our-journey-through-a-memory-leak-1fd9b43cc01e?source=rss----25cd379abb8---4</link>
            <guid isPermaLink="false">https://medium.com/p/1fd9b43cc01e</guid>
            <category><![CDATA[memory-leak]]></category>
            <category><![CDATA[python]]></category>
            <dc:creator><![CDATA[Jay Patel]]></dc:creator>
            <pubDate>Mon, 15 Dec 2025 19:31:01 GMT</pubDate>
            <atom:updated>2025-12-15T19:30:48.214Z</atom:updated>
            <content:encoded><![CDATA[<figure><img alt="Image generated with ChatGPT (OpenAI), 2025." src="https://cdn-images-1.medium.com/max/1024/1*QWBsfrlv7BNM8sZwFAsTfQ.png" /><figcaption><em>Image generated with ChatGPT (OpenAI), 2025.</em></figcaption></figure><h3>Intro</h3><p>When working with Python, memory management often feels like a solved problem. The garbage collector quietly does its job, and unlike C or C++, we rarely think about malloc or free. This doesn‚Äôt mean that there are no memory leaks in Python. Reference cycles, unreleased resources like connection pooling, global caches, etc can slowly inflate your process‚Äôs memory footprint. You might not notice it at first, until your worker starts OOM-ing, latency creeps up, or container restarts become mysteriously frequent.</p><p>In this post, we‚Äôll share the story of a real-world memory leak we encountered during a Python upgrade‚Ää‚Äî‚Äähow we discovered it, the tools and techniques we used to investigate, and the lessons we¬†learned.</p><h3>What happened after upgrading to Python¬†3.10?</h3><p>Back in the summer of 2024, we had an initiative at Lyft to upgrade all of our Python services from v3.8 to 3.10 as v3.8 was scheduled to be EoL by the end of 2024. You can find more details on how our awesome Backend Foundations team at Lyft does Python upgrade across hundreds of repos at scale <a href="https://eng.lyft.com/python-upgrade-playbook-1479145d52f4">here</a>. The upgrade involved two phases: the first phase was to upgrade all the dependencies to be Python 3.10 compatible, and the second phase was to upgrade the services to Python 3.10. The dependency upgrades went smoothly for all services and then the phase to upgrade all services to Python 3.10 rolled out. While all services were running Python 3.10 smoothly, there was one service for which the upgrade in the test environment caused a flurry of latency spikes, resulting in timeouts for downstream services.</p><figure><img alt="Graph: Increasing 5xx caused by timeouts after upgrading to Python 3.10" src="https://cdn-images-1.medium.com/max/1024/1*eZCQ8TRmMvT5_wSFiksPrA.png" /><figcaption><em>Increasing 5xx caused by timeouts after upgrading to Python¬†3.10</em></figcaption></figure><p>After profiling the APIs with increased latency with stats, we found that the source of latency were repository queries to the dynamo tables. Specifically, we had <a href="https://pynamodb.readthedocs.io/en/stable/">pynamodb</a> based repository queries which would spin up a bunch of greenlets to fetch data from multiple tables and combine the result which was showing increased timeouts. The individual queries themselves were fine; however, it was the thread join which took the longest time causing the worker to timeout (default = 30 seconds).</p><figure><img alt="Graph: Individual Dynamo queries taking &lt; 100 ms to finish" src="https://cdn-images-1.medium.com/max/1024/1*1Nh7ZK1j2fI0fWNNw9hUew.png" /><figcaption><em>Individual Dynamo queries taking &lt; 100 ms to¬†finish</em></figcaption></figure><figure><img alt="Graph: Gevent thread join takes 30 secs" src="https://cdn-images-1.medium.com/max/1024/1*rOsCnNVg66HEXNy1HqGCIg.png" /><figcaption>Gevent thread join takes 30¬†secs</figcaption></figure><p>The other interesting thing we found was memory consumption slowly creeping up with time in all of the¬†pods.</p><figure><img alt="Graph: Memory usage % of all pods" src="https://cdn-images-1.medium.com/max/1024/1*eTRt4KtT7amhI-lDska_rQ.png" /><figcaption>Memory usage % of all¬†pods</figcaption></figure><p>At this point, we weren‚Äôt sure if there was something up with gevent/greenlet causing the memory leak or the memory leak causing the latency since decreased memory availability can cause increasing page fetches from the disk. We first checked if the <a href="https://www.gevent.org/monitoring.html#blocking">gevent monitoring thread</a> detected any event loop blocks, which could potentially cause these timeouts. We then pivoted to find out the root cause of the memory leak. Fortunately, Lyft has an internal library which can help profile memory which is based on <a href="https://docs.python.org/3/library/tracemalloc.html">tracemalloc</a>.</p><h3>Memory profiling tool</h3><p>The Lyft memory profiling tool is based on <a href="https://docs.python.org/3/library/tracemalloc.html">tracemalloc</a>. To capture the memory trace for a given gunicorn process, we registered the worker process to listen to USR2 signal during the application initialization phase.</p><pre># app/__init__.py<br><br>MemoryProfiler().register_handlers()<br><br># mem_profiler.py pseudo code<br><br>class MemoryProfiler: <br>    def __init__(self) -&gt; None:<br>        self._state_machine = self._profiling_state_machine()<br><br>     def register_handlers(self) -&gt; None:<br>        # Register gunicorn worker to listen to USR2 to dump traces<br>        signal.signal(signal.SIGUSR2, self.handle_signal)  <br><br>    def handle_signal(self, signum: signal.Signals, frame: FrameType) -&gt; None:<br>        next(self._state_machine)<br><br>    def _profiling_state_machine(self) -&gt; Generator[None, None, None]:<br>        while True:<br>            try:<br>                self.start_tracing() # tracemalloc.start()<br>                self.memory_dump() # Create snapshot1<br>                yield<br>                self.memory_dump() # Create snaphot2,compare with snapshot1, and dump the difference in a file <br>            finally:<br>                if tracemalloc.is_tracing():<br>                    tracemalloc.stop()</pre><h3>Let‚Äôs start the¬†tracing!</h3><p>Ok, now that we had the memory profiler setup, we are ready for some tracing to find the source of the leak. To start the tracing, we send <strong>USR2</strong> signal to the gunicorn process in the K8s pod to start tracing and send the signal again after some time interval to capture the stack trace with highest memory¬†usage.</p><pre>ps aux</pre><figure><img alt="Command line output: Initial process list before sending USR2 signal" src="https://cdn-images-1.medium.com/max/1024/1*LWqnkq8g2_BB5cXEBuyozg.png" /><figcaption><em>Initial process list before sending USR2¬†signal</em></figcaption></figure><p>Now, we will send a USR2 signal to worker with pid¬†12</p><pre>kill -USR2 12</pre><p>Upon checking the process list¬†again‚Ä¶.</p><pre>ps aux</pre><figure><img alt="Command line output: Tracing killing the gunicorn worker with PID=12" src="https://cdn-images-1.medium.com/max/1024/1*27fhy9fAMFix-1-BorV4pA.png" /><figcaption><em>Tracing killing the gunicorn worker with¬†PID=12</em></figcaption></figure><p>‚Ä¶ <strong>we observed that the gunicorn process we planned to trace got killed¬†</strong>üôÅ</p><p>It took several hours of debugging and a journey back to one of my <a href="https://dartmouth.smartcatalogiq.com/en/2023/orc/departments-programs-undergraduate/computer-science/cosc-computer-science-undergraduate/cosc-58">favorite class</a> to find the root of the issue‚Ää‚Äî‚Ää<a href="https://docs.gunicorn.org/en/stable/settings.html#preload-app">preload</a>. To understand why preload caused the process to be killed, we first need to understand how gunicorn¬†works.</p><h3>Gunicorn</h3><p><a href="https://gunicorn.org/">Gunicorn</a> works on the pre-fork model. There is a leader process which forks a bunch of workers. There are two ways to fork the¬†workers:</p><p><strong>No Preload</strong></p><figure><img alt="Graph: Gunicorn forked workers with no preload" src="https://cdn-images-1.medium.com/max/1024/1*GIDNjLI8fwZoU7vTSsA6Og.png" /><figcaption><em>Gunicorn forked workers with no¬†preload</em></figcaption></figure><p>When the leader process forks a worker, the worker has its own application code. This results in the worker process having a larger memory footprint than the¬†leader.</p><pre>smem -a - sort=pid -k</pre><figure><img alt="Command line output: Service with no preload: Worker PSS mem = ~203MB" src="https://cdn-images-1.medium.com/max/1024/1*3CskCAq2yV0Lz2RKHAUhow.png" /><figcaption><em>Service with no preload: Worker PSS mem =¬†~203MB</em></figcaption></figure><p><strong>With Preload</strong></p><figure><img alt="Graph: Gunicorn forked workers with preload" src="https://cdn-images-1.medium.com/max/1024/1*xK8Eol5qNjHq5XRHV-FHqQ.png" /><figcaption><em>Gunicorn forked workers with¬†preload</em></figcaption></figure><p>Preload is a memory optimization based on the concept of<a href="https://en.wikipedia.org/wiki/Copy-on-write"> copy-on-write</a>. Essentially, the workers share the imports and application code with the leader and only modified pages are written to the worker‚Äôs¬†memory.</p><pre>smem -a - sort=pid -k</pre><figure><img alt="Command line output: Service with preload: Worker PSS mem reduced to ~41MB!!" src="https://cdn-images-1.medium.com/max/1024/1*howZgiwphM8tW7Yz5P5P2Q.png" /><figcaption><em>Service with preload: Worker PSS mem reduced to¬†~41MB!!</em></figcaption></figure><p><strong>So how does preload play a role with USR2 signal killing the¬†process?</strong></p><p>If you remember, we registered the signal during the app initialization by calling <strong>register_handlers().</strong></p><pre># app/__init__.py<br><br>MemoryProfiler().register_handlers()  <br><br><br># mem_profiler.py<br><br>class MemoryProfiler:<br><br>    def register_handlers(self) -&gt; None:<br>        # Register gunicorn worker to listen to USR2 to dump traces<br>        signal.signal(signal.SIGUSR2, self.handle_signal)</pre><p>Since the app had preload=True, only the leader process was registering the USR2 signal to handle the tracing. The worker process did not register due to copy-on-write and that causes any <strong>kill -USR2</strong> to actually kill the¬†process!</p><h3>Let‚Äôs start the tracing again (with no preload)!</h3><p>Now that we have figured out that preload caused the process to be killed, we turn off the preload option and start the tracing¬†again.</p><pre>ps aux </pre><figure><img alt="Command line output: Initial process list before sending USR2 signal" src="https://cdn-images-1.medium.com/max/1024/1*WWqAGDxKxuOaC7xd9CpZ3A.png" /><figcaption><em>Initial process list before sending USR2¬†signal</em></figcaption></figure><pre>kill -USR2 12</pre><figure><img alt="Command line output: Successful USR2 signal not killing the gunicorn worker" src="https://cdn-images-1.medium.com/max/1024/1*LtPhlX5-R_IWxkIovsQR5w.png" /><figcaption><em>Successful USR2 signal not killing the gunicorn¬†worker</em></figcaption></figure><p>The worker does not get¬†killed!</p><p>We created a script which iterates through all the K8s pods and sends a USR2 signal to all the workers to start the tracing and resends the signal to stop the tracing after a certain time interval. The trace had a lot of false positives since it collects dumps which may not necessarily be the source of the leak, but have not been garbage collected yet.</p><h3>Root causing</h3><p>The most interesting (and common) memory dump trace after sifting through hundreds of them was the following:</p><figure><img alt="Stack trace dump from memory profiler" src="https://cdn-images-1.medium.com/max/1024/1*mFiA_yzHXa2mzw463nnO5Q.png" /><figcaption><em>Stack trace dump from memory¬†profiler</em></figcaption></figure><p>If you remember the initial conclusion we had with the following graph, we knew that the increase in timeouts had something to do with pynamodb and gevent/greenlets since we saw thread joins taking a long¬†time:</p><figure><img alt="Graph: Our initial observation of gevent thread join takes 30 secs" src="https://cdn-images-1.medium.com/max/1024/1*rOsCnNVg66HEXNy1HqGCIg.png" /><figcaption>Our initial observation of <em>gevent thread join takes 30¬†secs</em></figcaption></figure><p>The stack trace combined with the graph above, narrowed down the issue to pynamo/botocore. After digging online, we found the following <a href="https://github.com/urllib3/urllib3/issues/3061">issue with urllib3 v1.26.16</a>. Essentially, in a highly concurrent environment using <strong>gevent</strong>, connections were not being returned to the pool which caused the pool to sit at its max size and block further requests. This particular stack trace confirmed our suspicion:</p><figure><img alt="Stack trace showing botocore/urllib3/connectionpool" src="https://cdn-images-1.medium.com/max/1024/1*wcV7ev9Zs45RSLesKlQsfw.png" /><figcaption>Stack trace showing botocore/urllib3/connectionpool</figcaption></figure><p>The root cause of the issue was some incompatibility between <a href="https://docs.python.org/3/library/weakref.html"><strong>weakref.finalize</strong></a> and gevent‚Äôs monkey patching causing non-deterministic deadlock which made the issue hard to reproduce. The immediate fix was to downgrade the urllib3 version to <strong>1.26.15</strong>, after which the timeouts and the memory leak were gone!! <strong>The actual fix which ensures urllib3 connection pooling is cooperative was released in April 2025 and we have seen no issues upgrading both gevent to </strong><a href="https://github.com/gevent/gevent/issues/1769"><strong>v25.4.1</strong></a><strong> as well as urllib3 to 1.26.16+.</strong></p><p>It is unclear though why the Python version upgrade exposed the issue. <strong>In fact, urllib3 upgrade was not part of the dependency upgrade we had done to prepare for the Python 3.10 upgrade! </strong>We had actually been running Python 3.8 with urllib v1.26.16 for about a year without any problem. Ironically, we had upgraded to v1.26.16 specifically because it <a href="https://pypi.org/project/urllib3/1.26.16/">logged</a> the total connections whenever connection pools were¬†full.</p><h3>Tip</h3><ol><li>If you run into memory leaks which are affecting the live production system, you can use gunicorn‚Äôs <a href="https://docs.gunicorn.org/en/stable/settings.html#max-requests"><strong>max-request</strong> </a>settings which recycles the worker processes after N requests. This ensures your process or container does not run into OOM. While this helps mitigate the issue, it is critical to continue investigating the source of the memory¬†leak.</li><li>Gevent <a href="https://www.gevent.org/monitoring.html#memory-usage">monitoring thread</a> has an option to print trace for greenlets which exceed a certain memory threshold. While I have personally never tried this, it could help find objects which are holding large amounts of memory, but not necessarily the source of a¬†leak.</li></ol><h3><strong>Closing Notes</strong></h3><p><strong>There is no silver bullet to debugging memory leaks</strong>; it is a hard issue to debug them. There a few things you can look for eg. unbounded global caches, unreleased resources tied to database/network pooling, recently upgraded libraries, etc. If you check the actual gevent/urllib3 <a href="https://github.com/urllib3/urllib3/issues/3061">issue</a>, none of them talked about memory leaks, only timeouts. We just happened to run into a memory leak and try to find the root cause of it¬†üòÄ</p><p>Lyft is hiring! If you‚Äôre passionate about efficient database connection management, visit <a href="https://www.lyft.com/careers">Lyft Careers</a> to see our openings.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=1fd9b43cc01e" width="1" height="1" alt=""><hr><p><a href="https://eng.lyft.com/from-python3-8-to-python3-10-our-journey-through-a-memory-leak-1fd9b43cc01e">From Python3.8 to Python3.10: Our Journey Through a Memory Leak</a> was originally published in <a href="https://eng.lyft.com">Lyft Engineering</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[LyftLearn Evolution: Rethinking ML Platform Architecture]]></title>
            <link>https://eng.lyft.com/lyftlearn-evolution-rethinking-ml-platform-architecture-547de6c950e1?source=rss----25cd379abb8---4</link>
            <guid isPermaLink="false">https://medium.com/p/547de6c950e1</guid>
            <category><![CDATA[distributed-systems]]></category>
            <category><![CDATA[artificial-intelligence]]></category>
            <category><![CDATA[machine-learning]]></category>
            <category><![CDATA[aws]]></category>
            <category><![CDATA[data-science]]></category>
            <dc:creator><![CDATA[Yaroslav Yatsiuk]]></dc:creator>
            <pubDate>Tue, 18 Nov 2025 18:16:05 GMT</pubDate>
            <atom:updated>2025-11-18T18:16:04.544Z</atom:updated>
            <content:encoded><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*-r1saxCbUiFJf47gTbTyXg.png" /></figure><p><em>Written by </em><a href="https://www.linkedin.com/in/yaroslav-yatsiuk-945931160/"><em>Yaroslav¬†Yatsiuk</em></a></p><p>At Lyft, machine learning (ML) is the engine behind our most critical business functions‚Ää‚Äî‚Ääfrom dispatch and pricing optimization to fraud detection and support automation. Our ML infrastructure serves thousands of production models making hundreds of millions of real-time predictions per day, supported by thousands of daily training jobs that keep ML models fresh and accurate.</p><p>As our scale grew, we faced a classic engineering challenge: the very complexity that powered our platform was becoming a bottleneck to its future growth. We needed to answer a fundamental question: How could we evolve our platform to accelerate innovation for our users while simplifying its underlying architecture?</p><p>This post explores how we rethought LyftLearn‚Äôs architecture to solve this problem. We‚Äôll walk through our transition from a fully Kubernetes-based system to a hybrid platform, combining the simplicity of managed compute on AWS SageMaker for offline workloads with the flexibility of Kubernetes for online model serving. Afterwards, we‚Äôll share the key technical decisions and trade-offs that made this evolution possible.</p><h3>LyftLearn Overview</h3><p>LyftLearn is Lyft‚Äôs end-to-end machine learning platform, managing the complete ML lifecycle from model development to production serving. Built to support hundreds of data scientists and ML engineers, it handles the full spectrum of ML workloads at scale. The platform is composed of three integrated products:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*SEJzxSPCjsnhPd0q5U2Zvg.png" /><figcaption>Figure 1: LyftLearn Components</figcaption></figure><p><strong>LyftLearn Compute (Offline Stack)</strong> handles model development and training workloads. ML Modelers use JupyterLab environments to prototype models, then run training jobs, batch processing, and hyperparameter optimization at scale. These workloads are elastic and on-demand‚Ää‚Äî‚Ääthey spin up when needed, process large datasets, and terminate when complete.</p><p><strong>LyftLearn Serving (Online Stack)</strong> powers production inference, serving millions of predictions per minute with millisecond latency. It provides online model serving with real-time ML capabilities, automated deployment and promotion workflows, and online validation to ensure model quality before production traffic.</p><p><strong>LyftLearn Observability</strong> monitors model health and detects degradation across the platform. It tracks performance drift, identifies anomalies, scores model health, and monitors model activity to ensure production models maintain quality as data and business conditions evolve.</p><p>While all three components work together to provide a unified ML platform, the offline and online stacks have fundamentally different operational characteristics. Offline workloads need elastic, cost-efficient compute that scales to zero between jobs. Online model serving requires always-on infrastructure with strict latency guarantees and tight operational control. These differences led us to adopt different infrastructure strategies for each‚Ää‚Äî‚Ääand it‚Äôs the evolution of our offline stack that transformed how we deliver LyftLearn Compute¬†today.</p><h3>The Original Architecture</h3><p>The original offline stack (LyftLearn Compute) ran entirely on Kubernetes‚Ää‚Äî‚Ääevery training job, batch prediction, hyperparameter optimization run, and JupyterLab notebook environment executed as a Kubernetes workload, orchestrated through a collection of custom-built services. We documented this architecture in detail in our 2021 blog post,<a href="https://eng.lyft.com/lyftlearn-ml-model-training-infrastructure-built-on-kubernetes-aef8218842bb"> LyftLearn: ML Model Training Infrastructure built on Kubernetes</a>.</p><p>The following diagram shows a high-level view of the LyftLearn Compute 1.0 architecture:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*a-_qAccCYkUbkOj4mfMj9w.png" /><figcaption>Figure 2: LyftLearn Compute 1.0 high-level architecture</figcaption></figure><p>To understand the operational complexity, let‚Äôs look at some of the key components and how they worked together:</p><p><strong>LyftLearn Service </strong>served as the backend API, receiving requests from three primary sources: the LyftLearn UI for ad-hoc jobs, Airflow DAGs for scheduled training and batch prediction pipelines, and CI/CD pipelines that registered models along with their Docker images during deployments. It managed model configurations, job metadata, and coordinated with downstream services.</p><p><strong>K8s Orchestration Service</strong> translated job requests into Kubernetes resources. When LyftLearn Service called it to create a training job, it¬†would:</p><ul><li>insert the job record in LyftLearn DB (so watchers could track¬†it)</li><li>construct the Kubernetes Job specification with containers, resource requests, environment variables, sidecars, references to docker images in AWS Elastic Container Registry (ECR), and other K8s resources</li><li>submit the job to the Kubernetes cluster</li></ul><p><strong>Background Watchers</strong> ran continuously to manage jobs lifecycle and infrastructure. We maintained multiple worker scripts handling different responsibilities:</p><ul><li>job status watcher (monitoring job state transitions and¬†timing)</li><li>container status watcher (tracking individual container states)</li><li>ingress status watcher (managing notebook endpoint¬†URLs)</li><li>job cleanup watcher (removing completed jobs from Kubernetes)</li><li>analytics event watcher (capturing usage¬†events)</li><li>additional scripts for EFS cleanup, spending tracking, and stats publishing</li></ul><p>Creating any job meant assembling a complete set of Kubernetes resources:<strong><em> </em></strong>Pod specifications with init and sidecar containers for secrets and metrics, <em>ConfigMaps</em> for hyperparameters, Secrets for credentials, <em>PersistentVolumeClaims</em> for notebook storage, <em>Services</em> and <em>Ingresses</em> for network access, and role-based access control (RBAC) policies (<em>ServiceAccounts, Roles, RoleBindings</em>) for cluster permissions. In essence, we owned the entire operational lifecycle‚Ää‚Äî‚Ääfrom scheduling and retries to cleanup and low-level resource management.</p><h4>What Worked¬†Well</h4><p>The Kubernetes-based architecture successfully powered production ML workloads for years and delivered some real technical advantages, including:</p><p><strong>Unified Infrastructure Stack</strong> <br>ML workloads ran on the same Kubernetes infrastructure as Lyft‚Äôs production services, using the same networking stack, observability tooling, security patterns, and operational processes. This meant the platform team leveraged existing infrastructure expertise and tooling rather than maintaining separate systems for ML workloads.</p><p><strong>Fast Job Startup<br></strong>Jobs could launch as fast as 30‚Äì45 seconds on existing K8s cluster infrastructure. Unlike on-demand compute provisioning which requires waiting for instances to start and initialize, jobs scheduled immediately onto available nodes with cached images, making the approach particularly effective for frequently running training jobs and batch processing workflows.</p><p><strong>Flexible Resource Specifications</strong> <br>Engineers could request any CPU/memory combination their workload needed. Memory-intensive preprocessing jobs could request 16 CPUs with 512GB RAM, while CPU-intensive training jobs used 64 CPUs with 128GB RAM. These ratios didn‚Äôt map cleanly to fixed AWS instance types, so this flexibility allowed precise resource allocation based on workload¬†needs.</p><p>This architecture served hundreds of engineers running thousands of daily jobs that powered business-critical ML workflows. However, as Lyft‚Äôs scale grew, so did the operational complexity of managing such a¬†system.</p><h4>Challenges of a Growing¬†Platform</h4><p>We identified several key challenges that were consuming an increasing amount of our¬†focus:</p><p><strong>The Feature Tax <br></strong>Every new capability we added to the platform, from distributed hyperparameter optimization using Katib/Vizier to distributed training with Kubeflow operators, required building, deploying, and maintaining a corresponding set of custom Kubernetes orchestration logic. While this approach gave us maximum control, it also meant that a significant portion of our development cycle was dedicated to building and managing the infrastructure for each new feature, rather than the feature¬†itself.</p><p><strong>Managing State in a Distributed System<br></strong>To keep our platform‚Äôs database synchronized with the cluster state, we relied on background watcher scripts that continuously monitored Kubernetes events for job status changes, container updates, and ingress resource availability.</p><p>The eventually-consistent nature of Kubernetes created operational complexity. Training containers could succeed while Kubernetes marked jobs as failed due to sidecar issues. Event streams would timeout or arrive out of order. Container statuses could transition between states as different watchers processed conflicting events. We developed sophisticated synchronization checks and logic to handle these cases, but managing state consistency for thousands of daily jobs required considerable on-call attention and directly impacted our development velocity.</p><p><strong>Kubernetes Cluster Management<br></strong>A persistent challenge in managing a large-scale ML compute platform is optimizing resource utilization for heterogeneous workloads. ML jobs often have distinct phases with conflicting resource profiles: data processing tends to be memory-intensive, while model training is often CPU- or GPU-intensive. This created a complex optimization puzzle, making it challenging to maximize node utilization across the¬†cluster.</p><p>As the platform grew, we also had to proactively manage resource contention during bursts of highly parallel workloads. Ensuring that the cluster autoscaler could provision capacity quickly enough to prevent job queuing for critical workflows required careful planning and continuous management.</p><p>The pattern was clear: as the platform scaled, so did the operational investment required to manage its low-level infrastructure. To continue innovating for our users, we needed to abstract away this underlying complexity and refocus our efforts on what mattered most: building new platform capabilities, optimizing ML workflows, and accelerating the entire ML development lifecycle<em>.</em></p><h3>The Journey to LyftLearn 2.0</h3><p>The growing operational complexity of our Kubernetes stack was limiting our development velocity. This reality pushed us to explore how we could simplify operations while delivering more powerful capabilities to our users. We began evaluating managed solutions to abstract this infrastructure complexity, which led us to a deep evaluation of <a href="https://aws.amazon.com/sagemaker/">AWS SageMaker</a>.</p><p>We evaluated SageMaker across both our <strong>online</strong> (LyftLearn Serving) and <strong>offline</strong> (LyftLearn Compute)¬†stacks.</p><p><strong>For LyftLearn Serving</strong>, adopting SageMaker would have required a fundamental re-architecture of our core workflows. Our model deployment, promotion, and serving solutions were deeply integrated with Lyft‚Äôs internal tooling. Observability relied on our standard monitoring infrastructure, not on AWS CloudWatch. Client services communicated via<a href="https://www.envoyproxy.io/"> Envoy</a>, not via SageMaker‚Äôs specific invocation and authentication patterns.</p><p>Our analysis confirmed that the existing Kubernetes-based stack was exceptionally reliable and efficient, performing well within our required latency requirements. We determined the right path forward was to retain our existing, battle-tested model serving infrastructure.</p><p><strong>For LyftLearn Compute</strong>, the evaluation pointed in a different direction. This was where our greatest operational complexity lived: managing eventually-consistent job states, optimizing cluster capacity for heterogeneous workloads, and building custom Kubernetes orchestration for new ML capabilities.</p><p>SageMaker‚Äôs managed infrastructure would address these challenges directly. It offered out-of-the-box support for a variety of job types, which would allow us to stop building and maintaining low-level orchestration logic. Its native state management would eliminate the need for our custom watcher system, and its elastic compute model would handle capacity automatically, removing the need for complex cluster planning and autoscaling management.</p><p>While SageMaker‚Äôs per-instance costs were higher, the Total Cost of Ownership (TCO) was clearly lower. By eliminating idle compute, cluster administration overhead, and the constant infrastructure firefighting, the economics of a managed service made¬†sense.</p><p>The evaluation led to a clear strategy: adopt SageMaker for LyftLearn Compute, where we had the greatest opportunity to reduce operational complexity, and retain Kubernetes for LyftLearn Serving, where our existing solution was already highly reliable and efficient.</p><p>The diagram below provides a high-level, conceptual view of how we wanted to transform the offline¬†stack:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*1GXjfr0fXp4tcFBtapk_hQ.png" /><figcaption>Figure 3: LyftLearn Compute Evolution Plan</figcaption></figure><p><strong>On the left, the original architecture:</strong> LyftLearn Service sent job requests to a K8s Orchestration Service, which constructs Kubernetes Job specifications and submits them to the Kubernetes API. This orchestration service was complex‚Ää‚Äî‚Ääit managed pod configurations, resource allocations, volumes, and all the low-level details of Kubernetes jobs. Background watchers continuously polled the Kubernetes API for events‚Ää‚Äî‚Ääjob completions, container status changes, resource updates‚Ää‚Äî‚Ääand wrote those updates back to the LyftLearn database. The compute layer ran on Lyft-managed Kubernetes clusters.</p><p><strong>On the right, the new architecture: </strong>Under the hood, this is a significantly simpler solution. LyftLearn Service interacts with a lean SageMaker Manager Service that only makes AWS SDK calls‚Ää‚Äî‚Ääit doesn‚Äôt manage any low-level infrastructure. We replaced the fleet of problematic background watchers with a single, reliable SQS consumer that processes status updates pushed from EventBridge. The heavy lifting of orchestration and state management is delegated to AWS. The goal was simplification without losing¬†power.</p><p>It looks simple on a diagram, but making this transition without disrupting critical ML workflows and hundreds of users was a significant engineering challenge. The following sections detail some of the most difficult technical challenges we solved to make this migration possible.</p><h4>Migration: Solving the Hard¬†Problems</h4><p>Our core principle for the migration was to replace the execution engine‚Ää‚Äî‚ÄäKubernetes to SageMaker‚Ää‚Äî‚Ääwhile keeping our ML workflows completely unchanged. The actual ML code‚Ää‚Äî‚Ääthe Python scripts that train models, process data, and run inference‚Ää‚Äî‚Äähad to work identically on both platforms. No modifications to model training logic, no changes to data preprocessing, no updates to inference code.</p><p>Forcing hundreds of users across dozens of teams to rewrite their business-critical ML workflows was not an option. The cost of such a disruption in terms of lost productivity and engineering effort would have made the migration untenable, which meant the burden of compatibility was entirely on our platform. The requirement of zero code changes transformed the project into a complex systems engineering challenge for the ML Platform team, but it was a necessary one. The real task wasn‚Äôt just running a container on a different platform‚Ää‚Äî‚Ääit was ensuring environmental parity.</p><p>During the transition, we solved numerous challenges across the stack. Here are a few of the most complex ones we solved to make this possible.</p><p><strong>Replicating the Kubernetes Runtime Environment<br></strong>Our Kubernetes environment provided automatic credential injection via webhooks, metrics collection through sidecars, and configuration management via <em>ConfigMaps</em>. SageMaker offered none of these primitives. We built a compatibility layer into cross-platform base Docker images to replicate this behavior:</p><ul><li><strong>Credentials</strong>: In Kubernetes, credentials from our internal secret management solution, <a href="https://lyft.github.io/confidant/">Confidant</a>¬†, were automatically injected at pod creation. SageMaker has no equivalent mechanism. We built a custom solution, as part of the container entrypoint script, that fetches credentials at job startup and exposes them exactly as Kubernetes did, ensuring user code worked identically on both platforms</li><li><strong>Environment Variables</strong>: SageMaker constrains the number of environment variables passed via its API. Similar to our credential solution, we moved most environment setup to runtime, fetching additional configuration at job¬†startup.</li><li><strong>Metrics</strong>: Kubernetes workloads sent <a href="https://github.com/statsd/statsd">StatsD</a> metrics to sidecar containers. SageMaker has no sidecar support, so we reconfigured the runtime and networking to connect directly to our metrics aggregation gateway. The user-facing API remained unchanged.</li><li><strong>Hyperparameters</strong>: In Kubernetes, hyperparameters were stored in <em>ConfigMaps</em> and mounted as files. SageMaker‚Äôs API has much stricter size limits than K8s, making direct parameter passing impossible for our use cases. We developed a solution to upload hyperparameters to AWS S3 before each job and have SageMaker automatically download them to its standard input path. This overcame the API limitation while still using SageMaker‚Äôs native capabilities.</li></ul><p>These represent only a subset of the environmental differences we systematically solved across the migration.</p><p><strong>Building for the Hybrid Architecture<br></strong>We developed new SageMaker-compatible base images to replace our old LyftLearn images. The critical design requirement was that these images must work across our entire hybrid platform: in SageMaker (for training and batch processing) and in Kubernetes (for serving). This meant the same Docker image that trained a model would also serve it, guaranteeing consistency. These base images serve as a foundation that teams extend with their own dependencies.</p><p>We built SageMaker-compatible base images with different capabilities to match our workload diversity. Here are some of the most important ones:</p><ul><li><strong>LyftLearn image:</strong><em> </em>For traditional ML workloads</li><li><strong>LyftLearn Distributed image:</strong> Adds Spark ecosystem integration for distributed processing</li><li><strong>LyftLearn DL image:</strong> Adds GPU support and libraries for deep learning workloads</li></ul><p>The Spark-compatible images presented the biggest challenge. They needed to maintain full compatibility with our existing Spark infrastructure‚Ää‚Äî‚Ääcustom wrappers, executor configurations, and JAR (Java Archive) dependencies. But they also had to run correctly in three distinct execution contexts: SageMaker Jobs, SageMaker Studio notebooks, and Model serving in¬†K8s.</p><p>These images detect their execution environment at runtime and adapt. They automatically configure different environment variables, use different users and permissions, and set up Spark appropriately for each context, all while preserving an identical core¬†runtime.</p><p><strong>Matching Kubernetes Job Launch Times<br></strong>In Kubernetes, notebooks, training, and processing jobs could start quickly because nodes were warm due to a significant percentage of cluster resources sitting idle. SageMaker provisions instances on-demand‚Ää‚Äî‚Ääno idle waste, but slower¬†startup.</p><p>For JupyterLab notebooks, we adopted <em>SOCI</em> (Seekable Open Container Initiative) indexes. <em>SOCI</em> enables lazy loading: SageMaker fetches only the filesystem layers needed immediately rather than pulling entire multi-gigabyte images. This cut notebook startup times by¬†40‚Äì50%.</p><p>For training and batch processing jobs, <em>SOCI</em> wasn‚Äôt available. We optimized our Docker image sizes, which were sufficient for most of our workloads. However, this wasn‚Äôt enough for our most latency-sensitive workflows. Some models retrain every 15 minutes, making slower startup times unacceptable. For this subset of jobs, we adopted <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/train-warm-pools-how-to-use.html">SageMaker‚Äôs warm pools</a>, which keep instances alive between¬†runs.</p><p>These optimizations gave us Kubernetes-like startup times with fully serverless infrastructure.</p><p><strong>Cross-Cluster Networking for Spark<br></strong>Many of our ML Platform users rely heavily on the interactive Spark experience in JupyterLab notebooks. In Kubernetes, this was simple, as the driver and executors ran in the same cluster. The new architecture, however, required the Spark driver to run in a SageMaker Studio notebook while the executors remained on our EKS K8s¬†cluster.</p><p>This hybrid model presented a major networking challenge, as shown in the diagram below. Spark client mode requires bidirectional communication:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*0TVs7AcCQMI_t7FrJFpKrQ.png" /><figcaption>Figure 4: Spark Networking Architecture in LyftLearn 2.0</figcaption></figure><ul><li>The driver (in SageMaker) must call the EKS API Server Endpoint to request executor¬†pods.</li><li>The executor pods must be able to establish inbound connections directly back to the driver‚Äôs SageMaker Instance Elastic Network Interface (ENI).</li></ul><p>The default SageMaker Studio networking blocked these critical inbound connections, breaking Spark‚Äôs communication model. This issue was a fundamental blocker that could jeopardize the entire migration. Without a solution for interactive Spark, we could not move our users to SageMaker Studio. To resolve this, we partnered closely with the AWS team. As a result of this collaboration, they introduced networking changes to the Studio Domains in our account that enabled the required inbound traffic from our EKS cluster. Despite the cross-cluster setup, Spark performance remained the same, and the interactive experience for ML Platform users was identical to the original Kubernetes environment.</p><h3>LyftLearn 2.0: The Hybrid Architecture</h3><p>As a result of this architectural transformation, we arrived at the hybrid architecture we planned: SageMaker for LyftLearn Compute and Kubernetes for LyftLearn Serving.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*FDqyuwxPjGjobOPxWkfMCQ.png" /><figcaption><em>Figure 5: Complete LyftLearn 2.0 High-Level Architecture</em></figcaption></figure><p>As the diagram illustrates, the two systems are fully decoupled, each operating as a purpose-built stack:</p><p><strong>LyftLearn Serving</strong> runs on Kubernetes, powering a distributed architecture for real-time inference. Dozens of ML teams deploy their own model serving services‚Ää‚Äî‚Ääeach containing their team‚Äôs models with custom prediction handlers and configurations‚Ää‚Äî‚Äähandling production predictions for specific use cases (pricing, fraud, dispatch, ETA, etc.). The Model Registry Service coordinates model deployments across these services. (We detailed this serving architecture in our 2023 blog post: <a href="https://eng.lyft.com/powering-millions-of-real-time-decisions-with-lyftlearn-serving-9bb1f73318dc">Powering Millions of Real-Time Decisions with LyftLearn Serving</a>.)</p><p><strong>LyftLearn Compute</strong> runs on SageMaker, where the SageMaker Manager Service orchestrates training, batch processing, Hyperparameter Optimization (HPO), and JupyterLab notebooks through AWS SDK calls. EventBridge and SQS provide event-driven state management, replacing our background watchers.</p><p>Integration happens through the Model Registry and S3. Training jobs in SageMaker generate model binaries and save them to S3. The Model Registry tracks these artifacts, and model serving services pull them for deployment. Docker images flow from CI/CD through ECR to both platforms. The LyftLearn database maintains job metadata and model configurations across both¬†stacks.</p><p>Each LyftLearn product operates independently while maintaining seamless end-to-end ML workflows.</p><h3>Putting It All¬†Together</h3><p>We rolled out changes repository by repository, running both infrastructures in parallel. Our approach was systematic: build a comprehensive compatibility layer that made SageMaker feel like Kubernetes to ML code, validate each workflow type thoroughly, then migrate teams incrementally. Each repository required minimal changes‚Ää‚Äî‚Äätypically updating configuration files and workflow APIs‚Ää‚Äî‚Ääwhile the actual ML code remained untouched.</p><p>For our users, the migration was nearly invisible. But behind the scenes, the operational improvements were substantial. We reduced ML training and batch processing compute costs by eliminating idle cluster resources and moving to on-demand provisioning. System reliability improved significantly, with infrastructure-related incidents becoming rare occurrences. Most importantly, this stability and the serverless nature of the new compute freed our team to focus on building platform capabilities rather than managing low-level infrastructure components.</p><h4>Key Lessons</h4><p><strong>Build versus buy is a pragmatic decision, not an ideology</strong> <br>We adopted SageMaker for training because managing custom batch compute infrastructure was consuming engineering capacity better spent on ML platform capabilities. We kept our serving infrastructure custom-built because it delivered the cost efficiency and control we needed. The decision wasn‚Äôt about preferring managed services or custom infrastructure‚Ää‚Äî‚Ääit was about choosing the right tool for each specific workload.</p><p><strong>Abstract complexity from users</strong>. <br>The migration succeeded because we absorbed all the complexity. Users didn‚Äôt rewrite ML code or learn SageMaker APIs‚Ää‚Äî‚Ääthey continued their work while we handled secrets management, networking, metrics collection, and environmental parity. The platform‚Äôs job is to evolve infrastructure while preserving velocity and avoiding disruptions, not to distribute migration work across hundreds of¬†teams.</p><p><strong>Invest in compatibility layers<br></strong> The cross-platform base images were the foundation of the migration‚Äôs success. They enabled gradual, repository-by-repository migration with easy rollbacks. Most importantly, they guaranteed that the same Docker image for model training in SageMaker would serve it in Kubernetes, eliminating train-serve inconsistencies. The upfront investment in cross-platform compatibility paid dividends throughout the migration.</p><blockquote>The best platform engineering isn‚Äôt about the technology stack you run‚Ää‚Äî‚Ääit‚Äôs about the complexity you hide and the velocity you¬†unlock.</blockquote><h4>Acknowledgment</h4><p>This platform evolution was a massive team effort. Special thanks to <a href="https://ua.linkedin.com/in/vladyermakov"><strong>Vlad Yermakov</strong></a><strong>, </strong><a href="https://ua.linkedin.com/in/herman-khivrenko-ab488618b"><strong>Herman Khivrenko</strong></a><strong>, </strong><a href="https://ca.linkedin.com/in/nimanasiri"><strong>Nima Nasiri</strong></a><strong> and </strong><a href="https://www.linkedin.com/in/andyrosales"><strong>Andy Rosales-Elias</strong></a><strong> </strong>for their exceptional work making it a¬†success.</p><p>We‚Äôre also grateful to <a href="https://www.linkedin.com/in/rajesh-bagwe-1995762/"><strong>Raj Bagwe</strong></a> and <a href="https://www.linkedin.com/in/vikramsawant/"><strong>Vikram Sawant</strong></a>, our partners from AWS, for their invaluable support on this initiative.</p><p><strong><em>Lyft is hiring!</em></strong><em> If you‚Äôre passionate about building AI/ML platforms and applications at scale, visit </em><a href="https://www.lyft.com/careers"><em>Lyft Careers</em></a><em> to see our openings.</em></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=547de6c950e1" width="1" height="1" alt=""><hr><p><a href="https://eng.lyft.com/lyftlearn-evolution-rethinking-ml-platform-architecture-547de6c950e1">LyftLearn Evolution: Rethinking ML Platform Architecture</a> was originally published in <a href="https://eng.lyft.com">Lyft Engineering</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[My Starter Project on the Lyft Rider Data Science Team]]></title>
            <link>https://eng.lyft.com/my-starter-project-on-the-lyft-rider-data-science-team-86a60dddd935?source=rss----25cd379abb8---4</link>
            <guid isPermaLink="false">https://medium.com/p/86a60dddd935</guid>
            <category><![CDATA[causal-inference]]></category>
            <category><![CDATA[data-science]]></category>
            <category><![CDATA[rideshare]]></category>
            <category><![CDATA[machine-learning]]></category>
            <category><![CDATA[artifical-intellegence]]></category>
            <dc:creator><![CDATA[Jacob Nogas]]></dc:creator>
            <pubDate>Tue, 07 Oct 2025 14:41:38 GMT</pubDate>
            <atom:updated>2025-10-07T14:41:34.115Z</atom:updated>
            <content:encoded><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/450/1*XmS2ckVGZJ6GDLfnTwp3UQ.png" /><figcaption>Credit to Brian Wu for creating the illustrations in this¬†post.</figcaption></figure><p>I joined Lyft in January of 2024, as a Data Scientist‚Ää‚Äî‚ÄäDecisions, on the Rider Science Core Experience team. My journey at Lyft began with a starter project, which focussed on using the Rider Experience Score (RES) tool to measure long-term effects of various rider experiences at¬†Lyft.</p><p>In this blog post, I will discuss my experience at Lyft as a new hire, focusing on this starter¬†project.</p><h3>What is¬†RES?</h3><h4>Motivation</h4><p>At Lyft, we aim to deliver seamless and reliable experiences for our riders. To continuously improve the platform, it‚Äôs important to understand which rider experiences most impact our riders, and how those experiences influence their decision to continue using Lyft over time (rider retention).</p><p>For example, we can imagine a hypothetical scenario where a rider experiences a lower than normal ETA (the estimated request to pickup time). Experiencing lower ETA can potentially be an improved experience for riders, motivating us to make a product change which drives a decrease in ETA. To justify the introduction of this product change, we first want to quantify how low ETA impacts long-term rider retention. Using an A/B test to evaluate the long-term rider retention impact of low ETA may be problematic, since an A/B test typically runs for 2‚Äì6 weeks, which may not be a long enough time period to accurately measure long-term effects. In other cases, A/B tests may not be possible, such as when introducing a new feature which is rolled out to all¬†users.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*RjN8moizEc9uQPwhu40Mug.png" /><figcaption>Figure 1: Illustration of total rides per customer vs time. Group 1 had a positive experience (such as low ETA), whereas Group 2 did not. Œî shows the difference in long-term total rides across Group 1 and 2. This image is adapted from the presentation Customers Obsessed Experimentation and Metrics by Ricky Chachra (December 14,¬†2023).</figcaption></figure><p>RES is a tool for estimating how various user experiences (low ETA, early driver arrival, etc.) impact long-term rides taken, which is the Œî in Figure 1, without requiring an A/B¬†test.</p><h4>Challenges in Estimating the Effects of User Experiences</h4><p>In order to explain RES, I will first discuss the challenges in estimating how user experiences impact rider retention.</p><p>The essence of this problem is estimating the causal effect of exposing riders to a particular experience on rider retention. For simplicity, we define the rider retention effect as the impact on the number of rides taken in the future 28 days. The true causal effect of a rider experiencing low ETA would be obtained by observing the future 28 day rides for a rider session in which low ETA is encountered, and comparing that to the future 28 day rides for the exact same rider session, except low ETA is not encountered (counterfactual). But, it‚Äôs impossible to observe counterfactuals; in reality, for a given session, we only observe the session where the rider experiences low ETA or not, but not both (the Fundamental Problem of Causal Inference).</p><p>A simple potential solution to overcome the challenge of not being able to observe the counterfactual outcome might be to look at historical observational data for riders that experienced low ETA, and compare their average ride retention to riders who didn‚Äôt experience low ETA, which is the average treatment effect (ATE) estimated by difference-in-means estimator.</p><p>However, this naive approach can lead us astray. Let‚Äôs see what happens when we apply this method. Figure 2 shows hypothetical results (not real data) that illustrate the problem we might encounter.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/450/1*rshk2JsslTUutmNRp_tvOg.png" /><figcaption>Figure 2: Plotting low ETA vs. # of rides in the next 28 days for all regions. Left of the dashed line didn‚Äôt see low ETA, right¬†did.</figcaption></figure><p>Based on the observations in Figure 2, we would conclude that low ETA actually has a negative effect on rider retention, which must be incorrect. To understand how we arrived at this incorrect conclusion, we segment our data into regions City A and City B, and also indicate on the x-axis whether or not the rider experienced low ETA (left of dashed line didn‚Äôt see low ETA, right did); results are shown in Figure¬†3.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/450/1*XmS2ckVGZJ6GDLfnTwp3UQ.png" /><figcaption>Figure 3: Plotting low ETA vs. # of rides in the next 28 days with regions highlighted.</figcaption></figure><p>We now see that within City B, and within City A, the effect of low ETA is showing a positive trend, as expected. We also see that City B is much less likely to be in the ‚Äútreatment‚Äù group (right of dash, low ETA), and that City B has higher baseline rides in the next 28 days. We thus see that our groups for low and not low ETA are biased (selection bias), which results in an incorrect estimate of the causal effect of low ETA. The difference-in-means estimator works well for randomized experiments, but is biased for non-randomized cases; in the above example, region is correlated with both the treatment ‚Äòlow ETA‚Äô, as well as the outcome # of rides in the next 28 days (region is a confounder). This is an example of an issue that can arise when estimating causal effects from observational data.</p><p>The gold standard approach for estimating causal effects is randomized experimentation (A/B test). With randomization, we would end up with a roughly equal split of City A and City B across control and treatment groups, thus mitigating the bias discussed above. But, a limitation of an A/B test is that we typically can only run them for a fixed short period of time, which won‚Äôt allow measuring long-term effects on rider retention. We thus can benefit from methodology that can mitigate the bias in causal effects estimated from observational data.</p><h4>RES Methodology</h4><p>The RES tool employs causal inference methodology to mitigate the bias in causal effects measurements obtained with observational data.</p><p>There are various methods to control for confounding variables. Propensity score methods model the relationship between confounders X and treatments W (i.e. with an ML model), outcome methods model the relationship between confounders X and the outcome Y, and double ML methods model both the relationship between X and Y and W and¬†X.</p><p>RES employs Augmented Inverse Propensity Score Weighting estimator (AIPW; more info on AIPW can be found <a href="https://www.law.berkeley.edu/files/AIPW(1).pdf">here</a>), which is an example of a double ML¬†Method.</p><p>At a high level, AIPW consists of estimating potential outcome functions of treatment and control experiences (Direct Method), as well as adjusting estimation bias with propensity weighted residuals. AIPW has the desirable theoretical properties of Neyman Orthogonality (robust to ML estimation errors), and doubly robustness. We also apply a <a href="https://academic.oup.com/ectj/article-abstract/21/1/C1/5056401?redirectedFrom=fulltext">cross-fitting</a> procedure (data split) to get Double-ML properties (unbiased estimation).</p><p>More precisely, we compute the treatment effect as¬†follows:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*Mu1m_p4C_ofzN0ywAIVW5w.png" /></figure><p>In order to compute the treatment effect, we train three XGBoost or LightGBM¬†models</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/490/1*bGEjtKvZLmL5YodYR3Oy6A.png" /></figure><p>For every observation i, using the above models (and cross-fitting), we predict the outcome <em>Œº‚ÇÅ*(x·µ¢)</em>, <em>Œº‚ÇÄ*(x·µ¢)</em>, and <em>e*(x·µ¢)</em>. Finally, we plug <em>Œº‚ÇÅ*(x·µ¢)</em>, <em>Œº‚ÇÄ*(x·µ¢)</em>, and <em>e*(x·µ¢)</em> into the equation for AIPW to get an estimate for the ATE for low ETA. Though the relationships between outcomes, treatment, and confounders in the above low ETA example are relatively simple, using machine learning models for <em>e*(x·µ¢) </em>and <em>Œº*(x·µ¢) </em>allows us to model complex non-linear relationships which may arise in other settings.</p><p>In the example above, AIPW helps us avoid making a wrong conclusion in two¬†ways:</p><ol><li>By modelling the relationship between region and 28-day rides (<em>Œº*(x·µ¢)</em>), we are able to isolate the effect of region on our outcome Y, and capture that City B riders are more likely to have more rides in 28 days than City¬†A.</li><li>By modelling the relationship between region and low vs. normal ETA (<em>e*(x·µ¢)</em>), we capture that City B riders are less likely to experience low ETA than City A riders. AIPW reweights observations inversely to their propensity scores, effectively upweighting points in the bottom-left and top-right of Figure 3. This reweighting reveals the underlying upward slope, allowing us to conclude that low ETA has a positive impact on 28-day¬†rides.</li></ol><p>The AIPW estimator combines both 1. and 2., which gives favorable statistical properties, such as treatment effect estimation being insensitive to errors in models <em>e*(x·µ¢)</em> and¬†<em>Œº*(x·µ¢)</em>.</p><h3>What I did as a new¬†hire</h3><p>The existing RES estimates needed an update, and there was also a need to add additional rider experiences to the RES pipeline. RES generally needed a refresh, and I was tasked with doing¬†so.</p><p>I started by gaining more familiarity with AIPW. Reading through the internal causal inference lecture series at Lyft was extremely helpful. I also found Stefan Wager‚Äôs STATS 361 <a href="https://web.stanford.edu/~swager/stats361.pdf">course notes</a> very useful for learning about¬†AIPW.</p><p>Next, I spent time learning about how to use <a href="https://eng.lyft.com/lyftlearn-ml-model-training-infrastructure-built-on-kubernetes-aef8218842bb">LyftLearn</a>, Lyft‚Äôs internal computing platform for Big Data and Machine Learning. I then became familiar with how to use the RES codebase. I analyzed the RES codebase to see if there were any opportunities to improve reliability and efficiency of the RES code. I identified some aspects of the RES code which could be improved, and then submitted a pull request with the corresponding changes. For example, there was a subtle issue which prevented model diagnostics from completing in certain cases, which I was able to¬†fix.</p><p>With the unblocking of the RES pipeline, I sought to compute estimates of long-term effects of various Lyft user experiences. I began with identifying the most important experiences to add to the RES pipeline.</p><p>I reached out to other teams to see which experiences would be most important to their work. Examples of experiences that were identified are ‚ÄúImproved Match Time Prediction‚Äù, and ‚ÄúImproved ETA Reliability‚Äù.</p><p>After gathering a list of experiences, I prioritized and selected 23 based on discussions with my manager about their potential impact. This process provided great insight into the experiences that matter most to our riders, and where we could have the greatest positive impact on retention. I also refreshed estimates for the pre-existing reliability experiences (with newly added High Value Mode sub group analysis).</p><p>A major challenge I faced in computing estimates for my chosen experiences was selecting confounders. Internal RES documentation provides excellent guidance on selecting confounders, but significant trial and error was still required. For example, I computed RES estimates for Prime Time experience, where Prime Time refers to a multiplier on ride price during high demand periods, and I had included Neighborhood Supply as a confounder. The ROC AUC of our trained model was suspiciously high, which we realized was partly due to Neighborhood Supply being a leaky confounder for Prime Time. A leaky confounder is a covariate which contains information that is concurrent or subsequent to the experience; this is an issue, as it can cause some of the treatment effect to be attributed to the confounder, leading to biased estimates.</p><p>For each of the 23 experiences I worked on, I had to make sure to include key confounders, and also avoid including problematic confounders, which was very time consuming. But, this process provided a great opportunity to learn about various data sources that exist at Lyft, and to reflect on how key covariates may be related causally to Lyft rider experiences.</p><h3>Conclusion</h3><p>Though I faced many challenges in my starter project, I had excellent support from my colleagues at Lyft, and was able to successfully generate causal estimates for the 23 user experiences identified.</p><p>Navigating the various challenges I encountered was a very intellectually stimulating and fulfilling experience. The insights from RES play a crucial role in helping teams focus on the experiences that matter most to our riders; contributing to a workstream with such a high impact on improving rider experience has been very rewarding.</p><p>Getting started at Lyft has been an enriching journey. I‚Äôve had the opportunity to apply real-world causal inference techniques and collaborate with amazing colleagues. I‚Äôm excited to continue contributing to impactful projects, and to see what the future¬†holds.</p><p><em>Lyft is hiring! If you‚Äôre passionate about Data Science, visit </em><a href="https://www.lyft.com/careers"><em>Lyft Careers</em></a><em> to see our openings.</em></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=86a60dddd935" width="1" height="1" alt=""><hr><p><a href="https://eng.lyft.com/my-starter-project-on-the-lyft-rider-data-science-team-86a60dddd935">My Starter Project on the Lyft Rider Data Science Team</a> was originally published in <a href="https://eng.lyft.com">Lyft Engineering</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Migrating Lyft‚Äôs Android Codebase to Kotlin]]></title>
            <link>https://eng.lyft.com/migrating-lyfts-android-codebase-to-kotlin-53b231dfecb5?source=rss----25cd379abb8---4</link>
            <guid isPermaLink="false">https://medium.com/p/53b231dfecb5</guid>
            <category><![CDATA[android]]></category>
            <category><![CDATA[kotlin]]></category>
            <category><![CDATA[programming]]></category>
            <category><![CDATA[lyft]]></category>
            <dc:creator><![CDATA[Oleksii Chyrkov]]></dc:creator>
            <pubDate>Tue, 09 Sep 2025 20:34:03 GMT</pubDate>
            <atom:updated>2025-09-09T13:48:32.011Z</atom:updated>
            <content:encoded><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*9HXuWJOvxM-TqitBdgZ6oQ.jpeg" /></figure><p><strong>Introduction</strong></p><p>Lyft started adopting Kotlin into our Android codebase in 2018. Fast forward 7 years, and we are finally done! Lyft Rider, Driver and Lyft Urban Solutions apps are now fully Kotlin-based.</p><p>I joined Lyft in 2022, so this post will describe the efforts undertaken after¬†that.</p><p>Our motivation included several¬†points:</p><ul><li>Kotlin code is more concise, and oftentimes, way more concise than Java. In some cases, 10 lines of Java could be turned into a 1-liner in¬†Kotlin.</li><li>We get compile speed benefits by using the newest K2 compiler.</li><li>All the new UIs in Lyft are written using Compose‚Ää‚Äî‚Ääthe modern declarative UI framework approach which is the industry standard. All the existing UIs will eventually be migrated to Compose. That also means Kotlin was the only option, as Compose only supports¬†Kotlin.</li><li>We started adopting Coroutines‚Ää‚Äî‚Ääthe structured concurrency framework which greatly simplifies writing asynchronous code. Coroutines are part of the Kotlin standard library, so that was an extra argument to adopt Kotlin¬†faster.</li><li>Our codebase is huge, so we often run automated migrations, which require adopting migration scripts for Java as¬†well.</li><li>Working entirely in Kotlin is a big plus for engineers considering Lyft.</li></ul><p><strong>Pre-migration</strong></p><p>The first thing which needs to be done when undertaking a project this large is to know where we are standing. To achieve this, Lyft has an internal tool called Migration Tracker, which tracks all of the migrations in both Android and iOS codebases. Examples of migrations are:</p><ul><li>Migrating from RxJava to Coroutines</li><li>Switching from our old UI approach to the new declarative one</li><li>Eliminating uses of Java in favor of¬†Kotlin</li></ul><p>A daily cron job runs the Migration Tracker and updates an internal website, presenting graphs which help us ensure we meet the migration deadlines.</p><p>As of Feb 24, 2025, the Kotlin migration was 85% ready. That means we still needed to migrate about 1,000 files scattered across 20+ teams and 150+ Bazel¬†modules.</p><p><strong>Migration</strong></p><p>Fellow developer Oleksii Zaiats built a tool which greatly sped up the migration process: the Migration Script. It leveraged Android Studio IDE Scripting, which is a powerful but rarely used tool, fitting perfectly for this kind of¬†task.</p><p>Put simply, the script flow was as¬†follows:</p><ul><li>For a given team, it found all modules owned by the¬†team.</li><li>For the given module, it ran the automatic migration mechanism for each Java¬†file.</li><li>It automatically fixed some common imperfections of Android Studio‚Äôs built-in Java to Kotlin converter.</li><li>After all the files in the module were migrated to Kotlin, it created a git commit with all the changes, naming the git branch appropriately.</li><li>The owning team was notified of the changes and reviews were requested.</li></ul><p>This simple approach was not without its flaws, but it gave us a real productivity boost and allowed us to migrate a couple of modules per¬†day.</p><p><strong>Challenges and¬†Caveats</strong></p><p>The major pain point in the migration process was that the automatic migration tool was not¬†perfect:</p><ul><li>In many cases, it uses nullable-types where non-nullable types are fine, resulting in code like <strong>Observable&lt;Optional&lt;List&lt;Ride?&gt;?&gt;?&gt;?</strong></li><li>It is unnecessarily verbose, using explicit types everywhere.</li><li>It is not smart enough to convert an explicit for loop into a Kotlin one-liner using <strong>map</strong> or¬†<strong>filter</strong>.</li><li>It cannot automatically use <strong>lateinit var</strong> which is often needed when writing View-based UIs.</li></ul><p>In addition, the structure of the legacy code itself presented additional complications. We had some very old code implementing a hand-written <strong>INullable</strong> interface, which was similar to <strong>Optional</strong> but not quite. The semantics of <strong>INullable</strong> required us to do extensive code review each time we touched one of these¬†classes.</p><p>Last but not least, once we encountered a class which absolutely had to be written in Java! It was implementing an interface with a signature like¬†so:</p><pre>public void onTouchEvent(@NonNull Float x, @NonNull Float y)</pre><p>However, on some devices, the API contract was broken, so <strong>x </strong>and <strong>y</strong> could in fact be null! In Java this was totally fine, but in Kotlin this resulted in a crash. Thankfully, we have rewritten the entire screen using another approach, not using this interface anymore.</p><p><strong>Post-migration</strong></p><p>After we were finally done, we needed to ensure that no one accidentally adds a Java file to our codebase. To achieve this, we have added a Lint check, integrated in our CI system, which checks every pull request and explicitly prohibits Java¬†code.</p><p><strong>Conclusion</strong></p><p>After concluding this years-long effort, Lyft developers can skip the hassle of Java-Kotlin interop and concentrate on solving the problem that actually matters‚Ää‚Äî‚Ääproviding our users with the world‚Äôs best transportation!</p><p><em>Lyft is hiring! If you‚Äôre passionate about working on a Kotlin-only app, visit </em><a href="https://www.lyft.com/careers"><em>Lyft Careers</em></a><em> to see our openings.</em></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=53b231dfecb5" width="1" height="1" alt=""><hr><p><a href="https://eng.lyft.com/migrating-lyfts-android-codebase-to-kotlin-53b231dfecb5">Migrating Lyft‚Äôs Android Codebase to Kotlin</a> was originally published in <a href="https://eng.lyft.com">Lyft Engineering</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Intern Experience at Lyft]]></title>
            <link>https://eng.lyft.com/intern-experience-at-lyft-9b7338e7c8bb?source=rss----25cd379abb8---4</link>
            <guid isPermaLink="false">https://medium.com/p/9b7338e7c8bb</guid>
            <category><![CDATA[lyft]]></category>
            <category><![CDATA[machine-learning]]></category>
            <category><![CDATA[internships]]></category>
            <category><![CDATA[causal-inference]]></category>
            <dc:creator><![CDATA[Iraklikhorguani]]></dc:creator>
            <pubDate>Thu, 14 Aug 2025 19:34:57 GMT</pubDate>
            <atom:updated>2025-08-14T19:34:57.281Z</atom:updated>
            <content:encoded><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*f5bDERtosR0e_DlgBWek-Q.jpeg" /></figure><p><em>Written by </em><a href="https://www.linkedin.com/in/taiebat/">Morteza Taiebat</a> <em>and </em><a href="https://www.linkedin.com/in/han-gong-20237214a/">Han Gong</a> <em>at¬†Lyft.</em></p><h3>Introduction</h3><p>This post is authored by two current Lyft Data Scientists, <a href="https://www.linkedin.com/in/taiebat/">Morteza Taiebat</a> and <a href="https://www.linkedin.com/in/han-gong-20237214a/">Han Gong</a>, who both began their journeys at the company as interns. They share their experiences leading up to their internships, the work they did while at Lyft, and what ultimately motivated them to return as full-time employees.</p><p>Whether you‚Äôre a prospective intern, a new hire, a candidate in the interview process, or simply curious about life at Lyft, this post offers insights into what it‚Äôs like to grow your career¬†here.</p><h3>Story from Morteza¬†Taiebat</h3><h4>Background Before¬†Lyft</h4><p>My journey at Lyft began through the <a href="https://edfclimatecorps.org/engagement/lyft-inc-morteza-taiebat-2020">EDF Climate Corps program</a>, which places fellows in top organizations through a rigorous technical assessment process. Coming from an academic background, my research focused on <a href="https://deepblue.lib.umich.edu/handle/2027.42/170040">studying</a> the impacts of emerging transportation trends like automation, electrification, and shared mobility on travel behavior and sustainability outcomes. Joining Lyft gave me the opportunity to apply my expertise in a way that aligned perfectly with my passion for sustainability and shaping the future of transportation.</p><h4>Team at¬†Lyft</h4><p>I joined Lyft‚Äôs Sustainability team in May 2020, around the time the company began outlining its vision and strategy to scale electric vehicle adoption across the platform. During my internship, I focused on building causal models to assess EV suitability and impact on driver productivity. This involved evaluating which types of driving patterns and operational contexts were most suited for EV adoption, in order to maximize driver benefits and emissions reductions. I assessed EV suitability based on factors like driving patterns, cost savings, and charging infrastructure availability, while also analyzing how EV adoption influenced driver engagement. By leveraging these models, we were able to quantify the impact of EV conversion on driver productivity and inform strategies for sustainable adoption across the different lines of business on the Lyft platform.</p><h4>Intern Project</h4><p>I focused on analyzing Driver-Hour (DH), Lyft‚Äôs preferred metric for measuring driver productivity, and on understanding the causal impact of EV conversion on this metric. To evaluate the effect of EV adoption on DH, I developed a <a href="https://en.wikipedia.org/wiki/Difference_in_differences">difference-in-differences (DiD)</a> model that compared changes in driver productivity before and after EV conversion. This <em>quasi-experimental design</em> contrasted the productivity trends of drivers who switched to EVs (treatment group) with those who continued using internal combustion engine vehicles (control¬†group).</p><p>The DiD design hinged on the comparison between two groups over a specified pre and post conversion period:</p><ul><li><strong>Treatment Group:</strong> Drivers who switched to EVs in the specific past X¬†months.</li><li><strong>Control Group:</strong> Drivers who continued using gas-powered vehicles.</li></ul><p>The equation at the heart of this model¬†was:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*CNlVlcNE_svx0o3WEi5yYA.png" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*JK0cXiJ8Aiw8o7Ql" /><figcaption><em>Schematic representation of difference-in-differences (DiD) approach for analysis of EV conversion effect on driver productivity</em></figcaption></figure><p>A major challenge was understanding driver behavior. Many drivers on the Lyft platform are app-switchers, meaning they drive on multiple rideshare or delivery platforms. This limited our visibility into their complete driving patterns. To address this, I worked on integrating third-party data and making assumptions based on observable patterns on the Lyft platform. We also faced the issue of limited access to home charging, which affects drivers‚Äô ability to make EVs viable for daily use. By incorporating projections of public charging station growth, we were able to better understand future feasibility for¬†drivers.</p><p>This model became the backbone for identifying driver segments and usage patterns associated with successful EV transitions that minimize productivity disruptions, helping inform outreach strategies and support Lyft‚Äôs long-term sustainability goals.</p><h4>Returning to¬†Lyft</h4><p>One of the things I appreciated most during my internship at Lyft was the culture of collaboration and focus on real impact. Everything in the rideshare space can be turned into a math problem, but the key is knowing which problems are worth solving. The work pushes you to think practically‚Ää‚Äî‚Ääwhat‚Äôs going to move the needle the most? That mindset stuck with me. I found myself applying the 80/20 rule constantly: 80% of the results come from focusing on the most impactful 20% of the¬†efforts.</p><p>During my time on the Sustainability team, for example, we were exploring how EV adoption affected the driver experience. There were several analytical paths we could‚Äôve taken‚Ää‚Äî‚Äämodeling charging downtime, emissions savings, changes in vehicle maintenance costs‚Ää‚Äî‚Ääbut I chose to prioritize earnings impact, since that‚Äôs what drivers cared about most and what would influence policy decisions the fastest. That choice paid off: the work shaped internal strategy and was later <a href="https://doi.org/10.1016/j.apenergy.2022.119246">published</a> in a respected journal. It was one of those moments that made me realize how data science, when focused and well-scoped, can lead to real-world change.</p><p>Unlike academic research, where refining a problem can take unlimited time, at Lyft the focus is on building solutions that deliver meaningful results quickly, and then improving on them over time. That practical, fast-moving environment was a big part of what drew me back. After completing my fellowship, I rejoined the Sustainability team in 2021 and continued the work I‚Äôd started as an intern. Later, as Lyft formalized its EV team, I had the chance to interview for a full-time data science role and became Lyft‚Äôs first EV Data Scientist‚Ää‚Äî‚Ääa milestone that was incredibly fulfilling. Transitioning into the Algorithms archetype and eventually joining the Marketplace team deepened my understanding of the rideshare landscape. These steps have paved the way for my continued growth, as I tackle an ever-widening range of challenges in a dynamic, two-sided marketplace.</p><h4>Advice for Future Interns/Employees</h4><p>For those coming from an academic background, I recommend learning early how to balance execution speed with quality‚Ää‚Äî‚Ääa skill that‚Äôs not always emphasized in academia. Be proactive in seeking mentorship and take full ownership of your projects. The transition from intern to full-time data scientist has been a journey of continuous growth and learning. Whether it‚Äôs shaping EV adoption strategies or optimizing marketplace algorithms, working at Lyft has enabled me to engage with projects that truly align with my passions and deliver tangible impact. If I could go back, I would focus on building partnerships with industry leaders and attending more conferences to better understand the challenges they face, and to invest in my skills accordingly.</p><p>If you‚Äôre considering an internship at Lyft, I encourage you to go for it‚Ää‚Äî‚Ääthere‚Äôs no better place to learn and make a difference!</p><h3>Story from Han¬†Gong</h3><h4>Background Before¬†Lyft</h4><p>In the summer of 2021, I was pursuing my graduate degree in MIDS (Master of Interdisciplinary Data Science) and looking to transition into a true Data Science role. Previously, I had interned in FMCG (fast-moving consumer goods) and tech, but my work felt more like writing SQL queries than solving meaningful data problems. Despite my data science background, I wasn‚Äôt sure what being a Data Scientist really meant in industry‚Ää‚Äî‚Ääor if I‚Äôd even enjoy¬†it.</p><p>I chose Lyft for its strong data science community and the chance to contribute to an app I actually¬†used.</p><h4>Team at¬†Lyft</h4><p>I joined as a ‚ÄúData Scientist, Product Intern‚Äù on the Driver Loyalty team which focuses on acquiring, identifying, and retaining top-tiered drivers through initiatives like segmentation, driving score, and the Lyft Rewards loyalty program. My primary focus was to grow Lyft Rewards as a tiered loyalty program where I worked with PMs, scientists and engineers to define performance metrics, understand behaviors that predict engagement trends, and optimize incentives and perks strategy to maximize efficiency.</p><h4>Intern Project</h4><p>During the internship, I worked on an analysis and experimentation plan to launch a tiered referral bonus for existing drivers acting as referrers. The hypothesis suggests that Platinum drivers, who represent Lyft‚Äôs top-tier drivers, may have a tendency to refer new drivers who demonstrate a higher likelihood of remaining active and performing well (e.g., receiving high ratings, maintaining a low cancellation rate, driving more hours, and having a longer retention rate) in comparison to referrals from other drivers. The goal was to understand how drivers across different tiers respond to referral incentives and how their referrals differ in terms of retention and productivity. We also aimed to evaluate whether the current bonus structure aligns with the value generated by referred¬†drivers.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/840/0*XTXAu0RyWd0apabH" /><figcaption>New drivers referred by Platinum referrers drove 8.5 more hours on average within the first 60 days post-activation (including the initial 28-day bonus period). They also demonstrated longer retention even at 30, 60, and 90 days after the incentive period¬†ended.</figcaption></figure><p>Observational data was used to analyze how different referral bonus offers and referrer tiers correlated with lead quality, approval and activation rates, 28-day retention, driving hours (DH), and driver scores (DVS) of referees. Given the strong influence of seasonality and regional variations, a Hierarchical Linear Model (<a href="https://en.wikipedia.org/wiki/Multilevel_model">HLM</a>) with Fixed &amp; Random Effects was applied to isolate the true impact of referral bonuses while accounting for market fluctuations.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*-KrluJ4q8hyevuiUvaL7og.png" /></figure><p>To evaluate the cost-effectiveness of referral incentives, CPIDH (Cost Per Incremental Driver Hour) was used as the objective metric to compare the financial cost of bonus offers against the incremental gain in driver hours. The model predicted CPIDH by simulating different bonus structures and estimating incremental gains. By normalizing referrer and referee bonuses across regions and evaluating how bonus increases influenced referral activation rates, an optimal cost-benefit threshold was identified.</p><p>The analysis indicated that top-tier drivers often refer new drivers who tend to drive more hours and demonstrate a higher retention rate. These insights informed the team‚Äôs regional experimentation roadmap for incentive design and inspired strategies to better recognize and encourage high-quality referrals, particularly among Lyft Rewards top-tiered drivers.</p><h4>Returning to¬†Lyft</h4><p>After my internship, coming back to Lyft was a no-brainer. The science team culture, great mentorship, and collaborative environment made learning exciting and engaging. What made it even more compelling were the unique problems we got to tackle in the rideshare marketplace‚Ää‚Äî‚Äälike navigating supply crunches while maintaining marketplace efficiency or aligning incrementality measurement across pricing, rider coupons, and driver incentives to ensure cohesive strategy decisions. People at Lyft sometimes refer to the company as a giant math problem: dynamic and full of interdependent variables.</p><p>When I returned full time, I joined the Central Marketplace Management team, shifting from improving specific Lyft products to managing the growth of our two-sided marketplace. I began expanding my interest into causal prediction and optimization, focusing on growth-related challenges like budget allocation, long-term value modeling, and coupon strategy design. One particularly impactful project involved developing models to estimate the incremental gains at different budget levels. These models became a core part of our budget allocation framework and significantly improved incentive efficiency by directing spend toward the most responsive markets and programs.</p><h4>Advice for Future Interns/Employees</h4><p>Get involved in the science community‚Ää‚Äî‚Ääjoin sharing sessions, see what others are working on, and don‚Äôt be afraid to ask questions. Talk to PMs and engineers to understand how your work fits into the bigger picture, and don‚Äôt skip brainstorming sessions (or just catch-ups): the best ideas come from open-ended discussions. Beyond the data, cultivate business sense so your insights drive real impact, and most importantly, focus on what excites¬†you.</p><h3>Conclusion</h3><p>Our journey from interns to full-time data scientists at Lyft has been incredibly rewarding, filled with growth opportunities, meaningful projects, and a supportive community. Whether you‚Äôre just starting your career or looking to make a change, we hope our experiences provide valuable insights into what makes Lyft such a great place to work and grow. If you‚Äôre interested in joining our team and experiencing the same opportunities for growth and impact, we encourage you to explore our current openings and internship programs. Visit our careers page to learn more about available positions and start your own journey at¬†Lyft.</p><p>Ready to start your journey? Check out our <a href="https://www.lyft.com/careers#openings">internship opportunities and current openings</a> to see where you might fit in our growing¬†team.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=9b7338e7c8bb" width="1" height="1" alt=""><hr><p><a href="https://eng.lyft.com/intern-experience-at-lyft-9b7338e7c8bb">Intern Experience at Lyft</a> was originally published in <a href="https://eng.lyft.com">Lyft Engineering</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Solving Dispatch in a Ridesharing Problem Space]]></title>
            <link>https://eng.lyft.com/solving-dispatch-in-a-ridesharing-problem-space-821d9606c3ff?source=rss----25cd379abb8---4</link>
            <guid isPermaLink="false">https://medium.com/p/821d9606c3ff</guid>
            <category><![CDATA[optimization]]></category>
            <category><![CDATA[data-science]]></category>
            <category><![CDATA[matching]]></category>
            <dc:creator><![CDATA[Oussama Hanguir]]></dc:creator>
            <pubDate>Thu, 31 Jul 2025 17:43:14 GMT</pubDate>
            <atom:updated>2025-07-31T17:43:14.671Z</atom:updated>
            <content:encoded><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*cUPR97yd89hZobjB" /></figure><p>Imagine having to assemble a dynamic jigsaw puzzle with millions of new pieces every second. How would you approach such a problem, ensuring that each piece fits? Ridesharing platforms like Lyft solve these problems on a continuous basis; a complex puzzle where each rider and driver is a unique piece with specific edges‚Ää‚Äî‚Ääpreferences, locations, and destinations. The challenge lies in fitting these pieces together in real-time, ensuring that each connection is seamless and efficient, ultimately creating a coherent picture of urban mobility.</p><p>Efficiently matching drivers to riders ensures that riders get picked up quickly and reliably, and that the drivers‚Äô time is best utilized to maximize their earnings. At Lyft, our dispatch team is entrusted with the mission of providing optimal matching decisions, sifting through an initial space of millions of possible decisions every¬†second.</p><p>In this blog post, we will provide background on classical matching problems, as well as the challenges of solving matching problems in ridesharing applications.</p><p><strong>Matchings and Graph¬†Theory</strong></p><p>The concept of matching drivers to riders can be captured through the lens of graph theory. Thinking of ridesharing matching as a graph comes naturally as the problem inherently involves connecting two distinct groups‚Ää‚Äî‚Ääriders and drivers. It is also advantageous for several reasons; including the flexibility in modeling relationships (capturing factors like distance, time, and compatibility through weighted edges), as well as offering an armada of efficient and scalable algorithms to identify optimal structures.</p><p>One specific graph that captures the ridesharing settings (and more generally any two-sided marketplace setting) is the bipartite graph. Formally, a <a href="https://en.wikipedia.org/wiki/Bipartite_graph">bipartite graph</a> is a graph where the vertices can be divided into two disjoint sets such that all edges connect a vertex in one set to a vertex in another set. There are no edges between vertices within the same¬†set.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*1mp7obsyFzBwNzlH" /><figcaption><em>Figure 1: Constructing a bipartite graph to connect drivers to¬†riders</em></figcaption></figure><p>In most cases, our matching graphs are bipartite because we match one rider to one driver. However, there exist more involved situations, like shared rides, where the graph does not satisfy this condition. For simplicity, we only focus on bipartite cases.</p><p>Another practical layer that can be added to the concept of bipartite graphs is weighted edges. For our purposes, a weight represents the benefit accrued from a particular match.</p><p>You might now be wondering: a graph seems like a fairly intuitive object, but what exactly is a matching? Given a bipartite graph, a matching is a subset of the edges for which every vertex belongs to exactly one of the edges. Given a set of riders and drivers, the number of possible matchings can be exponentially large, but usually, we look for a matching that maximizes the sum of¬†weights.</p><p><strong>Mathematical Matching Formulation:</strong></p><p>If we define a binary decision variable <em>x‚Çë</em> for each edge e in the graph, <em>x‚Çë=1</em> will correspond to the inclusion of edge e in the matching. The matching problem is then equivalent to the <a href="https://en.wikipedia.org/wiki/Integer_programming">integer linear program</a>¬†(ILP).</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*QmOru9x34BRbsgm6" /></figure><p>The objective function maximizes the weight of the edges in the matching, while the constraints limit us to one edge per vertex (no more than one assignment per driver and per¬†rider).</p><p><strong>In plain terms:</strong><br>This mathematical formulation helps decide which driver gets paired with which rider to maximize overall benefit. We aim to select the matches that provide the highest total value, considering factors like proximity, pickup time, and cost. The first constraint ensures that no driver or rider is involved in more than one match‚Ää‚Äî‚Ääeach person can only be matched once. Lastly, the variables can only be 0 or 1, meaning either the match happens or it¬†doesn‚Äôt.</p><p>If we replace the 0‚Äì1 constraints on the selection variables with inequalities of the form <em>0 ‚â§ x‚Çë ‚â§1</em> we get the linear program relaxation of this ILP. We don‚Äôt need to include these inequalities explicitly, since the form of the standard linear program already constrains the <em>x‚Çë</em> to be non-negative and consequently, the other 2 constraints prevent them from being larger than 1. Dealing with the linear program (LP) relaxation instead of the original ILP is helpful because LPs are much faster and easier to solve. Furthermore, <a href="https://www.cambridge.org/core/journals/canadian-journal-of-mathematics/article/paths-trees-and-flowers/08B492B72322C4130AE800C0610E0E21">Edmonds</a> shows that for bipartite graphs, the LP relaxation gives us integer solutions anyway, even though no explicit constraint on integrality is enforced. This enables the use of fast and efficient algorithms to solve the problem. Theoretically, the <a href="https://en.wikipedia.org/wiki/Hungarian_algorithm">Hungarian</a> algorithm solves the problem in polynomial time. In practice, commercial solvers are faster and can even handle some binary constraints (multiple routes, many-to-one matchings etc.)</p><p><strong>Specificity of Lyft‚Äôs (or ridesharing in general) matching instances</strong></p><p>Solving matching problems in the context of ridesharing presents significant challenges primarily due to the dynamic nature of the data, requiring the generation and processing of bipartite matching graphs at frequent intervals, such as every few seconds, on a rolling¬†basis.</p><p>Each time we generate a graph, edge weights‚Ää‚Äî‚Ääreflecting factors like driver location, rider demand, and market conditions‚Ää‚Äî‚Äämust be recalculated to reflect the most current and accurate information. Because this data changes rapidly (drivers constantly on the move, demand levels fluctuating, traffic conditions), our system needs to update weights in real-time without excessive computational overhead. Achieving this requires efficient data handling and smart preprocessing to skip edges unlikely to be part of the optimal solution, balancing speed and accuracy.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/821/1*cyYqJiv6HNhe6t7JHtKWYw.png" /><figcaption><em>Figure 2: The cycle of generating and solving a matching¬†problem.</em></figcaption></figure><p>When calculating matchings, we must carefully choose the time interval for batching nearby drivers and riders‚Ää‚Äî‚Ääa decision that delicately balances computational efficiency with service quality. Longer batches give a fuller picture of supply and demand, often producing better matches, lower wait times, and higher vehicle utilization, but they can delay ride assignments and risk cancellations. Shorter intervals speed up matches and keep riders and drivers engaged, but may lead to suboptimal pairings and higher computational demands. The challenge is finding the right frequency to maximize efficiency and customer satisfaction amid fluctuating demand, supply, and resource constraints.</p><p>Once we‚Äôve set the batch size, we can compute the optimal matches for that snapshot using algorithms like the Hungarian method. However, optimizing each batch in isolation can be short-sighted, prioritizing immediate gains over long-term efficiency. A match that looks optimal now might increase wait times or reduce driver availability later. The figure below illustrates this: at time t=0, two drivers are available for one ride request, leading to the white car being dispatched. At t=1, a new request appears, but with only the black car left, resulting in a longer¬†wait.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*qcP56cI--vsQO-vM" /><figcaption><em>Figure 3: Example of how myopic decision making can lead to long term inefficiency.</em></figcaption></figure><p>To mitigate the sub-optimality arising from myopic matching between batches, several strategies can be adopted. For instance, we can use predictive analytics to forecast future demand and supply patterns, and augment the matching graph with the ‚Äúprojected future demand‚Äù from high density areas. In our example, this can reserve the white car for future batches, and result in a matching that minimizes the average waiting time. Other techniques can also include (1) incorporating long-term objectives in the weights of the matching graph, such as minimizing total wait time over a longer period or maximizing driver utilization rates, as well as (2) Dynamic Rebalancing, to continuously adjust the allocation of drivers based on real-time data, through incentives and¬†bonuses.</p><p>To conclude, matching riders to drivers may seem straightforward, but doing it well‚Ää‚Äî‚Ääat scale, in real time, and under uncertainty‚Ää‚Äî‚Ääis a deeply complex optimization challenge. In this post, we introduced the core concepts behind how we model and solve matching problems at Lyft, highlighting both the mathematical structure and the practical constraints of real-world ridesharing systems.</p><p>In <strong>Part 2</strong>, we‚Äôll explore how <strong>Machine Learning and Optimization work together</strong> to produce high-quality dispatch decisions, and discuss <strong>ongoing efforts to reduce the impact of uncertainty</strong> through forecasting, dynamic rebalancing, and smarter long-term decision-making.</p><h4><strong>Acknowledgments</strong></h4><p>We would like to thank <a href="https://www.linkedin.com/in/briannwu/">Brian</a> for his contributions in designing the graphs used in this post, as well as the editing team (<a href="https://www.linkedin.com/in/hongru-liu/">Hongru</a>, <a href="https://www.linkedin.com/in/jeana-choi/">Jeana</a>, <a href="https://www.linkedin.com/in/hongru-liu/">Kelly</a>, <a href="https://www.linkedin.com/in/miriam-leon/">Miriam</a>) for their thoughtful reviews and constructive feedback.</p><p><em>Lyft is hiring! If you‚Äôre passionate about solving this kind of optimization problem, visit </em><a href="https://www.lyft.com/careers"><em>https://www.lyft.com/careers</em></a><em> to see our openings.</em></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=821d9606c3ff" width="1" height="1" alt=""><hr><p><a href="https://eng.lyft.com/solving-dispatch-in-a-ridesharing-problem-space-821d9606c3ff">Solving Dispatch in a Ridesharing Problem Space</a> was originally published in <a href="https://eng.lyft.com">Lyft Engineering</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[How science inspires our ETA models]]></title>
            <link>https://eng.lyft.com/how-science-inspires-our-eta-models-bf229e3148e8?source=rss----25cd379abb8---4</link>
            <guid isPermaLink="false">https://medium.com/p/bf229e3148e8</guid>
            <category><![CDATA[travel-time-prediction]]></category>
            <category><![CDATA[statistics]]></category>
            <category><![CDATA[machine-learning]]></category>
            <dc:creator><![CDATA[Mohamad Elmasri]]></dc:creator>
            <pubDate>Wed, 28 May 2025 19:24:48 GMT</pubDate>
            <atom:updated>2025-05-28T19:24:48.804Z</atom:updated>
            <content:encoded><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*mPFyKHuB-2Oqttx5" /></figure><h4>Part I: Micro patterns in traffic¬†chaos</h4><p>Have you ever driven alongside another vehicle for an extended period? You‚Äôve likely experienced this peculiar phenomenon: despite sharing the same route and traffic signals, you inevitably encounter a red light while the other vehicle passes through seconds earlier. For a moment, you might think they‚Äôll reach their destination first. However, as you continue, you‚Äôre surprised to find them waiting at another red light just a few blocks ahead. This dance continues for a while. The ‚Äònotes‚Äô of this ‚Äòsong‚Äô are the micro-random events inherent in traffic: a flock of pigeons crossing the road, a cyclist approaching, a sudden lane change by the vehicle in front. Some factors are more deterministic: weather conditions, road closures, or construction delays, others less¬†so.</p><p>As a seasoned data scientist, your mission is to uncover hidden patterns within chaotic systems and translate them into mathematical insights. These insights inform the decisions, both big and small, of engineering and science organizations, and support its continual operational strategy. This blog translates a seemingly random traffic pattern into a comprehensible behavior, which we will use to build a statistical model for travel¬†time.</p><p>Let‚Äôs get back to the core question: how do seemingly chaotic patterns help us build models? One observation we repeatedly made is that the <em>distance</em> of a ride significantly influences our understanding of travel time uncertainty. The longer you are on the road, the more chance you can expect traffic, this leads to the belief that travel uncertainty is substantially higher for longer rides. However, we found the opposite: travel time predictions are often more accurate for longer journeys. For instance, ETAs between your house and the next town tend to be more precise than those for a trip to the coffee shop next door. This relative travel uncertainty is particularly pronounced for short rides during rush hour, where unexpected traffic can significantly impact your ETA to the coffee shop. Conversely, when driving to the next city, the accumulated delays from rush hour congestion tend to smooth out over the longer distance. Therefore, two rides following the same route can ultimately arrive at similar times. If a ride encounters traffic at a specific spot, the other ride is also likely to encounter traffic at another spot. Where traffic is encountered might differ for different rides, yet the likelihood is similar. This phenomenon is more likely to occur with longer journeys.</p><p>What we‚Äôve just described is a macro-level example of the ‚Äòtraffic light dance‚Äô we discussed at the beginning of this blog. Rush hour is simply a larger-scale traffic event compared to a flock of pigeons crossing the road. We can visualize this phenomenon by plotting the trajectories of two rides taking a long route. If we represent congestion (like traffic lights) with red circles, these circles will appear at different points along each ride‚Äôs trajectory, leading to short-term variations in travel¬†time.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*AYiH9fEkBGs-Y7h7" /><figcaption>Two vehicles traveling a 100-edge route, with œÑi(e)¬†, i=1,2, denoting the travel times up to road e. Red circles represent congestion events.</figcaption></figure><p>As illustrated in the figure above, the travel time up to road ‚Äòe‚Äô varies between the two rides. However, these short-term discrepancies tend to even out over longer distances, as demonstrated by the occasional intersections of the two curves, visually representing the ‚Äòdance‚Äô we described earlier.</p><p>This insight is valuable, but how can we translate it into a more practical framework? First, let‚Äôs express it mathematically. These findings suggest that the difference in travel time between rides following the same route may possess strong statistical properties. While individual segments of a trip may exhibit randomness, the cumulative effect of this randomness can be highly predictable and statistically well-behaved.</p><p>Let‚Äôs expand our analysis by examining a larger number of rides and observing the impact on average travel time along the route. We‚Äôll track the cumulative average travel time over the first 1, 10, 50, and 100 kilometers. The average is calculated by dividing the total travel time by the number of road segments (edges) within the respective distance. This allows us to visualize how travel time fluctuations change as the distance increases, demonstrating the shift from short-term variability to more consistent long-term trends.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/932/0*ttkxK7OFxRhAzn6T" /></figure><p>Given our above intuition, it‚Äôs unsurprising that the average travel time converges as the distance increases. The first kilometer exhibits a heavily skewed distribution with significant variability in travel times. This is because a single congestion event within a 1-kilometer route can have a substantial impact on the overall travel time on the first kilometer. However, the distribution of average travel time per road segment gradually becomes more symmetric.</p><p>This pattern reveals two key insights:</p><ul><li>Travel time along a route converges towards an asymptotic (long-term) distribution as distance increases.</li><li>Aggregating travel data from multiple trips to estimate road segment speeds can approximate this distribution.</li></ul><p>What is the nature of this asymptotic distribution, and how can we formalize it mathematically? Essentially, we‚Äôve been examining rides along a consistent route, calculating the average travel time per road segment. This involves summing the travel times for each segment along the route and dividing by the number of road segments (n), which serves as a proxy for distance.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*YmuCdF4Na2D7rs2FnZ_vng.png" /></figure><p>Doesn‚Äôt this pattern resemble the <a href="https://en.wikipedia.org/wiki/Central_limit_theorem">Central Limit Theorem (CLT)</a>, which states that the sum of independent, identically distributed random variables converges to a normal distribution? Can we apply a similar principle here? While our variables (travel times across road segments) are not strictly independent, we observe an empirical convergence towards normality. This suggests that, even though individual road segment travel times exhibit variability due to stochastic congestion, their aggregated effect across numerous segments approximates a normal distribution. That¬†is,</p><figure><img alt="Test" src="https://cdn-images-1.medium.com/max/1024/1*Sz7KdxWUwWbtGRJC8FOqOg.png" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*Q4kpM2T7E8mRC9ri-x9OVw.png" /></figure><p>This is a significant finding about the long-term statistical stability of traffic patterns.</p><p>Formally satisfying the CLT conditions is mathematically complex, as it requires more in-depth analysis of travel time, understanding the type of dependencies amount road segments, and how to deal with them mathematically. A great topic for our next blog in the¬†series!</p><p>For now, let‚Äôs further validate our intuition with data by looking at one of our most frequent routes between 8‚Äì9AM in the Bay Area. This ride goes from Howard St and 5th St to 17th St and De Haro St, in San Francisco.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*5iIIYqGWT6lo1hZm4R0jAg.png" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/828/1*xe7sg5gvshoqr4raHDeOog.png" /><figcaption>Route (A) (solid pink)‚Ää‚Äî‚Ää2.4km long, Howard St to 17th St, in San Francisco, CA. The average duration is 615 sec, with standard deviation of 94¬†sec.</figcaption></figure><p>The graph shows that the majority of trips fall within a predictable range around the average of 615 seconds (about 10 minutes). The narrowness of the distribution (indicated by the standard deviation) suggests that travel times on this route are relatively consistent. This insight already allows us to provide more reliable ETAs to our users, improving their overall experience.</p><p>Try it yourself, if you drive from home to work daily, record your travel time for a month and plot them! You‚Äôll find that they are normally distributed.</p><p>To truly maximize the accuracy of our ETAs, we must examine a broader range of routes, validate the general applicability of our CLT assumption, and provide more insight into the conditions under which it holds. In our next post, we‚Äôll explore these complexities in greater detail. We will connect our findings to random walks on stochastic networks and leverage a three-decade-old discovery by David Aldous. As noted in his 1991 paper, random walks frequently emerge in contexts where the initial question wasn‚Äôt explicitly about them‚Ää‚Äî‚Ääand transportation networks are no exception. All these random rides on our network can be stitched together to exhibit behavior akin to a random walk. This perspective offers a powerful approach to analyzing travel time variability and long-term behavior within the¬†network.</p><p><strong>Are you as excited as we¬†are?</strong></p><p><em>Check out more exciting blogs in </em><a href="https://eng.lyft.com/"><em>Lyft Data Science</em></a><em> or the </em><a href="https://www.lyft.com/careers#openings"><em>Lyft Careers</em></a><em> page for information about current openings!</em></p><p>Thanks to Annegret Muller for drafting assistance.</p><h4>References</h4><p>Mohamad Elmasri. Aur√©lie Labbe. Denis Larocque. Laurent Charlin. ‚ÄúPredictive inference for travel time on transportation networks.‚Äù Ann. Appl. Stat. 17 (4) 2796‚Äì2820, December 2023. <a href="https://doi.org/10.1214/23-AOAS1737">https://doi.org/10.1214/23-AOAS1737</a></p><p>David Aldous. ‚ÄúApplications of random walks on finite graphs.‚Äù <em>Lecture Notes-Monograph Series</em> (1991): 12‚Äì26. <a href="https://www.jstor.org/stable/4355644">https://www.jstor.org/stable/4355644</a></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=bf229e3148e8" width="1" height="1" alt=""><hr><p><a href="https://eng.lyft.com/how-science-inspires-our-eta-models-bf229e3148e8">How science inspires our ETA models</a> was originally published in <a href="https://eng.lyft.com">Lyft Engineering</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Beyond Query Optimization: Aurora Postgres Connection Pooling with SQLAlchemy & RDSProxy]]></title>
            <link>https://eng.lyft.com/beyond-query-optimization-aurora-postgres-connection-pooling-with-sqlalchemy-rdsproxy-200db7f562d7?source=rss----25cd379abb8---4</link>
            <guid isPermaLink="false">https://medium.com/p/200db7f562d7</guid>
            <category><![CDATA[connection-pooling]]></category>
            <category><![CDATA[aurora]]></category>
            <category><![CDATA[postgres]]></category>
            <category><![CDATA[database]]></category>
            <category><![CDATA[rds]]></category>
            <dc:creator><![CDATA[Jay Patel]]></dc:creator>
            <pubDate>Tue, 20 May 2025 13:04:18 GMT</pubDate>
            <atom:updated>2025-05-20T15:23:28.960Z</atom:updated>
            <content:encoded><![CDATA[<p><em>Written by </em><a href="https://www.linkedin.com/in/jay-p-73461749/"><em>Jay Patel</em></a><em> and </em><a href="https://www.linkedin.com/in/crestonjamison/"><em>Creston¬†Jamison</em></a><em>.</em></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/848/1*8-pRKqJFIeOYAbomjyIL8g.png" /><figcaption>Photo by <a href="https://commons.wikimedia.org/wiki/User:RRZEicons">RRZEicons</a> UNDER <a href="https://creativecommons.org/licenses/by-sa/3.0/deed.en">CC BY-SA 3.0¬†DEED</a></figcaption></figure><p>Efficient database connection management is often overlooked when optimizing database performance and scaling. When we think about database performance, we focus on query optimization and indexing while underestimating the impact of managing database connections effectively. Without connection pooling, each request may incur the costly overhead of repeatedly opening and closing connections, leading to performance bottlenecks and resource exhaustion. In modern web scale applications, a well-implemented connection pool can mean the difference between smooth scalability and crippling database overload. At Lyft, we saw several benefits of adopting a proxy based connection pooler which we will discuss extensively in this blog¬†post.</p><h3>Connection Pooling</h3><h4>What is connection pooling?</h4><p>In an ideal world, our application process or threads could scale infinitely and each have a dedicated database (DB) connection. In reality, this would be wildly inefficient and a waste of resources. Yes, application threads frequently use database connections, but that is not all they do. If an application thread is making an API call to an external service, it is not doing any database work. Therefore, it is more efficient for these database connections to be shared amongst your application threads so the DB connections can only be used when¬†needed.</p><p>It is also advantageous to minimize the number of connections to a relational database like PostgreSQL. The consistency Postgres guarantees relies on locking and shared memory structures. Therefore, fewer connections to the database allow less contention for these limited resources.</p><p>This is where connection pooling comes in. It creates a pool of connections that your application threads can ‚Äúcheck-out‚Äù and use when needed. After a database call is complete, the application thread returns the connection back to the¬†pool.</p><h4>Why use connection pooling?</h4><ol><li><strong>Reduces Connection Overhead:</strong> Opening and closing database connections is expensive in terms of time and resources. A connection pool keeps a set of ready-to-use connections, avoiding the need to repeatedly establish and tear down connections.</li><li><strong>Improves Performance &amp; Latency:</strong> By reusing existing connections, applications can serve requests faster, reducing the time spent waiting for new database connections to be established.</li><li><strong>Controls the Number of Concurrent Connections:</strong> Without pooling, an application might create too many concurrent connections, overwhelming the database and leading to degraded performance or even crashes. Each Postgres connection is a process that spawns on establishment, resulting in overhead for the lock manager to coordinate the locking between different transactions.</li><li><strong>Optimizes Resource Utilization:</strong> Connection pooling ensures that database resources (e.g., memory, CPU) are used efficiently by avoiding unnecessary connection creation and destruction.</li><li><strong>Avoids Connection Leaks:</strong> Poorly-managed connections (i.e., those not closed properly) can lead to resource leaks. A connection pool manages the lifecycle of connections, ensuring they are properly closed or¬†reused.</li><li><strong>Improves Application Availability:</strong> Connection poolers like RDSProxy, reduces the unavailability of the database significantly when a <a href="https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/rds-proxy.howitworks.html#rds-proxy-failover">failover</a> occurs by <a href="https://aws.amazon.com/rds/proxy/">66%</a> according to¬†AWS.</li></ol><h3>Flask SQLAlchemy DB connection setup</h3><p>Before we jump to RDSProxy, it is important to understand how a DB connection is set up with SQLAlchemy. At Lyft, we use <a href="https://flask-sqlalchemy.readthedocs.io/en/stable/">Flask SQLAlchemy</a> to power our API server and connect to Aurora Postgres DB. We use the following <a href="https://flask-sqlalchemy.readthedocs.io/en/stable/config/">config</a> in settings.py for SQLAlchemy:</p><pre>AURORA_WRITER_BIND = &#39;aurora_writer_bind&#39;<br>AURORA_READER_BIND = &#39;aurora_reader_bind&#39;<br><br>SQLALCHEMY_BINDS = {<br>   AURORA_WRITER_BIND: &#39;postgresql://{username}:{password}@{host}:{port}/{db}&#39;.format(<br>    username=&#39;scott&#39;,<br>    password=&#39;tiger&#39;,<br>    host=&#39;aurora.writer.com&#39;,<br>    port=5432,<br>    db=&#39;mydb&#39;<br>),<br>  AURORA_READER_BIND: &#39;postgresql://{username}:{password}@{host}:{port}/{db}&#39;.format(<br>    username=&#39;scott&#39;,<br>    password=&#39;tiger&#39;,<br>    host=&#39;aurora.reader.com&#39;,<br>    port=5432,<br>    db=&#39;mydb&#39;)<br>}<br><br>SQLALCHEMY_DATABASE_URI = SQLALCHEMY_BINDS[AURORA_WRITER_BIND] # Default bind</pre><p>The Flask APIs can now use the reader or the writer¬†DB:</p><pre>def my_write_api():<br>  db.session.set_bind(AURORA_WRITER_BIND):<br> // API Logic<br><br><br>def my_read_api():<br>  db.session.set_bind(AURORA_READER_BIND):<br> // API Logic</pre><p>For connection pooling, here is an example configuration:</p><pre>SQLALCHEMY_MAX_OVERFLOW = 5<br>SQLALCHEMY_POOL_SIZE = 10<br>SQLALCHEMY_POOL_TIMEOUT = 5</pre><h4>How does it¬†work?</h4><ol><li>When an application queries the database, it checks the pool for an available connection for the¬†bind.</li><li>If a connection is available, it uses¬†it.</li><li>If no connections are available, and the pool size is below SQLALCHEMY_POOL_SIZE, a new connection is created (lazy instantiation).</li><li>If more connections are needed, SQLAlchemy can create up to SQLALCHEMY_MAX_OVERFLOW additional connections.</li><li>These overflow connections do not stay in the pool. They are closed when released.</li><li>If both SQLALCHEMY_POOL_SIZE and SQLALCHEMY_MAX_OVERFLOW limits are reached, new connection requests will wait for an available connection (up to SQLALCHEMY_POOL_TIMEOUT seconds) before¬†failing.</li></ol><h4>Limitations of application-based connection pooling</h4><p>If SQLAlchemy can create connection pools, what is the¬†problem?</p><p>At Lyft, we use <a href="https://gunicorn.org/">gunicorn</a> with <a href="https://www.gevent.org/">gevent</a> to create multiple worker processes to handle requests efficiently and deploy them using Kubernetes. This means that each pod will have multiple gunicorn processes and each process will have the Flask app running with a max SQLALCHEMY_POOL_SIZE + SQLALCHEMY_MAX_OVERFLOW connection pools, i.e. max_connection_pool_per_pod = num_gunicorn_process * (SQLALCHEMY_POOL_SIZE + SQLALCHEMY_MAX_OVERFLOW). Please note that this represents the max connection pool per gunicorn process. In reality, it will be less than that given that the load would be distributed across multiple¬†pods.</p><p>The application-based connection pooling has the following shortcomings:</p><ul><li><strong>Application Scaling:</strong> In the configuration above, we will have a max of <strong>4*(10 + 5) = 60</strong> total connection pool count for reader and the same for writer if there are 4 gunicorn processes spawned in the pod. If the application scales out, the number of connections to the database also increases. So more pods, more connections, more Postgres processes, more overhead for the lock manager to run concurrent queries! Our application running ~40 pods during peak hours would easily create 1400+ connections to the writer DB. This is especially problematic considering Aurora has a single¬†writer.</li><li><strong>Database Maintenance:</strong> Whenever a database maintenance operation is run which causes locking (DDL, partition management, etc.), the query latency and timeouts increase. This would cause a retry storm from downstream services resulting in more connection pools getting created if not available and overwhelms the lock¬†manager.</li></ul><h3>Proxy-based connection pooling</h3><p>Application-based connection pools are great for applications to reuse database connections. However, as mentioned above, you can still run into several issues. First, if you keep deploying additional pods, you will eventually run out of database connections. As opposed to waiting for a connection to be freed, the database simply denies the connection request. Second, at least with Aurora Postgres, Postgres‚Äô process-based model means it has to spawn an additional process for each new connection request. This increases the amount of time it takes before a new connection can service a request. All of these processes also expend database resources. This is where a <em>separate proxy</em> layer that handles pooling and efficiently manages database connections for all applications can be leveraged.</p><p>Probably the most well-known external connection pooler for Postgres is <a href="https://www.pgbouncer.org">pgBouncer</a>. It uses a thread-based model making it a highly efficient connection pooler. It maintains connections to your database and accepts connections from your application with low latency. Plus, if you use transaction pooling, you can have many more client application connections compared to the number of database server connections (known as multiplexing) as each transaction is assigned to one of the server connections.</p><p>However, Amazon AWS also offers a proxy solution that requires less maintenance and management called <a href="https://aws.amazon.com/rds/proxy/">RDSProxy</a>. We choose this option for our proxy-based pooler solution.</p><h3>Setup AWS¬†RDSProxy</h3><p>Setting up RDSProxy can be done through the AWS console, through the AWS API, or via an Infrastructure as Code tool like Terraform. Here are the steps we followed to set up our RDSProxy:</p><ol><li><a href="https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/rds-proxy-network-prereqs.html">Network</a>: Plan for the new proxy to be placed in the same VPC as your databases and ensure your application instances can connect to it as¬†well.</li><li><a href="https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/rds-proxy-secrets-arns.html">AWS Secrets Manager</a>: RDSProxy requires using AWS Secrets Manager. A secret containing the username and password must be stored for each database user you want to connect through RDSProxy.</li><li><a href="https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/rds-proxy-iam-setup.html">Configure AWS IAM Authentication</a>. Your policy needs to include access to the Secrets Manager secret(s) and associate it with the RDS¬†service.</li><li><a href="https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/rds-proxy-creating.html">Create Proxy</a>: With these in place, you can now create the proxy. Be sure to specify the Secrets you created from step 2 and the IAM policy you created in step 3. You also define the target database cluster you want to connect to as well as if you want a read-only endpoint.</li><li><a href="https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/rds-proxy-connecting.html">Connecting</a>: Once this is complete, you should be able to test a connection through the proxy using one of the proxy endpoints and the user and password you set up in the Secrets Manager secret. We used the Postgres client psql for a quick test of connectivity.</li></ol><h3>RDSProxy</h3><h4>Setting up SQLAlchemy with¬†RDSProxy</h4><p>Recall that we were using reader and writer binds earlier to connect to the reader and writer Aurora instances. Now that we have the proxy endpoints setup, we can create binds for the reader and the writer proxy endpoints.</p><pre>AURORA_WRITER_BIND = &#39;aurora_writer_bind&#39;<br>AURORA_READER_BIND = &#39;aurora_reader_bind&#39;<br>AURORA_WRITER_PROXY_BIND = &#39;aurora_writer_proxy_bind&#39;<br>AURORA_READER_PROXY_BIND = &#39;aurora_reader_proxy_bind&#39;<br><br>SQLALCHEMY_BINDS = {<br>   AURORA_WRITER_BIND: &#39;postgresql://{username}:{password}@{host}:{port}/{db}&#39;.format(<br>    username=&#39;scott&#39;,<br>    password=&#39;tiger&#39;,<br>    host=&#39;aurora.writer.com&#39;,<br>    port=5432,<br>    db=&#39;mydb&#39;<br>  ),<br>  AURORA_READER_BIND: &#39;postgresql://{username}:{password}@{host}:{port}/{db}&#39;.format(<br>    username=&#39;scott&#39;,<br>    password=&#39;tiger&#39;,<br>    host=&#39;aurora.reader.com&#39;,<br>    port=5432,<br>    db=&#39;mydb&#39;),<br>   AURORA_WRITER_PROXY_BIND: &#39;postgresql://{username}:{password}@{host}:{port}/{db}&#39;.format(<br>    username=&#39;scott&#39;,<br>    password=&#39;tiger&#39;,<br>    host=&#39;aurora.proxy.writer.com&#39;,<br>    port=5432,<br>    db=&#39;mydb&#39;<br>  ),<br> AURORA_READER_PROXY_BIND: &#39;postgresql://{username}:{password}@{host}:{port}/{db}&#39;.format(<br>    username=&#39;scott&#39;,<br>    password=&#39;tiger&#39;,<br>    host=&#39;aurora.proxy.reader.com&#39;,<br>    port=5432,<br>    db=&#39;mydb&#39;<br>  ),<br>}<br><br>SQLALCHEMY_DATABASE_URI = SQLALCHEMY_BINDS[AURORA_WRITER_PROXY_BIND] # Change default bind to writer proxy</pre><p>Now we can switch to using the proxy binds for our read and write¬†APIs.</p><pre>def my_write_api():<br>  db.session.set_bind(AURORA_WRITER_PROXY_BIND)<br>   // API Logic<br><br>def my_read_api():<br>  db.session.set_bind(AURORA_READER_PROXY_BIND)<br>   // API Logic</pre><h3>Gotchas with¬†RDSProxy</h3><p>The main limitation of using an external proxy with transaction pooling is that Postgres‚Äô session-based features will not work correctly. This means any kind of ‚Äú<strong>SET XXXX</strong>‚Äù will not work. Also, historically, prepared statements have also been a problem. However, AWS introduced some enhancements to RDSProxy to <a href="https://aws.amazon.com/blogs/database/amazon-rds-proxy-multiplexing-support-for-postgresql-extended-query-protocol/">support prepared statements in late 2023</a> and we never encountered any issues. We encountered several issues when rolling out RDSProxy in our test/prod environment:</p><h4>Statement timeout</h4><p>When connecting with the database using the proxy, we received the following error:</p><pre>sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at &quot;aurora.reader.proxy.com&quot; (10.0.146.4), port 5432 failed: FATAL: Feature not supported: RDS Proxy currently doesn&#39;t support command-line options.</pre><p>The reason for this was the following configuration in our settings¬†files:</p><pre>SQLALCHEMY_ENGINE_OPTIONS = {&#39;connect_args&#39;: {&#39;options&#39;: &#39;-c statement_timeout={timeout}&#39;.format(<br>timeout=str_env(&#39;POSTGRES_STATEMENT_TIMEOUT&#39;)<br>)}}</pre><p>To overcome this issue, we decided to set the statement_timeout at the user level and removed the SQLALCHEMY_ENGINE_OPTIONS:</p><pre>ALTER ROLE scott SET statement_timeout = &#39;5s&#39;;</pre><h4>Session pinning</h4><p>This is probably the most important issue to be mindful of when moving to RDSProxy. Normally, RDSProxy optimizes database connections by multiplexing multiple client requests over fewer actual database connections. However, some database session states require a persistent connection to a specific backend instance, forcing RDSProxy to ‚Äúpin‚Äù that session. The pinning of sessions reduces how many connections are available for reuse. To get the most benefit from connection pooling, it is worth minimizing behaviors that trigger session¬†pinning.</p><h4>Session pinning: How to catch¬†them?</h4><p>Conveniently, RDSProxy charts all occurrences of session pinning in the monitoring tab of the RDSProxy called ‚Äú<strong>DatabaseConnectionsCurrentlySessionPinned</strong>.‚Äù</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*hAhd-IIrw35HAjt2" /><figcaption>Session pinning metrics from AWS cloudwatch</figcaption></figure><p>In addition, it logs the cause of the session pinning as well. These logs can be found in CloudWatch at a <strong>WARN</strong> level and include the text, ‚Äú<strong>The client session was pinned to the database connection¬†.¬†.¬†.</strong>‚Äù.</p><p>We did encounter some rare queries which are session pinned, for which we never found the source of this session pinning where the SQL query exceeded the 16384 byte¬†limit:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*veaec0n2gkimPSD5" /><figcaption>Example session pinning Cloudwatch log</figcaption></figure><p>RDSProxy does not log these SQL statements even with <strong>Enhanced Logging</strong> enabled. We attempted to find this query on the database side. Unfortunately, we could never find this query in pg_stat_statements. Probably because pg_stat_statements stores a parameterized version of the query that could be much shorter than the actual one. Especially, if it is a query with an IN clause looking for a lot of values. The only other alternative is logging every statement in the database to try and find them. But, given how rare the session pinning incidents are, we have not attempted to do that as of¬†yet.</p><p>Apart from this instance, we went ahead and addressed each of the following session pinning¬†issues:</p><h4>Advisory lock_timeout</h4><p>We had several flows in our application using <a href="https://www.postgresql.org/docs/current/explicit-locking.html#ADVISORY-LOCKS">advisory locks</a> with varying timeouts.</p><pre>db.session.execute(&#39;SET lock_timeout=3000&#39;);</pre><p>This changes the state of the session and requires session pinning. One option was to set the lock_timeout at the user level. However, because each flow had a different timeout requirement, we opted to use SET LOCAL which is transaction-scoped and does not create session¬†pinning.</p><pre>db.session.execute(&#39;SET LOCAL lock_timeout=3000&#39;);</pre><h4>Sequence numbers</h4><p>We had some legacy flows relying on sequence number generation in our write APIs. We had to refactor our code to ensure that we rely on the auto-incremented value generated on writes to the DB and not generate the sequence using<strong> </strong>nextval().</p><p>There are many other reasons why session pinning occurs. A full list is available in the <a href="https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/rds-proxy-pinning.html">official AWS documentation</a></p><h4>Query latency</h4><p>Due to an extra network hop added by the proxy, there will be some latency introduced. For our application, most of the APIs did not see any significant increase in latency. There was one API which was latency sensitive and we chose to directly connect with the reader instead of using the proxy. We will evaluate this API in future so see if we can change the query patterns.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*w5QIduu0IibVSoTg" /><figcaption>Sample query for which the p95 latency increased due to the proxy¬†rollout</figcaption></figure><h3>RDSProxy Live!</h3><p>As soon as we switched the writer to RDSProxy, we immediately saw the number of connections to it dropping by¬†<strong>~56%.</strong></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*zxt5Yk2Tf7h3Lddf" /><figcaption>Drop in writer connections as soon as we rolled¬†out</figcaption></figure><p>We can also clearly see that the <strong>writer connections were below 800, compared to not using proxy where the connection fluctuated between 900 and 1400 over a period of 7¬†hours</strong>.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*3UTnydXI0-xBS447" /><figcaption>Writer connections fluctuation with and without¬†proxy</figcaption></figure><p>We also see a slight improvement in CPU performance for the¬†writer.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*RLKW7dcFxo07evxq" /><figcaption>Writer CPU % with and without¬†proxy</figcaption></figure><p>Aurora supports multiple readers (up to 15) and each reader on an average dropped connections by <strong>~16%. </strong>We can also see how the proxy is able to smooth out the number of connections compared to not using the proxy where the connections fluctuate. We also saw significant improvement in CPU consumption for each¬†reader.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*aI_Z2DBKOA2IKXye" /><figcaption>Reader connections fluctuation with and without¬†proxy</figcaption></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*114aX7-XxC47z8HI" /><figcaption>Reader CPU % with and without¬†proxy</figcaption></figure><p>AWS provides <a href="https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/rds-proxy.monitoring.html">metrics</a> on RDSProxy for the number of client connections, DB connections (combined from all readers + writer), connections borrowed, etc. In live traffic, we saw the total client connections to the proxy and how they translated to the DB connections to readers and the¬†writer.</p><p>There were some write queries generating session pinning and after fixing them,<strong> we saw the connection to the writer dropping further by 33% (from 750 to 500!)</strong>. We can also see how the proxy is able to handle auto scaling of the application well when there is no session pinning keeping a flat connection count of¬†500.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*6M87V5OM_WFyArpo" /><figcaption>Writer connections after fixing session pinning with¬†RDSProxy</figcaption></figure><h4><strong>How efficient was RDSProxy in managing the connections?</strong></h4><p>The efficiency of proxy is calculated using the multiplexing ratio which is <strong>ClientConnections/DatabaseConnections</strong></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*fERPcHJ8LFWg4i1e" /><figcaption>Connections to RDSProxy for both reads and¬†writes</figcaption></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*iMs1QQShDV2tyVX7" /><figcaption>Total connections to the database using¬†RDSProxy</figcaption></figure><p>From the CloudWatch graphs above, we can see that the proxy had a multiplexing ratio of<strong> ~2.4 (3.9k/1.65k)</strong></p><p><strong>Note: The CloudWatch statistics include both the traffic to the reader and the writer when counting ClientConnections and the DatabaseConnections.</strong></p><p>Here‚Äôs a comprehensive summary of the usage of proxy and the connection improvements we saw with the readers and writer during peak hours with ~40¬†pods:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*KsktustDiySDLW4LgweGzg.png" /></figure><h3>RDSProxy scale¬†testing</h3><p>To understand the benefits of the proxy, we conducted some scale testing for the writer and reader DB. For the scope of this blog post, we will restrict the discussion to only the writer since the writer cannot be horizontally scaled out unlike the readers. In this test, we did not increase the application traffic, but rather increased the number of application pods connecting to the writer database to better understand how efficiently proxy manages connection. We divided our test with session pinning and without session pinning to understand the connection multiplexing efficiency.</p><h4>With session¬†pinning</h4><p>We used a different user for queries which we knew caused session pinning so that we could have a breakdown in pg_stat_activity by username. We scaled out to ~200 pods which created ~1700 connections to the DB. We discovered that <strong>out of the 1700 writer connections, the session pinned connections were ~1100</strong>. This demonstrates how critical it is to fix session pinning for efficient connection management. The total DB connections including the readers was¬†~2900.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*ZpZ38cHoYrIwASWM" /><figcaption>Connections to the readers and the writer after scaling out to 200¬†pods</figcaption></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*v8Q1tl77vuCLmP17" /><figcaption>Connections to RDSProxy for both reads and¬†writes</figcaption></figure><p>The multiplexing ratio here was <strong>~2.24</strong> (6500/2900).</p><h4>Post session pinning¬†fixes</h4><p>After refactoring our code to fix session pinning, we observed the following metric in CloudWatch:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*13gt_cwstx_4nxFA" /><figcaption>Connections to RDSProxy for both reads and writes with 250 pods after fixing session¬†pinning</figcaption></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*tTnAA-wrLG1G1YbH" /><figcaption>Total connections to the database using¬†RDSProxy</figcaption></figure><p>As you can see, we were able to achieve a multiplexing ratio of <strong>~4.4 </strong>(7.1k/1.6k) and that too with 250¬†pods!</p><h4><strong>So how does our writer perform now after eliminating session¬†pinning?</strong></h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*Va96Ff_MFKHH-vPm" /></figure><p>If you compare the results from session pinning, we reached 1.5k connections to the writer with 200 pods. <strong>After eliminating session pinning, our connections were flat at ~500 connections!</strong> <strong>Contrast this with what we had before RDSProxy: 1400 connections occurring with just 40 pods</strong>. This demonstrates how important it is to eliminate or minimize session pinning as much as possible to efficiently multiplex the connections to the DB and scaling of the application.</p><p>To summarize our findings:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*8jE9iy3KZrJkMKXYLBF08Q.png" /></figure><h3>Closing notes &amp; next¬†steps</h3><p>RDSProxy has proven beneficial so far for us to ensure our application can scale gracefully without throttling the Aurora Postgres DB. The key to ensure efficient multiplexing is minimizing session pinning as much as possible. The AWS monitoring definitely needs some improvement to help catch the remaining session pinned queries. While query latency caused by the extra network hop did not cause significant issues for us, it is vital to evaluate RDSProxy if your application APIs are latency sensitive. As a next step, given the proxy‚Äôs efficiency, we will evaluate our clusters to see if we can downsize our readers/writers to lower instance size and possibly also scale down the number of readers to reduce our¬†cost.</p><p><em>Lyft is hiring! If you‚Äôre passionate about solving these kind of distributed system problems, visit </em><a href="https://www.lyft.com/careers"><em>Lyft Careers</em></a><em> to see our openings.</em></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=200db7f562d7" width="1" height="1" alt=""><hr><p><a href="https://eng.lyft.com/beyond-query-optimization-aurora-postgres-connection-pooling-with-sqlalchemy-rdsproxy-200db7f562d7">Beyond Query Optimization: Aurora Postgres Connection Pooling with SQLAlchemy &amp; RDSProxy</a> was originally published in <a href="https://eng.lyft.com">Lyft Engineering</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Real-Time Spatial Temporal Forecasting @ Lyft]]></title>
            <link>https://eng.lyft.com/real-time-spatial-temporal-forecasting-lyft-fa90b3f3ec24?source=rss----25cd379abb8---4</link>
            <guid isPermaLink="false">https://medium.com/p/fa90b3f3ec24</guid>
            <category><![CDATA[machine-learning]]></category>
            <category><![CDATA[mlops]]></category>
            <category><![CDATA[apache-beam]]></category>
            <category><![CDATA[distributed-systems]]></category>
            <category><![CDATA[forecasting]]></category>
            <dc:creator><![CDATA[Rakesh Kumar]]></dc:creator>
            <pubDate>Mon, 05 May 2025 17:21:24 GMT</pubDate>
            <atom:updated>2025-05-05T17:50:12.462Z</atom:updated>
            <content:encoded><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*0eiKINrJ8k6dlZG3K4m0IQ.jpeg" /></figure><p><em>Written by </em><a href="https://www.linkedin.com/in/joshxiaominxi/"><em>Josh Xi</em></a><em> &amp; </em><a href="https://www.linkedin.com/in/rakeshkumar1007/"><em>Rakesh Kumar</em></a><em> at¬†Lyft.</em></p><p>From real-time rider pricing and driver incentives to long-term budget allocation and strategic planning, forecasting at Lyft plays a pivotal role in providing a foresight of our market conditions for efficient operations and facilitating millions of rides daily across North America. This article explores real-time spatial temporal forecasting models and system designs used for predicting market conditions, focusing on how their complexity and rapid nature affect model performance, selection, and forecasting system¬†design.</p><h3>Introduction: Real-Time Spatial Temporal Forecasting</h3><h4>Definition</h4><p>Real-time spatial temporal forecasting is often used to predict market signals, ranging from a few minutes to a couple of hours, at fine spatial and temporal granularity. For example, we can predict rideshare demand and supply at <a href="https://www.ibm.com/docs/en/streams/4.3.0?topic=334-geohashes">geohash-6</a> level for every 5 minute interval in the next hour for an entire city or region. The forecast also runs at a high frequency (e.g. every minute) with real-time input of the most refreshed data to capture the latest marketplace conditions in fine detail, including local spikes and¬†dips.</p><p>Lyft currently operates in hundreds of North American cities and regions. Our spatial temporal forecasting models predict forecast values for 4M geohashes per minute, per¬†signal.</p><h4>Use Cases</h4><p>Predictions from real-time forecasting are often used for inputs into levers that balance real-time demand and supply across space. For¬†example:</p><ul><li><strong>Dynamic pricing:</strong> As a platform, maintaining marketplace balance is one of Lyft‚Äôs top missions. Dynamic pricing maintains balance in real-time by raising prices to dampen demand when drivers are scarce and lowering prices to encourage rides when there are more¬†drivers.</li><li><strong>Real-time driver incentives:</strong> In contrast to dynamic pricing managing demand, real-time driver incentives achieve marketplace balance by raising drivers‚Äô earnings to attract drivers to come online during undersupply or relocate from oversupplied to undersupplied areas. Similarly, understanding the current and near-future marketplace conditions in different locations is essential to the incentive model.</li></ul><h4>Challenges</h4><p>The high dimension, high frequency nature of real-time spatial temporal forecasting presents unique challenges compared to low dimension, low frequency time series forecasting.</p><blockquote>Large Computation Restricts Practical Model Choices and System¬†Design</blockquote><p>More computation power is needed for heavy spatial and temporal data processing and fast online inference, which can restrict our model choices and system design in practice. Large complex models, such as deep neural networks, might give more accurate forecasts in theory, but they also cost more to run, take longer, and can be harder to keep stable and scale up. This can cancel out the accuracy¬†gains.</p><blockquote>Noisy Signals Reduce Forecasting Accuracy</blockquote><p>When analyzing signals with greater spatial and temporal detail, such as minutely geohash-6 level rideshare demand and supply, they can become noisier compared to when viewed at a more aggregated level. Specifically, we¬†observe:</p><ul><li>Sparser signals with many zero observations;</li><li>Increase in intermittent local spikes and dips, usually lasting from a few minutes to about 30¬†minutes.</li></ul><p>These local fluctuations are often due to events like concerts, sports, and community gatherings, which may not be noticeable at broader hourly, daily, or regional levels, but become significant when viewed in¬†detail.</p><p>Modeling such impact can be challenging for multiple¬†reasons:</p><ul><li><strong>Data availability &amp; accuracy:</strong> Getting comprehensive event data can be costly in practice. Even with access to events data, it can be hard to predict key information such as event end time or impact time at the desired accuracy level. For example, although an American football game displays 5 minutes left on the screen, the actual event end time can vary from 5 to 30¬†minutes.</li><li><strong>Compound effect:</strong> In our experience, the actual impact of events on rideshare demand is usually compounded with other factors; for example:<br><strong><em>* </em></strong>Venue operation like shuttle services and designated rideshare pickups can affect both demand and supply spatially.<br><strong><em>*</em></strong><em> </em>Availability of public transit and parking facilities can affect travelers‚Äô mode choice.<br><strong><em>*</em></strong><em> </em>Nearby amenities, like restaurants, can affect pre- and post-event activities and travel plans.<br><strong><em>*</em></strong><em> </em>Time of day, day of week, and seasonality can also affect public transit, venue and business operations, affecting demand patterns.</li></ul><p>Due to this ‚Äúnoise‚Äù, the spatial and temporal correlation and stability at a detailed level can drop significantly from those at an aggregated level. We will discuss how these changes could affect forecast accuracy in the model performance section.</p><h3>Forecasting Models</h3><p>We have explored two distinct sets of models for real-time spatial temporal forecasting: classical time-series models and neural network models. In this section, we introduce a few representative models that we have explored at Lyft and/or have been well studied in research papers. We also compare the model performance in terms of forecast accuracy and engineering cost from our implementation. Note that although it‚Äôs mentioned in the introduction that local events can contribute to fluctuations in our signals, we will not explicitly discuss events or events modeling here as it is a complicated topic worth another discussion of its¬†own.</p><h4>Time Series¬†Models</h4><p>Some of the time series models we have explored¬†include:</p><ul><li><strong>Linear regression models like auto-regression and ARIMA:</strong> These simple time series models use past data (linear combinations or weighted averages) to predict the future. To adapt single time series models on spatial data (multiple correlated time series), we can either assume the same model weights for all regional geohashes or divide the region into partitions based on signal history, assigning specific weights to each partition.</li><li><strong>Spatial temporal covariance models:</strong> Instead of treating different geohashes (or partitions) as independent time series, we can estimate and model spatial temporal correlations explicitly (see <a href="https://marcgenton.github.io/2021.CGS.ARSIA.pdf">Chen et al.¬†[2021]</a>).</li><li><strong>Spatial temporal correlation through dimension reduction: </strong>Assuming that many geohashes are correlated, we can apply dimension reduction approaches like SVD and PCA to project thousands of time series to a few dozen, with correlation embedded in the dimension reduction process. We then apply traditional time series models on the reduced dimensions for forecasting, and finally project the forecasts back to the original geohash dimension (see <a href="https://www.sciencedirect.com/science/article/abs/pii/S0960148114002432">Skittides Fruh [2014]</a>, <a href="https://dl.acm.org/doi/abs/10.1145/3538637.3539764">Grother &amp; Reiger¬†[2022]</a>).</li></ul><h3>Neural Net¬†Models</h3><p>Deep neural network (DNN) models have emerged as powerful alternatives due to their capacity to automatically handle complex non-linear spatial and temporal patterns without extensive feature engineering (<a href="https://www.mdpi.com/2078-2489/14/11/598">Casolaro et al. [2023]</a>; <a href="https://link.springer.com/article/10.1007/s11831-025-10244-5">Mojtahedi et al. 2025</a>). Some of the DNNs we tested¬†are:</p><ul><li>Recurrent neural networks (RNN) and variants like long short-term memory (LTSM): <a href="https://arxiv.org/pdf/2108.11875">He et al [2021]</a>, <a href="http://abduljabbar">Abduljabbar et al. [2021]</a>. RNNs‚Äô fundamental architecture for sequence modeling is perfect for learning temporal correlation by memory retention from previous time steps. However, standard RNNs suffer from vanishing gradient problems when processing long sequences. LSTM networks address this limitation through gating mechanisms.</li><li>Convolutional neural networks (CNN): <a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/09/DeepST-SIGSPATIAL2016_Zheng-2.pdf">Zhang et al. [2016]</a>, <a href="https://ieeexplore.ieee.org/abstract/document/8684259">Guo et al. [2019]</a>. CNNs were initially applied to image processing, and recently have been adapted for spatial temporal modeling, treating signal values at each map timestamp like a snapshot of pixels in an¬†image.</li></ul><p>A few other emerging DNN models, which we haven‚Äôt tested yet but give similar performance in literature, are:</p><ul><li>Graphic neural networks (GNN): <a href="https://link.springer.com/article/10.1007/s10489-021-02587-w">Bui et al. [2022]</a>, <a href="https://www.sciencedirect.com/science/article/abs/pii/S0020025522003978">Geng et al. [2022]</a>, <a href="https://arxiv.org/pdf/2410.22377">Corradini et al. [2024]</a>. GNNs have been well-applied to graph-structured data in applications like behavior detection, traffic control, and molecular structure study, where nodes and edges represent entities and their relationships. In spatial temporal forecasting, it is assumed that the signal value of a node depends on its own history and the history of its neighbors, whose weights are estimated by specific blocks or modules in the GNN structure.</li><li>Transformer models: <a href="https://arxiv.org/abs/2001.08317">Wu et al. [2020]</a>, <a href="https://arxiv.org/pdf/2202.07125">Wen et al. [2022]</a>. Inspired by their success in natural language processing, transformer models have been increasingly applied to time series forecasting. However, literature has shown mixed results so far (<a href="https://ojs.aaai.org/index.php/AAAI/article/view/26317">Zeng et al.¬†[2023]</a>).</li></ul><h4>Online Implementation &amp; Refitting</h4><p>Our marketplace is constantly changing from day-to-day and minute-to-minute. In our experience, a model trained on historical data can quickly become obsolete, sometimes in a few days but sometimes in a dozen minutes, in the occurrence of some unknown or unmodeled local events. Therefore, frequently retraining the model is critical to the accuracy of the forecasts. We have implemented the following in our forecasting:</p><ul><li>Refit time series models every minute using the latest observations before running an inference.<br><strong><em>* </em></strong>Due to the simplicity of the model structure and smaller model weights, these models are effortlessly re-trainable without causing significant latency or memory issues.<br><strong><em>* </em></strong>It‚Äôs worth noting that some of the time series models, such as those based on dimension reduction, are only partially re-trainable in real-time. While new data samples can update model weights in the reduced dimension, this process requires a longer history, which can be time consuming and can only be completed offline at less frequent intervals.</li><li>Refit DNN models multiple times a day.<br><strong><em>* </em></strong>While theoretically all neural network models can be refitted on new data samples, their model size can cause high latency, making real-time refitting less ideal. Instead, we refit the model separately offline multiple times daily using recent data¬†batches.</li></ul><h3>Model Performance</h3><p>In our experience, model choice is a trade-off between forecast accuracy and engineering cost. In this section, we share some learnings from our testing and implementation of time series and DNN models. For those interested in or familiar with forecasting modes, the ‚ÄúAccuracy‚Äù section offers technical insights into why certain models outperform others. Alternatively, you can focus on the key learnings highlighted in¬†<strong>bold</strong>.</p><h4>Accuracy</h4><p>While most literature finds that DNN models provide better forecast accuracy than classical time series models, we find it to be only partially true. Below are some of our learnings:</p><h4>DNNs outperform time series without latency consideration</h4><p>When latency is not considered and forecasts are simulated at the same refitting frequency (from daily, hourly, to minutely), DNN models overall generate more accurate forecasts than classical time series models. However, as the refitting frequency increases, the accuracy gap reduces significantly; and in some cases, time series models can even outperform DNN¬†models.</p><h4>Time series outperforms DNNs with latency consideration</h4><p>When considering latency, simulating time series refitted minutely and DNN refitted hourly results in time series models having better overall accuracy than DNN. Specifically,</p><ol><li><strong>Time series models are more accurate for forecasting short-term horizons, such as the next 5 to 30 or 45 minutes. In contrast, DNN models can outperform time series in longer horizons, beyond 30 or 45 minutes. </strong><br>Our real-time signal has strong near-term autocorrelation. In other words, what will happen in the next 5 minutes can be similar to what happened in the past 5 minutes; hence, even a simple autocorrelation model can generate a good forecast.<br>This is also true during temporal local spikes caused by irregular events. To illustrate, imagine riders leaving a concert. Rideshare demand usually spikes up fast, stays high in the first 15‚Äì20 minutes, then gradually returns to normal in the next 15‚Äì20 minutes. As the peak occurs, refitting an autoregression model can quickly pick up the autocorrelation and update the forecast accordingly. Meanwhile, a local spike can temporarily change the spatial correlation across nearby geohashes, and without refitting, it can throw off the forecasts of a DNN model‚Äôs estimation of spatial correlation.<br>As the forecast moves to a longer horizon, the near-term autocorrelation gets weaker. In the concert example above, a demand spike in the past 5 minutes does not guarantee a spike in an hour. Instead, factors like seasonality, trend, and spatial correlation become more important predictors, which DNN models seem to capture better than classical time series¬†models.</li><li><strong>Between demand and supply signals, both time series and DNN models tend to give better accuracy on supply; however, time series models are more likely to outperform DNN on demand signals. </strong><br>The rationale behind this is due to different levels of signal noise. Drivers tend to stay online for a while after logging on, resulting in smoother supply patterns than for demand, with less temporary spikes or dips. As a result, supply signals have more stable spatial and temporal correlations, making both sets of models perform better. Meanwhile, riders make on-demand requests based on their individual schedules, which tends to cause more fluctuations in demand pattern, making time series with fast refitting a better option. <br>In general, the underlying signal generation process influences the spatial temporal correlation and stability, affecting forecast accuracy. For example, average traffic speed in different locations of a city resembles a Gaussian process, and hence is likely to have a much stronger spatial correlation and more stable temporal pattern compared to a demand signal from a Poisson¬†process.</li><li><strong>For regions with complicated terrain structures (like lots of hills and lakes), DNN models tend to perform worse than time series models</strong>.<br>Our conjecture is that complicated terrain structures can weaken spatial correlation; hence, making some of the DNN models less powerful. For example, a city with many mountains and waters can have more pockets with zero demand and supply; and a city with many venues for irregular events can cause more local spikes and¬†dips.</li></ol><p>From our learnings, we can conclude that forecast accuracy is heavily dependent on the signal characteristics. Before choosing your model, you should evaluate signals for their spatial and temporal correlation and stability.</p><h4>Engineering Cost</h4><p>Real-time spatial temporal forecasting is big in size, and requires a large amount of memory and computation power for heavy data processing and fast online inference. Hence, scalability, stability (e.g. low latency), and computation cost affect the final model and system¬†design.</p><p>Given the size of the DNN models, it‚Äôs no surprise that they are more costly than time series models. In particular:</p><ul><li><strong>Training cost:</strong> DNN models require training on GPU, which can be 100x more expensive than classical time series models that train on CPU. For example, training a DNN model on a few weeks of data of a single region can take a couple of hours on a 128GB GPU, while a classical time series model takes less than a minute on an 8GB CPU. These cost differences can be non-trivial when training separate models for hundreds of regions on dozens of¬†signals.</li><li><strong>Engineering reliability:</strong> In our experience, DNN models are more prone to issues like training failures, out-of-memory errors, and high latency, incurring higher maintenance costs.</li><li><strong>Forecast interpretability &amp; debuggability:</strong> Forecasts from conventional time-series models are usually more interpretable, making it easy to debug performance issues and overwrite forecasts manually if necessary. For example, a time series can be broken down into trend and seasonality, with events and weather impacts added. Each component can be further examined and overwritten if expert knowledge or external data provides a different projection.</li></ul><h3>Forecasting Architecture &amp; Tech¬†Stack</h3><h4>Architecture</h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*3vmpUOqOOaLbotj8" /><figcaption>Figure 1: Architecture diagram for forecasting pipelines</figcaption></figure><p>The current architecture uses the following technology stack:</p><ul><li><a href="https://beam.apache.org/">Apache Beam</a> (<a href="https://flink.apache.org/">Apache Flink</a> runner) streaming pipelines</li><li>Amazon Services (<a href="https://aws.amazon.com/msk/">MSK</a>, <a href="https://aws.amazon.com/kinesis/">Kinesis</a>, <a href="https://aws.amazon.com/pm/dynamodb/?gclid=Cj0KCQiA8fW9BhC8ARIsACwHqYqKfsD8rEN51OItEvKNW9k4EBaFlcQUOQJPf-XVMFXc7M5VV5kCeNAaAiXUEALw_wcB&amp;trk=94bf4df1-96e1-4046-a020-b07a2be0d712&amp;sc_channel=ps&amp;ef_id=Cj0KCQiA8fW9BhC8ARIsACwHqYqKfsD8rEN51OItEvKNW9k4EBaFlcQUOQJPf-XVMFXc7M5VV5kCeNAaAiXUEALw_wcB:G:s&amp;s_kwcid=AL!4422!3!610000101513!e!!g!!dynamodb!11205224796!115505094368">DynamoDB</a>)</li><li><a href="https://eng.lyft.com/powering-millions-of-real-time-decisions-with-lyftlearn-serving-9bb1f73318dc">Lyft Machine Learning Platform</a> (<a href="https://airflow.apache.org/">Apache Airflow</a>, <a href="https://aws.amazon.com/sagemaker">AWS SageMaker</a>)</li><li><a href="https://clickhouse.com/">ClickHouse</a></li></ul><h4><strong>Execution flow</strong></h4><ol><li>Before we can run any online inference, an offline model training pipeline needs to be executed to construct online model structure and initialize model weights. These are saved into a model artifacts database for online models to access. This pipeline can also be used to retrain model weights at a scheduled frequency or ad-hocly, and push updated weights to the artifacts database.</li><li>The system gets the data based on analytics events. These events are generated by our services and client¬†apps.</li><li>The feature generation pipeline aggregates the events (read about them in this <a href="https://eng.lyft.com/evolution-of-streaming-pipelines-in-lyfts-marketplace-74295eaf1eba">blog post</a>) and passes them through Kafka¬†topics.</li><li>Features are indexed in OLAP DB (ClickHouse) as short-term historical features with a one day¬†TTL.</li><li>Our forecasting pipeline takes these features from Kafka topics and OLAP DB. This ensures data consistency across different features based on the window end time of those features. We pass them to our forecasting models hosted on Lyft‚Äôs Machine Learning Platform (MLP). Our models also use older historical features (days/weeks), refreshed daily via Airflow¬†DAGs.</li><li>MLP is responsible for model online inference and asynchronously syncing results to a Kafka topic. If online refitting is implemented, our models can refit using the most recent input and adjust their weights before each inference.</li><li>This topic is subscribed by downstream services (Rider Pricing, Driver Earnings, etc.) who are interested in forecasted values and want to consume them in real-time. Forecasted values are stored in the Feature Store in case users want to consume them in asynchronous fashion.</li><li>The forecasted values are also indexed in the OLAP DB, so we can calculate and monitor model performance metrics in real-time.</li></ol><p>We have chosen an asynchronous design mainly for scaling and performance reasons.</p><h4>Forecasted Feature Guarantee</h4><p>It is important that features come with a quality and reliability guarantee, otherwise it would be difficult for our customers to build products with confidence. For each signal, we define a quality guarantee and measure through our internal systems. Specifically, one system generates model performance metrics (e.g. bias, mean absolute percent error) based on the forecasted and historical values stored in the OLAP DB. Another system constantly monitors these metrics, and if the metrics are outside the expected bounds, then it alerts our engineering team.</p><h3>Conclusion</h3><p>Forecasting performance is heavily dependent on the characteristics of the forecasted signals, such as spatial temporal granularity, level of noise, spatial temporal correlation, and its stability. In our experience, simple time series models with real-time refitting provide overall better accuracy when the signal is more granular with less stable spatial temporal correlations, and/or when forecasting the near-term horizons. Although a complex DNN model can improve forecasting accuracy if refitted real-time or forecasting for longer horizons, they may not be the best solution for your business due to high computation cost and latency. Businesses often prioritize simpler models with greater interpretability, lower inference latency, a simplified retraining process and, <em>crucially, </em>a lower total cost of ownership. These factors are essential for choosing cost-effective, maintainable solutions in real-world applications.</p><h3>Acknowledgements</h3><p>We would like to thank all our forecasting team members (<a href="https://www.linkedin.com/in/jimchiang/">Jim</a>, <a href="https://www.linkedin.com/in/glennazhang/">Glenna</a>, <a href="https://www.linkedin.com/in/quinn-liu/">Quinn</a>, <a href="https://www.linkedin.com/in/hongru-liu/">Hongru</a>, <a href="https://www.linkedin.com/in/binli98004/">Bin</a>, <a href="https://www.linkedin.com/in/lees28/">Soo</a>, <a href="https://www.linkedin.com/in/kyle-bilton/">Kyle</a>, <a href="https://www.linkedin.com/in/ido-bright-a1808429/">Ido</a>, <a href="https://www.linkedin.com/in/evanwils/">Evan</a>) for their contribution to model and architecture development, as well as the editing team (<a href="https://www.linkedin.com/in/jeana-choi/">Jeana</a>) for valuable suggestions and support during writing this blog¬†post.</p><p>Want to build ML solutions that impact millions? Join Lyft! We‚Äôre leveraging machine learning to solve problems at scale. If you‚Äôre passionate about building impactful, real-world applications, explore our openings at <a href="https://www.lyft.com/careers">Lyft¬†Careers</a>.</p><h3>Relevant Posts</h3><ul><li>Learn how we solve other forecasting problems at Lyft: <a href="https://eng.lyft.com/causal-forecasting-at-lyft-part-2-418f1febca5a">causal forecasting</a>, <a href="https://eng.lyft.com/making-long-term-forecasts-at-lyft-fac475b3ba52">cohort-based long-term forecasts</a>, <a href="https://eng.lyft.com/how-to-deal-with-the-seasonality-of-a-market-584cc94d6b75">seasonality</a>.</li><li>Check how we build <a href="https://eng.lyft.com/building-real-time-machine-learning-foundations-at-lyft-6dd99b385a4e">real-time machine learning foundations</a> and <a href="https://eng.lyft.com/ml-feature-serving-infrastructure-at-lyft-d30bf2d3c32a">ML feature¬†service</a>.</li><li>Check out how we <a href="https://eng.lyft.com/evolution-of-streaming-pipelines-in-lyfts-marketplace-74295eaf1eba">evolved our streaming pipelines</a> for realtime ML feature generation.</li><li>Discover how <a href="https://eng.lyft.com/gotchas-of-streaming-pipelines-profiling-performance-improvements-301439f46412">Lyft identified and fixed performance issues</a> in our streaming pipelines.</li><li>Learn <a href="https://eng.lyft.com/gotchas-of-stream-processing-data-skewness-cfba58eb45d4">how data skewness</a> can affect performance of streaming pipelines.</li></ul><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=fa90b3f3ec24" width="1" height="1" alt=""><hr><p><a href="https://eng.lyft.com/real-time-spatial-temporal-forecasting-lyft-fa90b3f3ec24">Real-Time Spatial Temporal Forecasting @ Lyft</a> was originally published in <a href="https://eng.lyft.com">Lyft Engineering</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[From manual fixes to automatic upgrades‚Ää‚Äî‚Ääbuilding the Codemod Platform at Lyft]]></title>
            <link>https://eng.lyft.com/from-manual-fixes-to-automatic-upgrades-building-the-codemod-platform-at-lyft-74c4f9df4680?source=rss----25cd379abb8---4</link>
            <guid isPermaLink="false">https://medium.com/p/74c4f9df4680</guid>
            <category><![CDATA[frontend-engineering]]></category>
            <category><![CDATA[automation]]></category>
            <category><![CDATA[codemod]]></category>
            <category><![CDATA[developer-tooling]]></category>
            <dc:creator><![CDATA[Anatolii (Nate) Kurochkin]]></dc:creator>
            <pubDate>Wed, 30 Apr 2025 15:24:59 GMT</pubDate>
            <atom:updated>2025-06-05T16:58:19.705Z</atom:updated>
            <content:encoded><![CDATA[<h3>From manual fixes to automatic upgrades‚Ää‚Äî‚Ääbuilding the Codemod Platform at¬†Lyft</h3><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*JpuIl6aS0EAFxl75QWlnsg.jpeg" /></figure><p>In software development, keeping your code up-to-date with the latest libraries and APIs is both crucial and frustrating‚Ää‚Äî‚Ääespecially in large-scale organizations. There‚Äôs always this trade-off between building shiny new features and slogging through tech debt to upgrade dependencies. And let‚Äôs be honest, new features almost always win. They‚Äôre the ones driving business value, after¬†all.</p><p>What if we flipped the script? Imagine upgrading libraries, handling breaking changes, and adopting new features happening seamlessly‚Ää‚Äî‚Ääwithout pulling developers away from their real work. Not only would that be cool, but it would also help engineers stay focused on delivering business value by removing the toil that often makes reducing tech debt such a tough tradeoff. Well, that‚Äôs exactly the challenge we set out to¬†tackle.</p><p>My team, Frontend Developer Experience, maintains the server-side rendering (SSR) web platform and a common components library used across all 100+ Lyft frontend microservices. Keeping these core tools updated was always a big effort, so we set out to build something that could automate upgrades‚Ää‚Äî‚Äähandling breaking changes and rolling out new features seamlessly.</p><p>To make this happen, we decided to build a Codemod Platform to handle code transformations at scale. We already had some codemod tools, but they were tied to specific libraries or versions. We wanted something better‚Ää‚Äî‚Ääa platform that works for any library or framework and makes updates easy and reusable.</p><p><em>If you‚Äôve never used codemods, they‚Äôre scripts that transform code by parsing it into a tree (</em><a href="https://en.wikipedia.org/wiki/Abstract_syntax_tree"><em>AST‚Ää‚Äî‚ÄäAbstract Syntax Tree</em></a><em>), making changes, and converting it¬†back.</em></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*c5QcnGfsG3CdlmSS" /></figure><p>With the right library, you can automate almost any change, but handling all edge cases takes time and patience.</p><h4>Our Goals</h4><p>To tackle this challenge, we started with several clear¬†goals:</p><ul><li><strong>Automate dependency upgrades: </strong>minor version updates often introduce new features, while major versions come with breaking changes that require fixes. Instead of developers manually reading docs, testing changes, and updating code, we would love codemods to handle everything automatically‚Ää‚Äî‚Ääseamlessly upgrading APIs and applying necessary fixes without human intervention.</li><li><strong>Make codemods easier to write: </strong>reduce the learning curve by providing helper functions and clear documentation, making it easier for developers to create their own transforms.</li><li><strong>Make it accessible to all developers:</strong> to ensure codemods could run anywhere with Node.js access, we provide our own CLI tool. By executing it with <a href="https://docs.npmjs.com/cli/v11/commands/npx">npx</a>, developers can run codemods without needing a global installation or adding them to frontend (FE) repositories.</li><li><strong>Standardize codemods across Lyft:</strong> last but not least, we aimed to unify the different codemod implementations across¬†Lyft.</li></ul><h4>Requirements</h4><p>Choosing the right library was key. There aren‚Äôt many options for transforming code in frontend, and <a href="https://github.com/facebook/jscodeshift">jscodeshift</a> was the best fit‚Ää‚Äî‚Ääit provides parsing, transformation, and writing in one place. Since we needed to handle TS, TSX, JS, and JSX files, jscodeshift worked out of the box. However, it also had some limitations we wanted to¬†address:</p><ul><li><strong>Reusing code:</strong> out of the box, jscodeshift provides a <a href="https://github.com/facebook/jscodeshift?tab=readme-ov-file#usage-cli">CLI</a> to run a single transform module. However, we wanted more flexibility‚Ää‚Äî‚Ääallowing one transform to execute another or run multiple transformations together.</li><li><strong>Detecting eligible services:</strong> for dependency upgrades, we needed to skip services that didn‚Äôt use the dependency to avoid wasting resources. This was especially important when running codemods across hundreds of microservices in¬†CI.</li><li><strong>Evergreen codemods:</strong> some transforms need extra setup before running. For example, upgrading @lyft/service (core library for Lyft FE services, <a href="https://eng.lyft.com/changing-lanes-how-lyft-is-migrating-100-frontend-microservices-to-next-js-42199aaebd5f">learn more</a>) to v2 requires installing sass to avoid breaking changes. Since every @lyft/service version from v2 onward has this requirement, we needed a way to ensure it was always handled automatically. We introduced evergreen codemods‚Ää‚Äî‚Ääpre-checks that set up things before running the main transform. We later expanded this to include post-transform checks as¬†well.</li><li><strong>Non-JS transforms</strong>: In addition to TS and JS files, we wanted to support transformations for YAML, JSON, and¬†.env files. This allowed us to extend codemods beyond code changes, handling configuration updates, environment variable adjustments, and other use cases, making the platform even more powerful.</li><li><strong>Naming convention:</strong> A clear and consistent naming convention makes the CLI easier to use and understand. To keep codemods predictable and maintainable, we established a set of naming rules:<br>‚Äì Clear and descriptive‚Ää‚Äî‚Ääthe name should clearly state what the transformation does, use a predictable format like verb-noun or verb-noun-adjective to make names easy to interpret.<br>‚Äì Versioning‚Ää‚Äî‚Ääif a codemod applies to a specific library update, include the version number (e.g., react-18.3.1, next-15.1) to a transform name.<br>‚Äì Allowed characters‚Ää‚Äî‚Ääuse only lowercase letters, numbers, ‚Äú.‚Äù and ‚Äú-‚Äù (e.g., react-18.3.1 instead of react_1831). This prevents issues with version mix-ups, such as distinguishing 18.3.1 from¬†1.83.1.</li><li><strong>Run 3rd-party codemods: </strong>many Frontend open-source libraries provide their own codemods to simplify migrations. For example, Next.js has the <a href="https://nextjs.org/docs/app/building-your-application/upgrading/codemods">@next/codemod</a> CLI; React and Storybook offer similar tools. We wanted our Codemod Platform to support running these existing transforms, allowing us to reuse them instead of reinventing the¬†wheel.</li><li><strong>Helper functions:</strong> last but not least, we aimed to minimize boilerplate. Whether adding a new import or removing a JSX prop from a React component, we wanted reusable functions to handle these common tasks automatically.</li></ul><h4>Design</h4><p>To address these challenges, we built a solution that covers all these gaps. Here‚Äôs how we did¬†it:</p><p><strong>@lyft/codemod CLI</strong></p><p>A typical codemod execution from the terminal would look like¬†this:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/2e54072acaf01eb493b47a4548e2a35d/href">https://medium.com/media/2e54072acaf01eb493b47a4548e2a35d/href</a></iframe><p>For the CLI to find and execute the correct transform, we created a helper function called executeUpgrade:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/4f25a11cd07882f7d39b71c0315ef649/href">https://medium.com/media/4f25a11cd07882f7d39b71c0315ef649/href</a></iframe><p>As you might have noticed, we execute the UpgradeClass, which sits at the core of our platform. This class was designed to handle all the requirements we outlined, ensuring every transformation runs smoothly and consistently. Let‚Äôs take a closer look at its implementation:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/757fa73d1d57a9846de3d065ed27719e/href">https://medium.com/media/757fa73d1d57a9846de3d065ed27719e/href</a></iframe><p>To see how everything fits together, let‚Äôs walk through a real¬†example.</p><p>Suppose we have a component library called core-ui, and in version 2, we removed the compact prop from the Button component. Our goal is to create a @lyft/codemod transform to automatically fix this breaking¬†change.</p><p>To start, we create a new folder in our transforms directory:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/59e395a10652d67eca8a335b30f10a84/href">https://medium.com/media/59e395a10652d67eca8a335b30f10a84/href</a></iframe><p>The index.ts file will export a class extending UpgradeBase with the following implementation:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/872f9a944b2a004730bfd02328ff7e17/href">https://medium.com/media/872f9a944b2a004730bfd02328ff7e17/href</a></iframe><p>The transform-buttons.ts file follows the same structure as a standard <a href="https://github.com/facebook/jscodeshift?tab=readme-ov-file#transform-module">jscodeshift transform module</a>. This made migrating existing jscodeshift transforms from other projects as simple as copying and pasting them into the new platform.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/01564536de76a3fe6b12139e4ef45003/href">https://medium.com/media/01564536de76a3fe6b12139e4ef45003/href</a></iframe><p>Or, even better if using helper functions to reduce boilerplate:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/f51b76768fb29e658a45ebadfbc22441/href">https://medium.com/media/f51b76768fb29e658a45ebadfbc22441/href</a></iframe><p><strong>Execution flow</strong></p><ol><li>@lyft/codemod -t core-ui-2 is being¬†executed</li><li>executeUpgrade searches for a folder named core-ui-2</li><li>core-ui-2 must contain an index.ts file exporting UpgradeBase as the¬†default</li><li>UpgradeBase.execute is called to run the¬†codemod</li><li>Execute checks if the service is¬†eligible</li><li>If eligible, runUpgrade executes the transform-buttons transformation</li><li>The compact prop has been removed from the Button component across all files under the¬†pathname</li><li>üéâ Profit</li></ol><p>This example covers a single use case, but multiple transformations can be combined within one folder and executed sequentially. Now, imagine applying this across hundreds of microservices!</p><p><strong>More complex use¬†cases</strong></p><p>Some codemods could be a lot more complex. For example, it could be running one codemod, and then executing another transform file, and then running 3rd-party codemod. All these can be easily handled by the platform. Here‚Äôs the example of the index file of the transform as well as how the whole file structure would look¬†like:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/6eafd416b432cc2dab2e8d0a85240369/href">https://medium.com/media/6eafd416b432cc2dab2e8d0a85240369/href</a></iframe><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/95b9a5af8bc0e3134198a198b43a43dc/href">https://medium.com/media/95b9a5af8bc0e3134198a198b43a43dc/href</a></iframe><p>It could be much more than that, but hopefully, this example gives you a sense of what codemods can¬†achieve.</p><h4>Releasing</h4><p>The Codemod Platform is essentially a library with its own CLI. To manage different versions, we package it as an internal npm package called @lyft/codemod, which any Lyft developer can execute¬†using:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/f7c5861542678d46b54e96997da69f80/href">https://medium.com/media/f7c5861542678d46b54e96997da69f80/href</a></iframe><p>Versioning follows <a href="https://semver.org/">semver</a> to maintain consistency. Locally, developers can use any available version by specifying it explicitly:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/5b76e72bc84a9dddf521048f5c9feff7/href">https://medium.com/media/5b76e72bc84a9dddf521048f5c9feff7/href</a></iframe><p>In CI, we¬†run:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/61cd1f49e1ed27c70ac054de7ec29c0a/href">https://medium.com/media/61cd1f49e1ed27c70ac054de7ec29c0a/href</a></iframe><p>This ensures that CI always uses the latest version of codemods, instantly applying all new transforms and bug¬†fixes.</p><p>Keeping the @lyft/codemod package separate from our internal libraries has also been key. It allowed us to develop and iterate quickly, without being blocked by or introducing changes to library¬†code.</p><h4>Testing</h4><p>To test our changes before it goes out we use <a href="https://github.com/facebook/jscodeshift?tab=readme-ov-file#definetest">defineTest</a> from jscodeshift because it makes testing codemods simple and readable. It compares two fixture files‚Ää‚Äî‚Ääone before and one after the transform‚Ää‚Äî‚Ääto verify that the changes work as expected.</p><p>For example, transform-buttons fixtures structure and content would look like¬†this</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/1bee68ffd7f89fdd1b772dc277738737/href">https://medium.com/media/1bee68ffd7f89fdd1b772dc277738737/href</a></iframe><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/684d3ab10fd41572fa9b4782fa3e4041/href">https://medium.com/media/684d3ab10fd41572fa9b4782fa3e4041/href</a></iframe><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/7e5f7254a6b27f0b5fb1c8ab0dd9df8f/href">https://medium.com/media/7e5f7254a6b27f0b5fb1c8ab0dd9df8f/href</a></iframe><p>Making it very easy to maintain and read by developers!</p><p>Another useful tool for testing and writing codemods is <a href="https://astexplorer.net/">AST Explorer</a>, which we use frequently. Since jscodeshift transforms code into an AST before modifying it, using the right API and node types is¬†crucial.</p><p>AST Explorer makes this easy by providing a visual representation of the AST and a real-time editor to experiment with transformations. For example, here‚Äôs how console.log(‚ÄòHello, World‚Äô) looks in AST¬†world:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*PL_VLsl2-3Jg1RnIVltZNQ.png" /></figure><p>Even though the AST might look complicated at first, it is a powerful tool that can help you analyze and transform the source code in a structured way. As you can see, each node has a type, and you can traverse the tree to find the nodes and methods you are interested in.</p><h4>Outcome</h4><p>So, what did we actually get out of this? Was all the effort really worth it? Let‚Äôs break it¬†down!</p><p><strong>Converted multiple Web Platform releases from major to¬†minor</strong></p><p>By fully automating breaking changes, we turned what would have been major releases into minor ones. Even better, for minor updates, we made Refactorator pull requests (PRs) fully auto-mergeable, meaning engineers no longer had to review PRs, read docs, or manually test¬†changes.</p><p><em>At Lyft, Refactorator is our internal tool that automates large-scale code changes by generating and managing PRs across all projects.</em></p><p>With 100+ FE microservices and multiple releases per year, this cut down on tedious manual work, saved thousands of developer hours, and made upgrades a lot smoother.</p><p><strong>Integrated codemods into automatic dependencies upgrade</strong></p><p>At Lyft, we already had a system for automating npm dependency upgrades, but version bumps alone weren‚Äôt enough‚Ää‚Äî‚Ääbreaking changes still needed manual fixes. By integrating the Codemods CLI into the process, upgrade PRs now do more than just update dependencies. They fix breaking changes and adopt new features automatically, all in a single¬†PR.</p><p>For example, automating 80% of breaking changes in our components library significantly boosted adoption, leading to ~30% of microservices migrating within 2 weeks‚Ää‚Äî‚Ääsomething that would have previously taken months. This not only made upgrades smoother but also gave developers a way to write their own codemods, making it easier to contribute meaningful changes and grow their¬†impact.</p><p>Codemods have now been executed in thousands of dependency upgrades to fix breaking changes and support new feature adoption. This automated process has helped reduce the total number of outdated dependencies across microservices by over 1,000. While this number is dynamic‚Ää‚Äî‚Ääsince new library versions are released daily‚Ää‚Äî‚Ääthe combination of automated package upgrades and codemods has had a lasting impact on keeping our ecosystem up to¬†date.</p><h4>Future plans</h4><p>To make an even bigger impact, we‚Äôve built a set of cleanup codemods that run on a schedule across FE services. These handle tasks like removing duplicate TypeScript compiler options, cleaning up redundant ESLint rules already covered by our base config, or eliminating unexpected duplicate dependencies in FE services.</p><p>We‚Äôre expanding codemods into this space as well. So far, we‚Äôve created 40+ transforms in just a year, and we‚Äôre just getting¬†started.</p><p>Looking ahead, we plan to make codemods even more accessible by integrating them into local development workflows and CI pipelines, giving engineers early feedback. We also plan to explore AI-assisted codemods that can suggest or even generate code transformations based on diff patterns, upgrade guides, or documentation. This could further reduce engineering effort and unlock new levels of automation in how we maintain and modernize our codebase.</p><h4>Acknowledgments</h4><p>Many thanks to my team‚Ää‚Äî‚ÄäDiana Cubas, Jonatan Santa Cruz, Betsabe Ortegon, Alfredo Campos Tams, Allen Arturo Jimenez‚Ää‚Äî‚Ääfor continuously improving this platform and adding more transforms to automate manual¬†work.</p><p>Additionally, thanks to Glen Cheney and Mario Garcia for automating breaking changes in some libraries and creating new helper functions used across transformations!</p><p><em>Lyft is hiring! If you‚Äôre passionate about developer tooling and automation, visit </em><a href="https://www.lyft.com/careers"><em>Lyft Careers</em></a><em> to see our openings.</em></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=74c4f9df4680" width="1" height="1" alt=""><hr><p><a href="https://eng.lyft.com/from-manual-fixes-to-automatic-upgrades-building-the-codemod-platform-at-lyft-74c4f9df4680">From manual fixes to automatic upgrades‚Ää‚Äî‚Ääbuilding the Codemod Platform at Lyft</a> was originally published in <a href="https://eng.lyft.com">Lyft Engineering</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
    </channel>
</rss>