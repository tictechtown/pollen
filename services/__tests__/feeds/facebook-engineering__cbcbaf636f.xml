<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	>

<channel>
	<title>Engineering at Meta</title>
	<atom:link href="https://engineering.fb.com/feed/" rel="self" type="application/rss+xml" />
	<link>https://engineering.fb.com/</link>
	<description>Engineering at Meta Blog</description>
	<lastBuildDate>Fri, 19 Dec 2025 17:16:56 +0000</lastBuildDate>
	<language>en-US</language>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>https://wordpress.org/?v=6.9</generator>
<site xmlns="com-wordpress:feed-additions:1">147945108</site>	<item>
		<title>DrP: Meta&#8217;s Root Cause Analysis Platform at Scale</title>
		<link>https://engineering.fb.com/2025/12/19/data-infrastructure/drp-metas-root-cause-analysis-platform-at-scale/</link>
		
		<dc:creator><![CDATA[]]></dc:creator>
		<pubDate>Fri, 19 Dec 2025 17:35:13 +0000</pubDate>
				<category><![CDATA[Data Infrastructure]]></category>
		<category><![CDATA[ML Applications]]></category>
		<guid isPermaLink="false">https://engineering.fb.com/?p=23496</guid>

					<description><![CDATA[<p>Incident investigation can be a daunting task in today’s digital landscape, where large-scale systems comprise numerous interconnected components and dependencies DrP is a root cause analysis (RCA) platform, designed by Meta, to programmatically automate the investigation process, significantly reducing the mean time to resolve (MTTR) for incidents and alleviating on-call toil Today, DrP is used [...]</p>
<p><a class="btn btn-secondary understrap-read-more-link" href="https://engineering.fb.com/2025/12/19/data-infrastructure/drp-metas-root-cause-analysis-platform-at-scale/">Read More...</a></p>
<p>The post <a rel="nofollow" href="https://engineering.fb.com/2025/12/19/data-infrastructure/drp-metas-root-cause-analysis-platform-at-scale/">DrP: Meta&#8217;s Root Cause Analysis Platform at Scale</a> appeared first on <a rel="nofollow" href="https://engineering.fb.com">Engineering at Meta</a>.</p>
]]></description>
										<content:encoded><![CDATA[<p><span style="font-weight: 400;">Incident investigation can be a daunting task in today’s digital landscape, where large-scale systems comprise numerous interconnected components and dependencies</span></p>
<p><a href="https://arxiv.org/abs/2512.04250"><span style="font-weight: 400;">DrP</span></a><span style="font-weight: 400;"> is a root cause analysis (RCA) platform, designed by Meta, to programmatically automate the investigation process, significantly reducing the mean time to resolve (MTTR) for incidents and alleviating on-call toil</span></p>
<p><span style="font-weight: 400;">Today, DrP is used by over 300 teams at Meta, running 50,000 analyses daily, and has been effective in reducing MTTR by 20-80% </span></p>
<p><span style="font-weight: 400;">By understanding DrP and its capabilities, we can unlock new possibilities for efficient incident resolution and improved system reliability.</span></p>
<h2><span style="font-weight: 400;">What It Is</span></h2>
<p><span style="font-weight: 400;">DrP is an end-to-end platform that automates the investigation process for large-scale systems. It addresses the inefficiencies of manual investigations, which often rely on outdated playbooks and ad-hoc scripts. These traditional methods can lead to prolonged downtimes and increased on-call toil as engineers spend countless hours triaging and debugging incidents.</span></p>
<p><span style="font-weight: 400;">DrP offers a comprehensive solution by providing an expressive and flexible SDK to author investigation playbooks, known as analyzers. These analyzers are executed by a scalable backend system, which integrates seamlessly with mainstream workflows such as alerts and incident management tools. Additionally, DrP includes a post-processing system to automate actions based on investigation results, such as mitigation steps.</span></p>
<p><img fetchpriority="high" decoding="async" class="alignnone size-full wp-image-23498" src="https://engineering.fb.com/wp-content/uploads/2025/12/Meta-DrP-image-1.png" alt="" width="1999" height="1125" srcset="https://engineering.fb.com/wp-content/uploads/2025/12/Meta-DrP-image-1.png 1999w, https://engineering.fb.com/wp-content/uploads/2025/12/Meta-DrP-image-1.png?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2025/12/Meta-DrP-image-1.png?resize=916,516 916w, https://engineering.fb.com/wp-content/uploads/2025/12/Meta-DrP-image-1.png?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2025/12/Meta-DrP-image-1.png?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2025/12/Meta-DrP-image-1.png?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2025/12/Meta-DrP-image-1.png?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2025/12/Meta-DrP-image-1.png?resize=192,108 192w" sizes="(max-width: 992px) 100vw, 62vw" /></p>
<p><span style="font-weight: 400;">DrP’s key components include: </span></p>
<ol>
<li style="font-weight: 400;" aria-level="1"><b>Expressive SDK</b><span style="font-weight: 400;">: The DrP SDK allows engineers to codify investigation workflows into analyzers. It provides a rich set of helper libraries and machine learning (ML) algorithms for data access and problem isolation analysis, such as anomaly detection, event isolation, time series correlation and dimension analysis.</span></li>
<li style="font-weight: 400;" aria-level="1"><b>Scalable backend</b><span style="font-weight: 400;">: The backend system executes the analyzers, providing both multi-tenant and isolated execution environments. It ensures that analyzers can be run at scale, handling thousands of automated analyses per day.</span></li>
<li style="font-weight: 400;" aria-level="1"><b>Integration with workflows</b><span style="font-weight: 400;">: DrP integrates with alerting and incident management tools, allowing for the auto-triggering of analyzers on incidents. This integration ensures that investigation results are immediately available to on-call engineers.</span></li>
<li style="font-weight: 400;" aria-level="1"><b>Post-processing system</b><span style="font-weight: 400;">: After an investigation, the post-processing system can take automated actions based on the analysis results. For example, it can create tasks or pull requests to mitigate issues identified during the investigation.</span></li>
</ol>
<h2><span style="font-weight: 400;">How It Works </span></h2>
<h3><span style="font-weight: 400;">Authoring Workflow</span></h3>
<p><img decoding="async" class="alignnone size-full wp-image-23499" src="https://engineering.fb.com/wp-content/uploads/2025/12/Meta-DrP-image-2.jpg" alt="" width="1999" height="1125" srcset="https://engineering.fb.com/wp-content/uploads/2025/12/Meta-DrP-image-2.jpg 1999w, https://engineering.fb.com/wp-content/uploads/2025/12/Meta-DrP-image-2.jpg?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2025/12/Meta-DrP-image-2.jpg?resize=916,516 916w, https://engineering.fb.com/wp-content/uploads/2025/12/Meta-DrP-image-2.jpg?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2025/12/Meta-DrP-image-2.jpg?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2025/12/Meta-DrP-image-2.jpg?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2025/12/Meta-DrP-image-2.jpg?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2025/12/Meta-DrP-image-2.jpg?resize=192,108 192w" sizes="(max-width: 992px) 100vw, 62vw" /></p>
<p><span style="font-weight: 400;">The process of creating automated playbooks, or analyzers, begins with the DrP SDK. Engineers enumerate the investigation steps, listing inputs and potential paths to isolate problem areas. The SDK provides APIs and libraries to codify these workflows, allowing engineers to capture all required input parameters and context in a type-safe manner.</span></p>
<ol>
<li style="font-weight: 400;" aria-level="1"><b>Enumerate investigation steps</b><span style="font-weight: 400;">: Engineers start by listing the steps required to investigate an incident, including inputs and potential paths to isolate the problem.</span></li>
<li style="font-weight: 400;" aria-level="1"><b>Bootstrap code</b><span style="font-weight: 400;">: The DrP SDK provides bootstrap code to create a template analyzer with pre-populated boilerplate code. Engineers extend this code to capture all necessary input parameters and context.</span></li>
<li style="font-weight: 400;" aria-level="1"><b>Data access and analysis</b><span style="font-weight: 400;">: The SDK includes libraries for data access and analysis, such as dimension analysis and time series correlation. Engineers use these libraries to code the main investigation decision tree into the analyzer.</span></li>
<li style="font-weight: 400;" aria-level="1"><b>Analyzer chaining</b><span style="font-weight: 400;">: For dependent service analysis, the SDK’s APIs allow for seamless chaining of analyzers, passing context and obtaining outputs.</span></li>
<li style="font-weight: 400;" aria-level="1"><b>Output and post-processing</b><span style="font-weight: 400;">: The output method captures findings from the analysis, using special data structures for both text and machine-readable formats. Post-processing methods automate actions based on analyzer findings.</span></li>
</ol>
<p><span style="font-weight: 400;">Once created, analyzers are tested and sent for code review. DrP offers automated backtesting integrated into code review tools, ensuring high-quality analyzers before deployment.</span></p>
<h3><span style="font-weight: 400;">Consumption Workflow</span></h3>
<p><span style="font-weight: 400;">In production, analyzers integrate with tools like UI, CLI, alerts, and incident management systems. Analyzers can automatically trigger upon alert activation, providing immediate results to on-call engineers and improving response times. The DrP backend manages a queue for requests and a worker pool for secure execution, with results returning asynchronously.</span></p>
<ol>
<li style="font-weight: 400;" aria-level="1"><b>Integration with alerts</b><span style="font-weight: 400;">: DrP is integrated with alerting systems, allowing analyzers to trigger automatically when an alert is activated. This provides immediate analysis results to on-call engineers.</span></li>
<li style="font-weight: 400;" aria-level="1"><b>Execution and monitoring</b><span style="font-weight: 400;">: The backend system manages a queue for analyzer requests and a worker pool for execution. It monitors execution, ensuring that analyzers run securely and efficiently.</span></li>
<li style="font-weight: 400;" aria-level="1"><b>Post-processing and insights</b><span style="font-weight: 400;">: A separate post-processing system handles analysis results, annotating alerts with findings. The DrP Insights system periodically analyzes outputs to identify and rank top alert causes, aiding teams in prioritizing reliability improvements.</span></li>
</ol>
<h2><span style="font-weight: 400;">Why It Matters</span></h2>
<h3><span style="font-weight: 400;">Reducing MTTR</span></h3>
<p><span style="font-weight: 400;">DrP has demonstrated significant improvements in reducing MTTR across various teams and use cases. By automating manual investigations, DrP enables faster triage and mitigation of incidents, leading to quicker system recovery and improved availability.</span></p>
<ol>
<li style="font-weight: 400;" aria-level="1"><b>Efficiency</b><span style="font-weight: 400;">: Automated investigations reduce the time engineers spend on manual triage, allowing them to focus on more complex tasks. This efficiency translates to faster incident resolution and reduced downtime.</span></li>
<li style="font-weight: 400;" aria-level="1"><b>Consistency</b><span style="font-weight: 400;">: By codifying investigation workflows into analyzers, DrP ensures consistent and repeatable investigations. This consistency reduces the likelihood of errors and improves the reliability of incident resolution.</span></li>
<li style="font-weight: 400;" aria-level="1"><b>Scalability</b><span style="font-weight: 400;">: DrP can handle thousands of automated analyses per day, making it suitable for large-scale systems with complex dependencies. Its scalability ensures that it can support the needs of growing organizations.</span></li>
</ol>
<h3><span style="font-weight: 400;">Enhancing On-Call Productivity</span></h3>
<p><span style="font-weight: 400;">The automation provided by DrP reduces the on-call effort during investigations, saving engineering hours and reducing on-call fatigue. By automating repetitive and time-consuming steps, DrP allows engineers to focus on more complex tasks, improving overall productivity.</span></p>
<h3><span style="font-weight: 400;">Scalability and Adoption</span></h3>
<p><span style="font-weight: 400;">DrP has been successfully deployed at scale at Meta, covering over 300 teams and 2000 analyzers, executing 50,000 automated analyses per day. Its integration into mainstream workflows, such as alerting systems, has facilitated widespread adoption and demonstrated its value in real-world scenarios.</span></p>
<ol>
<li style="font-weight: 400;" aria-level="1"><b>Widespread adoption</b><span style="font-weight: 400;">: DrP has been adopted by hundreds of teams across various domains, demonstrating its versatility and effectiveness in addressing diverse investigation needs.</span></li>
<li style="font-weight: 400;" aria-level="1"><b>Proven impact</b><span style="font-weight: 400;">: DrP has been in production for over five years, with proven results in reducing MTTR and improving on-call productivity. Its impact is evident in the positive feedback received from users and the significant improvements in incident resolution times.</span></li>
<li style="font-weight: 400;" aria-level="1"><b>Continuous improvement</b><span style="font-weight: 400;">: DrP is continuously evolving, with ongoing enhancements to its ML algorithms, SDK, backend system, and integrations. </span><span style="font-weight: 400;">This commitment to continuous improvement ensures that DrP remains a cutting-edge solution for incident investigations, while its growing adoption across teams enables existing workflows and analyzers to be reused by others, compounding the shared knowledge base and making it increasingly valuable across the organization.</span></li>
</ol>
<h2><span style="font-weight: 400;">What’s Next</span></h2>
<p><span style="font-weight: 400;">Looking ahead, DrP aims to evolve into an AI-native platform, playing a central role in advancing Meta’s broader AI4Ops vision</span><span style="font-weight: 400;">, enabling more powerful and automated investigations. This transformation will enhance analysis by delivering more accurate and insightful results, while also simplifying the user experience through streamlined ML algorithms, SDKs, UI, and integrations facilitating effortless authoring and execution of analyzers.</span></p>
<h2><span style="font-weight: 400;">Read the Paper</span></h2>
<p><a href="https://arxiv.org/abs/2512.04250"><span style="font-weight: 400;">DrP: Meta&#8217;s Efficient Investigations Platform at Scale</span></a></p>
<h2>Acknowledgements</h2>
<p><i><span style="font-weight: 400;">We wish to thank contributors to this effort across many teams throughout Meta</span></i></p>
<p><i><span style="font-weight: 400;">Team &#8211;  </span></i><i><span style="font-weight: 400;" data-rich-links="{&quot;per_n&quot;:&quot;Eduardo Hernandez&quot;,&quot;per_e&quot;:&quot;heduardo@fb.com&quot;,&quot;type&quot;:&quot;person&quot;}">Eduardo Hernandez</span></i><i><span style="font-weight: 400;">, </span></i><i><span style="font-weight: 400;" data-rich-links="{&quot;per_n&quot;:&quot;Jimmy Wang&quot;,&quot;per_e&quot;:&quot;jimmw@meta.com&quot;,&quot;type&quot;:&quot;person&quot;}">Jimmy Wang</span></i><i><span style="font-weight: 400;">, Akash Jothi, Kshitiz Bhattarai, </span></i><i><span style="font-weight: 400;" data-rich-links="{&quot;per_n&quot;:&quot;Shreya Shah&quot;,&quot;per_e&quot;:&quot;shreyashah@meta.com&quot;,&quot;type&quot;:&quot;person&quot;}">Shreya Shah</span></i><i><span style="font-weight: 400;">, </span></i><i><span style="font-weight: 400;" data-rich-links="{&quot;per_n&quot;:&quot;Neeru Sharma&quot;,&quot;per_e&quot;:&quot;neerusharma@meta.com&quot;,&quot;type&quot;:&quot;person&quot;}">Neeru Sharma</span></i><i><span style="font-weight: 400;">, Alex He, Juan-Pablo E, Oswaldo R, Vamsi Kunchaparthi, Daniel An, Rakesh Vanga, Ankit Agarwal, Narayanan Sankaran, Vlad Tsvang, Khushbu Thakur, </span></i><i><span style="font-weight: 400;" data-rich-links="{&quot;per_n&quot;:&quot;Srikanth Kamath&quot;,&quot;per_e&quot;:&quot;srkamath@fb.com&quot;,&quot;type&quot;:&quot;person&quot;}">Srikanth Kamath</span></i><i><span style="font-weight: 400;">, Chris Davis, </span></i><i><span style="font-weight: 400;" data-rich-links="{&quot;per_n&quot;:&quot;Rohit JV&quot;,&quot;per_e&quot;:&quot;rohitjv@fb.com&quot;,&quot;type&quot;:&quot;person&quot;}">Rohit JV</span></i><i><span style="font-weight: 400;">, </span></i><i><span style="font-weight: 400;" data-rich-links="{&quot;per_n&quot;:&quot;Ohad Yahalom&quot;,&quot;per_e&quot;:&quot;oyahalom@meta.com&quot;,&quot;type&quot;:&quot;person&quot;}">Ohad Yahalom</span></i><i><span style="font-weight: 400;">, Bao Nguyen, Viraaj Navelkar, Arturo Lira, Nikolay Laptev, Sean Lee, Yulin Chen</span></i></p>
<p><em><span style="font-weight: 400;">Leadership &#8211; Sanjay Sundarajan, John Ehrhardt, Ruben Badaro, </span><span style="font-weight: 400;" data-rich-links="{&quot;per_n&quot;:&quot;Nitin Gupta&quot;,&quot;per_e&quot;:&quot;nkg@fb.com&quot;,&quot;type&quot;:&quot;person&quot;}">Nitin Gupta</span><span style="font-weight: 400;">, Victoria Dudin, </span><span style="font-weight: 400;" data-rich-links="{&quot;per_n&quot;:&quot;Benjamin Renard&quot;,&quot;per_e&quot;:&quot;benj@meta.com&quot;,&quot;type&quot;:&quot;person&quot;}">Benjamin Renard</span><span style="font-weight: 400;">, Gautam Shanbhag, Barak Yagour, Aparna Ramani</span></em></p>
<p>The post <a rel="nofollow" href="https://engineering.fb.com/2025/12/19/data-infrastructure/drp-metas-root-cause-analysis-platform-at-scale/">DrP: Meta&#8217;s Root Cause Analysis Platform at Scale</a> appeared first on <a rel="nofollow" href="https://engineering.fb.com">Engineering at Meta</a>.</p>
]]></content:encoded>
					
		
		
		<post-id xmlns="com-wordpress:feed-additions:1">23496</post-id>	</item>
		<item>
		<title>How We Built Meta Ray-Ban Display: From Zero to Polish</title>
		<link>https://engineering.fb.com/2025/12/17/virtual-reality/meta-ray-ban-display-from-zero-to-polish/</link>
		
		<dc:creator><![CDATA[]]></dc:creator>
		<pubDate>Wed, 17 Dec 2025 14:00:17 +0000</pubDate>
				<category><![CDATA[Culture]]></category>
		<category><![CDATA[Virtual Reality]]></category>
		<category><![CDATA[Meta Tech Podcast]]></category>
		<guid isPermaLink="false">https://engineering.fb.com/?p=23477</guid>

					<description><![CDATA[<p>We’re going behind the scenes of the Meta Ray-Ban Display, Meta’s most advanced AI glasses yet. In a previous episode we met the team behind the Meta Neural Band, the EMG wristband packaged with the Ray-Ban Display. Now we’re delving into the glasses themselves. Kenan and Emanuel, from Meta’s Wearables org, join Pascal Hartig on [...]</p>
<p><a class="btn btn-secondary understrap-read-more-link" href="https://engineering.fb.com/2025/12/17/virtual-reality/meta-ray-ban-display-from-zero-to-polish/">Read More...</a></p>
<p>The post <a rel="nofollow" href="https://engineering.fb.com/2025/12/17/virtual-reality/meta-ray-ban-display-from-zero-to-polish/">How We Built Meta Ray-Ban Display: From Zero to Polish</a> appeared first on <a rel="nofollow" href="https://engineering.fb.com">Engineering at Meta</a>.</p>
]]></description>
										<content:encoded><![CDATA[<p>We’re going behind the scenes of the <a href="https://about.fb.com/news/2025/09/meta-ray-ban-display-ai-glasses-emg-wristband/?ref=engineeringatmeta">Meta Ray-Ban Display</a>, Meta’s most advanced AI glasses yet. In a previous episode we met the team behind the Meta Neural Band, the EMG wristband packaged with the Ray-Ban Display. Now we’re delving into the glasses themselves.</p>
<p>Kenan and Emanuel, from Meta’s Wearables org, join <a href="https://www.threads.com/@passy_" target="_blank" rel="noopener">Pascal Hartig</a> on the Meta Tech Podcast to talk about all the unique challenges of designing game-changing wearable technology, from the unique display technology to emerging UI patterns for display glasses.</p>
<p>You’ll also learn what particle physics and hardware design have in common and how to celebrate even the incremental wins in a fast-moving culture.</p>
<p>Download or listen to the episode below:</p>
<p><iframe style="border: none;" title="Libsyn Player" src="//html5-player.libsyn.com/embed/episode/id/39382865/height/90/theme/custom/thumbnail/yes/direction/forward/render-playlist/no/custom-color/000000/" width="100%" height="90" scrolling="no" allowfullscreen="allowfullscreen"></iframe></p>
<p>You can also find the episode wherever you get your podcasts, including:</p>
<ul>
<li><a href="https://open.spotify.com/episode/1xMZDrCSW74orGphqGLFE5?ref=engineeringatmeta" target="_blank" rel="noopener">Spotify</a></li>
<li><a href="https://podcasts.apple.com/gb/podcast/from-zero-to-polish-building-meta-ray-ban-display/id1370910331?i=1000741025569" target="_blank" rel="noopener">Apple Podcasts</a></li>
<li><a href="https://pca.st/3dhpd4np?ref=engineeringatmeta" target="_blank" rel="noopener">Pocket Casts</a></li>
</ul>
<p>The <a href="https://insidefacebookmobile.libsyn.com/" target="_blank" rel="noopener">Meta Tech Podcast</a> is a podcast, brought to you by Meta, where we highlight the work Meta’s engineers are doing at every level – from low-level frameworks to end-user features.</p>
<p>Send us feedback on <a href="https://instagram.com/metatechpod" target="_blank" rel="noopener">Instagram</a>, <a href="https://threads.net/@metatechpod" target="_blank" rel="noopener">Threads</a>, or <a href="https://twitter.com/metatechpod" target="_blank" rel="noopener">X</a>.</p>
<p>And if you’re interested in learning more about career opportunities at Meta visit the <a href="https://www.metacareers.com/?ref=engineering.fb.com" target="_blank" rel="noopener">Meta Careers</a> page.</p>
<p>The post <a rel="nofollow" href="https://engineering.fb.com/2025/12/17/virtual-reality/meta-ray-ban-display-from-zero-to-polish/">How We Built Meta Ray-Ban Display: From Zero to Polish</a> appeared first on <a rel="nofollow" href="https://engineering.fb.com">Engineering at Meta</a>.</p>
]]></content:encoded>
					
		
		
		<post-id xmlns="com-wordpress:feed-additions:1">23477</post-id>	</item>
		<item>
		<title>How AI Is Transforming the Adoption of Secure-by-Default Mobile Frameworks</title>
		<link>https://engineering.fb.com/2025/12/15/android/how-ai-transforming-secure-by-default-mobile-frameworks-adoption/</link>
		
		<dc:creator><![CDATA[]]></dc:creator>
		<pubDate>Mon, 15 Dec 2025 17:00:25 +0000</pubDate>
				<category><![CDATA[Android]]></category>
		<category><![CDATA[iOS]]></category>
		<category><![CDATA[ML Applications]]></category>
		<category><![CDATA[Security & Privacy]]></category>
		<guid isPermaLink="false">https://engineering.fb.com/?p=23460</guid>

					<description><![CDATA[<p>Meta’s secure-by-default frameworks wrap potentially unsafe OS and third-party functions, making security the default while preserving developer speed and usability. These frameworks are designed to closely mirror existing APIs, rely on public and stable interfaces, and maximize developer adoption by minimizing friction and complexity. Generative AI and automation accelerate the adoption of secure frameworks at [...]</p>
<p><a class="btn btn-secondary understrap-read-more-link" href="https://engineering.fb.com/2025/12/15/android/how-ai-transforming-secure-by-default-mobile-frameworks-adoption/">Read More...</a></p>
<p>The post <a rel="nofollow" href="https://engineering.fb.com/2025/12/15/android/how-ai-transforming-secure-by-default-mobile-frameworks-adoption/">How AI Is Transforming the Adoption of Secure-by-Default Mobile Frameworks</a> appeared first on <a rel="nofollow" href="https://engineering.fb.com">Engineering at Meta</a>.</p>
]]></description>
										<content:encoded><![CDATA[<ul>
<li style="font-weight: 400;" aria-level="1"><span style="font-weight: 400;">Meta’s secure-by-default frameworks wrap potentially unsafe OS and third-party functions, making security the default while preserving developer speed and usability.</span></li>
<li style="font-weight: 400;" aria-level="1"><span style="font-weight: 400;">These frameworks are designed to closely mirror existing APIs, rely on public and stable interfaces, and maximize developer adoption by minimizing friction and complexity.</span></li>
<li style="font-weight: 400;" aria-level="1"><span style="font-weight: 400;">Generative AI and automation accelerate the adoption of secure frameworks at scale, enabling consistent security enforcement and efficient migration across Meta’s vast codebase.</span></li>
</ul>
<p><span style="font-weight: 400;">Sometimes functions within operating systems or provided by third parties come with a risk of misuse that could compromise security. To mitigate this, we wrap or replace these functions using our own secure-by-default frameworks. These frameworks play an important role in helping our security and software engineers maintain and improve the security of our codebases while maintaining developer speed.</span></p>
<p><span style="font-weight: 400;">But implementing these frameworks comes with practical challenges, like design tradeoffs. Building a secure framework on top of Android APIs, for example, requires a thoughtful balance between security, usability, and maintainability.</span></p>
<p><span style="font-weight: 400;">With the emergence of AI-driven tools and automation we can scale the adoption of these frameworks across Meta&#8217;s large codebase. AI can assist in identifying insecure usage patterns, suggesting or automatically applying secure framework replacements and continuously monitoring compliance. This not only accelerates migration but also ensures consistent security enforcement at scale.</span></p>
<p><span style="font-weight: 400;">Together, these strategies empower our development teams to ship well-secured software efficiently, safeguarding user data and trust while maintaining high developer productivity across Meta’s vast ecosystem.</span></p>
<h2><span style="font-weight: 400;">How We Design Secure-by-Default Frameworks at Meta</span></h2>
<p><span style="font-weight: 400;">Designing secure-by-default frameworks for use by a large number of developers shipping vastly different features across multiple apps is an interesting challenge. There are a lot of competing concerns such as discoverability, usability, maintainability, performance, and security benefits. </span></p>
<p><span style="font-weight: 400;">Practically speaking, developers only have a finite amount of time to code each day. The goal of our frameworks is to improve product security while being largely invisible and friction-free to avoid slowing developers down unnecessarily. This means that we have to correctly balance all those competing concerns discussed above. If we strike the wrong balance, some developers could avoid using our frameworks, which could reduce our ability to prevent security vulnerabilities. </span></p>
<p><span style="font-weight: 400;">For example, if we design a framework that improves product security in one area but introduces three new concepts and requires developers to provide five additional pieces of information per call site, some app developers may try to find a way around using them. Conversely, if we provide these same frameworks that are trivially easy to use, but they consume noticeable amounts of CPU and RAM, some app developers may, again, seek ways around using them, albeit for different reasons.</span></p>
<p><span style="font-weight: 400;">These examples might seem a bit obvious, but they are taken from real experiences over the last 10+ years developing ~15 secure-by-default frameworks targeting Android and iOS. Over that time, we&#8217;ve established some best practices for designing and implementing these new frameworks.</span></p>
<p><span style="font-weight: 400;">To the maximum extent possible, an effective framework should embody the following principles: </span></p>
<ul>
<li style="font-weight: 400;" aria-level="1"><b>The secure framework API should resemble the existing API.</b><span style="font-weight: 400;"> This reduces the cognitive burden on framework users, forces security framework developers to minimize the complexity of the changes, and makes it easier to perform automated code conversion from the insecure to secure API usage.</span></li>
<li style="font-weight: 400;" aria-level="1"><b>The framework should itself be built on public and stable APIs</b><span style="font-weight: 400;">. APIs from OS vendors and third parties change all the time, especially the non-public ones. Even if access to those APIs is technically allowed in some cases, building on top of private APIs is a recipe for constant fire drills (best case) and dead-end investment in frameworks that simply can’t work with newer versions of operating systems and libraries (worst case).</span></li>
<li style="font-weight: 400;" aria-level="1"><b>The framework should cover the maximum number of application users, not security use cases</b><span style="font-weight: 400;">. There shouldn’t be one security framework that covers all security issues, and not every security issue is general enough to deserve its own framework. However, each security framework should be usable across all apps and OS versions for a particular platform. Small libraries are faster to build and deploy, and easier to maintain and explain to app developers.</span></li>
</ul>
<p><span style="font-weight: 400;">Now that we’ve looked at the design philosophy behind our frameworks, let’s look at one of our most widely used Android security frameworks, SecureLinkLauncher.</span></p>
<h2><span style="font-weight: 400;">SecureLinkLauncher: Preventing Android Intent Hijacking</span></h2>
<p><span style="font-weight: 400;">SecureLinkLauncher (SLL) is one of our widely-used secure frameworks. SLL is designed to prevent sensitive data from spilling through the</span><a href="https://developer.android.com/guide/components/intents-filters" target="_blank" rel="noopener"> <span style="font-weight: 400;">Android intents system</span></a><span style="font-weight: 400;">. It exemplifies our approach to secure-by-default frameworks by wrapping native Android intent launching methods with scope verification and security checks, preventing common vulnerabilities such as intent hijacking without sacrificing developer velocity or familiarity.</span></p>
<p><span style="font-weight: 400;">The system consists of intent senders and intent receivers. SLL is targeted to intent senders.</span></p>
<p><span style="font-weight: 400;">SLL offers a semantic API that closely mirrors the familiar Android Context API for launching intents, including methods like</span> <span style="font-weight: 400; font-family: 'courier new', courier;">startActivity()</span><span style="font-weight: 400;"> and </span><span style="font-weight: 400; font-family: 'courier new', courier;">startActivityForResult()</span><span style="font-weight: 400;">. </span><span style="font-weight: 400;">Instead of invoking the potentially insecure Android API directly, such as</span> <span style="font-weight: 400; font-family: 'courier new', courier;">context.startActivity(intent);</span><span style="font-weight: 400;">, </span><span style="font-weight: 400;">developers use SecureLinkLauncher with a similar method call pattern, for example, </span><span style="font-weight: 400; font-family: 'courier new', courier;">SecureLinkLauncher.launchInternalActivity(intent, context);</span><span style="font-weight: 400;">. </span><span style="font-weight: 400;">Internally, SecureLinkLauncher delegates to the stable Android </span><span style="font-weight: 400; font-family: 'courier new', courier;">startActivity()</span> <span style="font-weight: 400;">API, ensuring that all intent launches are securely verified and protected by the framework.</span></p>
<pre class="line-numbers"><code class="language-java">public void launchInternalActivity(Intent intent, Context context) {
   // Verify that the target activity is internal (same package)
   if (!isInternalActivity(intent, context)) {
       throw new SecurityException("Target activity is not internal");
   }
   // Delegate to Android's startActivity to launch the intent
   context.startActivity(intent);
}
</code></pre>
<p>Similarly, instead of calling <span style="font-family: 'courier new', courier;">context.startActivityForResult(intent, code);</span> directly, developers use <span style="font-family: 'courier new', courier;">SecureLinkLauncher.launchInternalActivityForResult(intent, code, context);</span>. SecureLinkLauncher (SLL) wraps Android’s <span style="font-family: 'courier new', courier;">startActivity()</span> and related methods, enforcing scope verification before delegating to the native Android API. This approach provides security by default while preserving the familiar Android intent launching semantics.</p>
<p><span style="font-weight: 400;">One of the most common ways that data is spilled through intents is due to incorrect targeting of the intent. As an example, following intent isn’t targeting a specific package. This means it can be received by any app with a matching &lt;intent-filter&gt;. While the intention of the developer might be that their Intent ends up in the Facebook app based on the URL, the reality is that any app, including a malicious application, could add an &lt;intent-filter&gt; that handles that URL and receive the intent. </span></p>
<pre class="line-numbers"><code class="language-java">Intent intent = new Intent(FBLinks.PREFIX + "profile");
intent.setExtra(SECRET_INFO, user_id);
startActivity(intent); 
// startActivity can’t ensure who the receiver of the intent would be</code></pre>
<p><span style="font-weight: 400;">In the example below, SLL ensures that the intent is directed to one of the family apps, as specified by the developer’s scope for implicit intents. Without SLL, these intents can resolve to both family and non-family apps,potentially exposing SECRET_INFO to third-party or malicious apps on the user&#8217;s device. By enforcing this scope, SLL can prevent such information leaks.</span></p>
<pre class="line-numbers"><code class="language-java">SecureLinkLauncher.launchFamilyActivity(intent, context); 
// launchFamilyActivity would make sure intent goes to the meta family apps</code></pre>
<p><span style="font-weight: 400;">In a typical Android environment, two scopes – internal and external – might seem sufficient for handling intents within the same app and between different apps. However, Meta&#8217;s ecosystem is unique, comprising multiple apps such as Facebook, Instagram, Messenger, WhatsApp, and their variants (e.g., WhatsApp Business). The complexity of inter-process communication between these apps demands more nuanced control over intent scoping. To address this need, SLL provides a more fine-grained approach to intent scoping, offering scopes that cater to specific use cases:</span></p>
<ul>
<li style="font-weight: 400;" aria-level="1"><b>Family scope</b><span style="font-weight: 400;">: Enables secure communication between Meta-owned apps, ensuring that intents are only sent from one Meta app to another.</span></li>
<li style="font-weight: 400;" aria-level="1"><b>Same-key scope</b><span style="font-weight: 400;">: Restricts intent sending to Meta apps signed with the same key (not all Meta apps are signed by the same key), providing an additional layer of security and trust.</span></li>
<li style="font-weight: 400;" aria-level="1"><b>Internal scope</b><span style="font-weight: 400;">: Restricts intent sending within the app itself.</span></li>
<li style="font-weight: 400;" aria-level="1"><b>Third-party scope</b><span style="font-weight: 400;">: Allows intents to be sent to third-party apps, while preventing them from being handled by Meta&#8217;s own apps.</span></li>
</ul>
<p><span style="font-weight: 400;">By leveraging these scopes, developers can ensure that sensitive data is shared securely and intentionally within the Meta ecosystem, while also protecting against unintended or malicious access. SLL’s fine-grained intent scoping capabilities, which are built upon the secure-by-default framework principles discussed above, empower developers to build more robust and secure applications that meet the unique demands of Meta&#8217;s complex ecosystem.</span></p>
<h2><span style="font-weight: 400;">Leveraging Generative AI To Deploy Secure-by-Default Frameworks at Scale</span></h2>
<p><span style="font-weight: 400;">Adopting these frameworks in a large codebase is non-trivial. The main complexity is choosing the correct scope, as that choice relies on information that is not readily available at existing call sites. While one could imagine a deterministic analysis attempting to infer the scope based on dataflows, that would be a large undertaking. Furthermore, it would likely have some precision-scalability trade-off. </span></p>
<p><span style="font-weight: 400;">Instead, we explored using Generative AI for this case. AI can read the surrounding code and attempt to infer the scope based on variable names and comments surrounding the call site. While this approach isn’t always perfect, it doesn’t need to be. It just needs to provide good enough guesses, such that code owners can one-click accept suggested patches. </span></p>
<p><span style="font-weight: 400;">If the patches are correct in most cases, this is a big timesaver that enables efficient adoption of the framework. This complements our </span><a href="https://engineering.fb.com/2025/04/29/ai-research/autopatchbench-benchmark-ai-powered-security-fixes/" target="_blank" rel="noopener"><span style="font-weight: 400;">recent work on AutoPatchBench</span></a><span style="font-weight: 400;">, a benchmark designed to evaluate AI-powered patch generators that leverage large language models (LLMs) to automatically recommend and apply security patches. Secure-by-default frameworks are a great example of the kinds of code modifications that an automatic patching system can apply to improve the security of a code base.</span></p>
<p><span style="font-weight: 400;">We’ve built a framework leveraging Llama as the core technology, which takes locations in the codebase that we want to migrate and suggests patches for code owners to accept:</span></p>
<p><img loading="lazy" decoding="async" class="alignnone size-full wp-image-23462" src="https://engineering.fb.com/wp-content/uploads/2025/12/Meta-Secure-By-Default-frameworks.png" alt="" width="1999" height="750" srcset="https://engineering.fb.com/wp-content/uploads/2025/12/Meta-Secure-By-Default-frameworks.png 1999w, https://engineering.fb.com/wp-content/uploads/2025/12/Meta-Secure-By-Default-frameworks.png?resize=916,344 916w, https://engineering.fb.com/wp-content/uploads/2025/12/Meta-Secure-By-Default-frameworks.png?resize=768,288 768w, https://engineering.fb.com/wp-content/uploads/2025/12/Meta-Secure-By-Default-frameworks.png?resize=1024,384 1024w, https://engineering.fb.com/wp-content/uploads/2025/12/Meta-Secure-By-Default-frameworks.png?resize=1536,576 1536w, https://engineering.fb.com/wp-content/uploads/2025/12/Meta-Secure-By-Default-frameworks.png?resize=96,36 96w, https://engineering.fb.com/wp-content/uploads/2025/12/Meta-Secure-By-Default-frameworks.png?resize=192,72 192w" sizes="auto, (max-width: 992px) 100vw, 62vw" /></p>
<h3><span style="font-weight: 400;">Prompt Creation</span></h3>
<p><span style="font-weight: 400;">The AI workflow starts with a call site we want to migrate including its file path and line number. The location is used to extract a code snippet from the code base. This means opening the file where the call site is present, copying 10-20 lines before and after the call site location, and pasting this into the prompt template that gives general instructions as to how to perform the migration. This description is very similar to what would be written as an onboarding guide to the framework for human engineers.</span></p>
<h3><span style="font-weight: 400;">Generative AI</span></h3>
<p><span style="font-weight: 400;">The prompt is then provided to a Llama model (llama4-maverick-17b-128e-instruct). The model is asked to output two things: the modified code snippet, where the call site has been migrated; and, optionally, some actions (like adding an import to the top of a file). The main purpose of actions is to work around the limitations of this approach where all code changes are not local and limited to the code snippet. Actions enable the model fix to reach outside the snippet for some limited, deterministic changes. This is useful for adding imports or dependencies, which are rarely local to the code snippet, but are necessary for the code to compile. The code snippet is then inserted back to the code base and any actions are applied. </span></p>
<h3><span style="font-weight: 400;">Validation</span></h3>
<p><span style="font-weight: 400;">Finally, we perform a series of validations on the code base. We run all of these with and without the AI changes and only report the difference:</span></p>
<ul>
<li style="font-weight: 400;" aria-level="1"><span style="font-weight: 400;">Lints</span><span style="font-weight: 400;">: We run the linters again to confirm the lint issue was fixed and no new lint errors were introduced by the changes.</span></li>
<li style="font-weight: 400;" aria-level="1"><span style="font-weight: 400;">Compiling</span><span style="font-weight: 400;">: We compile and run tests covering the targeted file. This is not intended to catch all bugs (we rely on continuous integration for that), but give the AI some early feedback on its changes (such as compile errors). </span></li>
<li style="font-weight: 400;" aria-level="1"><span style="font-weight: 400;">Formatting:</span><span style="font-weight: 400;"> The code is formatted to avoid formatting issues. We do not feed the formatting errors back to the AI.</span></li>
</ul>
<p><span style="font-weight: 400;">If any errors arise during the validation, their error messages are included in the prompt (along with the “fixed” code snippet) and the AI is asked to try again. We repeat this loop five times and give up if no successful fix is created. If the validation succeeds, we submit a patch for human review.</span></p>
<h2><span style="font-weight: 400;">Thoughtful Framework Design Meets Intelligent Automation</span></h2>
<p><span style="font-weight: 400;">By adhering to core design principles such as providing an API that closely resembles existing OS patterns, relying solely on public and stable OS APIs, and designing frameworks that cover broad user bases rather than niche use cases, developers can create robust, secure-by-default features that integrate seamlessly into existing codebases.</span><span style="font-weight: 400;"><br />
</span><span style="font-weight: 400;"><br />
</span><span style="font-weight: 400;">These same design principles help us leverage AI for smoothly adopting frameworks at scale. While there are still challenges around the accuracy of generated code – for example, the AI choosing the incorrect scope, using incorrect syntax, etc., the internal feedback loop design allows the LLM to automatically move past easily solvable problems without human intervention, increasing scalability and reducing developer frustration.</span></p>
<p><span style="font-weight: 400;">Internally, this project helped prove that AI could be impactful for adopting security frameworks across a diverse codebase in a way that is minimally disruptive to our developers. There are now a variety of projects tackling similar problems across a variety of codebases and languages – including C/++ – using diverse models and validation techniques. We expect this trend to continue and accelerate in 2026 as developers become more comfortable with state of the art AI tools and the quality of code that they are capable of producing.</span></p>
<p><span style="font-weight: 400;">As our codebase grows and security threats become more sophisticated, the combination of thoughtful framework design and intelligent automation will be essential to protecting user data and maintaining trust at scale.</span></p>
<p>The post <a rel="nofollow" href="https://engineering.fb.com/2025/12/15/android/how-ai-transforming-secure-by-default-mobile-frameworks-adoption/">How AI Is Transforming the Adoption of Secure-by-Default Mobile Frameworks</a> appeared first on <a rel="nofollow" href="https://engineering.fb.com">Engineering at Meta</a>.</p>
]]></content:encoded>
					
		
		
		<post-id xmlns="com-wordpress:feed-additions:1">23460</post-id>	</item>
		<item>
		<title>Zoomer: Powering AI Performance at Meta&#8217;s Scale Through Intelligent Debugging and Optimization</title>
		<link>https://engineering.fb.com/2025/11/21/data-infrastructure/zoomer-powering-ai-performance-meta-intelligent-debugging-optimization/</link>
		
		<dc:creator><![CDATA[]]></dc:creator>
		<pubDate>Fri, 21 Nov 2025 21:00:15 +0000</pubDate>
				<category><![CDATA[Data Center Engineering]]></category>
		<category><![CDATA[Data Infrastructure]]></category>
		<category><![CDATA[ML Applications]]></category>
		<guid isPermaLink="false">https://engineering.fb.com/?p=23436</guid>

					<description><![CDATA[<p>We’re introducing Zoomer, Meta’s comprehensive, automated debugging and optimization platform for AI.  Zoomer works across all of our training and inference workloads at Meta and provides deep performance insights that enable energy savings, workflow acceleration, and efficiency gains in our AI infrastructure.  Zoomer has delivered training time reductions, and significant QPS improvements, making it the [...]</p>
<p><a class="btn btn-secondary understrap-read-more-link" href="https://engineering.fb.com/2025/11/21/data-infrastructure/zoomer-powering-ai-performance-meta-intelligent-debugging-optimization/">Read More...</a></p>
<p>The post <a rel="nofollow" href="https://engineering.fb.com/2025/11/21/data-infrastructure/zoomer-powering-ai-performance-meta-intelligent-debugging-optimization/">Zoomer: Powering AI Performance at Meta&#8217;s Scale Through Intelligent Debugging and Optimization</a> appeared first on <a rel="nofollow" href="https://engineering.fb.com">Engineering at Meta</a>.</p>
]]></description>
										<content:encoded><![CDATA[<ul>
<li style="font-weight: 400;" aria-level="1"><span style="font-weight: 400;">We’re introducing Zoomer, Meta’s comprehensive, automated debugging and optimization platform for AI. </span></li>
<li style="font-weight: 400;" aria-level="1"><span style="font-weight: 400;">Zoomer works across all of our training and inference workloads at Meta and provides deep performance insights that enable energy savings, workflow acceleration, and efficiency gains in our AI infrastructure. </span></li>
<li style="font-weight: 400;" aria-level="1"><span style="font-weight: 400;">Zoomer has delivered training time reductions, and significant QPS improvements, making it the de-facto tool for AI performance optimization across Meta&#8217;s entire AI infrastructure.</span></li>
</ul>
<p><span style="font-weight: 400;">At the scale that Meta’s AI infrastructure operates, poor performance debugging can lead to massive energy inefficiency, increased operational costs, and suboptimal hardware utilization across hundreds of thousands of GPUs. The fundamental challenge is achieving maximum computational efficiency while minimizing waste. Every percentage point of utilization improvement translates to significant capacity gains that can be redirected to innovation and growth.</span></p>
<p><span style="font-weight: 400;">Zoomer is Meta&#8217;s automated, one-stop-shop platform for performance profiling, debugging, analysis, and optimization of AI training and inference workloads. Since its inception, Zoomer has become the de-facto tool across Meta for GPU workload optimization, </span><span style="font-weight: 400;">generating tens of thousands of profiling reports daily</span><span style="font-weight: 400;"> for teams across all of our apps. </span></p>
<h2><span style="font-weight: 400;">Why Debugging Performance Matters</span></h2>
<p><a href="https://engineering.fb.com/2025/09/29/data-infrastructure/metas-infrastructure-evolution-and-the-advent-of-ai/" target="_blank" rel="noopener"><span style="font-weight: 400;">Our AI infrastructure</span></a><span style="font-weight: 400;"> supports </span><a href="https://engineering.fb.com/2024/06/12/production-engineering/maintaining-large-scale-ai-capacity-meta/" target="_blank" rel="noopener"><span style="font-weight: 400;">large-scale and advanced workloads across a global fleet of GPU clusters, continually evolving to meet the growing scale and complexity of generative AI</span></a><span style="font-weight: 400;">.</span></p>
<p><span style="font-weight: 400;">At the training level it supports a diverse range of workloads, including powering models for </span><a href="https://engineering.fb.com/2025/11/10/ml-applications/metas-generative-ads-model-gem-the-central-brain-accelerating-ads-recommendation-ai-innovation/" target="_blank" rel="noopener"><span style="font-weight: 400;">ads ranking</span></a><span style="font-weight: 400;">, </span><a href="https://engineering.fb.com/2025/05/21/production-engineering/journey-to-1000-models-scaling-instagrams-recommendation-system/" target="_blank" rel="noopener"><span style="font-weight: 400;">content recommendations</span></a><span style="font-weight: 400;">, and </span><a href="https://engineering.fb.com/2025/05/20/web/metas-full-stack-hhvm-optimizations-for-genai/" target="_blank" rel="noopener"><span style="font-weight: 400;">GenAI features</span></a><span style="font-weight: 400;">.  </span></p>
<p><span style="font-weight: 400;">At the inference level, we serve</span><span style="font-weight: 400;"> hundreds of trillions of AI model executions per day.</span></p>
<p><span style="font-weight: 400;">Operating at this scale means putting a high priority on eliminating GPU underutilization. Training inefficiencies delay model iterations and product launches, while inference bottlenecks limit our ability to serve user requests at scale. Removing resource waste and accelerating workflows helps us train larger models more efficiently, serve more users, and reduce our environmental footprint.</span></p>
<h2><span style="font-weight: 400;">AI Performance Optimization Using Zoomer</span></h2>
<p><span style="font-weight: 400;">Zoomer is an automated debugging and optimization platform that works across all of our AI model types (ads recommendations, GenAI, computer vision, etc.) and both training and inference paradigms, providing deep performance insights that enable energy savings, workflow acceleration, and efficiency gains.  </span></p>
<p><span style="font-weight: 400;">Zoomer&#8217;s architecture consists of three essential layers that work together to deliver comprehensive AI performance insights: </span></p>
<h3><span style="font-weight: 400;">Infrastructure and Platform Layer</span></h3>
<p><span style="font-weight: 400;">The foundation provides the enterprise-grade scalability and reliability needed to profile workloads across Meta&#8217;s massive infrastructure. This includes distributed storage systems using </span><a href="https://www.youtube.com/watch?v=tddb-zbmnTo"><span style="font-weight: 400;">Manifold</span></a><span style="font-weight: 400;"> (Meta’s blob storage platform) for trace data, fault-tolerant processing pipelines that handle huge trace files, and low-latency data collection with automatic profiling triggers across thousands of hosts simultaneously. The platform maintains high availability and scale through redundant processing workers and can handle huge numbers of profiling requests during peak usage periods.</span></p>
<h3><span style="font-weight: 400;">Analytics and Insights Engine</span></h3>
<p><span style="font-weight: 400;">The core intelligence layer delivers deep analytical capabilities through multiple specialized analyzers. This includes: GPU trace analysis via Kineto integration and NVIDIA DCGM, CPU profiling through </span><a href="https://engineering.fb.com/2025/01/21/production-engineering/strobelight-a-profiling-service-built-on-open-source-technology/" target="_blank" rel="noopener"><span style="font-weight: 400;">StrobeLight</span></a><span style="font-weight: 400;"> integration, host-level metrics analysis via </span><a href="https://developers.facebook.com/blog/post/2022/11/16/dynolog-open-source-system-observability/" target="_blank" rel="noopener"><span style="font-weight: 400;">dyno telemetry</span></a><span style="font-weight: 400;">, communication pattern analysis for distributed training, straggler detection across distributed ranks, memory allocation profiling (including GPU memory snooping), request/response profiling for inference workloads, and much more. The engine automatically detects performance anti-patterns and also provides actionable recommendations.</span></p>
<h3><span style="font-weight: 400;">Visualization and User Interface Layer</span></h3>
<p><span style="font-weight: 400;">The presentation layer transforms complex performance data into intuitive, actionable insights. This includes interactive timeline visualizations showing GPU activity across thousands of ranks, multi-iteration analysis for long-running training workloads, drill-down dashboards with percentile analysis across devices, trace data visualization integrated with Perfetto for kernel-level inspection, heat map visualizations for identifying outliers across GPU deployments, and automated insight summaries that highlight critical bottlenecks and optimization opportunities.</span></p>
<figure id="attachment_23438" aria-describedby="caption-attachment-23438" style="width: 1928px" class="wp-caption alignnone"><img loading="lazy" decoding="async" class="size-full wp-image-23438" src="https://engineering.fb.com/wp-content/uploads/2025/11/Meta-Zoomer-architecture.png" alt="" width="1928" height="1508" srcset="https://engineering.fb.com/wp-content/uploads/2025/11/Meta-Zoomer-architecture.png 1928w, https://engineering.fb.com/wp-content/uploads/2025/11/Meta-Zoomer-architecture.png?resize=916,716 916w, https://engineering.fb.com/wp-content/uploads/2025/11/Meta-Zoomer-architecture.png?resize=768,601 768w, https://engineering.fb.com/wp-content/uploads/2025/11/Meta-Zoomer-architecture.png?resize=1024,801 1024w, https://engineering.fb.com/wp-content/uploads/2025/11/Meta-Zoomer-architecture.png?resize=1536,1201 1536w, https://engineering.fb.com/wp-content/uploads/2025/11/Meta-Zoomer-architecture.png?resize=96,75 96w, https://engineering.fb.com/wp-content/uploads/2025/11/Meta-Zoomer-architecture.png?resize=192,150 192w" sizes="auto, (max-width: 992px) 100vw, 62vw" /><figcaption id="caption-attachment-23438" class="wp-caption-text">The three essential layers of Zoomer&#8217;s architecture.</figcaption></figure>
<h2><span style="font-weight: 400;">How Zoomer Profiling Works: From Trigger to Insights</span></h2>
<p><span style="font-weight: 400;">Understanding how Zoomer conducts a complete performance analysis provides insight into its sophisticated approach to AI workload optimization.</span></p>
<h3><span style="font-weight: 400;">Profiling Trigger Mechanisms</span></h3>
<p><span style="font-weight: 400;">Zoomer operates through both automatic and on-demand profiling strategies tailored to different workload types. For training workloads, which involve multiple iterations and can run for days or weeks, Zoomer automatically triggers profiling around iteration 550-555 to capture stable-state performance while avoiding startup noise. For inference workloads, profiling can be triggered on-demand for immediate debugging or through integration with automated load testing and benchmarking systems for continuous monitoring.</span></p>
<h3><span style="font-weight: 400;">Comprehensive Data Capture</span></h3>
<p><span style="font-weight: 400;">During each profiling session, Zoomer simultaneously collects multiple data streams to build a holistic performance picture: </span></p>
<ul>
<li style="font-weight: 400;" aria-level="1"><b>GPU Performance Metrics</b><span style="font-weight: 400;">: SM utilization, GPU memory utilization, GPU busy time, memory bandwidth, Tensor Core utilization, power consumption, clock frequencies, and power consumption data via DCGM integration.</span></li>
<li style="font-weight: 400;" aria-level="1"><b>Detailed Execution Traces</b><span style="font-weight: 400;">: Kernel-level GPU operations, memory transfers, CUDA API calls, and communication collectives via <a href="https://docs.pytorch.org/tutorials/recipes/recipes/profiler_recipe.html" target="_blank" rel="noopener">PyTorch Profiler</a> and <a href="https://github.com/pytorch/kineto" target="_blank" rel="noopener">Kineto</a>.</span></li>
<li style="font-weight: 400;" aria-level="1"><b>Host-Level Performance Data</b><span style="font-weight: 400;">: CPU utilization, memory usage, network I/O, storage access patterns, and system-level bottlenecks via dyno telemetry.</span></li>
<li style="font-weight: 400;" aria-level="1"><b>Application-Level Annotations</b><span style="font-weight: 400;">: Training iterations, forward/backward passes, optimizer steps, data loading phases, and custom user annotations.</span></li>
<li style="font-weight: 400;" aria-level="1"><b>Inference-Specific Data</b><span style="font-weight: 400;">: Rate of inference requests, server latency, active requests, GPU memory allocation patterns, request latency breakdowns via Strobelight’s Crochet profiler, serving parameter analysis, and thrift request-level profiling.</span></li>
<li style="font-weight: 400;" aria-level="1"><b>Communication Analysis</b><span style="font-weight: 400;">: NCCL collective operations, inter-node communication patterns, and network utilization for distributed workloads</span></li>
</ul>
<h3><span style="font-weight: 400;">Distributed Analysis Pipeline</span></h3>
<p><span style="font-weight: 400;">Raw profiling data flows through sophisticated processing systems that deliver multiple types of automated analysis including:</span></p>
<ul>
<li style="font-weight: 400;" aria-level="1"><b>Straggler Detection</b><span style="font-weight: 400;">: Identifies slow ranks in distributed training through comparative analysis of execution timelines and communication patterns.</span></li>
<li style="font-weight: 400;" aria-level="1"><b>Bottleneck Analysis</b><span style="font-weight: 400;">: Automatically detects CPU-bound, GPU-bound, memory-bound, or communication-bound performance issues.</span></li>
<li style="font-weight: 400;" aria-level="1"><b>Critical Path Analysis</b><span style="font-weight: 400;">: Systematically identifies the longest execution paths to focus optimization efforts on highest-impact opportunities.</span></li>
<li style="font-weight: 400;" aria-level="1"><b>Anti-Pattern Detection</b><span style="font-weight: 400;">: Rule-based systems that identify common efficiency issues and generate specific recommendations.</span></li>
<li style="font-weight: 400;" aria-level="1"><b>Parallelism Analysis</b><span style="font-weight: 400;">: Deep understanding of tensor, pipeline, data, and expert parallelism interactions for large-scale distributed training.</span></li>
<li style="font-weight: 400;" aria-level="1"><b>Memory Analysis</b><span style="font-weight: 400;">: Comprehensive analysis of GPU memory usage patterns, allocation tracking, and leak detection.</span></li>
<li style="font-weight: 400;" aria-level="1"><b>Load Imbalance Analysis</b><span style="font-weight: 400;">: Detects workload distribution issues across distributed ranks and recommendations for optimization.</span></li>
</ul>
<h3><span style="font-weight: 400;">Multi-Format Output Generation</span></h3>
<p><span style="font-weight: 400;">Results are presented through multiple interfaces tailored to different user needs: interactive timeline visualizations showing activity across all ranks and hosts, comprehensive metrics dashboards with drill-down capabilities and percentile analysis, trace viewers integrated with Perfetto for detailed kernel inspection, automated insights summaries highlighting key bottlenecks and recommendations, and actionable notebooks that users can clone to rerun jobs with suggested optimizations.</span></p>
<h3><span style="font-weight: 400;">Specialized Workload Support</span></h3>
<p><span style="font-weight: 400;">For massive distributed training for specialized workloads, like GenAI, Zoomer contains a purpose-built platform supporting LLM workloads that offers specialized capabilities including GPU efficiency heat maps and N-dimensional parallelism visualization. For inference, specialized analysis covers everything from single GPU models, soon expanding to massive distributed inference across thousands of servers.</span></p>
<h2><span style="font-weight: 400;">A Glimpse Into Advanced Zoomer Capabilities</span></h2>
<p><span style="font-weight: 400;">Zoomer offers an extensive suite of advanced capabilities designed for different AI workload types and scales. While a comprehensive overview of all features would require multiple blog posts, here&#8217;s a glimpse at some of the most compelling capabilities that demonstrate Zoomer&#8217;s depth:</span></p>
<p><b>Training Powerhouse Features</b><span style="font-weight: 400;">:</span></p>
<ul>
<li style="font-weight: 400;" aria-level="1"><b>Straggler Analysis</b><span style="font-weight: 400;">: Helps identify ranks in distributed training jobs that are significantly slower than others, causing overall job delays due to synchronization bottlenecks. Zoomer provides information that helps diagnose root causes like sharding imbalance or hardware issues.</span></li>
<li style="font-weight: 400;" aria-level="1"><b>Critical Path Analysis</b><span style="font-weight: 400;">: Identification of the longest execution paths in PyTorch applications, enabling accurate performance improvement projections</span><span style="font-weight: 400;">. </span></li>
<li style="font-weight: 400;" aria-level="1"><b>Advanced Trace Manipulation</b><span style="font-weight: 400;">: Sophisticated tools for compression, filtering, combination, and segmentation of massive trace files (2GB+ per rank), enabling analysis of previously impossible-to-process large-scale training jobs</span></li>
</ul>
<p><b>Inference Excellence Features</b><span style="font-weight: 400;">:</span></p>
<ul>
<li style="font-weight: 400;" aria-level="1"><b>Single-Click QPS Optimization</b><span style="font-weight: 400;">: A workflow that identifies </span><span style="font-weight: 400;">bottlenecks and</span><span style="font-weight: 400;"> triggers automated load tests with one click, reducing optimization time </span><span style="font-weight: 400;">while delivering QPS improvements of +2% to +50% across different models, depending on model characteristics.</span><span style="font-weight: 400;"> </span></li>
<li style="font-weight: 400;" aria-level="1"><b>Request-Level Deep Dive</b><span style="font-weight: 400;">: Integration with Crochet profiler provides </span><a href="https://engineering.fb.com/2014/02/20/open-source/under-the-hood-building-and-open-sourcing-fbthrift/"><span style="font-weight: 400;">Thrift </span></a><span style="font-weight: 400;">request-level analysis, enabling identification of queue time bottlenecks and serving inefficiencies that traditional metrics miss.</span></li>
<li style="font-weight: 400;" aria-level="1"><b>Realtime Memory Profiling</b><span style="font-weight: 400;">: GPU memory allocation tracking, providing live insights into memory leaks, allocation patterns, and optimization opportunities.</span></li>
</ul>
<p><b>GenAI Specialized Features</b><span style="font-weight: 400;">:</span></p>
<ul>
<li style="font-weight: 400;" aria-level="1"><b>LLM Zoomer for Scale</b><span style="font-weight: 400;">: A purpose-built platform supporting 100k+ GPU workloads with N-dimensional parallelism visualization, GPU efficiency heat maps across thousands of devices, and specialized analysis for tensor, pipeline, data, and expert parallelism interactions.</span></li>
<li style="font-weight: 400;" aria-level="1"><b>Post-Training Workflow Support</b><span style="font-weight: 400;">: Enhanced capabilities for GenAI post-training tasks including SFT, DPO, and ARPG workflows with generator and trainer profiling separation.</span></li>
</ul>
<p><b>Universal Intelligence Features</b><span style="font-weight: 400;">:</span></p>
<ul>
<li style="font-weight: 400;" aria-level="1"><b>Holistic Trace Analysis (HTA)</b><span style="font-weight: 400;">: Advanced framework for diagnosing distributed training bottlenecks across communication overhead, workload imbalance, and kernel inefficiencies, with automatic load balancing recommendations.</span></li>
<li style="font-weight: 400;" aria-level="1"><b>Zoomer Actionable Recommendations Engine (Zoomer AR)</b><span style="font-weight: 400;">: Automated detection of efficiency anti-patterns with machine learning-driven recommendation systems that generate auto-fix diffs, optimization notebooks, and one-click job re-launches with suggested improvements.</span></li>
<li style="font-weight: 400;" aria-level="1"><b>Multi-Hardware Profiling</b><span style="font-weight: 400;">: Native support across NVIDIA GPUs, AMD MI300X, </span><a href="https://ai.meta.com/blog/next-generation-meta-training-inference-accelerator-AI-MTIA/"><span style="font-weight: 400;">MTIA</span></a><span style="font-weight: 400;">, and CPU-only workloads with consistent analysis and optimization recommendations regardless of hardware platform.</span></li>
</ul>
<h2><span style="font-weight: 400;">Zoomer’s Optimization Impact: From Debugging to Energy Efficiency</span></h2>
<p><span style="font-weight: 400;">Performance debugging with Zoomer creates a cascading effect that transforms low-level optimizations into massive efficiency gains. </span></p>
<p><span style="font-weight: 400;">The optimization pathway flows from: identifying bottlenecks → improving key metrics → accelerating workflows → reducing resource consumption → saving energy and costs.</span></p>
<h3><span style="font-weight: 400;">Zoomer’s Training Optimization Pipeline</span></h3>
<p><span style="font-weight: 400;">Zoomer&#8217;s training analysis identifies bottlenecks in GPU utilization, memory bandwidth, and communication patterns. </span></p>
<p><b>Example of Training Efficiency Wins: </b></p>
<ul>
<li style="font-weight: 400;" aria-level="1"><b>Algorithmic Optimizations</b><span style="font-weight: 400;">: We delivered </span><b>power savings</b><span style="font-weight: 400;"> through systematic efficiency improvements across the training fleet, by fixing reliability issues for low efficiency jobs.</span></li>
<li style="font-weight: 400;" aria-level="1"><b>Training Time Reduction Success</b><span style="font-weight: 400;">: In 2024, we observed a 75% training time reduction for Ads relevance models, leading to 78% reduction in power consumption.</span></li>
<li style="font-weight: 400;" aria-level="1"><b>Memory Optimizations</b><span style="font-weight: 400;">: One-line code changes for performance issues due to inefficient memory copy identified by Zoomer, delivered </span><b>20% QPS improvements</b><span style="font-weight: 400;"> with minimal engineering effort. </span></li>
</ul>
<h3><span style="font-weight: 400;">Inference Optimization Pipeline:</span></h3>
<p><span style="font-weight: 400;">Inference debugging focuses on latency reduction, throughput optimization, and serving efficiency. Zoomer identifies opportunities in kernel execution, memory access patterns, and serving parameter tuning to maximize requests per GPU.</span></p>
<p><b>Inference Efficiency Wins:</b></p>
<ul>
<li style="font-weight: 400;" aria-level="1"><b>GPU and CPU Serving parameters Improvements</b><span style="font-weight: 400;">: Automated GPU and CPU bottleneck identification and parameter tuning, </span><span style="font-weight: 400;">leading to 10% to 45% reduction in power consumption</span><span style="font-weight: 400;">.</span></li>
<li style="font-weight: 400;" aria-level="1"><b>QPS Optimization</b><span style="font-weight: 400;">: GPU trace analysis used to boost serving QPS and optimize serving capacity.</span></li>
</ul>
<h3><span style="font-weight: 400;">Zoomer’s GenAI and Large-Scale Impact</span></h3>
<p><span style="font-weight: 400;">For massive distributed workloads, even small optimizations compound dramatically. </span><b>32k GPU benchmark optimizations</b><span style="font-weight: 400;"> achieved 30% speedups through broadcast issue resolution, while </span><b>64k GPU configurations</b><span style="font-weight: 400;"> delivered 25% speedups in just one day of optimization.</span></p>
<h2><span style="font-weight: 400;">The Future of AI Performance Debugging</span></h2>
<p><span style="font-weight: 400;">As AI workloads expand in size and complexity, Zoomer is advancing to meet new challenges focused on several innovation fronts: broadening unified performance insights across heterogeneous hardware (including MTIA and next-gen accelerators), building advanced analyzers for proactive optimization, enabling inference performance tuning through serving param optimization, and democratizing optimization with automated, intuitive tools for all engineers. As Meta&#8217;s AI infrastructure continues its rapid growth, Zoomer plays an important role in helping us innovate efficiently and sustainably.</span></p>
<h2><i><span style="font-weight: 400;">Acknowledgments</span></i></h2>
<p><em><span style="font-weight: 400;">I would like to thank my entire team and our partner teams — Ganga Barani Balakrishnan, Qingyun Bian, Harshavardhan Reddy Bommireddy, Haibo Chen, Anubhav Chaturvedi, Wenbo Cui, Jon Dyer, Fatemeh Elyasi, Hrishikesh Gadre, Wenqin Huangfu, Arda Icmez, Amit Katti, Karthik Kambatla, Prakash KL, Raymond Li, Phillip Liu, Ya Liu, Majid Mashhadi, Abhishek Maroo, Paul Meng, Hassan Mousavi, Gil Nahmias, Manali Naik, Jackie Nguyen, Brian Mohammed Catraguna, Shiva Ramaswami, Shyam Sundar Chandrasekaran, Daylon Srinivasan, Sudhansu Singh, Michael Au-Yeung, Mengtian Xu, Zhiqiang Zang, Charles Yoon, John Wu, Uttam Thakore — for their dedication, technical excellence, and collaborative spirit in building Zoomer into the comprehensive AI profiling platform it is today.</span></em></p>
<p><em><span style="font-weight: 400;">I would also like to thank past team members and partners including Valentin Andrei, Brian Mohammed Catraguna, Patrick Lu, Majid Mashhadi, Chen Pekker, Wei Sun, Sreen Tallam, Chenguang Zhu — for laying the foundational vision and early technical contributions that made Zoomer&#8217;s evolution possible.</span></em></p>
<p>The post <a rel="nofollow" href="https://engineering.fb.com/2025/11/21/data-infrastructure/zoomer-powering-ai-performance-meta-intelligent-debugging-optimization/">Zoomer: Powering AI Performance at Meta&#8217;s Scale Through Intelligent Debugging and Optimization</a> appeared first on <a rel="nofollow" href="https://engineering.fb.com">Engineering at Meta</a>.</p>
]]></content:encoded>
					
		
		
		<post-id xmlns="com-wordpress:feed-additions:1">23436</post-id>	</item>
		<item>
		<title>Key Transparency Comes to Messenger</title>
		<link>https://engineering.fb.com/2025/11/20/security/key-transparency-comes-to-messenger/</link>
		
		<dc:creator><![CDATA[]]></dc:creator>
		<pubDate>Thu, 20 Nov 2025 17:00:21 +0000</pubDate>
				<category><![CDATA[Security & Privacy]]></category>
		<guid isPermaLink="false">https://engineering.fb.com/?p=23382</guid>

					<description><![CDATA[<p>We&#8217;re excited to share another advancement in the security of your conversations on Messenger: the launch of key transparency verification for end-to-end encrypted chats.  This new feature enables an additional level of assurance that only you — and the people you&#8217;re communicating with — can see or listen to what is sent, and that no [...]</p>
<p><a class="btn btn-secondary understrap-read-more-link" href="https://engineering.fb.com/2025/11/20/security/key-transparency-comes-to-messenger/">Read More...</a></p>
<p>The post <a rel="nofollow" href="https://engineering.fb.com/2025/11/20/security/key-transparency-comes-to-messenger/">Key Transparency Comes to Messenger</a> appeared first on <a rel="nofollow" href="https://engineering.fb.com">Engineering at Meta</a>.</p>
]]></description>
										<content:encoded><![CDATA[<ul>
<li style="font-weight: 400;" aria-level="1"><span style="font-weight: 400;">We&#8217;re excited to share another advancement in the security of your conversations on Messenger: the launch of key transparency verification for end-to-end encrypted chats. </span></li>
<li style="font-weight: 400;" aria-level="1"><span style="font-weight: 400;">This new feature enables an additional level of assurance that only you — and the people you&#8217;re communicating with — can see or listen to what is sent, and that no one else, not even Meta, can do so.</span></li>
</ul>
<p><a href="https://www.facebook.com/help/messenger-app/1084673321594605" target="_blank" rel="noopener"><span style="font-weight: 400;">End-to-end encryption on Messenger</span></a><span style="font-weight: 400;"> already ensures that the content of your direct messages and calls are protected from the moment they leave your device to the moment they reach the receiver’s device. </span><span style="font-weight: 400;">As part of our end-to-end encrypted chat platform, we believe it&#8217;s also important that anyone can verify that the public keys (used by the sender’s device for encrypting each message) belong to the intended recipients and haven&#8217;t been tampered with.</span></p>
<p><span style="font-weight: 400;">This launch builds upon the valuable work and experiences shared by others in the industry. </span><a href="https://engineering.fb.com/2023/04/13/security/whatsapp-key-transparency/" target="_blank" rel="noopener"><span style="font-weight: 400;">WhatsApp&#8217;s implementation of key transparency</span></a><span style="font-weight: 400;"> in 2023 demonstrated the feasibility of this technology for large-scale encrypted messaging. We&#8217;ve extended these pioneering efforts in our Messenger implementation to deliver a robust and reliable solution with similar security properties.</span></p>
<h2>What Is Key Transparency?</h2>
<p><span style="font-weight: 400;">Key transparency provides messaging users with a verifiable and auditable record of public keys. It allows them to confirm that their conversations are indeed encrypted with the correct keys for their contacts, and that these keys haven&#8217;t been maliciously swapped by a compromised server. This means you can be more confident that your messages are only accessible to the people you intend to communicate with.</span></p>
<p><img loading="lazy" decoding="async" class="alignnone size-full wp-image-23383" src="https://engineering.fb.com/wp-content/uploads/2025/11/Meta-Key-Transparency-Messenger.png" alt="" width="1999" height="1646" srcset="https://engineering.fb.com/wp-content/uploads/2025/11/Meta-Key-Transparency-Messenger.png 1999w, https://engineering.fb.com/wp-content/uploads/2025/11/Meta-Key-Transparency-Messenger.png?resize=916,754 916w, https://engineering.fb.com/wp-content/uploads/2025/11/Meta-Key-Transparency-Messenger.png?resize=768,632 768w, https://engineering.fb.com/wp-content/uploads/2025/11/Meta-Key-Transparency-Messenger.png?resize=1024,843 1024w, https://engineering.fb.com/wp-content/uploads/2025/11/Meta-Key-Transparency-Messenger.png?resize=1536,1265 1536w, https://engineering.fb.com/wp-content/uploads/2025/11/Meta-Key-Transparency-Messenger.png?resize=96,79 96w, https://engineering.fb.com/wp-content/uploads/2025/11/Meta-Key-Transparency-Messenger.png?resize=192,158 192w" sizes="auto, (max-width: 992px) 100vw, 62vw" /></p>
<p><span style="font-weight: 400;">You can already </span><a href="https://www.facebook.com/help/messenger-app/147596532316790/Check+your+keys+for+end-to-end+encrypted+chats+on+Messenger" target="_blank" rel="noopener"><span style="font-weight: 400;">check your keys for end-to-end encrypted chats on Messenger</span></a><span style="font-weight: 400;">, but this can be cumbersome for people who have logged in to Messenger on multiple devices, each of which has its own key. Moreover, these keys change when new devices are added or are re-registered, which necessitates another check of the key every time this happens. </span></p>
<p><span style="font-weight: 400;">To address this, we’ve added a new security feature, based on key transparency, that allows users to verify these keys without having to compare them manually with their contacts. Of course, anyone who wishes to continue manually verifying their keys is free to do so.</span></p>
<h2>How We’re Handling Messenger Keys at Scale</h2>
<p><span style="font-weight: 400;">Our key transparency implementation leverages the </span><a href="https://github.com/facebook/akd" target="_blank" rel="noopener"><span style="font-weight: 400;">Auditable Key Directory (AKD) library</span></a><span style="font-weight: 400;">, mirroring the system already in place for WhatsApp. This system allows Meta to securely distribute and verify users&#8217; public keys. To further enhance the security of this process, we use </span><a href="https://developers.cloudflare.com/key-transparency/" target="_blank" rel="noopener"><span style="font-weight: 400;">Cloudflare’s key transparency auditor</span></a><span style="font-weight: 400;"> to provide an additional layer of verification, ensuring that the distribution of keys is transparent and verifiable by anyone. Cloudflare’s auditor maintains a live log of the latest entries on the </span><a href="https://dash.key-transparency.cloudflare.com/" target="_blank" rel="noopener"><span style="font-weight: 400;">Key Transparency Dashboard</span></a><span style="font-weight: 400;">, for both the WhatsApp and Messenger directories.</span></p>
<p><span style="font-weight: 400;">Implementing key transparency on the scale of Messenger presented unique engineering challenges. One significant factor was the sheer volume and frequency of key updates. Messenger indexes keys for each and every device someone has logged in on, which means that a single user often has multiple, frequently-changing keys associated with their account.</span></p>
<p><span style="font-weight: 400;">This increased complexity leads to a much higher frequency of key updates being sequenced into our key transparency directory. Currently, we&#8217;re observing an epoch frequency of approximately 2 minutes per publish, with hundreds of thousands of new keys added in each epoch. Since we began indexing, our database has already grown to billions of key entries. We’ve implemented a number of advancements in our infrastructure and libraries to help manage this massive and constantly growing dataset, while ensuring high availability and real-time verification:</span></p>
<p><span style="font-weight: 400;">We improved the algorithmic efficiency of the existing key lookup and verification operations in the AKD library by optimizing for smaller proof sizes, even as the number of updates (versions) for a single key grows. Previously, these proofs grew linearly with the height of the transparency tree, which was still difficult to manage given the number of nodes in the tree.</span></p>
<p><span style="font-weight: 400;">We also updated our existing infrastructure to be more resilient to temporary outages and improved the process for recovering from long delays in key sequencing. These improvements were adapted from lessons learned from running WhatsApp’s key transparency log for the past two years.</span></p>
<p><span style="font-weight: 400;">With key transparency now live on Messenger, users will have the ability to automatically verify the authenticity of their contacts&#8217; encryption keys for one-on-one chats. </span><span style="font-weight: 400;">This represents another step forward in our ongoing investment in providing</span> <span style="font-weight: 400;">a secure and private</span> <span style="font-weight: 400;">service</span><b>. </b></p>
<p><span style="font-weight: 400;">Stay tuned for more updates as we continue to enhance the security and privacy of end-to-end encryption in Messenger.</span></p>
<p>The post <a rel="nofollow" href="https://engineering.fb.com/2025/11/20/security/key-transparency-comes-to-messenger/">Key Transparency Comes to Messenger</a> appeared first on <a rel="nofollow" href="https://engineering.fb.com">Engineering at Meta</a>.</p>
]]></content:encoded>
					
		
		
		<post-id xmlns="com-wordpress:feed-additions:1">23382</post-id>	</item>
		<item>
		<title>Efficient Optimization With Ax, an Open Platform for Adaptive Experimentation</title>
		<link>https://engineering.fb.com/2025/11/18/open-source/efficient-optimization-ax-open-platform-adaptive-experimentation/</link>
		
		<dc:creator><![CDATA[]]></dc:creator>
		<pubDate>Tue, 18 Nov 2025 17:00:24 +0000</pubDate>
				<category><![CDATA[Open Source]]></category>
		<guid isPermaLink="false">https://engineering.fb.com/?p=23315</guid>

					<description><![CDATA[<p>We’ve released Ax 1.0, an open-source platform that uses machine learning to automatically guide complex, resource-intensive experimentation. Ax is used at scale across Meta to improve AI models, tune production infrastructure, and accelerate advances in ML and even hardware design. Our accompanying paper, &#8220;Ax: A Platform for Adaptive Experimentation&#8221; explains Ax’s architecture, methodology, and how it [...]</p>
<p><a class="btn btn-secondary understrap-read-more-link" href="https://engineering.fb.com/2025/11/18/open-source/efficient-optimization-ax-open-platform-adaptive-experimentation/">Read More...</a></p>
<p>The post <a rel="nofollow" href="https://engineering.fb.com/2025/11/18/open-source/efficient-optimization-ax-open-platform-adaptive-experimentation/">Efficient Optimization With Ax, an Open Platform for Adaptive Experimentation</a> appeared first on <a rel="nofollow" href="https://engineering.fb.com">Engineering at Meta</a>.</p>
]]></description>
										<content:encoded><![CDATA[<ul>
<li class="xdj266r x14z9mp xat24cr x1lziwak">We’ve released <b>Ax 1.0</b>, an open-source platform that uses machine learning to automatically guide complex, resource-intensive experimentation.</li>
<li class="xdj266r x14z9mp xat24cr x1lziwak">Ax is used at scale across Meta to improve AI models, tune production infrastructure, and accelerate advances in ML and even hardware design.</li>
<li class="xdj266r x14z9mp xat24cr x1lziwak">Our accompanying paper, <span style="font-weight: 400;">&#8220;</span><a style="background-color: #ffffff; font-size: 1rem;" href="https://openreview.net/forum?id=U1f6wHtG1g&amp;ref=engineeringatmeta" target="_blank" rel="noopener"><span>Ax: A Platform for Adaptive Experimentation</span></a><span style="font-weight: 400;">&#8221; </span>explains Ax’s architecture, methodology, and how it compares to other state-of-the-art black-box optimization libraries.</li>
</ul>
<p><span style="font-weight: 400;">How can researchers effectively understand and optimize AI models or systems that have a vast number of possible configurations? This is a challenge that is particularly prevalent in domains characterized by complex, interacting systems, such as modern AI development and deployment. Optimizing under these settings demands experimentation, and efficiency is of the utmost importance when evaluating a single configuration is extremely resource- and/or time-intensive.</span></p>
<p><span style="font-weight: 400;">Adaptive experimentation offers a solution to this problem by actively proposing new configurations for sequential evaluation, leveraging insights gained from previous evaluations</span></p>
<p><span style="font-weight: 400;">This year, we released version 1.0 of Ax, an open source adaptive experimentation platform that leverages machine learning to guide and automate the experimentation process. </span><span style="font-weight: 400;">Ax employs Bayesian optimization to enable researchers and developers to conduct efficient experiments, identifying optimal configurations to optimize their systems and processes.</span></p>
<p><span style="font-weight: 400;">In conjunction with this major release, we published a paper titled, &#8220;</span><a href="https://openreview.net/forum?id=U1f6wHtG1g&amp;ref=engineeringatmeta" target="_blank" rel="noopener"><span style="font-weight: 400;">Ax: A Platform for Adaptive Experimentation</span></a><span style="font-weight: 400;">&#8221; that explores Ax’s core architecture, provides a deeper explanation of the methodology powering the optimization, and compares Ax&#8217;s performance against other black-box optimization libraries.</span></p>
<p><span style="font-weight: 400;">Ax has been successfully applied across various disciplines at Meta, including:</span></p>
<ul>
<li style="font-weight: 400;" aria-level="1"><span style="font-weight: 400;">Traditional machine learning tasks, such as hyperparameter optimization and architecture search.</span></li>
<li style="font-weight: 400;" aria-level="1"><span style="font-weight: 400;">Addressing key challenges in GenAI, including discovering optimal data mixtures for training AI models.</span></li>
<li style="font-weight: 400;" aria-level="1"><span style="font-weight: 400;">Tuning infrastructure or compiler flags in production settings.</span></li>
<li style="font-weight: 400;" aria-level="1"><span style="font-weight: 400;">Optimizing design parameters in physical engineering tasks, such as designing AR/VR devices.</span></li>
</ul>
<p><span style="font-weight: 400;">By utilizing Ax, developers can employ state-of-the-art methodology to conduct complex experiments, ultimately gaining a deeper understanding and optimizing their underlying systems.</span></p>
<h2><span style="font-weight: 400;">How to Get Started With Ax</span></h2>
<p><span style="font-weight: 400;">To start using Ax to efficiently tune parameters in complex systems install the latest version of the library via `pip install ax-platform` and visit </span><a href="https://ax.dev/" target="_blank" rel="noopener"><span style="font-weight: 400;">the Ax website</span></a><span style="font-weight: 400;"> for a quickstart guide, tutorials, and deep dives on the methods that Ax uses under the hood.</span></p>
<h2><span style="font-weight: 400;">Ax Is for Real World Experimentation</span></h2>
<p><span style="font-weight: 400;">Adaptive experiments are incredibly useful, but can be challenging to run. Not only do these experiments require the use of sophisticated machine learning methods to drive the optimization, they also demand specialized infrastructure for managing experiment state, automating orchestration, providing useful analysis and diagnostics, and more. Additionally, the goals of any given experiment are often more complex than simply improving a single metric. In practice experimentation is usually a careful balance between multiple objective metrics subject to multiple constraints and guardrails.</span></p>
<p><span style="font-weight: 400;">We built Ax to empower users to easily configure and run these dynamic experiments using state-of-the-art techniques, and to provide a robust and mature platform for researchers to integrate cutting-edge methods directly into production systems.</span></p>
<h2><span style="font-weight: 400;">Ax for Understanding</span></h2>
<p><span style="font-weight: 400;">In addition to finding optimal configurations efficiently, Ax is a powerful tool for understanding the underlying system being optimized. Ax provides a suite of analyses (plots, tables, etc) which helps its users understand how the optimization is progressing over time, tradeoffs between different metrics via a </span><a href="https://en.wikipedia.org/wiki/Pareto_front"><span style="font-weight: 400;">Pareto frontier</span></a><span style="font-weight: 400;">, visualize the effect of one or two parameters across the input space, and explain how much each input parameter contributes to the results (via sensitivity analysis). </span></p>
<p><span style="font-weight: 400;">These tools allow experimenters to walk away with both an optimal configuration to deploy to production and a deeper understanding of their system, which can inform decisions moving forward.</span></p>
<p><img loading="lazy" decoding="async" class="alignnone size-full wp-image-23323" src="https://engineering.fb.com/wp-content/uploads/2025/11/Meta-Adaptive-Experimentation-Ax-image1.png" alt="" width="1674" height="1094" srcset="https://engineering.fb.com/wp-content/uploads/2025/11/Meta-Adaptive-Experimentation-Ax-image1.png 1674w, https://engineering.fb.com/wp-content/uploads/2025/11/Meta-Adaptive-Experimentation-Ax-image1.png?resize=916,599 916w, https://engineering.fb.com/wp-content/uploads/2025/11/Meta-Adaptive-Experimentation-Ax-image1.png?resize=768,502 768w, https://engineering.fb.com/wp-content/uploads/2025/11/Meta-Adaptive-Experimentation-Ax-image1.png?resize=1024,669 1024w, https://engineering.fb.com/wp-content/uploads/2025/11/Meta-Adaptive-Experimentation-Ax-image1.png?resize=1536,1004 1536w, https://engineering.fb.com/wp-content/uploads/2025/11/Meta-Adaptive-Experimentation-Ax-image1.png?resize=96,63 96w, https://engineering.fb.com/wp-content/uploads/2025/11/Meta-Adaptive-Experimentation-Ax-image1.png?resize=192,125 192w" sizes="auto, (max-width: 992px) 100vw, 62vw" /> <img loading="lazy" decoding="async" class="alignnone size-full wp-image-23327" src="https://engineering.fb.com/wp-content/uploads/2025/11/Meta-Adaptive-Experimentation-Ax-image-2.png" alt="" width="1674" height="1094" srcset="https://engineering.fb.com/wp-content/uploads/2025/11/Meta-Adaptive-Experimentation-Ax-image-2.png 1674w, https://engineering.fb.com/wp-content/uploads/2025/11/Meta-Adaptive-Experimentation-Ax-image-2.png?resize=916,599 916w, https://engineering.fb.com/wp-content/uploads/2025/11/Meta-Adaptive-Experimentation-Ax-image-2.png?resize=768,502 768w, https://engineering.fb.com/wp-content/uploads/2025/11/Meta-Adaptive-Experimentation-Ax-image-2.png?resize=1024,669 1024w, https://engineering.fb.com/wp-content/uploads/2025/11/Meta-Adaptive-Experimentation-Ax-image-2.png?resize=1536,1004 1536w, https://engineering.fb.com/wp-content/uploads/2025/11/Meta-Adaptive-Experimentation-Ax-image-2.png?resize=96,63 96w, https://engineering.fb.com/wp-content/uploads/2025/11/Meta-Adaptive-Experimentation-Ax-image-2.png?resize=192,125 192w" sizes="auto, (max-width: 992px) 100vw, 62vw" /> <img loading="lazy" decoding="async" class="alignnone size-full wp-image-23328" src="https://engineering.fb.com/wp-content/uploads/2025/11/Meta-Adaptive-Experimentation-Ax-image-3.png" alt="" width="1674" height="1094" srcset="https://engineering.fb.com/wp-content/uploads/2025/11/Meta-Adaptive-Experimentation-Ax-image-3.png 1674w, https://engineering.fb.com/wp-content/uploads/2025/11/Meta-Adaptive-Experimentation-Ax-image-3.png?resize=916,599 916w, https://engineering.fb.com/wp-content/uploads/2025/11/Meta-Adaptive-Experimentation-Ax-image-3.png?resize=768,502 768w, https://engineering.fb.com/wp-content/uploads/2025/11/Meta-Adaptive-Experimentation-Ax-image-3.png?resize=1024,669 1024w, https://engineering.fb.com/wp-content/uploads/2025/11/Meta-Adaptive-Experimentation-Ax-image-3.png?resize=1536,1004 1536w, https://engineering.fb.com/wp-content/uploads/2025/11/Meta-Adaptive-Experimentation-Ax-image-3.png?resize=96,63 96w, https://engineering.fb.com/wp-content/uploads/2025/11/Meta-Adaptive-Experimentation-Ax-image-3.png?resize=192,125 192w" sizes="auto, (max-width: 992px) 100vw, 62vw" /> <img loading="lazy" decoding="async" class="alignnone size-full wp-image-23324" src="https://engineering.fb.com/wp-content/uploads/2025/11/Meta-Adaptive-Experimentation-Ax-image-4.png" alt="" width="1674" height="1094" srcset="https://engineering.fb.com/wp-content/uploads/2025/11/Meta-Adaptive-Experimentation-Ax-image-4.png 1674w, https://engineering.fb.com/wp-content/uploads/2025/11/Meta-Adaptive-Experimentation-Ax-image-4.png?resize=916,599 916w, https://engineering.fb.com/wp-content/uploads/2025/11/Meta-Adaptive-Experimentation-Ax-image-4.png?resize=768,502 768w, https://engineering.fb.com/wp-content/uploads/2025/11/Meta-Adaptive-Experimentation-Ax-image-4.png?resize=1024,669 1024w, https://engineering.fb.com/wp-content/uploads/2025/11/Meta-Adaptive-Experimentation-Ax-image-4.png?resize=1536,1004 1536w, https://engineering.fb.com/wp-content/uploads/2025/11/Meta-Adaptive-Experimentation-Ax-image-4.png?resize=96,63 96w, https://engineering.fb.com/wp-content/uploads/2025/11/Meta-Adaptive-Experimentation-Ax-image-4.png?resize=192,125 192w" sizes="auto, (max-width: 992px) 100vw, 62vw" /></p>
<h2><span style="font-weight: 400;">How Ax Works</span></h2>
<p><span style="font-weight: 400;">By default Ax uses Bayesian optimization, an effective adaptive experimentation method that excels at balancing </span><b>exploration</b><span style="font-weight: 400;"> – learning how new configurations  perform, and </span><b>exploitation</b><span style="font-weight: 400;"> – refining configurations previously observed to be good. Ax relies on </span><a href="https://botorch.org/" target="_blank" rel="noopener"><span style="font-weight: 400;">BoTorch</span></a><span style="font-weight: 400;"> for its implementation of Bayesian optimization components.</span></p>
<p><img loading="lazy" decoding="async" class="alignnone size-full wp-image-23325" src="https://engineering.fb.com/wp-content/uploads/2025/11/Meta-Adaptive-Experimentation-Ax-image-5.png" alt="" width="1920" height="1120" srcset="https://engineering.fb.com/wp-content/uploads/2025/11/Meta-Adaptive-Experimentation-Ax-image-5.png 1920w, https://engineering.fb.com/wp-content/uploads/2025/11/Meta-Adaptive-Experimentation-Ax-image-5.png?resize=916,534 916w, https://engineering.fb.com/wp-content/uploads/2025/11/Meta-Adaptive-Experimentation-Ax-image-5.png?resize=768,448 768w, https://engineering.fb.com/wp-content/uploads/2025/11/Meta-Adaptive-Experimentation-Ax-image-5.png?resize=1024,597 1024w, https://engineering.fb.com/wp-content/uploads/2025/11/Meta-Adaptive-Experimentation-Ax-image-5.png?resize=1536,896 1536w, https://engineering.fb.com/wp-content/uploads/2025/11/Meta-Adaptive-Experimentation-Ax-image-5.png?resize=96,56 96w, https://engineering.fb.com/wp-content/uploads/2025/11/Meta-Adaptive-Experimentation-Ax-image-5.png?resize=192,112 192w" sizes="auto, (max-width: 992px) 100vw, 62vw" /></p>
<p><span style="font-weight: 400;">Bayesian optimization is an iterative approach to solving the global optimization problem <img decoding="async" src="https://s0.wp.com/latex.php?latex=argmax_%7Bx+%5Cin+X%7D+f%28x%29&#038;bg=ffffff&#038;fg=000&#038;s=0&#038;c=20201002" alt="argmax_{x &#92;in X} f(x)" class="latex" /> which does not assume any information about the form of the function f. In practice, this means optimizing systems by evaluating some candidate configurations <img decoding="async" src="https://s0.wp.com/latex.php?latex=x+%5C+in+X&#038;bg=ffffff&#038;fg=000&#038;s=0&#038;c=20201002" alt="x &#92; in X" class="latex" /> (i.e., trying some configurations out and measuring their effect), building a surrogate model using this data, using that surrogate to identify the most promising configuration to evaluate next, and repeating until an optimal solution has been found or the experimental budget is exhausted.</span></p>
<p><span style="font-weight: 400;">Under typical settings Ax uses a Gaussian process (GP) as the  surrogate model during the Bayesian optimization loop, a flexible model which can make predictions while quantifying uncertainty and is especially effective with very few data points. Ax then uses an acquisition function from a family called expected improvement (EI) to suggest the next candidate configurations to evaluate by capturing the expected value of any new configuration compared to the best previously evaluated configuration.</span></p>
<p><span style="font-weight: 400;">The following animation shows this loop with a GP modeling the goal metric plotted above in blue and EI plotted below in black; the highest value of EI informs the next value of x to evaluate. Once the new value of x has been evaluated, the GP is re-fit with the new data point and we calculate the next EI value.</span></p>
<p><img loading="lazy" decoding="async" class="alignnone wp-image-23326" src="https://engineering.fb.com/wp-content/uploads/2025/11/Meta-Adaptive-Experimentation-Ax-image-6.gif?w=916" alt="" width="600" height="340" /></p>
<p><span style="font-weight: 400;">This 1-dimensional example can be expanded for many input and output dimensions, allowing Ax to optimize problems with many (potentially hundreds) of tunable parameters and outcomes. In fact, higher-dimensional settings, in which covering the search space becomes exponentially more costly, is where the surrogate-based approach really shines compared to other approaches.</span></p>
<p><span style="font-weight: 400;">You can read more about Bayesian optimization on Ax website’s </span><a href="https://ax.dev/docs/next/intro-to-bo" target="_blank" rel="noopener"><span style="font-weight: 400;">Introduction to Bayesian Optimization</span> <span style="font-weight: 400;">page</span></a><span style="font-weight: 400;">.</span></p>
<h2><span style="font-weight: 400;">How We Use Ax at Meta</span></h2>
<p><span style="font-weight: 400;">Ax has been deployed at scale at Meta to solve some of the company’s most challenging optimization problems. Thousands of developers at Meta use Ax for tasks like hyperparameter optimization and architecture search for AI models, tuning parameters for online recommender and ranking models, infrastructure optimizations, and simulation optimization for AR and VR hardware design.</span></p>
<p><span style="font-weight: 400;">These experiments optimize nuanced goals and leverage sophisticated algorithms. For instance, we’ve used multi-objective optimization to simultaneously improve a machine learning model’s accuracy while minimizing its resource usage. When researchers were tasked with shrinking natural language models to fit on the first generation of  Ray-Ban Stories </span><a href="https://research.facebook.com/blog/2021/7/optimizing-model-accuracy-and-latency-using-bayesian-multi-objective-neural-architecture-search/" target="_blank" rel="noopener"><span style="font-weight: 400;">they used Ax</span></a><span style="font-weight: 400;"> to search for models that optimally traded off size and performance. Additionally, Meta engineers use constrained optimization techniques for </span><a href="https://engineering.fb.com/2023/08/09/ml-applications/scaling-instagram-explore-recommendations-system/" target="_blank" rel="noopener"><span style="font-weight: 400;">tuning recommender systems</span></a><span style="font-weight: 400;"> to optimize key metrics while avoiding regressions in others.</span></p>
<p><span style="font-weight: 400;">Recently, Ax was used to design </span><a href="https://engineering.fb.com/2025/07/16/data-center-engineering/ai-make-lower-carbon-faster-curing-concrete/" target="_blank" rel="noopener"><span style="font-weight: 400;">new faster curing, low carbon concrete mixes</span></a><span style="font-weight: 400;"> that were deployed at one of our data center construction sites. These new mixes are playing an important role in advancing our </span><a href="https://sustainability.fb.com/wp-content/uploads/2023/07/Meta-2023-Path-to-Net-Zero.pdf" target="_blank" rel="noopener"><span style="font-weight: 400;">goal of net zero emissions in 2030</span></a><span style="font-weight: 400;">.</span></p>
<p><span style="font-weight: 400;">We see problems across every domain where the ultimate quality of a system is affected by parameters whose interactions are complex to reason about without experimentation and where experimentation has a meaningful cost: Ax addresses these challenges by employing a data-driven approach to adapt experiments as they unfold, enabling us to solve these problems efficiently and effectively.</span></p>
<h2><span style="font-weight: 400;">The Future of Ax</span></h2>
<p><span style="font-weight: 400;">We are always working to improve Ax by building new features for representing innovative experiment designs, exciting new optimization methods, or integrations for using Ax with external platforms. </span><a href="https://github.com/facebook/Ax/"><span style="font-weight: 400;">Ax is proud to be open source</span></a><span style="font-weight: 400;"> (MIT license), and we invite both the practitioner and research communities to contribute to the project whether that be through improved surrogate models or acquisition  functions,  extensions used for individual research applications that may benefit the larger community, or simply bug fixes or improvements to the core capabilities. Please reach out to the team via </span><a href="https://github.com/facebook/Ax/issues"><span style="font-weight: 400;">Github Issues</span></a><span style="font-weight: 400;">.</span></p>
<h2><span style="font-weight: 400;">Read the Paper</span></h2>
<p><a href="https://openreview.net/forum?id=U1f6wHtG1g&amp;ref=engineeringatmeta" target="_blank" rel="noopener"><span style="font-weight: 400;">Ax: A Platform for Adaptive Experimentation</span></a></p>
<p>To learn more about Meta Open Source, visit our <a href="https://opensource.fb.com/" target="_blank" rel="noopener">website</a>, subscribe to our <a href="https://www.youtube.com/channel/UCCQY962PmHabTjaHv2wJzfQ" target="_blank" rel="noopener">YouTube channel</a>, or follow us on <a href="https://www.facebook.com/MetaOpenSource" target="_blank" rel="noopener">Facebook</a>, <a href="https://www.threads.net/@metaopensource" target="_blank" rel="noopener">Threads</a>, <a href="https://x.com/MetaOpenSource" target="_blank" rel="noopener">X</a>, <a href="https://bsky.app/profile/metaopensource.bsky.social" target="_blank" rel="noopener">Bluesky</a> and <a href="https://www.linkedin.com/showcase/meta-open-source?fbclid=IwZXh0bgNhZW0CMTEAAR2fEOJNb7zOi8rJeRvQry5sRxARpdL3OpS4sYLdC1_npkEy60gBS1ynXwQ_aem_mJUK6jEUApFTW75Emhtpqw" target="_blank" rel="noopener">LinkedIn</a>.</p>
<h2><span style="font-weight: 400;">Acknowledgements</span></h2>
<p><em><span style="font-weight: 400;">Ax was created by Meta’s Adaptive Experimentation team: Sebastian Ament, Eytan Bakshy, Max Balandat, Bernie Beckerman, Sait Cakmak, Cesar Cardoso, Ethan Che, Sam Daulton, David Eriksson, Mia Garrard, Matthew Grange, Carl Hvarfner, Paschal Igusti, Lena Kashtelyan, Cristian Lara, Ben Letham, Andy Lin, Jerry Lin, Jihao Andreas Lin, Samuel Müller, Miles Olson, Eric Onofrey, Shruti Patel, Elizabeth Santorella, Sunny Shen, Louis Tiao, and Kaiwen Wu.</span></em></p>
<p>The post <a rel="nofollow" href="https://engineering.fb.com/2025/11/18/open-source/efficient-optimization-ax-open-platform-adaptive-experimentation/">Efficient Optimization With Ax, an Open Platform for Adaptive Experimentation</a> appeared first on <a rel="nofollow" href="https://engineering.fb.com">Engineering at Meta</a>.</p>
]]></content:encoded>
					
		
		
		<post-id xmlns="com-wordpress:feed-additions:1">23315</post-id>	</item>
		<item>
		<title>Announcing the Completion of the Core 2Africa System: Building the Future of Connectivity Together</title>
		<link>https://engineering.fb.com/2025/11/17/connectivity/core-2africa-system-completion-future-connectivity/</link>
		
		<dc:creator><![CDATA[]]></dc:creator>
		<pubDate>Tue, 18 Nov 2025 06:00:58 +0000</pubDate>
				<category><![CDATA[Connectivity]]></category>
		<guid isPermaLink="false">https://engineering.fb.com/?p=23313</guid>

					<description><![CDATA[<p>Connecting Africa and the World We’re excited to share the completion of the core 2Africa infrastructure, the world&#8217;s longest open access subsea cable system. 2Africa is a landmark subsea cable system that sets a new standard for global connectivity. This project is the result of years of collaboration, innovation, and a shared vision to connect [...]</p>
<p><a class="btn btn-secondary understrap-read-more-link" href="https://engineering.fb.com/2025/11/17/connectivity/core-2africa-system-completion-future-connectivity/">Read More...</a></p>
<p>The post <a rel="nofollow" href="https://engineering.fb.com/2025/11/17/connectivity/core-2africa-system-completion-future-connectivity/">Announcing the Completion of the Core 2Africa System: Building the Future of Connectivity Together</a> appeared first on <a rel="nofollow" href="https://engineering.fb.com">Engineering at Meta</a>.</p>
]]></description>
										<content:encoded><![CDATA[<h2>Connecting Africa and the World</h2>
<p><b>We’re excited to share the completion of the core 2Africa infrastructure, </b><b>the world&#8217;s longest open access subsea cable system.</b><span style="font-weight: 400;"> 2Africa is</span> <span style="font-weight: 400;">a landmark subsea cable system that sets a new standard for global connectivity. This project is the result of years of collaboration, innovation, and a shared vision to connect communities, accelerate economic growth, and enable transformative digital experiences across Africa and beyond.</span></p>
<div style="width: 800px;" class="wp-video"><video class="wp-video-shortcode" id="video-23313-1" width="800" height="360" preload="metadata" controls="controls"><source type="video/mp4" src="https://engineering.fb.com/wp-content/uploads/2025/11/2Africa_Impact_Captions_1920x1080_Stereo_MPEG-4-1.mp4?_=1" /><a href="https://engineering.fb.com/wp-content/uploads/2025/11/2Africa_Impact_Captions_1920x1080_Stereo_MPEG-4-1.mp4">https://engineering.fb.com/wp-content/uploads/2025/11/2Africa_Impact_Captions_1920x1080_Stereo_MPEG-4-1.mp4</a></video></div>
<p>&nbsp;</p>
<h2>Unprecedented Scale and Reach</h2>
<p><b>2Africa is the first cable to connect East and West Africa in a continuous system and link Africa to the Middle East, South Asia, and Europe.</b><span style="font-weight: 400;"> With a current reach of 33 countries and still counting, we’re enabling connectivity for 3 billion people across Africa, Europe, and Asia – more than 30% of the world’s population. This scale is unprecedented and we are proud to have partnered with stakeholders across the ecosystem to deliver new levels of connectivity at such scale.</span></p>
<figure id="attachment_23340" aria-describedby="caption-attachment-23340" style="width: 1920px" class="wp-caption alignnone"><img loading="lazy" decoding="async" class="size-full wp-image-23340" src="https://engineering.fb.com/wp-content/uploads/2025/11/2Africa-2025_1117-Map.png" alt="" width="1920" height="1080" srcset="https://engineering.fb.com/wp-content/uploads/2025/11/2Africa-2025_1117-Map.png 1920w, https://engineering.fb.com/wp-content/uploads/2025/11/2Africa-2025_1117-Map.png?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2025/11/2Africa-2025_1117-Map.png?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2025/11/2Africa-2025_1117-Map.png?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2025/11/2Africa-2025_1117-Map.png?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2025/11/2Africa-2025_1117-Map.png?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2025/11/2Africa-2025_1117-Map.png?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2025/11/2Africa-2025_1117-Map.png?resize=192,108 192w" sizes="auto, (max-width: 992px) 100vw, 62vw" /><figcaption id="caption-attachment-23340" class="wp-caption-text">The 2Africa Subsea Cable reaches 3 continents and lands in 33 countries, connecting over 3 billion people.</figcaption></figure>
<h2>Building 2Africa: Partnership, Scale, and Open Access</h2>
<p><span style="font-weight: 400;">Africa’s digital future depends on robust, scalable infrastructure built in partnership with local communities and stakeholders. As demand for high-speed internet grows, a consortium of global partners led by Meta, including </span><a href="https://bayobab.africa/" target="_blank" rel="noopener"><span style="font-weight: 400;">Bayobab</span></a><span style="font-weight: 400;"> (MTN Group), </span><a href="https://center3.com/" target="_blank" rel="noopener"><span style="font-weight: 400;">center3</span></a><span style="font-weight: 400;"> (stc), </span><a href="https://www.chinamobileltd.com/en/global/home.php" target="_blank" rel="noopener"><span style="font-weight: 400;">CMI, </span></a><a href="https://www.orange.com/en" target="_blank" rel="noopener"><span style="font-weight: 400;">Orange</span></a><span style="font-weight: 400;">, </span><a href="https://www.te.eg/wps/portal/te/Personal" target="_blank" rel="noopener"><span style="font-weight: 400;">Telecom Egypt</span></a><span style="font-weight: 400;">, </span><a href="https://www.vodafone.com/" target="_blank" rel="noopener"><span style="font-weight: 400;">Vodafone Group</span></a><span style="font-weight: 400;">, and </span><a href="https://wiocc.net/" target="_blank" rel="noopener"><span style="font-weight: 400;">WIOCC</span></a><span style="font-weight: 400;">, came together to design and invest in what would become the world’s longest open access subsea cable system. With the </span><a href="https://engineering.fb.com/2021/09/28/connectivity/2africa-pearls/"><span style="font-weight: 400;">Pearls extension</span></a><span style="font-weight: 400;"> scheduled to go live in 2026, 2Africa&#8217;s complete system length of 45,000 kilometers is longer than the equivalent of the Earth’s circumference. </span></p>
<p><span style="font-weight: 400;">Realizing this vision required close collaboration across both private and public sectors. We managed the project and facilitated engagement with local partners for cable landing, construction, and regulatory processes. The deployment spanned 50 jurisdictions and nearly </span><span style="font-weight: 400;">six years of work, relying on the active engagement of regulators and policymakers to navigate requirements and keep progress on track.</span></p>
<p><span style="font-weight: 400;">The consortium’s shared goal is to develop an open, inclusive network that fosters competition, supports innovation, and unlocks new opportunities for millions. This open-access model ensures that multiple service providers can leverage the infrastructure, accelerating digital transformation and AI adoption across the region. New partners including Bharti Airtel and MainOne (an Equinix Company) collaborated on specific segments and data center integration, further expanding the cable’s impact and reach.</span></p>
<h2>Engineering Innovation and Overcoming Challenges</h2>
<p><span style="font-weight: 400;">Building 2Africa required us to push the boundaries of what’s possible in subsea infrastructure. We deployed advanced </span><a href="https://www.asn.com/sdm/" target="_blank" rel="noopener"><span style="font-weight: 400;">spatial division multiplexing (SDM) technology</span></a><span style="font-weight: 400;">, supporting up to 16 fiber pairs per cable. This is </span><b>double the capacity of older systems. </b><span style="font-weight: 400;">It is the</span><b> first 16-fiber-pair subsea cable to fully connect Africa</b><span style="font-weight: 400;">. We incorporated undersea optical wavelength switching, enabling flexible bandwidth management and supporting evolving demands for AI, cloud, and high-bandwidth applications.</span></p>
<p><img loading="lazy" decoding="async" class="alignnone size-full wp-image-23338" src="https://engineering.fb.com/wp-content/uploads/2025/11/2Africa-Blog-Post-Image-01.png" alt="" width="1920" height="1080" srcset="https://engineering.fb.com/wp-content/uploads/2025/11/2Africa-Blog-Post-Image-01.png 1920w, https://engineering.fb.com/wp-content/uploads/2025/11/2Africa-Blog-Post-Image-01.png?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2025/11/2Africa-Blog-Post-Image-01.png?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2025/11/2Africa-Blog-Post-Image-01.png?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2025/11/2Africa-Blog-Post-Image-01.png?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2025/11/2Africa-Blog-Post-Image-01.png?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2025/11/2Africa-Blog-Post-Image-01.png?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2025/11/2Africa-Blog-Post-Image-01.png?resize=192,108 192w" sizes="auto, (max-width: 992px) 100vw, 62vw" /></p>
<p><span style="font-weight: 400;">We increased 2Africa’s burial depth by 50% over previous systems and carefully routed the cable to avoid seabed hazards such as seamounts at </span><a href="https://www.nature.com/articles/s43247-022-00482-x" target="_blank" rel="noopener"><span style="font-weight: 400;">hot brine pools</span></a><span style="font-weight: 400;">, improving resilience and network availability. The system features two independent trunk powering architectures across its West, East, and Mediterranean segments, optimizing capacity and providing additional resiliency against electrical faults. Our branching unit switching capability allowed us to optimize for trunk capacity and reliability by utilizing routes much further offshore from hazards such as the </span><a href="https://www.bbc.co.uk/news/science-environment-57382529" target="_blank" rel="noopener"><span style="font-weight: 400;">Congo Canyon turbidity currents</span></a><span style="font-weight: 400;">, while efficiently serving branches to West African nations. To further ensure the integrity and reach of the cable, we engineered compatible crossing solutions for over 60 oil and gas pipelines. </span></p>
<p><img loading="lazy" decoding="async" class="alignnone size-full wp-image-23347" src="https://engineering.fb.com/wp-content/uploads/2025/11/2Africa-Blog-Post-Visual-02-updated.png" alt="" width="1920" height="1142" srcset="https://engineering.fb.com/wp-content/uploads/2025/11/2Africa-Blog-Post-Visual-02-updated.png 1920w, https://engineering.fb.com/wp-content/uploads/2025/11/2Africa-Blog-Post-Visual-02-updated.png?resize=916,545 916w, https://engineering.fb.com/wp-content/uploads/2025/11/2Africa-Blog-Post-Visual-02-updated.png?resize=768,457 768w, https://engineering.fb.com/wp-content/uploads/2025/11/2Africa-Blog-Post-Visual-02-updated.png?resize=1024,609 1024w, https://engineering.fb.com/wp-content/uploads/2025/11/2Africa-Blog-Post-Visual-02-updated.png?resize=1536,914 1536w, https://engineering.fb.com/wp-content/uploads/2025/11/2Africa-Blog-Post-Visual-02-updated.png?resize=96,57 96w, https://engineering.fb.com/wp-content/uploads/2025/11/2Africa-Blog-Post-Visual-02-updated.png?resize=192,114 192w" sizes="auto, (max-width: 992px) 100vw, 62vw" /></p>
<p><span style="font-weight: 400;">Over the course of construction, we deployed 35 offshore vessels, amounting to nearly 32 years of vessel operations, while dedicated shore-end operations required even more inshore vessels, locally mobilized for cable pulling, guarding, security, and dive support. In remote locations, we imported and mobilized specialist equipment such as dive decompression chambers and shore-end burial tooling to locally operated vessels.</span></p>
<h2>Economic Impact and Community Transformation</h2>
<p><span style="font-weight: 400;">2Africa is delivering a step change in international bandwidth for Africa, with technical capacity that far exceeds previous systems. For example, on the West segment, stretching from England to South Africa, and landing in countries such as </span><span style="font-weight: 400;">Senegal, Ghana, Cote d’Ivoire, Nigeria, Gabon, the Republic of Congo, DRC, and Angola</span><span style="font-weight: 400;">, the cable supports 21 terabits per second (Tbps) per fiber pair, with 8 fiber pairs on the trunk. This results in a total trunk capacity of up to 180 Tbps. </span></p>
<h3>But what does 180 Tbps mean for people?</h3>
<p><span style="font-weight: 400;">To put in perspective:</span></p>
<ul>
<li style="font-weight: 400;" aria-level="1"><span style="font-weight: 400;">180 Tbps is enough to stream over 36 million HD movies simultaneously (assuming 5 megabits per second (Mbps) per stream).</span></li>
<li style="font-weight: 400;" aria-level="1"><span style="font-weight: 400;">For an individual, this means the potential to download 15,000 full-length Nollywood films (each about 1.5 GB) per second, or enable students to access a remote university’s full library in a minute.</span></li>
<li style="font-weight: 400;" aria-level="1"><span style="font-weight: 400;">For a city like Lagos, it means millions of people can video call, stream, and work online at the same time – without experiencing slowdowns or congestion.</span></li>
</ul>
<p><span style="font-weight: 400;">This massive capacity ensures a near-limitless supply of international internet bandwidth, allowing internet service providers (ISPs) and mobile network operators (MNOs) to secure capacity at much lower wholesale prices. This creates market competition, redundancy, and supports modern digital infrastructure including cloud services, data centers, and 5G deployment. </span></p>
<p><span style="font-weight: 400;">The impact is profound: </span><span style="font-weight: 400;">2Africa is expected to contribute up to </span><a href="https://www.rti.org/publication/economic-impact-2africa/fulltext.pdf"><span style="font-weight: 400;">36.9 billion US dollars</span></a><span style="font-weight: 400;"> to Africa’s GDP within just the first two to three years of operation. The cable’s arrival will boost job creation, entrepreneurship, and innovation hubs in connected regions. Evidence from previous cable landings shows that fast internet access increases employment rates, improves productivity, and supports shifts toward higher-skill occupations. </span></p>
<p><span style="font-weight: 400;">Meta’s vision is to empower African entrepreneurs, creators, and businesses to innovate and collaborate. By partnering with policymakers, regulators, and stakeholders, we advance Africa’s digital transformation and support its position as an emerging major player in the global digital economy.</span></p>
<h2>Building Connections, Empowering Progress</h2>
<p><span style="font-weight: 400;">The completion of 2Africa is a defining moment for Africa’s digital future. By leading the design, funding, and deployment of the world’s longest subsea cable system to date, we are building the infrastructure that will power transformative new experiences, drive economic growth, and connect billions of people. We are laying the foundation for the next generation of digital experiences. This subsea cable will enable faster, more reliable internet and support AI-driven services through digital access.</span></p>
<p><span style="font-weight: 400;">2Africa is part of Meta’s mission to build the future of human connection, opening more pathways for communities across Africa to help shape and play a critical role in the next chapter of the global digital economy. </span></p>
<p>The post <a rel="nofollow" href="https://engineering.fb.com/2025/11/17/connectivity/core-2africa-system-completion-future-connectivity/">Announcing the Completion of the Core 2Africa System: Building the Future of Connectivity Together</a> appeared first on <a rel="nofollow" href="https://engineering.fb.com">Engineering at Meta</a>.</p>
]]></content:encoded>
					
		
		
		<post-id xmlns="com-wordpress:feed-additions:1">23313</post-id>	</item>
		<item>
		<title>Enhancing HDR on Instagram for iOS With Dolby Vision</title>
		<link>https://engineering.fb.com/2025/11/17/ios/enhancing-hdr-on-instagram-for-ios-with-dolby-vision/</link>
		
		<dc:creator><![CDATA[]]></dc:creator>
		<pubDate>Mon, 17 Nov 2025 17:30:25 +0000</pubDate>
				<category><![CDATA[iOS]]></category>
		<category><![CDATA[Video Engineering]]></category>
		<category><![CDATA[Instagram]]></category>
		<guid isPermaLink="false">https://engineering.fb.com/?p=23294</guid>

					<description><![CDATA[<p>We’re sharing how we’ve enabled Dolby Vision and ambient viewing environment (amve) on the Instagram iOS app to enhance the video viewing experience. HDR videos created on iPhones contain unique Dolby Vision and amve metadata that we needed to support end-to-end Instagram for iOS is now the first Meta app to support Dolby Vision video, [...]</p>
<p><a class="btn btn-secondary understrap-read-more-link" href="https://engineering.fb.com/2025/11/17/ios/enhancing-hdr-on-instagram-for-ios-with-dolby-vision/">Read More...</a></p>
<p>The post <a rel="nofollow" href="https://engineering.fb.com/2025/11/17/ios/enhancing-hdr-on-instagram-for-ios-with-dolby-vision/">Enhancing HDR on Instagram for iOS With Dolby Vision</a> appeared first on <a rel="nofollow" href="https://engineering.fb.com">Engineering at Meta</a>.</p>
]]></description>
										<content:encoded><![CDATA[<ul>
<li style="font-weight: 400;" aria-level="1"><span style="font-weight: 400;">We’re sharing how we’ve enabled Dolby Vision and ambient viewing environment (amve) on the Instagram iOS app to enhance the video viewing experience.</span></li>
<li style="font-weight: 400;" aria-level="1"><span style="font-weight: 400;">HDR videos created on iPhones contain unique Dolby Vision and amve metadata that we needed to support end-to-end</span></li>
<li style="font-weight: 400;" aria-level="1"><span style="font-weight: 400;">Instagram for iOS is now the first Meta app to support Dolby Vision video, with more support coming across all of Meta’s apps coming in the future.</span></li>
</ul>
<p><span style="font-weight: 400;">Every iPhone-produced HDR video encoding includes two additional pieces of metadata that help ensure the picture is consistent between different displays and viewing conditions:</span></p>
<ul>
<li style="font-weight: 400;" aria-level="1"><span style="font-weight: 400;">Ambient viewing environment (amve), which</span> <span style="font-weight: 400;">provides the characteristics of the nominal ambient viewing environment for displaying associated video content. This information enables the final device to adjust the rendering of the video if the actual ambient viewing conditions differ from those for which it was encoded.</span></li>
<li style="font-weight: 400;" aria-level="1"><span style="font-weight: 400;">Dolby Vision, which enhances color, brightness, and contrast to better match the video to the capabilities of the display.</span></li>
</ul>
<p><span style="font-weight: 400;">While the Instagram and Facebook iOS apps</span> <a href="https://engineering.fb.com/2023/07/17/video-engineering/hdr-video-reels-meta/" target="_blank" rel="noopener"><span style="font-weight: 400;">have supported high dynamic range (HDR) video</span></a><span style="font-weight: 400;"> since 2022, our initial rollout of HDR didn’t support Dolby Vision or amve delivery and playback. Our derived encodings were done with </span><a href="https://www.ffmpeg.org/"><span style="font-weight: 400;">FFmpeg</span></a><span style="font-weight: 400;">, which has traditionally lacked support for Dolby Vision and amve. Since our tooling was discarding this metadata, it meant that pictures were not entirely representative of the way they were meant to be viewed – something that was particularly noticeable at low screen brightness levels.</span></p>
<p><span style="font-weight: 400;">Now, after hearing feedback from people using our iOS apps, we’ve worked with our partners to preserve the iOS-produced amve and Dolby Vision metadata from end-to-end and significantly enhanced the HDR viewing experience on iOS devices.  </span></p>
<h2><span style="font-weight: 400;">How Meta Processes Video </span></h2>
<p><span style="font-weight: 400;">It may first be helpful to give some background on the </span><a href="https://engineering.fb.com/2021/04/05/video-engineering/how-facebook-encodes-your-videos/" target="_blank" rel="noopener"><span style="font-weight: 400;">lifecycle of a video at Meta</span></a><span style="font-weight: 400;">. </span></p>
<p><span style="font-weight: 400;">The majority of videos uploaded through our apps go through three main stages:</span></p>
<h3><span style="font-weight: 400;">1. Client Processing </span></h3>
<p><span style="font-weight: 400;">In the client processing stage, the creator&#8217;s device flattens their composition into a single video file at a size appropriate for upload. For HDR videos produced by iOS devices this means encoding with HEVC using the Main 10 profile. This is the stage in which amve and Dolby Vision metadata are produced, added to the encoded bitstream, and uploaded to Meta’s servers.</span></p>
<h3><span style="font-weight: 400;">2. </span><span style="font-weight: 400;"> </span><span style="font-weight: 400;">Server Processing</span></h3>
<p><span style="font-weight: 400;">In the server processing stage, our transcoding system generates different versions of the video for different consumers. As playback occurs across a variety of devices with different capabilities, we need to produce the video in a format which will be optimal for each device. In the scope of HDR uploads, this means producing an SDR version for devices that don&#8217;t support HDR, a VP9 version to satisfy the majority of players, and (for our most popular videos) an</span><a href="https://engineering.fb.com/2023/02/21/video-engineering/av1-codec-facebook-instagram-reels/"> <span style="font-weight: 400;">AV1 version</span></a><span style="font-weight: 400;"> with the highest quality at the lowest file size.</span></p>
<p><span style="font-weight: 400;">Each of these versions is produced at a different bitrate (essentially, file size) to ensure that consumers with varying network conditions are all able to play the video without waiting for a large download to complete (the tradeoff is that lower bitrates have lower quality). All of our derived encodings are created with FFmpeg, which historically lacked support for amve and Dolby Vision. This is the stage where metadata was getting dropped.</span></p>
<h3><span style="font-weight: 400;">3. Consumption</span></h3>
<p><span style="font-weight: 400;">In the consumption stage, the viewer&#8217;s device picks the version that will play back smoothly (without stalls), decodes it frame by frame, and draws each frame onto the screen. In the context of iOS, all HDR playback is done using Apple&#8217;s AVSampleBufferDisplayLayer (AVSBDL). This is the class that consumes amve and Dolby Vision metadata along with each decoded frame.</span></p>
<h2><span style="font-weight: 400;">How We Added Support for amve</span></h2>
<p><span style="font-weight: 400;">When we first set off to support amve in 2022, we noticed something interesting. As we operate on a decoupled architecture of lower-level components rather than a typical high-level AVPlayer setup, we were able to inspect an intact video encoding and get a look at the amve metadata in between the decoder and AVSBDL. We observed that every frame of every video seemed to have exactly the same metadata. This allowed us to hold ourselves over with a quick fix and hardcode these values directly into our player pipeline.</span></p>
<p><span style="font-weight: 400;">This was not a great situation to be in. Even though the value seemed to be static, there was nothing enforcing this. Maybe a new iPhone or iOS version would produce different values, then we&#8217;d be using the wrong ones. amve is also not a concept on Android, which would mean that viewing an Android-produced HDR encoding on iPhone would result in an image that was not technically accurate.</span></p>
<p><span style="font-weight: 400;">In 2024, we worked with the community to land amve support in FFmpeg. We also built in some logging, which showed that our two-year-old assertion that the values never change still stood. But if they ever do, we will be properly set up for it. </span></p>
<h2><span style="font-weight: 400;">Enabling Dolby Vision</span></h2>
<p><span style="font-weight: 400;">Dolby Vision was not as straightforward as amve to adopt.</span></p>
<p><b>Challenge #1: The extant specification was for carriage of metadata within an HEVC bitstream. We don&#8217;t deliver HEVC.</b></p>
<p><span style="font-weight: 400;">iPhone-produced HDR uses Dolby Vision profile 8.4, where 8 indicates a profile using HEVC (the video codec) and .4 means cross-compatible with HLG (the standard for HDR video that players without Dolby Vision support would adhere to). </span></p>
<p><span style="font-weight: 400;">In order to deliver Dolby Vision metadata we needed to carry it within a codec that we do deliver. Fortunately, Dolby has created Profile 10 for carriage of Dolby Vision within AV1. As VP9 does not offer a facility for carriage of additional metadata there is no support for Dolby Vision at this time, but we are interested in exploring alternate delivery mechanisms.</span></p>
<p><span style="font-weight: 400;">However, Dolby Vision Profiles 10 and 8 were not properly supported by our existing video processing tools, including FFmpeg and </span><a href="https://github.com/shaka-project/shaka-packager" target="_blank" rel="noopener"><span style="font-weight: 400;">Shaka</span></a><span style="font-weight: 400;"> packager. Based on the specifications from Dolby, we collaborated with the FFmpeg developers to fully implement support for Dolby Vision Profile 8 and Profile 10. In particular, we enabled support within FFmpeg to transcode HEVC with Profile 8.4 into AV1 with Profile 10.4 using both the libaom and libsvtav1 encoders, and made fixes to other parts of the stack, including dav1d decoder and Shaka packager, to properly support Dolby Vision metadata.</span></p>
<p><b>Challenge #2: Getting Dolby Vision into AVSampleBufferDisplayLayer</b></p>
<p><span style="font-weight: 400;">When you feed AVSBDL an encoded bitstream in a supported format, e.g., HEVC from an iPhone camera, Dolby Vision just works for free. But we feed buffers that we decode independently, as we need to be able to decode formats that Apple does not offer out of the box (AV1 on devices before the iPhone 15 Pro, for example). Given this setup, it&#8217;s only fair that we&#8217;d have to extract Dolby Vision independently as well.</span></p>
<p><span style="font-weight: 400;">Following the newly-minted specification for carriage of Profile 10 within an AV1 bitstream from Dolby, we implemented manual extraction of Dolby Vision metadata, packaged it into the same format that AVSBDL expected, and we were in business.</span></p>
<p><span style="font-weight: 400;">To prove that our setup was working as expected, we set up a series of identical Instagram posts with and without Dolby Vision metadata. Our partners at Dolby measured the brightness of each of these posts using a display color analyzer, at varying levels of screen brightness.</span></p>
<p><span style="font-weight: 400;">They captured the following:</span></p>
<figure id="attachment_23298" aria-describedby="caption-attachment-23298" style="width: 1920px" class="wp-caption alignnone"><img loading="lazy" decoding="async" class="size-full wp-image-23298" src="https://engineering.fb.com/wp-content/uploads/2025/11/Meta-Dolby-Vision-iOS-Instagram_image-1.jpg" alt="" width="1920" height="1193" srcset="https://engineering.fb.com/wp-content/uploads/2025/11/Meta-Dolby-Vision-iOS-Instagram_image-1.jpg 1920w, https://engineering.fb.com/wp-content/uploads/2025/11/Meta-Dolby-Vision-iOS-Instagram_image-1.jpg?resize=916,569 916w, https://engineering.fb.com/wp-content/uploads/2025/11/Meta-Dolby-Vision-iOS-Instagram_image-1.jpg?resize=768,477 768w, https://engineering.fb.com/wp-content/uploads/2025/11/Meta-Dolby-Vision-iOS-Instagram_image-1.jpg?resize=1024,636 1024w, https://engineering.fb.com/wp-content/uploads/2025/11/Meta-Dolby-Vision-iOS-Instagram_image-1.jpg?resize=1536,954 1536w, https://engineering.fb.com/wp-content/uploads/2025/11/Meta-Dolby-Vision-iOS-Instagram_image-1.jpg?resize=96,60 96w, https://engineering.fb.com/wp-content/uploads/2025/11/Meta-Dolby-Vision-iOS-Instagram_image-1.jpg?resize=192,119 192w" sizes="auto, (max-width: 992px) 100vw, 62vw" /><figcaption id="caption-attachment-23298" class="wp-caption-text">Screen brightness settings versus image brightness, with and without Dolby Vision.</figcaption></figure>
<p><span style="font-weight: 400;">In this chart, the X-axis represents the screen brightness setting and the Y-axis represents the observed image brightness. The results demonstrate that with Dolby Vision metadata present, the brightness of the content much more closely follows the brightness setting of the screen.</span></p>
<p><span style="font-weight: 400;">It worked!…But we were not done yet. </span></p>
<h2><span style="font-weight: 400;">Testing Our Dolby Vision Implementation</span></h2>
<p><span style="font-weight: 400;">At Meta, we A/B test new features before shipping them to ensure they are performing as we expect. How do we A/B test metadata embedded within a video bitstream? The answer is that we produced an additional version of every video containing the new metadata. We delivered this new version to a randomly distributed test population while the randomly distributed control population continued receiving the existing experience. At our scale, we can assert that roughly an equal population will watch both flavors of each video.</span></p>
<p><span style="font-weight: 400;">For each flavor, we collected statistics such as how long the video was watched, how long it took to load, what type of connection it was watched on, and whether any errors were encountered during playback. Then we analyzed in aggregate to see how the flavor with metadata compared to the flavor without.</span></p>
<p><span style="font-weight: 400;">We hypothesized that if the metadata works as expected, videos with the new metadata would receive more watch time. But when we ran our initial test on Instagram Reels in 2024 we found that, on average, videos with Dolby Vision metadata were actually watched less than their standard counterparts.</span></p>
<p><span style="font-weight: 400;">How could this be possible? Isn&#8217;t Dolby Vision supposed to improve the image?</span></p>
<h3><span style="font-weight: 400;">Our First A/B Test With Dolby Vision Metadata </span></h3>
<p><span style="font-weight: 400;">Our data indicated that people were watching less Dolby Vision video because the videos were taking too long to load and people were just moving on to the next Reel in their feed.</span></p>
<p><span style="font-weight: 400;">There was a reasonable cause for the longer load times: The new metadata added on the order of 100 kbps to every video on average. It sounds petty, but our encodings are highly optimized for all kinds of diverse viewing conditions. Every bit counts in some situations, and a 100-kbps overhead was enough to regress engagement at the margins.</span></p>
<p><span style="font-weight: 400;">The answer to this was a compressed metadata format. The team at Dolby offered another specification which would lower the metadata overhead by a factor of four, to 25 kbps on average.</span></p>
<p><span style="font-weight: 400;">Would it be enough? We had to run another test to find out. But there was more work to be done first.</span></p>
<p><span style="font-weight: 400;">We needed to implement support for Dolby Vision metadata compression (and decompression while we’re at it) in FFmpeg using a bitstream filter. Also, while the uncompressed format was something we could extract from the bitstream and hand off to Apple, the compressed format was not something that was supported by Apple out of the box. We had to implement client-side decompression on our own.</span></p>
<p><span style="font-weight: 400;">About 2000 lines of code later, we were ready.</span></p>
<h3><span style="font-weight: 400;">Our Successful A/B Test </span></h3>
<p><span style="font-weight: 400;">This time, we found that consumers viewing with Dolby Vision metadata were spending more time in the app. We attribute this to people spending more time watching HDR videos in lower-light environments, when their screens are set to lower brightness levels and the HDR videos with proper metadata are less taxing on the eyes.</span></p>
<p><span style="font-weight: 400;">Because including Dolby Vision metadata had a tangibly positive outcome, we were able to make the case for shipping it across Instagram for iOS, making it our first app to take advantage of Dolby Vision. As of June 2025, all of our delivered AV1 encodings derived from iPhone-produced HDR include Dolby Vision metadata.</span></p>
<h2><span style="font-weight: 400;">The Future of Dolby Vision Across Meta</span></h2>
<p><span style="font-weight: 400;">The final challenge in the scope of this post is that Dolby Vision is not widely supported within the web ecosystem across different browsers and displays. Thus, we cannot accurately show the difference that it makes on this page, and hope you will experience it on Instagram on iPhone for yourself. The support for Dolby vision and amve is now in our encoding recipes and as such it’s ready for deployment to other platforms as well as we’re currently working on extending the support to Facebook Reels.</span></p>
<p><span style="font-weight: 400;">In collaboration with Dolby, we&#8217;ve solved the perceptible problem of HDR metadata preservation and collaborated with the FFmpeg developers to implement its support and make it readily available to the  community to take advantage of.</span></p>
<p><span style="font-weight: 400;">This is just the beginning. We look forward to expanding Dolby Vision to other Meta apps and their corresponding operating systems.</span></p>
<h2><span style="font-weight: 400;">Acknowledgements</span></h2>
<p><i><span style="font-weight: 400;">We’d like to thank Haixia Shi, the team at Dolby, and Niklas Haas from FFmpeg for their work supporting this effort.</span></i></p>
<p>The post <a rel="nofollow" href="https://engineering.fb.com/2025/11/17/ios/enhancing-hdr-on-instagram-for-ios-with-dolby-vision/">Enhancing HDR on Instagram for iOS With Dolby Vision</a> appeared first on <a rel="nofollow" href="https://engineering.fb.com">Engineering at Meta</a>.</p>
]]></content:encoded>
					
		
		
		<post-id xmlns="com-wordpress:feed-additions:1">23294</post-id>	</item>
		<item>
		<title>Open Source Is Good for the Environment</title>
		<link>https://engineering.fb.com/2025/11/14/production-engineering/open-source-is-good-for-the-environment/</link>
		
		<dc:creator><![CDATA[]]></dc:creator>
		<pubDate>Fri, 14 Nov 2025 20:54:14 +0000</pubDate>
				<category><![CDATA[Data Center Engineering]]></category>
		<category><![CDATA[ML Applications]]></category>
		<category><![CDATA[Open Source]]></category>
		<category><![CDATA[Production Engineering]]></category>
		<category><![CDATA[Meta Tech Podcast]]></category>
		<guid isPermaLink="false">https://engineering.fb.com/?p=23306</guid>

					<description><![CDATA[<p>Most people have heard of open-source software. But have you heard about open hardware? And did you know open source can have a positive impact on the environment? On this episode of the Meta Tech Podcast, Pascal Hartig sits down with Dharmesh and Lisa to talk about all things open hardware, and Meta’s biggest announcements [...]</p>
<p><a class="btn btn-secondary understrap-read-more-link" href="https://engineering.fb.com/2025/11/14/production-engineering/open-source-is-good-for-the-environment/">Read More...</a></p>
<p>The post <a rel="nofollow" href="https://engineering.fb.com/2025/11/14/production-engineering/open-source-is-good-for-the-environment/">Open Source Is Good for the Environment</a> appeared first on <a rel="nofollow" href="https://engineering.fb.com">Engineering at Meta</a>.</p>
]]></description>
										<content:encoded><![CDATA[<p>Most people have heard of open-source software. But have you heard about open hardware? And did you know open source can have a positive impact on the environment?</p>
<p>On this episode of the Meta Tech Podcast, <a href="https://www.threads.com/@passy_" target="_blank" rel="noopener">Pascal Hartig</a> sits down with Dharmesh and Lisa to talk about all things open hardware, and Meta’s biggest announcements from the <a href="https://engineering.fb.com/2025/10/13/data-infrastructure/ocp-summit-2025-the-open-future-of-networking-hardware-for-ai/" target="_blank" rel="noopener">2025 Open Compute Project (OCP) Summit</a> <span style="font-weight: 400;">– </span> including a new open methodology for <a href="https://engineering.fb.com/2025/10/14/data-center-engineering/how-meta-is-leveraging-ai-to-improve-the-quality-of-scope-3-emission-estimates-for-it-hardware/" target="_blank" rel="noopener">leveraging AI to understand Scope 3 emissions</a>.</p>
<p>Learn about the history of OCP and its growth into an organization with more than 400 companies contributing to it. You’ll also hear how AI and open hardware are helping Meta push to achieve <a href="https://sustainability.atmeta.com/climate/" target="_blank" rel="noopener">net zero emissions in 2030</a>, including how <a href="https://engineering.fb.com/2025/07/16/data-center-engineering/ai-make-lower-carbon-faster-curing-concrete/" target="_blank" rel="noopener">AI is being used to develop new concrete mixes for data center construction</a>.</p>
<p>Download or listen to the episode below:</p>
<p><iframe loading="lazy" style="border: none;" title="Libsyn Player" src="//html5-player.libsyn.com/embed/episode/id/39036615/height/90/theme/custom/thumbnail/yes/direction/forward/render-playlist/no/custom-color/000000/" width="100%" height="90" scrolling="no" allowfullscreen="allowfullscreen"></iframe></p>
<p>You can also find the episode wherever you get your podcasts, including:</p>
<ul>
<li><a href="https://open.spotify.com/episode/72stSBgoohgixM4t4eapmZ?ref=engineeringatmeta" target="_blank" rel="noopener">Spotify</a></li>
<li><a href="https://podcasts.apple.com/gb/podcast/lowering-emissions-with-the-open-compute-project/id1370910331?i=1000736776664?ref=engineeringatmeta" target="_blank" rel="noopener">Apple Podcasts</a></li>
<li><a href="https://pca.st/3dhpd4np?ref=engineeringatmeta" target="_blank" rel="noopener">Pocket Casts</a></li>
</ul>
<p>The <a href="https://insidefacebookmobile.libsyn.com/" target="_blank" rel="noopener">Meta Tech Podcast</a> is a podcast, brought to you by Meta, where we highlight the work Meta’s engineers are doing at every level – from low-level frameworks to end-user features.</p>
<p>Send us feedback on <a href="https://instagram.com/metatechpod" target="_blank" rel="noopener">Instagram</a>, <a href="https://threads.net/@metatechpod" target="_blank" rel="noopener">Threads</a>, or <a href="https://twitter.com/metatechpod" target="_blank" rel="noopener">X</a>.</p>
<p>And if you’re interested in learning more about career opportunities at Meta visit the <a href="https://www.metacareers.com/?ref=engineering.fb.com" target="_blank" rel="noopener">Meta Careers</a> page.</p>
<p>The post <a rel="nofollow" href="https://engineering.fb.com/2025/11/14/production-engineering/open-source-is-good-for-the-environment/">Open Source Is Good for the Environment</a> appeared first on <a rel="nofollow" href="https://engineering.fb.com">Engineering at Meta</a>.</p>
]]></content:encoded>
					
		
		
		<post-id xmlns="com-wordpress:feed-additions:1">23306</post-id>	</item>
	</channel>
</rss>
